[ns_server:info,2022-09-07T14:28:11.337Z,nonode@nohost:<0.145.0>:ns_server:init_logging:120]Started & configured logging
[ns_server:info,2022-09-07T14:28:11.344Z,nonode@nohost:<0.145.0>:ns_server:log_pending:29]Static config terms:
[{error_logger_mf_dir,"/opt/couchbase/var/lib/couchbase/logs"},
 {path_config_bindir,"/opt/couchbase/bin"},
 {path_config_etcdir,"/opt/couchbase/etc/couchbase"},
 {path_config_libdir,"/opt/couchbase/lib"},
 {path_config_datadir,"/opt/couchbase/var/lib/couchbase"},
 {path_config_tmpdir,"/opt/couchbase/var/lib/couchbase/tmp"},
 {path_config_secdir,"/opt/couchbase/etc/security"},
 {nodefile,"/opt/couchbase/var/lib/couchbase/couchbase-server.node"},
 {loglevel_default,debug},
 {loglevel_couchdb,info},
 {loglevel_ns_server,debug},
 {loglevel_error_logger,debug},
 {loglevel_user,debug},
 {loglevel_menelaus,debug},
 {loglevel_ns_doctor,debug},
 {loglevel_stats,debug},
 {loglevel_rebalance,debug},
 {loglevel_cluster,debug},
 {loglevel_views,debug},
 {loglevel_mapreduce_errors,debug},
 {loglevel_xdcr,debug},
 {loglevel_access,info},
 {loglevel_cbas,debug},
 {disk_sink_opts,[{rotation,[{compress,true},
                             {size,41943040},
                             {num_files,10},
                             {buffer_size_max,52428800}]}]},
 {disk_sink_opts_json_rpc,[{rotation,[{compress,true},
                                      {size,41943040},
                                      {num_files,2},
                                      {buffer_size_max,52428800}]}]},
 {net_kernel_verbosity,10}]
[ns_server:warn,2022-09-07T14:28:11.344Z,nonode@nohost:<0.145.0>:ns_server:log_pending:29]not overriding parameter error_logger_mf_dir, which is given from command line
[ns_server:warn,2022-09-07T14:28:11.344Z,nonode@nohost:<0.145.0>:ns_server:log_pending:29]not overriding parameter path_config_bindir, which is given from command line
[ns_server:warn,2022-09-07T14:28:11.344Z,nonode@nohost:<0.145.0>:ns_server:log_pending:29]not overriding parameter path_config_etcdir, which is given from command line
[ns_server:warn,2022-09-07T14:28:11.344Z,nonode@nohost:<0.145.0>:ns_server:log_pending:29]not overriding parameter path_config_libdir, which is given from command line
[ns_server:warn,2022-09-07T14:28:11.344Z,nonode@nohost:<0.145.0>:ns_server:log_pending:29]not overriding parameter path_config_datadir, which is given from command line
[ns_server:warn,2022-09-07T14:28:11.345Z,nonode@nohost:<0.145.0>:ns_server:log_pending:29]not overriding parameter path_config_tmpdir, which is given from command line
[ns_server:warn,2022-09-07T14:28:11.345Z,nonode@nohost:<0.145.0>:ns_server:log_pending:29]not overriding parameter path_config_secdir, which is given from command line
[ns_server:warn,2022-09-07T14:28:11.345Z,nonode@nohost:<0.145.0>:ns_server:log_pending:29]not overriding parameter nodefile, which is given from command line
[ns_server:warn,2022-09-07T14:28:11.345Z,nonode@nohost:<0.145.0>:ns_server:log_pending:29]not overriding parameter loglevel_default, which is given from command line
[ns_server:warn,2022-09-07T14:28:11.345Z,nonode@nohost:<0.145.0>:ns_server:log_pending:29]not overriding parameter loglevel_couchdb, which is given from command line
[ns_server:warn,2022-09-07T14:28:11.345Z,nonode@nohost:<0.145.0>:ns_server:log_pending:29]not overriding parameter loglevel_ns_server, which is given from command line
[ns_server:warn,2022-09-07T14:28:11.345Z,nonode@nohost:<0.145.0>:ns_server:log_pending:29]not overriding parameter loglevel_error_logger, which is given from command line
[ns_server:warn,2022-09-07T14:28:11.345Z,nonode@nohost:<0.145.0>:ns_server:log_pending:29]not overriding parameter loglevel_user, which is given from command line
[ns_server:warn,2022-09-07T14:28:11.345Z,nonode@nohost:<0.145.0>:ns_server:log_pending:29]not overriding parameter loglevel_menelaus, which is given from command line
[ns_server:warn,2022-09-07T14:28:11.345Z,nonode@nohost:<0.145.0>:ns_server:log_pending:29]not overriding parameter loglevel_ns_doctor, which is given from command line
[ns_server:warn,2022-09-07T14:28:11.345Z,nonode@nohost:<0.145.0>:ns_server:log_pending:29]not overriding parameter loglevel_stats, which is given from command line
[ns_server:warn,2022-09-07T14:28:11.345Z,nonode@nohost:<0.145.0>:ns_server:log_pending:29]not overriding parameter loglevel_rebalance, which is given from command line
[ns_server:warn,2022-09-07T14:28:11.345Z,nonode@nohost:<0.145.0>:ns_server:log_pending:29]not overriding parameter loglevel_cluster, which is given from command line
[ns_server:warn,2022-09-07T14:28:11.345Z,nonode@nohost:<0.145.0>:ns_server:log_pending:29]not overriding parameter loglevel_views, which is given from command line
[ns_server:warn,2022-09-07T14:28:11.345Z,nonode@nohost:<0.145.0>:ns_server:log_pending:29]not overriding parameter loglevel_mapreduce_errors, which is given from command line
[ns_server:warn,2022-09-07T14:28:11.345Z,nonode@nohost:<0.145.0>:ns_server:log_pending:29]not overriding parameter loglevel_xdcr, which is given from command line
[ns_server:warn,2022-09-07T14:28:11.345Z,nonode@nohost:<0.145.0>:ns_server:log_pending:29]not overriding parameter loglevel_access, which is given from command line
[ns_server:warn,2022-09-07T14:28:11.345Z,nonode@nohost:<0.145.0>:ns_server:log_pending:29]not overriding parameter loglevel_cbas, which is given from command line
[ns_server:warn,2022-09-07T14:28:11.346Z,nonode@nohost:<0.145.0>:ns_server:log_pending:29]not overriding parameter disk_sink_opts, which is given from command line
[ns_server:warn,2022-09-07T14:28:11.346Z,nonode@nohost:<0.145.0>:ns_server:log_pending:29]not overriding parameter disk_sink_opts_json_rpc, which is given from command line
[ns_server:warn,2022-09-07T14:28:11.346Z,nonode@nohost:<0.145.0>:ns_server:log_pending:29]not overriding parameter net_kernel_verbosity, which is given from command line
[ns_server:info,2022-09-07T14:28:11.355Z,nonode@nohost:dist_manager<0.197.0>:dist_manager:read_address_config_from_path:82]Reading ip config from "/opt/couchbase/var/lib/couchbase/ip_start"
[ns_server:info,2022-09-07T14:28:11.356Z,nonode@nohost:dist_manager<0.197.0>:dist_manager:read_address_config_from_path:82]Reading ip config from "/opt/couchbase/var/lib/couchbase/ip"
[ns_server:info,2022-09-07T14:28:11.356Z,nonode@nohost:dist_manager<0.197.0>:dist_manager:init:178]ip config not found. Looks like we're brand new node
[ns_server:info,2022-09-07T14:28:11.359Z,nonode@nohost:dist_manager<0.197.0>:dist_manager:bringup:244]Attempting to bring up net_kernel with name 'ns_1@cb.local'
[error_logger:info,2022-09-07T14:28:11.375Z,nonode@nohost:ssl_dist_admin_sup<0.200.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ssl_dist_admin_sup}
    started: [{pid,<0.201.0>},
              {id,ssl_pem_cache_dist},
              {mfargs,{ssl_pem_cache,start_link_dist,[[]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,4000},
              {child_type,worker}]

[error_logger:info,2022-09-07T14:28:11.375Z,nonode@nohost:ssl_dist_admin_sup<0.200.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ssl_dist_admin_sup}
    started: [{pid,<0.202.0>},
              {id,ssl_dist_manager},
              {mfargs,{ssl_manager,start_link_dist,[[]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,4000},
              {child_type,worker}]

[error_logger:info,2022-09-07T14:28:11.375Z,nonode@nohost:ssl_dist_sup<0.199.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ssl_dist_sup}
    started: [{pid,<0.200.0>},
              {id,ssl_dist_admin_sup},
              {mfargs,{ssl_dist_admin_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,4000},
              {child_type,supervisor}]

[error_logger:info,2022-09-07T14:28:11.378Z,nonode@nohost:tls_dist_sup<0.203.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,tls_dist_sup}
    started: [{pid,<0.204.0>},
              {id,dist_tls_connection},
              {mfargs,{tls_connection_sup,start_link_dist,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,4000},
              {child_type,supervisor}]

[error_logger:info,2022-09-07T14:28:11.380Z,nonode@nohost:tls_dist_server_sup<0.205.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,tls_dist_server_sup}
    started: [{pid,<0.206.0>},
              {id,dist_tls_socket},
              {mfargs,{ssl_listen_tracker_sup,start_link_dist,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,4000},
              {child_type,supervisor}]

[error_logger:info,2022-09-07T14:28:11.380Z,nonode@nohost:tls_dist_server_sup<0.205.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,tls_dist_server_sup}
    started: [{pid,<0.207.0>},
              {id,dist_tls_server_session_ticket},
              {mfargs,{tls_server_session_ticket_sup,start_link_dist,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,4000},
              {child_type,supervisor}]

[error_logger:info,2022-09-07T14:28:11.380Z,nonode@nohost:tls_dist_server_sup<0.205.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,tls_dist_server_sup}
    started: [{pid,<0.208.0>},
              {id,dist_ssl_server_session_cache_sup},
              {mfargs,
                  {ssl_upgrade_server_session_cache_sup,start_link_dist,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,4000},
              {child_type,supervisor}]

[error_logger:info,2022-09-07T14:28:11.380Z,nonode@nohost:tls_dist_sup<0.203.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,tls_dist_sup}
    started: [{pid,<0.205.0>},
              {id,dist_tls_server_sup},
              {mfargs,{tls_dist_server_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,4000},
              {child_type,supervisor}]

[error_logger:info,2022-09-07T14:28:11.380Z,nonode@nohost:ssl_dist_sup<0.199.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ssl_dist_sup}
    started: [{pid,<0.203.0>},
              {id,tls_dist_sup},
              {mfargs,{tls_dist_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,4000},
              {child_type,supervisor}]

[error_logger:info,2022-09-07T14:28:11.380Z,nonode@nohost:net_sup<0.198.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,net_sup}
    started: [{pid,<0.199.0>},
              {id,ssl_dist_sup},
              {mfargs,{ssl_dist_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2022-09-07T14:28:11.381Z,nonode@nohost:cb_dist<0.209.0>:cb_dist:info_msg:872]cb_dist: Starting cb_dist with config []
[error_logger:info,2022-09-07T14:28:11.382Z,nonode@nohost:net_sup<0.198.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,net_sup}
    started: [{pid,<0.209.0>},
              {id,cb_dist},
              {mfargs,{cb_dist,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,worker}]

[error_logger:info,2022-09-07T14:28:11.382Z,nonode@nohost:net_sup<0.198.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,net_sup}
    started: [{pid,<0.210.0>},
              {id,cb_epmd},
              {mfargs,{cb_epmd,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,2000},
              {child_type,worker}]

[error_logger:info,2022-09-07T14:28:11.384Z,nonode@nohost:net_sup<0.198.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,net_sup}
    started: [{pid,<0.211.0>},
              {id,auth},
              {mfargs,{auth,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,2000},
              {child_type,worker}]

[ns_server:debug,2022-09-07T14:28:11.386Z,nonode@nohost:cb_dist<0.209.0>:cb_dist:info_msg:872]cb_dist: Initial protos: [{external,inet_tcp_dist}], required protos: [{external,
                                                                        inet_tcp_dist}]
[ns_server:debug,2022-09-07T14:28:11.386Z,nonode@nohost:cb_dist<0.209.0>:cb_dist:info_msg:872]cb_dist: Starting {external,inet_tcp_dist} listener on 21100...
[ns_server:debug,2022-09-07T14:28:11.386Z,nonode@nohost:cb_dist<0.209.0>:cb_dist:info_msg:872]cb_dist: Started listener: inet_tcp_dist
[ns_server:debug,2022-09-07T14:28:11.458Z,ns_1@cb.local:cb_dist<0.209.0>:cb_dist:info_msg:872]cb_dist: Started acceptor inet_tcp_dist: <0.214.0>
[error_logger:info,2022-09-07T14:28:11.458Z,ns_1@cb.local:net_sup<0.198.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,net_sup}
    started: [{pid,<0.212.0>},
              {id,net_kernel},
              {mfargs,{net_kernel,start_link,
                                  [['ns_1@cb.local',longnames],
                                   false,net_sup_dynamic]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,2000},
              {child_type,worker}]

[error_logger:info,2022-09-07T14:28:11.458Z,ns_1@cb.local:kernel_sup<0.49.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,kernel_sup}
    started: [{pid,<0.198.0>},
              {id,net_sup_dynamic},
              {mfargs,
                  {erl_distribution,start_link,
                      [['ns_1@cb.local',longnames],false,net_sup_dynamic]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,supervisor}]

[ns_server:debug,2022-09-07T14:28:11.458Z,ns_1@cb.local:dist_manager<0.197.0>:dist_manager:configure_net_kernel:290]Set net_kernel vebosity to 10 -> 0
[ns_server:info,2022-09-07T14:28:11.462Z,ns_1@cb.local:dist_manager<0.197.0>:dist_manager:save_node:158]saving node to "/opt/couchbase/var/lib/couchbase/couchbase-server.node"
[ns_server:debug,2022-09-07T14:28:11.474Z,ns_1@cb.local:dist_manager<0.197.0>:dist_manager:bringup:260]Attempted to save node name to disk: ok
[ns_server:debug,2022-09-07T14:28:11.476Z,ns_1@cb.local:dist_manager<0.197.0>:dist_manager:wait_for_node:267]Waiting for connection to node 'babysitter_of_ns_1@cb.local' to be established
[error_logger:info,2022-09-07T14:28:11.476Z,ns_1@cb.local:net_kernel<0.212.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{connect,normal,'babysitter_of_ns_1@cb.local'}}
[ns_server:debug,2022-09-07T14:28:11.477Z,ns_1@cb.local:net_kernel<0.212.0>:cb_dist:info_msg:872]cb_dist: Setting up new connection to 'babysitter_of_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2022-09-07T14:28:11.477Z,ns_1@cb.local:cb_dist<0.209.0>:cb_dist:info_msg:872]cb_dist: Added connection {con,#Ref<0.2209594722.1843658754.36455>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2022-09-07T14:28:11.477Z,ns_1@cb.local:cb_dist<0.209.0>:cb_dist:info_msg:872]cb_dist: Updated connection: {con,#Ref<0.2209594722.1843658754.36455>,
                                  inet_tcp_dist,<0.216.0>,
                                  #Ref<0.2209594722.1843658754.36458>}
[ns_server:debug,2022-09-07T14:28:11.487Z,ns_1@cb.local:dist_manager<0.197.0>:dist_manager:wait_for_node:279]Observed node 'babysitter_of_ns_1@cb.local' to come up
[ns_server:info,2022-09-07T14:28:11.487Z,ns_1@cb.local:dist_manager<0.197.0>:dist_manager:save_address_config:145]Deleting irrelevant ip file "/opt/couchbase/var/lib/couchbase/ip_start": {error,
                                                                          enoent}
[ns_server:info,2022-09-07T14:28:11.487Z,ns_1@cb.local:dist_manager<0.197.0>:dist_manager:save_address_config:146]saving ip config to "/opt/couchbase/var/lib/couchbase/ip"
[ns_server:info,2022-09-07T14:28:11.500Z,ns_1@cb.local:dist_manager<0.197.0>:dist_manager:save_address_config:149]Persisted the address successfully
[error_logger:info,2022-09-07T14:28:11.506Z,ns_1@cb.local:root_sup<0.196.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,root_sup}
    started: [{pid,<0.197.0>},
              {id,dist_manager},
              {mfargs,{dist_manager,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2022-09-07T14:28:11.511Z,ns_1@cb.local:ns_server_cluster_sup<0.218.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_cluster_sup}
    started: [{pid,<0.219.0>},
              {id,local_tasks},
              {mfargs,{local_tasks,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[ns_server:info,2022-09-07T14:28:11.515Z,ns_1@cb.local:ns_server_cluster_sup<0.218.0>:log_os_info:start_link:19]OS type: {unix,linux} Version: {5,10,16}
Runtime info: [{otp_release,"24"},
               {erl_version,"12.1.5"},
               {erl_version_long,
                   "Erlang/OTP 24 [erts-12.1.5] [source-38e24c5] [64-bit] [smp:8:8] [ds:8:8:10] [async-threads:16] [jit]\n"},
               {system_arch_raw,"x86_64-pc-linux-gnu"},
               {system_arch,"x86_64-pc-linux-gnu"},
               {localtime,{{2022,9,7},{14,28,11}}},
               {memory,
                   [{total,44002216},
                    {processes,10601832},
                    {processes_used,10591960},
                    {system,33400384},
                    {atom,499905},
                    {atom_used,496889},
                    {binary,113168},
                    {code,10248126},
                    {ets,2609928}]},
               {loaded,
                   [ns_info,log_os_info,local_tasks,restartable,
                    ns_server_cluster_sup,ns_cluster,dist_util,ns_node_disco,
                    crypto,re,auth,tls_dist_server_sup,tls_dist_sup,
                    ale_error_logger_handler,ssl_dist_admin_sup,ssl_dist_sup,
                    inet_tls_dist,inet_tcp_dist,inet_tcp,gen_tcp,erl_epmd,
                    cb_epmd,gen_udp,inet_hosts,dist_manager,root_sup,
                    path_config,cb_dist,ns_server_stats,calendar,
                    ale_default_formatter,'ale_logger-metakv',
                    'ale_logger-rebalance','ale_logger-chronicle',
                    'ale_logger-menelaus','ale_logger-stats',
                    'ale_logger-json_rpc','ale_logger-access',
                    'ale_logger-ns_server','ale_logger-user',
                    'ale_logger-ns_doctor','ale_logger-cluster',
                    'ale_logger-xdcr',erl_bits,otp_internal,
                    cb_log_counter_sink,ns_log_sink,ale_disk_sink,misc,
                    couch_util,ns_server,timer,cpu_sup,filelib,memsup,disksup,
                    os_mon,unicode_util,string,io,release_handler,
                    alarm_handler,sasl,httpd_sup,httpc_handler_sup,
                    httpc_cookie,inets_trace,httpc_manager,httpc,
                    httpc_profile_sup,httpc_sup,inets_sup,inets_app,ssl,
                    lhttpc_manager,lhttpc_sup,lhttpc,
                    dtls_server_session_cache_sup,dtls_listener_sup,
                    dtls_server_sup,dtls_connection_sup,dtls_sup,
                    ssl_upgrade_server_session_cache_sup,
                    ssl_server_session_cache_sup,
                    tls_server_session_ticket_sup,ssl_listen_tracker_sup,
                    tls_server_sup,tls_connection_sup,tls_sup,
                    ssl_connection_sup,tls_client_ticket_store,
                    ssl_client_session_cache_db,ssl_config,ssl_manager,
                    ssl_pkix_db,ssl_pem_cache,ssl_admin_sup,ssl_sup,
                    logger_h_common,logger_std_h,ssl_logger,ssl_app,
                    'ale_logger-ale_logger','ale_logger-error_logger',
                    beam_opcodes,beam_dict,beam_asm,beam_z,beam_flatten,
                    beam_trim,beam_clean,beam_peep,beam_block,beam_utils,
                    beam_jump,beam_a,beam_validator,beam_ssa_codegen,
                    beam_ssa_pre_codegen,beam_ssa_throw,beam_ssa_dead,
                    beam_call_types,beam_types,beam_ssa_type,beam_ssa_bc_size,
                    beam_ssa_opt,beam_ssa_funs,beam_ssa_bsm,beam_ssa_recv,
                    beam_ssa_share,beam_ssa_bool,beam_ssa,beam_kernel_to_ssa,
                    v3_kernel,sys_core_bsm,sys_core_alias,erl_bifs,
                    cerl_clauses,sets,sys_core_fold,sys_core_inline,
                    cerl_trees,core_lib,cerl,v3_core,erl_expand_records,sofs,
                    erl_internal,ordsets,compile,dynamic_compile,ale_utils,
                    io_lib_pretty,io_lib_format,io_lib,ale_codegen,dict,ale,
                    ale_dynamic_sup,ale_sup,ale_app,ns_bootstrap,child_erlang,
                    raw_file_io,orddict,c,erl_signal_handler,maps,
                    logger_handler_watcher,logger_sup,kernel_refc,
                    kernel_config,user_io,user_sup,supervisor_bridge,
                    standard_error,global_group,erl_distribution,net_kernel,
                    global,rpc,epp,inet_gethost_native,inet_parse,inet,
                    inet_udp,inet_config,inet_db,unicode,os,gb_trees,gb_sets,
                    binary,erl_anno,proplists,erl_scan,queue,logger_olp,
                    logger_proxy,error_handler,heart,code,logger_filters,file,
                    file_server,application_controller,kernel,error_logger,
                    logger_simple_h,erl_eval,ets,logger_backend,filename,
                    logger_config,application_master,file_io_server,
                    application,logger,logger_server,gen_server,gen,proc_lib,
                    erl_parse,gen_event,code_server,lists,supervisor,erl_lint,
                    persistent_term,counters,atomics,
                    erts_dirty_process_signal_handler,
                    erts_literal_area_collector,erl_tracer,erts_internal,
                    erlang,erl_prim_loader,prim_zip,prim_net,prim_socket,
                    socket_registry,zlib,prim_file,prim_inet,prim_eval,
                    prim_buffer,init,erl_init,erts_code_purger]},
               {applications,
                   [{lhttpc,"Lightweight HTTP Client","1.3.0"},
                    {asn1,"The Erlang ASN1 compiler version 5.0.17","5.0.17"},
                    {public_key,"Public key infrastructure","1.11.3"},
                    {crypto,"CRYPTO","5.0.4"},
                    {ssl,"Erlang/OTP SSL application","10.5.3"},
                    {os_mon,"CPO  CXC 138 46","2.7.1"},
                    {sasl,"SASL  CXC 138 11","4.1"},
                    {inets,"INETS  CXC 138 49","7.4.2"},
                    {stdlib,"ERTS  CXC 138 10","3.16.1"},
                    {kernel,"ERTS  CXC 138 10","8.1.3"},
                    {ns_server,"Couchbase server","7.1.1-3175-enterprise"},
                    {ale,"Another Logger for Erlang","0.0.0"}]},
               {pre_loaded,
                   [persistent_term,counters,atomics,
                    erts_dirty_process_signal_handler,
                    erts_literal_area_collector,erl_tracer,erts_internal,
                    erlang,erl_prim_loader,prim_zip,prim_net,prim_socket,
                    socket_registry,zlib,prim_file,prim_inet,prim_eval,
                    prim_buffer,init,erl_init,erts_code_purger]},
               {process_count,150},
               {node,'ns_1@cb.local'},
               {nodes,[]},
               {registered,
                   [httpd_sup,ssl_dist_admin_sup,tls_server_sup,
                    ssl_connection_sup,sasl_safe_sup,cb_dist,
                    global_name_server,ssl_upgrade_server_session_cache_sup,
                    user,kernel_refc,inets_sup,tls_client_ticket_store,
                    'sink-disk_default',ns_server_cluster_sup,
                    tls_dist_connection_sup,dtls_server_session_cache_sup,
                    httpc_manager,tls_sup,'sink-disk_stats',rex,
                    tls_server_session_ticket_sup_dist,net_kernel,
                    standard_error_sup,local_tasks,'sink-disk_metakv',
                    root_sup,cpu_sup,ssl_dist_sup,ssl_sup,global_group,
                    dist_manager,file_server_2,ssl_listen_tracker_sup,
                    logger_sup,ale,dtls_sup,tls_dist_sup,
                    tls_server_session_ticket_sup,'sink-disk_xdcr',kernel_sup,
                    code_server,ssl_listen_tracker_sup_dist,ssl_manager,
                    erts_code_purger,'sink-disk_access_int',ale_sup,disksup,
                    os_mon_sup,httpc_profile_sup,erl_signal_server,
                    standard_error,kernel_safe_sup,ssl_pem_cache_dist,
                    logger_proxy,'sink-cb_log_counter',ssl_manager_dist,
                    erl_prim_loader,'sink-disk_debug',ssl_admin_sup,
                    application_controller,socket_registry,release_handler,
                    'sink-ns_log',httpc_sup,ssl_server_session_cache_sup,
                    timer_server,inet_db,'sink-disk_access',auth,
                    dtls_server_sup,init,lhttpc_manager,ale_stats_events,
                    alarm_handler,httpc_handler_sup,tls_dist_server_sup,
                    logger_handler_watcher,net_sup,ssl_pem_cache,erl_epmd,
                    dtls_connection_sup,tls_connection_sup,logger,sasl_sup,
                    memsup,'sink-disk_error',lhttpc_sup,'sink-disk_reports',
                    ssl_upgrade_server_session_cache_sup_dist,ale_dynamic_sup,
                    logger_std_h_ssl_handler,dtls_listener_sup,
                    'sink-disk_json_rpc']},
               {cookie,nocookie},
               {wordsize,8},
               {wall_clock,2}]
[ns_server:info,2022-09-07T14:28:11.524Z,ns_1@cb.local:ns_server_cluster_sup<0.218.0>:log_os_info:start_link:21]Manifest:
["<manifest>",
 "  <remote name=\"blevesearch\" fetch=\"https://github.com/blevesearch/\" />",
 "  <remote name=\"couchbase\" fetch=\"https://github.com/couchbase/\" review=\"review.couchbase.org\" />",
 "  <remote name=\"couchbase-priv\" fetch=\"ssh://git@github.com/couchbase/\" review=\"review.couchbase.org\" />",
 "  <remote name=\"couchbasedeps\" fetch=\"https://github.com/couchbasedeps/\" review=\"review.couchbase.org\" />",
 "  <remote name=\"couchbaselabs\" fetch=\"https://github.com/couchbaselabs/\" review=\"review.couchbase.org\" />",
 "  ","  <default remote=\"couchbase\" revision=\"master\" />","  ",
 "  <project name=\"HdrHistogram_c\" path=\"third_party/HdrHistogram_c\" remote=\"couchbasedeps\" revision=\"caed837aa163421a637222157b3f6353b4ca831a\" groups=\"kv\" />",
 "  <project name=\"analytics-dcp-client\" path=\"analytics/java-dcp-client\" revision=\"28a01945b085e906bdd25c5d35e92d3c8c87ff1a\" upstream=\"neo\" dest-branch=\"neo\" groups=\"notdefault,enterprise,analytics\" />",
 "  <project name=\"asterixdb\" path=\"analytics/asterixdb\" revision=\"344eeb2ebb60f453d6422aadc1bd310bc1339d1b\" groups=\"notdefault,enterprise,analytics\" />",
 "  <project name=\"backup\" remote=\"couchbase-priv\" revision=\"c0b230eba0c54681116f93d5806457089b24da2a\" groups=\"backup,notdefault,enterprise\" />",
 "  <project name=\"bbolt\" path=\"godeps/src/go.etcd.io/bbolt\" remote=\"couchbasedeps\" revision=\"68cc10a767ea1c6b9e8dcb9847317ff192d6d974\" />",
 "  <project name=\"bitset\" path=\"godeps/src/github.com/willf/bitset\" remote=\"couchbasedeps\" revision=\"28a4168144bb8ac95454e1f51c84da1933681ad4\" />",
 "  <project name=\"blance\" path=\"godeps/src/github.com/couchbase/blance\" revision=\"b2ec44a33677b3c64c516ed9e9c49721e266a58a\" />",
 "  <project name=\"bolt\" path=\"godeps/src/github.com/boltdb/bolt\" remote=\"couchbasedeps\" revision=\"51f99c862475898df9773747d3accd05a7ca33c1\" />",
 "  <project name=\"build\" path=\"cbbuild\" revision=\"e5d21de4a5e4ce048b7b330765529f71c6ff7e08\" upstream=\"neo\" dest-branch=\"neo\" groups=\"notdefault,build\">",
 "    <annotation name=\"RELEASE\" value=\"neo\" />",
 "    <annotation name=\"PRODUCT\" value=\"couchbase-server\" />",
 "    <annotation name=\"BLD_NUM\" value=\"3175\" />",
 "    <annotation name=\"VERSION\" value=\"7.1.1\" />",
 "    <annotation name=\"BSL_PRODUCT\" value=\"Couchbase Server\" />",
 "    <annotation name=\"BSL_VERSION\" value=\"7.1\" />",
 "    <annotation name=\"BSL_CHANGE_DATE\" value=\"April 1, 2026\" />",
 "  </project>",
 "  <project name=\"cbas\" path=\"goproj/src/github.com/couchbase/cbas\" remote=\"couchbase-priv\" revision=\"cf4628ae5f8b69a3402764b83aeae1668a9d7659\" upstream=\"neo\" dest-branch=\"neo\" groups=\"notdefault,enterprise,analytics\" />",
 "  <project name=\"cbas-core\" path=\"analytics\" remote=\"couchbase-priv\" revision=\"c41c19f8b112cff101a03e4b047738b7c2704d33\" groups=\"notdefault,enterprise,analytics\" />",
 "  <project name=\"cbas-ui\" revision=\"b6986a61412d59e297a62a93df4ac75bf8085dbe\" groups=\"notdefault,enterprise,analytics\" />",
 "  <project name=\"cbauth\" path=\"goproj/src/github.com/couchbase/cbauth\" revision=\"694763b234d64faaac75fbec427fc13132ccbcfe\" upstream=\"neo\" dest-branch=\"neo\" groups=\"backup\" />",
 "  <project name=\"cbbs\" remote=\"couchbase-priv\" revision=\"cf90ef81e93f36a09405211ee31661bbb78163ad\" groups=\"backup,notdefault,enterprise\" />",
 "  <project name=\"cbft\" revision=\"1e7554bde9f77b09429c7a2a506a2c7cbc215d8e\" upstream=\"7.1.1\" dest-branch=\"7.1.1\" />",
 "  <project name=\"cbftx\" remote=\"couchbase-priv\" revision=\"64469e336f363ed4d7b0fd51c6f54d0cd804752a\" groups=\"notdefault,enterprise\" />",
 "  <project name=\"cbgt\" revision=\"b7cd3e62474ad834adb44d9c4f0758cec77cd6d7\" />",
 "  <project name=\"cbsummary\" path=\"goproj/src/github.com/couchbase/cbsummary\" revision=\"083dc4dd2fd992caaaf3c9f1412ca3a8e87d0183\" />",
 "  <project name=\"chronicle\" path=\"ns_server/deps/chronicle\" revision=\"f53f803d409988d1bf3cdd2d7a5e9f55fb9dc2df\" />",
 "  <project name=\"clog\" path=\"godeps/src/github.com/couchbase/clog\" revision=\"f935d1fdfc36541b505cf86fea4822e4067f9c39\" />",
 "  <project name=\"cobra\" path=\"godeps/src/github.com/spf13/cobra\" remote=\"couchbasedeps\" revision=\"0f056af21f5f368e5b0646079d0094a2c64150f7\" />",
 "  <project name=\"context\" path=\"godeps/src/github.com/gorilla/context\" remote=\"couchbasedeps\" revision=\"215affda49addc4c8ef7e2534915df2c8c35c6cd\" />",
 "  <project name=\"couchbase-cli\" revision=\"7682cc259e6e129c80a583ffcb658495f7a7abd3\" groups=\"kv\" />",
 "  <project name=\"couchdb\" revision=\"27d1470742daa7829209763389f7d3ab1d1c8443\" />",
 "  <project name=\"couchdbx-app\" revision=\"64bdc899ba72d021a3c1dde1a1aa5b698f42ee06\" groups=\"notdefault,packaging\" />",
 "  <project name=\"couchstore\" revision=\"56b0f7b7a890a4896f321c5114c87412606120c4\" upstream=\"neo\" dest-branch=\"neo\" groups=\"kv\" />",
 "  <project name=\"crypto\" path=\"godeps/src/golang.org/x/crypto\" remote=\"couchbasedeps\" revision=\"bd6f299fb381e4c3393d1c4b1f0b94f5e77650c8\" />",
 "  <project name=\"cuckoofilter\" path=\"godeps/src/github.com/seiflotfy/cuckoofilter\" remote=\"couchbasedeps\" revision=\"d04838794ab86926d32b124345777e55e6f43974\" />",
 "  <project name=\"docloader\" path=\"goproj/src/github.com/couchbase/docloader\" revision=\"2e6af0c097c8bb98a596cbc81cdf6e169ae5b3cc\" />",
 "  <project name=\"errors\" path=\"godeps/src/github.com/pkg/errors\" remote=\"couchbasedeps\" revision=\"30136e27e2ac8d167177e8a583aa4c3fea5be833\" />",
 "  <project name=\"eventing\" path=\"goproj/src/github.com/couchbase/eventing\" revision=\"2f3487bea6ff76c09cc6acaa9fb5b03dd12e4c39\" upstream=\"7.1.1\" dest-branch=\"7.1.1\" groups=\"notdefault,enterprise\" />",
 "  <project name=\"eventing-ee\" path=\"goproj/src/github.com/couchbase/eventing-ee\" remote=\"couchbase-priv\" revision=\"14fd48dd7c506efac0607f3baa7e293646ed3fd0\" groups=\"notdefault,enterprise\" />",
 "  <project name=\"flatbuffers\" path=\"godeps/src/github.com/google/flatbuffers\" remote=\"couchbasedeps\" revision=\"1a8968225130caeddd16e227678e6f8af1926303\" />",
 "  <project name=\"forestdb\" revision=\"48c31dcb979ca7e152c2db570af49149c6d3e2a7\" groups=\"backup\" />",
 "  <project name=\"fwd\" path=\"godeps/src/github.com/philhofer/fwd\" remote=\"couchbasedeps\" revision=\"bb6d471dc95d4fe11e432687f8b70ff496cf3136\" />",
 "  <project name=\"geocouch\" revision=\"68f3b9d36630682d17ca5232770f1693b9b8fa18\" />",
 "  <project name=\"ghistogram\" path=\"godeps/src/github.com/couchbase/ghistogram\" revision=\"4ae3f06d0ac7b02081e33c1ec309daa22838d207\" />",
 "  <project name=\"go-bindata-assetfs\" path=\"godeps/src/github.com/elazarl/go-bindata-assetfs\" remote=\"couchbasedeps\" revision=\"57eb5e1fc594ad4b0b1dbea7b286d299e0cb43c2\" />",
 "  <project name=\"go-couchbase\" path=\"goproj/src/github.com/couchbase/go-couchbase\" revision=\"959eaf944140a6c660990f38b1db310ddd6d8e42\" groups=\"backup\" />",
 "  <project name=\"go-curl\" path=\"godeps/src/github.com/andelf/go-curl\" remote=\"couchbasedeps\" revision=\"f0b2afc926ec79be5d7f30393b3485352781a705\" upstream=\"20161221-couchbase\" dest-branch=\"20161221-couchbase\" />",
 "  <project name=\"go-curl\" path=\"godeps/src/github.com/couchbasedeps/go-curl\" remote=\"couchbasedeps\" revision=\"f0b2afc926ec79be5d7f30393b3485352781a705\" upstream=\"20161221-couchbase\" dest-branch=\"20161221-couchbase\" />",
 "  <project name=\"go-genproto\" path=\"godeps/src/google.golang.org/genproto\" remote=\"couchbasedeps\" revision=\"2b5a72b8730b0b16380010cfe5286c42108d88e7\" />",
 "  <project name=\"go-jsonpointer\" path=\"godeps/src/github.com/dustin/go-jsonpointer\" remote=\"couchbasedeps\" revision=\"75939f54b39e7dafae879e61f65438dadc5f288c\" />",
 "  <project name=\"go-metrics\" path=\"godeps/src/github.com/rcrowley/go-metrics\" remote=\"couchbasedeps\" revision=\"dee209f2455f101a5e4e593dea94872d2c62d85d\" />",
 "  <project name=\"go-runewidth\" path=\"godeps/src/github.com/mattn/go-runewidth\" remote=\"couchbasedeps\" revision=\"703b5e6b11ae25aeb2af9ebb5d5fdf8fa2575211\" />",
 "  <project name=\"go-slab\" path=\"godeps/src/github.com/couchbase/go-slab\" revision=\"e47646b420b3c9eb344cef022236a54e2554d40b\" groups=\"bsl\" />",
 "  <project name=\"go-unsnap-stream\" path=\"godeps/src/github.com/glycerine/go-unsnap-stream\" remote=\"couchbasedeps\" revision=\"62a9a9eb44fd8932157b1a8ace2149eff5971af6\" />",
 "  <project name=\"go-zookeeper\" path=\"godeps/src/github.com/samuel/go-zookeeper\" remote=\"couchbasedeps\" revision=\"fa6674abf3f4580b946a01bf7a1ce4ba8766205b\" />",
 "  <project name=\"go_json\" path=\"goproj/src/github.com/couchbase/go_json\" revision=\"39c6c3c3e21c5c0a0be9f696bdb9e496fde773f1\" />",
 "  <project name=\"go_n1ql\" path=\"godeps/src/github.com/couchbase/go_n1ql\" revision=\"0ed4bf93e31de2371f9180e424942bd3d5235397\" groups=\"bsl\" />",
 "  <project name=\"gocb\" path=\"godeps/src/github.com/couchbase/gocb/v2\" revision=\"40020eef484873e507745107edbf99c421927a93\" upstream=\"refs/tags/v2.2.5\" dest-branch=\"refs/tags/v2.2.5\" />",
 "  <project name=\"gocb\" path=\"godeps/src/gopkg.in/couchbase/gocb.v1\" revision=\"01c846cb025ddd50a2ef4c82a27992b40c230dbb\" upstream=\"refs/tags/v1.4.2\" dest-branch=\"refs/tags/v1.4.2\" />",
 "  <project name=\"gocbconnstr\" path=\"godeps/src/gopkg.in/couchbaselabs/gocbconnstr.v1\" remote=\"couchbaselabs\" revision=\"8f9a894d174b836c6362de9af75545cf585fc278\" />",
 "  <project name=\"gocbcore\" path=\"godeps/src/gopkg.in/couchbase/gocbcore.v7\" revision=\"441cb91f01ce26932514ec10d9e59e568ee27722\" upstream=\"refs/tags/v7.1.14\" dest-branch=\"refs/tags/v7.1.14\" />",
 "  <project name=\"gocbcore\" path=\"godeps/src/github.com/couchbase/gocbcore/v9\" revision=\"0ece206041d8cf5f5fcd919767446603691bdb69\" upstream=\"refs/tags/v9.1.6\" dest-branch=\"refs/tags/v9.1.6\" />",
 "  <project name=\"godbc\" path=\"goproj/src/github.com/couchbase/godbc\" revision=\"938b768d5e33f7b70c183db527b460d1648f5c52\" />",
 "  <project name=\"gofarmhash\" path=\"godeps/src/github.com/leemcloughlin/gofarmhash\" remote=\"couchbasedeps\" revision=\"0a055c5b87a8c55ce83459cbf2776b563822a942\" />",
 "  <project name=\"goforestdb\" path=\"godeps/src/github.com/couchbase/goforestdb\" revision=\"0b501227de0e8c55d99ed14e900eea1a1dbaf899\" />",
 "  <project name=\"gojson\" path=\"godeps/src/github.com/dustin/gojson\" remote=\"couchbasedeps\" revision=\"af16e0e771e2ed110f2785564ae33931de8829e4\" />",
 "  <project name=\"gojsonsm\" path=\"godeps/src/github.com/couchbaselabs/gojsonsm\" remote=\"couchbaselabs\" revision=\"eec4953dcb855282c483b8cd4fe03a8074e2f7a1\" />",
 "  <project name=\"golang-pkg-pcre\" path=\"godeps/src/github.com/glenn-brown/golang-pkg-pcre\" remote=\"couchbasedeps\" revision=\"48bb82a8b8ceea98f4e97825b43870f6ba1970d6\" />",
 "  <project name=\"golang-snappy\" path=\"godeps/src/github.com/golang/snappy\" remote=\"couchbasedeps\" revision=\"723cc1e459b8eea2dea4583200fd60757d40097a\" />",
 "  <project name=\"golang-tools\" path=\"godeps/src/golang.org/x/tools\" remote=\"couchbasedeps\" revision=\"a28dfb48e06b2296b66678872c2cb638f0304f20\" />",
 "  <project name=\"goleveldb\" path=\"godeps/src/github.com/syndtr/goleveldb\" remote=\"couchbasedeps\" revision=\"fa5b5c78794bc5c18f330361059f871ae8c2b9d6\" />",
 "  <project name=\"gomemcached\" path=\"goproj/src/github.com/couchbase/gomemcached\" revision=\"9ae3e1a53ee2393526383f0d37dab16cd936d92f\" groups=\"backup\" />",
 "  <project name=\"gometa\" path=\"goproj/src/github.com/couchbase/gometa\" revision=\"f26942c60986380e757967002c365d6f2b7fb219\" />",
 "  <project name=\"goskiplist\" path=\"godeps/src/github.com/ryszard/goskiplist\" remote=\"couchbasedeps\" revision=\"2dfbae5fcf46374f166f8969cb07e167f1be6273\" />",
 "  <project name=\"gosnappy\" path=\"godeps/src/github.com/syndtr/gosnappy\" remote=\"couchbasedeps\" revision=\"156a073208e131d7d2e212cb749feae7c339e846\" />",
 "  <project name=\"goutils\" path=\"goproj/src/github.com/couchbase/goutils\" revision=\"73dda2bf44424b5c588579948399e86e5de4be6c\" groups=\"bsl\" />",
 "  <project name=\"goxdcr\" path=\"goproj/src/github.com/couchbase/goxdcr\" revision=\"86f3852dbd814c861bf82de1bd3a5e98761e3746\" groups=\"bsl\" />",
 "  <project name=\"grpc-go\" path=\"godeps/src/google.golang.org/grpc\" remote=\"couchbasedeps\" revision=\"df014850f6dee74ba2fc94874043a9f3f75fbfd8\" upstream=\"refs/tags/v1.17.0\" dest-branch=\"refs/tags/v1.17.0\" />",
 "  <project name=\"gsl-lite\" path=\"third_party/gsl-lite\" remote=\"couchbasedeps\" revision=\"e1c381746c2625a76227255f999ae9f14a062208\" upstream=\"refs/tags/v0.38.1\" dest-branch=\"refs/tags/v0.38.1\" groups=\"kv\" />",
 "  <project name=\"gtreap\" path=\"godeps/src/github.com/steveyen/gtreap\" remote=\"couchbasedeps\" revision=\"0abe01ef9be25c4aedc174758ec2d917314d6d70\" />",
 "  <project name=\"httprouter\" path=\"godeps/src/github.com/julienschmidt/httprouter\" remote=\"couchbasedeps\" revision=\"975b5c4c7c21c0e3d2764200bf2aa8e34657ae6e\" />",
 "  <project name=\"indexing\" path=\"goproj/src/github.com/couchbase/indexing\" revision=\"ab452e223878afe5313d78c63a56c1132afdd688\" groups=\"bsl\" />",
 "  <project name=\"json-iterator-go\" path=\"godeps/src/github.com/json-iterator/go\" remote=\"couchbasedeps\" revision=\"f7279a603edee96fe7764d3de9c6ff8cf9970994\" />",
 "  <project name=\"jsonparser\" path=\"godeps/src/github.com/buger/jsonparser\" remote=\"couchbasedeps\" revision=\"df3ea76ece10095374fd1c9a22a4fb85a44efc42\" />",
 "  <project name=\"jsonschema\" path=\"godeps/src/github.com/santhosh-tekuri/jsonschema\" remote=\"couchbasedeps\" revision=\"137f44a49015e5060a447c331aa37de6e0f50267\" />",
 "  <project name=\"jsonx\" path=\"godeps/src/gopkg.in/couchbaselabs/jsonx.v1\" remote=\"couchbaselabs\" revision=\"03f375ceefb769799cfa0d64352fdcc9f1192368\" />",
 "  <project name=\"kv_engine\" revision=\"84f34ca1acbcfc79e1434d4695ebd83c8bff56bc\" groups=\"kv,bsl\" />",
 "  <project name=\"libcouchbase\" revision=\"e4de408c96550b48745b3a142d9827c898d4e96f\" upstream=\"refs/tags/3.2.5\" dest-branch=\"refs/tags/3.2.5\" />",
 "  <project name=\"liner\" path=\"godeps/src/github.com/peterh/liner\" remote=\"couchbasedeps\" revision=\"6f820f8f90ce9482ffbd40bb15f9ea9932f4942d\" />",
 "  <project name=\"liner\" path=\"godeps/src/github.com/sbinet/liner\" remote=\"couchbasedeps\" revision=\"d9335eee40a45a4f5d74524c90040d6fe6013d50\" />",
 "  <project name=\"logstats\" path=\"godeps/src/github.com/couchbase/logstats\" revision=\"24ba9753289f155ab6d43a9a2585b9248da79791\" groups=\"bsl\" />",
 "  <project name=\"magma\" remote=\"couchbase-priv\" revision=\"91b88afd959db2835ae94a9a5d622fd28f3cf56b\" groups=\"notdefault,enterprise,kv_ee\" />",
 "  <project name=\"mmap-go\" path=\"godeps/src/github.com/blevesearch/mmap-go\" remote=\"blevesearch\" revision=\"99940f54c59671cf69e10b2e4041fabce88eb9b2\" />",
 "  <project name=\"mmap-go\" path=\"godeps/src/github.com/edsrzf/mmap-go\" remote=\"couchbasedeps\" revision=\"935e0e8a636ca4ba70b713f3e38a19e1b77739e8\" />",
 "  <project name=\"moss\" path=\"godeps/src/github.com/couchbase/moss\" revision=\"4fae7b31078a3e2bd5848a7029754885cdc495e0\" groups=\"bsl\" />",
 "  <project name=\"mossScope\" path=\"godeps/src/github.com/couchbase/mossScope\" revision=\"9e34f3688e0abd1b057ea2196f02e45f830506f8\" groups=\"bsl\" />",
 "  <project name=\"mousetrap\" path=\"godeps/src/github.com/inconshreveable/mousetrap\" remote=\"couchbasedeps\" revision=\"76626ae9c91c4f2a10f34cad8ce83ea42c93bb75\" />",
 "  <project name=\"msgp\" path=\"godeps/src/github.com/tinylib/msgp\" remote=\"couchbasedeps\" revision=\"5bb5e1aed7ba5bcc93307153b020e7ffe79b0509\" />",
 "  <project name=\"mux\" path=\"godeps/src/github.com/gorilla/mux\" remote=\"couchbasedeps\" revision=\"043ee6597c29786140136a5747b6a886364f5282\" />",
 "  <project name=\"n1fty\" path=\"goproj/src/github.com/couchbase/n1fty\" revision=\"ea4db331c572f6f0157d3947084141fb5e6d6243\" upstream=\"7.1.1\" dest-branch=\"7.1.1\" groups=\"bsl\" />",
 "  <project name=\"net\" path=\"godeps/src/golang.org/x/net\" remote=\"couchbasedeps\" revision=\"44b7c21cbf19450f38b337eb6b6fe4f6496fb5b3\" />",
 "  <project name=\"nitro\" path=\"goproj/src/github.com/couchbase/nitro\" revision=\"2575ec52bf5cf4c7ccc2cbc161eb38e46ce7b4a8\" groups=\"bsl\" />",
 "  <project name=\"npipe\" path=\"godeps/src/github.com/natefinch/npipe\" remote=\"couchbasedeps\" revision=\"272c8150302e83f23d32a355364578c9c13ab20f\" />",
 "  <project name=\"ns_server\" revision=\"954eaa16df8ec2378baa16b80850dab69d230289\" groups=\"bsl\" />",
 "  <project name=\"opentracing-go\" path=\"godeps/src/github.com/opentracing/opentracing-go\" remote=\"couchbasedeps\" revision=\"1949ddbfd147afd4d964a9f00b24eb291e0e7c38\" />",
 "  <project name=\"participle\" path=\"godeps/src/github.com/alecthomas/participle\" remote=\"couchbasedeps\" revision=\"d638c6e1953ed899e05a34da3935146790c60e46\" />",
 "  <project name=\"pflag\" path=\"godeps/src/github.com/spf13/pflag\" remote=\"couchbasedeps\" revision=\"a232f6d9f87afaaa08bafaff5da685f974b83313\" />",
 "  <project name=\"phosphor\" revision=\"2eb6c244d6910baf2834513ece579ba88e4f9b9d\" groups=\"bsl,kv\" />",
 "  <project name=\"pierrec-lz4\" path=\"godeps/src/github.com/pierrec/lz4\" remote=\"couchbasedeps\" revision=\"ed8d4cc3b461464e69798080a0092bd028910298\" />",
 "  <project name=\"pierrec-xxHash\" path=\"godeps/src/github.com/pierrec/xxHash\" remote=\"couchbasedeps\" revision=\"a0006b13c722f7f12368c00a3d3c2ae8a999a0c6\" />",
 "  <project name=\"pkcs8\" path=\"godeps/src/github.com/youmark/pkcs8\" remote=\"couchbasedeps\" revision=\"1be2e3e5546da8a58903ff4adcfab015022538ea\" upstream=\"refs/tags/v1.1\" dest-branch=\"refs/tags/v1.1\" />",
 "  <project name=\"plasma\" path=\"goproj/src/github.com/couchbase/plasma\" remote=\"couchbase-priv\" revision=\"38d8516ac67737cb6f2ad727258184b979713156\" groups=\"notdefault,enterprise\" />",
 "  <project name=\"platform\" revision=\"ebae7a1442bec210dddab6a2c98722476f669a50\" upstream=\"neo\" dest-branch=\"neo\" groups=\"bsl,kv\" />",
 "  <project name=\"product-metadata\" revision=\"ff7c24047435e393698847ec5147716017048636\" groups=\"notdefault,packaging\" />",
 "  <project name=\"product-texts\" revision=\"7ca232481faa1a9240564da4907b6d51ef8d7883\" upstream=\"neo\" dest-branch=\"neo\" />",
 "  <project name=\"protobuf\" path=\"godeps/src/github.com/golang/protobuf\" remote=\"couchbasedeps\" revision=\"ddf22928ea3c56eb4292a0adbbf5001b1e8e7d0d\" />",
 "  <project name=\"query\" path=\"goproj/src/github.com/couchbase/query\" revision=\"7bb63a13eef84a5b1143310186944d19dc12cbff\" upstream=\"7.1.1\" dest-branch=\"7.1.1\" groups=\"bsl\" />",
 "  <project name=\"query-ee\" path=\"goproj/src/github.com/couchbase/query-ee\" remote=\"couchbase-priv\" revision=\"69e90e12caa6084e0d78139d354e55fdc606e04a\" upstream=\"7.1.1\" dest-branch=\"7.1.1\" groups=\"notdefault,enterprise\" />",
 "  <project name=\"query-ui\" revision=\"96b9be252e5068a19f10ae5e956ea4c707a71b4b\" groups=\"bsl\" />",
 "  <project name=\"retriever\" path=\"godeps/src/github.com/couchbase/retriever\" revision=\"295b11134f91d9451c3ae21895f5615fc7a61e31\" groups=\"bsl\" />",
 "  <project name=\"roaring\" path=\"godeps/src/github.com/RoaringBitmap/roaring\" remote=\"couchbasedeps\" revision=\"4208ad825dda03a6a3d2197df8ec57948aebcc12\" />",
 "  <project name=\"sigar\" revision=\"e66221577adc083bfce0b17ea2f833fb49f28081\" groups=\"kv\" />",
 "  <project name=\"subjson\" revision=\"0820f83427d69c6eb737876eb2f2cf6aefa45802\" upstream=\"neo\" dest-branch=\"neo\" groups=\"bsl,kv\" />",
 "  <project name=\"sys\" path=\"godeps/src/golang.org/x/sys\" remote=\"couchbasedeps\" revision=\"12a6c2dcc1e4cb348b57847c73987099e261714b\" />",
 "  <project name=\"testrunner\" revision=\"5e88163df228ac2c4b7bb264260953cef2a26907\" upstream=\"master\" dest-branch=\"master\" />",
 "  <project name=\"text\" path=\"godeps/src/golang.org/x/text\" remote=\"couchbasedeps\" revision=\"22f1617af38ed4cd65b3b96e02bab267e560155c\" upstream=\"refs/tags/v0.3.4\" dest-branch=\"refs/tags/v0.3.4\" />",
 "  <project name=\"tlm\" revision=\"3623279137a5e48ccb3bbe6b22fec22af926f473\" upstream=\"neo\" dest-branch=\"neo\" groups=\"bsl,kv\">",
 "    <copyfile src=\"GNUmakefile\" dest=\"GNUmakefile\" />",
 "    <copyfile src=\"Makefile\" dest=\"Makefile\" />",
 "    <copyfile src=\"CMakeLists.txt\" dest=\"CMakeLists.txt\" />",
 "    <copyfile src=\"dot-clang-format\" dest=\".clang-format\" />",
 "    <copyfile src=\"dot-clang-tidy\" dest=\".clang-tidy\" />",
 "    <copyfile src=\"third-party-CMakeLists.txt\" dest=\"third_party/CMakeLists.txt\" />",
 "  </project>",
 "  <project name=\"udf-api\" path=\"goproj/src/github.com/couchbase/udf-api\" revision=\"b2788ae3d412356a330b36d7f38ad2c66edb5879\" />",
 "  <project name=\"uuid\" path=\"godeps/src/github.com/google/uuid\" remote=\"couchbasedeps\" revision=\"dec09d789f3dba190787f8b4454c7d3c936fed9e\" />",
 "  <project name=\"vbmap\" revision=\"946986b5cf372f16c25ac5e67136ceac3b720fd2\" upstream=\"neo\" dest-branch=\"neo\" />",
 "  <project name=\"voltron\" remote=\"couchbase-priv\" revision=\"43a9e30bc8371f6904a7d944120a403270fe53a6\" upstream=\"neo\" dest-branch=\"neo\" groups=\"notdefault,packaging\" />",
 "  <project name=\"zstd\" path=\"godeps/src/github.com/DataDog/zstd\" remote=\"couchbasedeps\" revision=\"aebefd9fcb99f22cd691ef778a12ed68f0e6a1ab\" />",
 "</manifest>"]

[error_logger:info,2022-09-07T14:28:11.530Z,ns_1@cb.local:ns_server_cluster_sup<0.218.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_cluster_sup}
    started: [{pid,<0.220.0>},
              {id,timeout_diag_logger},
              {mfargs,{timeout_diag_logger,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2022-09-07T14:28:11.531Z,ns_1@cb.local:ns_server_cluster_sup<0.218.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_cluster_sup}
    started: [{pid,<0.221.0>},
              {id,ns_cookie_manager},
              {mfargs,{ns_cookie_manager,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2022-09-07T14:28:11.536Z,ns_1@cb.local:chronicle_local<0.222.0>:chronicle_local:init:54]Ensure chronicle is started
[error_logger:info,2022-09-07T14:28:11.555Z,ns_1@cb.local:chronicle_sup<0.227.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,chronicle_sup}
    started: [{pid,<0.228.0>},
              {id,chronicle_events},
              {mfargs,{chronicle_events,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2022-09-07T14:28:11.555Z,ns_1@cb.local:chronicle_sup<0.227.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,chronicle_sup}
    started: [{pid,<0.229.0>},
              {id,chronicle_external_events},
              {mfargs,{gen_event,start_link,
                                 [{local,chronicle_external_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2022-09-07T14:28:11.557Z,ns_1@cb.local:chronicle_sup<0.227.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,chronicle_sup}
    started: [{pid,<0.230.0>},
              {id,chronicle_ets},
              {mfargs,{chronicle_ets,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2022-09-07T14:28:11.558Z,ns_1@cb.local:chronicle_sup<0.227.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,chronicle_sup}
    started: [{pid,<0.231.0>},
              {id,chronicle_settings},
              {mfargs,{chronicle_settings,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2022-09-07T14:28:11.562Z,ns_1@cb.local:chronicle_agent_sup<0.232.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,chronicle_agent_sup}
    started: [{pid,<0.233.0>},
              {id,chronicle_snapshot_mgr},
              {mfargs,{chronicle_snapshot_mgr,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2022-09-07T14:28:11.562Z,ns_1@cb.local:chronicle_agent_sup<0.232.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,chronicle_agent_sup}
    started: [{pid,<0.234.0>},
              {id,chronicle_rsm_events},
              {mfargs,{chronicle_events,start_link,[chronicle_rsm_events]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[chronicle:info,2022-09-07T14:28:11.626Z,ns_1@cb.local:chronicle_agent<0.235.0>:chronicle_storage:open_current_log:220]Error while opening log file "/opt/couchbase/var/lib/couchbase/config/chronicle/logs/0.log": no_header
[chronicle:info,2022-09-07T14:28:11.626Z,ns_1@cb.local:chronicle_agent<0.235.0>:chronicle_storage:create_log:268]Creating log file /opt/couchbase/var/lib/couchbase/config/chronicle/logs/0.log (high seqno = 0)
[chronicle:info,2022-09-07T14:28:11.643Z,ns_1@cb.local:chronicle_agent<0.235.0>:chronicle_agent:maybe_seed_storage:2444]Found empty storage. Seeding it with default metadata:
#{committed_seqno => 0,history_id => <<"no-history">>,peer => nonode@nohost,
  peer_id => <<>>,pending_branch => undefined,state => not_provisioned,
  term => {0,nonode@nohost}}
[error_logger:info,2022-09-07T14:28:11.649Z,ns_1@cb.local:chronicle_agent_sup<0.232.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,chronicle_agent_sup}
    started: [{pid,<0.235.0>},
              {id,chronicle_agent},
              {mfargs,{chronicle_agent,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2022-09-07T14:28:11.649Z,ns_1@cb.local:chronicle_sup<0.227.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,chronicle_sup}
    started: [{pid,<0.232.0>},
              {id,chronicle_agent_sup},
              {mfargs,{chronicle_agent_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2022-09-07T14:28:11.652Z,ns_1@cb.local:chronicle_sup<0.227.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,chronicle_sup}
    started: [{pid,<0.237.0>},
              {id,chronicle_secondary_sup},
              {mfargs,{chronicle_secondary_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2022-09-07T14:28:11.653Z,ns_1@cb.local:application_controller<0.44.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    application: chronicle
    started_at: 'ns_1@cb.local'

[ns_server:debug,2022-09-07T14:28:11.654Z,ns_1@cb.local:chronicle_local<0.222.0>:chronicle_local:init:58]Chronicle state is: not_provisioned
[ns_server:debug,2022-09-07T14:28:11.654Z,ns_1@cb.local:chronicle_local<0.222.0>:chronicle_local:provision:139]Provision chronicle on this node
[chronicle:debug,2022-09-07T14:28:11.656Z,ns_1@cb.local:chronicle_agent<0.235.0>:chronicle_agent:handle_provision:1199]Provisioning with history <<"1933459bb8a062c61b619e931c483c80">>. Config:
{config,undefined,0,undefined,
        #{'ns_1@cb.local' =>
              #{id => <<"fb54a3882f4ab57cf6f0be39357bb948">>,role => voter}},
        undefined,
        #{chronicle_config_rsm => {rsm_config,chronicle_config_rsm,[]},
          kv => {rsm_config,chronicle_kv,[]}},
        #{},undefined,
        [{<<"1933459bb8a062c61b619e931c483c80">>,0}]}
[error_logger:info,2022-09-07T14:28:11.663Z,ns_1@cb.local:<0.238.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.238.0>,dynamic_supervisor}
    started: [{pid,<0.239.0>},
              {id,chronicle_leader},
              {mfargs,{chronicle_leader,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2022-09-07T14:28:11.669Z,ns_1@cb.local:chronicle_secondary_restartable_sup<0.240.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,chronicle_secondary_restartable_sup}
    started: [{pid,<0.241.0>},
              {id,chronicle_status},
              {mfargs,{chronicle_status,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2022-09-07T14:28:11.670Z,ns_1@cb.local:chronicle_secondary_restartable_sup<0.240.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,chronicle_secondary_restartable_sup}
    started: [{pid,<0.242.0>},
              {id,chronicle_failover},
              {mfargs,{chronicle_failover,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2022-09-07T14:28:11.671Z,ns_1@cb.local:<0.238.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.238.0>,dynamic_supervisor}
    started: [{pid,<0.240.0>},
              {id,chronicle_secondary_restartable_sup},
              {mfargs,{chronicle_secondary_restartable_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2022-09-07T14:28:11.672Z,ns_1@cb.local:<0.238.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.238.0>,dynamic_supervisor}
    started: [{pid,<0.243.0>},
              {id,chronicle_server},
              {mfargs,{chronicle_server,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[chronicle:info,2022-09-07T14:28:11.682Z,ns_1@cb.local:chronicle_config_rsm<0.247.0>:chronicle_rsm:get_incarnation:1590]No incarnation file found at '/opt/couchbase/var/lib/couchbase/config/chronicle/rsms/chronicle_config_rsm/incarnation'
[chronicle:debug,2022-09-07T14:28:11.696Z,ns_1@cb.local:chronicle_server<0.243.0>:chronicle_server:handle_register_rsm:361]Registering RSM chronicle_config_rsm with pid <0.247.0>
[error_logger:info,2022-09-07T14:28:11.696Z,ns_1@cb.local:<0.246.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.246.0>,chronicle_single_rsm_sup}
    started: [{pid,<0.247.0>},
              {id,chronicle_config_rsm},
              {mfargs,{chronicle_rsm,start_link,
                                     [chronicle_config_rsm,
                                      <<"fb54a3882f4ab57cf6f0be39357bb948">>,
                                      chronicle_config_rsm,[]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2022-09-07T14:28:11.697Z,ns_1@cb.local:<0.245.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.245.0>,dynamic_supervisor}
    started: [{pid,<0.246.0>},
              {id,chronicle_config_rsm},
              {mfargs,
                  {chronicle_single_rsm_sup,start_link,
                      [chronicle_config_rsm,
                       <<"fb54a3882f4ab57cf6f0be39357bb948">>,
                       chronicle_config_rsm,[]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2022-09-07T14:28:11.699Z,ns_1@cb.local:<0.249.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.249.0>,chronicle_single_rsm_sup}
    started: [{pid,<0.250.0>},
              {id,'kv-events'},
              {mfargs,{gen_event,start_link,[{local,'kv-events'}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[chronicle:info,2022-09-07T14:28:11.703Z,ns_1@cb.local:kv<0.251.0>:chronicle_rsm:get_incarnation:1590]No incarnation file found at '/opt/couchbase/var/lib/couchbase/config/chronicle/rsms/kv/incarnation'
[chronicle:debug,2022-09-07T14:28:11.717Z,ns_1@cb.local:chronicle_server<0.243.0>:chronicle_server:handle_register_rsm:361]Registering RSM kv with pid <0.251.0>
[error_logger:info,2022-09-07T14:28:11.717Z,ns_1@cb.local:<0.249.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.249.0>,chronicle_single_rsm_sup}
    started: [{pid,<0.251.0>},
              {id,kv},
              {mfargs,{chronicle_rsm,start_link,
                                     [kv,
                                      <<"fb54a3882f4ab57cf6f0be39357bb948">>,
                                      chronicle_kv,[]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2022-09-07T14:28:11.718Z,ns_1@cb.local:<0.245.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.245.0>,dynamic_supervisor}
    started: [{pid,<0.249.0>},
              {id,kv},
              {mfargs,
                  {chronicle_single_rsm_sup,start_link,
                      [kv,<<"fb54a3882f4ab57cf6f0be39357bb948">>,chronicle_kv,
                       []]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2022-09-07T14:28:11.718Z,ns_1@cb.local:<0.238.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.238.0>,dynamic_supervisor}
    started: [{pid,<0.244.0>},
              {id,chronicle_rsm_sup},
              {mfargs,{chronicle_rsm_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2022-09-07T14:28:11.718Z,ns_1@cb.local:ns_server_cluster_sup<0.218.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_cluster_sup}
    started: [{pid,<0.222.0>},
              {id,chronicle_local},
              {mfargs,{chronicle_local,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[ns_server:info,2022-09-07T14:28:11.720Z,ns_1@cb.local:ns_cluster<0.253.0>:ns_cluster:handle_info:519]Chronicle state is: provisioned
[error_logger:info,2022-09-07T14:28:11.720Z,ns_1@cb.local:ns_server_cluster_sup<0.218.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_cluster_sup}
    started: [{pid,<0.253.0>},
              {id,ns_cluster},
              {mfargs,{ns_cluster,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[ns_server:info,2022-09-07T14:28:11.722Z,ns_1@cb.local:sigar<0.255.0>:sigar:spawn_sigar:135]Spawing sigar process 'portsigar for ns_1@cb.local'("/opt/couchbase/bin/sigar_port") with babysitter pid: 43
[error_logger:info,2022-09-07T14:28:11.722Z,ns_1@cb.local:ns_server_cluster_sup<0.218.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_cluster_sup}
    started: [{pid,<0.255.0>},
              {id,sigar},
              {mfargs,{sigar,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[ns_server:info,2022-09-07T14:28:11.724Z,ns_1@cb.local:ns_config_sup<0.256.0>:ns_config_sup:init:26]loading static ns_config from "/opt/couchbase/etc/couchbase/config"
[error_logger:info,2022-09-07T14:28:11.725Z,ns_1@cb.local:ns_config_sup<0.256.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_config_sup}
    started: [{pid,<0.257.0>},
              {id,tombstone_keeper},
              {mfargs,{tombstone_keeper,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2022-09-07T14:28:11.725Z,ns_1@cb.local:ns_config_sup<0.256.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_config_sup}
    started: [{pid,<0.258.0>},
              {id,ns_config_events},
              {mfargs,{gen_event,start_link,[{local,ns_config_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2022-09-07T14:28:11.725Z,ns_1@cb.local:ns_config_sup<0.256.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_config_sup}
    started: [{pid,<0.259.0>},
              {id,ns_config_events_local},
              {mfargs,{gen_event,start_link,[{local,ns_config_events_local}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[ns_server:info,2022-09-07T14:28:11.762Z,ns_1@cb.local:ns_config<0.260.0>:ns_config:load_config:1108]Loading static config from "/opt/couchbase/etc/couchbase/config"
[ns_server:info,2022-09-07T14:28:11.764Z,ns_1@cb.local:ns_config<0.260.0>:ns_config:load_config:1122]Loading dynamic config from "/opt/couchbase/var/lib/couchbase/config/config.dat"
[ns_server:info,2022-09-07T14:28:11.764Z,ns_1@cb.local:ns_config<0.260.0>:ns_config:load_config:1127]No dynamic config file found. Assuming we're brand new node
[ns_server:debug,2022-09-07T14:28:11.767Z,ns_1@cb.local:ns_config<0.260.0>:ns_config:load_config:1130]Here's full dynamic config we loaded:
[[]]
[ns_server:info,2022-09-07T14:28:11.771Z,ns_1@cb.local:ns_config<0.260.0>:ns_config:load_config:1152]Here's full dynamic config we loaded + static & default config:
[{auto_failover_cfg,
  [{enabled,true},
   {timeout,120},
   {count,0},
   {failover_on_data_disk_issues,[{enabled,false},{timePeriod,120}]},
   {failover_server_group,false},
   {max_count,1},
   {failed_over_server_groups,[]},
   {can_abort_rebalance,true}]},
 {retry_rebalance,[{enabled,false},{after_time_period,300},{max_attempts,1}]},
 {{node,'ns_1@cb.local',{project_intact,is_vulnerable}},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780091}}]}|
   false]},
 {{node,'ns_1@cb.local',backup_grpc_port},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780091}}]}|
   9124]},
 {{node,'ns_1@cb.local',backup_https_port},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780091}}]}|
   18097]},
 {{node,'ns_1@cb.local',backup_http_port},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780091}}]}|
   8097]},
 {{node,'ns_1@cb.local',prometheus_http_port},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780091}}]}|
   9123]},
 {{node,'ns_1@cb.local',cbas_debug_port},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780091}}]}|-1]},
 {{node,'ns_1@cb.local',cbas_parent_port},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780091}}]}|
   9122]},
 {{node,'ns_1@cb.local',cbas_metadata_port},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780091}}]}|
   9121]},
 {{node,'ns_1@cb.local',cbas_replication_port},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780091}}]}|
   9120]},
 {{node,'ns_1@cb.local',cbas_metadata_callback_port},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780091}}]}|
   9119]},
 {{node,'ns_1@cb.local',cbas_messaging_port},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780091}}]}|
   9118]},
 {{node,'ns_1@cb.local',cbas_result_port},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780091}}]}|
   9117]},
 {{node,'ns_1@cb.local',cbas_data_port},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780091}}]}|
   9116]},
 {{node,'ns_1@cb.local',cbas_cluster_port},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780091}}]}|
   9115]},
 {{node,'ns_1@cb.local',cbas_console_port},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780091}}]}|
   9114]},
 {{node,'ns_1@cb.local',cbas_cc_client_port},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780091}}]}|
   9113]},
 {{node,'ns_1@cb.local',cbas_cc_cluster_port},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780091}}]}|
   9112]},
 {{node,'ns_1@cb.local',cbas_cc_http_port},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780091}}]}|
   9111]},
 {{node,'ns_1@cb.local',cbas_admin_port},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780091}}]}|
   9110]},
 {{node,'ns_1@cb.local',cbas_ssl_port},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780091}}]}|
   18095]},
 {{node,'ns_1@cb.local',cbas_http_port},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780091}}]}|
   8095]},
 {{node,'ns_1@cb.local',eventing_https_port},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780091}}]}|
   18096]},
 {{node,'ns_1@cb.local',eventing_debug_port},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780091}}]}|
   9140]},
 {{node,'ns_1@cb.local',eventing_http_port},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780091}}]}|
   8096]},
 {{node,'ns_1@cb.local',fts_grpc_ssl_port},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780091}}]}|
   19130]},
 {{node,'ns_1@cb.local',fts_grpc_port},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780091}}]}|
   9130]},
 {{node,'ns_1@cb.local',fts_ssl_port},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780091}}]}|
   18094]},
 {{node,'ns_1@cb.local',fts_http_port},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780091}}]}|
   8094]},
 {{node,'ns_1@cb.local',indexer_https_port},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780091}}]}|
   19102]},
 {{node,'ns_1@cb.local',indexer_stmaint_port},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780091}}]}|
   9105]},
 {{node,'ns_1@cb.local',indexer_stcatchup_port},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780091}}]}|
   9104]},
 {{node,'ns_1@cb.local',indexer_stinit_port},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780091}}]}|
   9103]},
 {{node,'ns_1@cb.local',indexer_http_port},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780091}}]}|
   9102]},
 {{node,'ns_1@cb.local',indexer_scan_port},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780091}}]}|
   9101]},
 {{node,'ns_1@cb.local',indexer_admin_port},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780091}}]}|
   9100]},
 {{node,'ns_1@cb.local',ssl_query_port},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780091}}]}|
   18093]},
 {{node,'ns_1@cb.local',query_port},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780091}}]}|
   8093]},
 {{node,'ns_1@cb.local',projector_ssl_port},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780091}}]}|
   9999]},
 {{node,'ns_1@cb.local',projector_port},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780091}}]}|
   9999]},
 {{node,'ns_1@cb.local',ssl_capi_port},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780091}}]}|
   18092]},
 {{node,'ns_1@cb.local',capi_port},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780091}}]}|
   8092]},
 {{node,'ns_1@cb.local',memcached_prometheus},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780091}}]}|
   11280]},
 {{node,'ns_1@cb.local',memcached_dedicated_ssl_port},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780091}}]}|
   11206]},
 {{node,'ns_1@cb.local',xdcr_rest_port},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780091}}]}|
   9998]},
 {{node,'ns_1@cb.local',ssl_rest_port},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780091}}]}|
   18091]},
 {{node,'ns_1@cb.local',rest},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780091}}]},
   {port,8091},
   {port_meta,global}]},
 {rest,[{port,8091}]},
 {password_policy,[{min_length,6},{must_present,[]}]},
 {auto_reprovision_cfg,[{enabled,true},{max_nodes,1},{count,0}]},
 {log_redaction_default_cfg,[{redact_level,none}]},
 {replication,[{enabled,true}]},
 {alert_limits,
  [{max_overhead_perc,50},{max_disk_used,90},{max_indexer_ram,75}]},
 {email_alerts,
  [{recipients,["root@localhost"]},
   {sender,"couchbase@localhost"},
   {enabled,false},
   {email_server,
    [{user,[]},{pass,"*****"},{host,"localhost"},{port,25},{encrypt,false}]},
   {alerts,
    [auto_failover_node,auto_failover_maximum_reached,
     auto_failover_other_nodes_down,auto_failover_cluster_too_small,
     auto_failover_disabled,ip,disk,overhead,ep_oom_errors,
     ep_item_commit_failed,audit_dropped_events,indexer_ram_max_usage,
     ep_clock_cas_drift_threshold_exceeded,communication_issue,
     time_out_of_sync,disk_usage_analyzer_stuck]},
   {pop_up_alerts,
    [auto_failover_node,auto_failover_maximum_reached,
     auto_failover_other_nodes_down,auto_failover_cluster_too_small,
     auto_failover_disabled,ip,disk,overhead,ep_oom_errors,
     ep_item_commit_failed,audit_dropped_events,indexer_ram_max_usage,
     ep_clock_cas_drift_threshold_exceeded,communication_issue,
     time_out_of_sync,disk_usage_analyzer_stuck]}]},
 {{node,'ns_1@cb.local',event_log},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780091}}]},
   {filename,"/opt/couchbase/var/lib/couchbase/event_log"}]},
 {{node,'ns_1@cb.local',ns_log},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780091}}]},
   {filename,"/opt/couchbase/var/lib/couchbase/ns_log"}]},
 {{node,'ns_1@cb.local',port_servers},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780091}}]}]},
 {secure_headers,[]},
 {buckets,[{configs,[]}]},
 {cbas_memory_quota,1529},
 {fts_memory_quota,512},
 {memory_quota,4046},
 {{node,'ns_1@cb.local',memcached_config},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780091}}]}|
   {[{interfaces,{memcached_config_mgr,get_interfaces,[]}},
     {client_cert_auth,{memcached_config_mgr,client_cert_auth,[]}},
     {connection_idle_time,connection_idle_time},
     {privilege_debug,privilege_debug},
     {breakpad,
      {[{enabled,breakpad_enabled},
        {minidump_dir,{memcached_config_mgr,get_minidump_dir,[]}}]}},
     {admin,{"~s",[admin_user]}},
     {verbosity,verbosity},
     {audit_file,{"~s",[audit_file]}},
     {rbac_file,{"~s",[rbac_file]}},
     {dedupe_nmvb_maps,dedupe_nmvb_maps},
     {tracing_enabled,tracing_enabled},
     {datatype_snappy,{memcached_config_mgr,is_snappy_enabled,[]}},
     {xattr_enabled,true},
     {scramsha_fallback_salt,{memcached_config_mgr,get_fallback_salt,[]}},
     {collections_enabled,{memcached_config_mgr,collections_enabled,[]}},
     {enforce_tenant_limits_enabled,
      {memcached_config_mgr,should_enforce_limits,[]}},
     {max_connections,max_connections},
     {system_connections,system_connections},
     {num_reader_threads,num_reader_threads},
     {num_writer_threads,num_writer_threads},
     {num_auxio_threads,num_auxio_threads},
     {num_nonio_threads,num_nonio_threads},
     {num_storage_threads,num_storage_threads},
     {logger,
      {[{filename,{"~s/~s",[log_path,log_prefix]}},
        {cyclesize,log_cyclesize}]}},
     {external_auth_service,
      {memcached_config_mgr,get_external_auth_service,[]}},
     {active_external_users_push_interval,
      {memcached_config_mgr,get_external_users_push_interval,[]}},
     {prometheus,{memcached_config_mgr,prometheus_cfg,[]}}]}]},
 {{node,'ns_1@cb.local',memcached},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780091}}]},
   {port,11210},
   {dedicated_port,11209},
   {dedicated_ssl_port,11206},
   {ssl_port,11207},
   {admin_user,"@ns_server"},
   {other_users,
    ["@cbq-engine","@projector","@goxdcr","@index","@fts","@eventing","@cbas",
     "@backup"]},
   {admin_pass,"*****"},
   {engines,
    [{membase,
      [{engine,"/opt/couchbase/lib/memcached/ep.so"},
       {static_config_string,"failpartialwarmup=false"}]},
     {memcached,
      [{engine,"/opt/couchbase/lib/memcached/default_engine.so"},
       {static_config_string,"vb0=true"}]}]},
   {config_path,"/opt/couchbase/var/lib/couchbase/config/memcached.json"},
   {audit_file,"/opt/couchbase/var/lib/couchbase/config/audit.json"},
   {rbac_file,"/opt/couchbase/var/lib/couchbase/config/memcached.rbac"},
   {log_path,"/opt/couchbase/var/lib/couchbase/logs"},
   {log_prefix,"memcached.log"},
   {log_generations,20},
   {log_cyclesize,10485760},
   {log_rotation_period,39003}]},
 {{node,'ns_1@cb.local',memcached_defaults},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780091}}]},
   {max_connections,65000},
   {system_connections,5000},
   {connection_idle_time,0},
   {verbosity,0},
   {privilege_debug,false},
   {breakpad_enabled,true},
   {breakpad_minidump_dir_path,"/opt/couchbase/var/lib/couchbase/crash"},
   {dedupe_nmvb_maps,false},
   {je_malloc_conf,undefined},
   {tracing_enabled,true},
   {datatype_snappy,true},
   {num_reader_threads,<<"default">>},
   {num_writer_threads,<<"default">>},
   {num_auxio_threads,<<"default">>},
   {num_nonio_threads,<<"default">>},
   {num_storage_threads,<<"default">>}]},
 {memcached,[]},
 {{node,'ns_1@cb.local',audit},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780091}}]}]},
 {audit,
  [{auditd_enabled,false},
   {rotate_interval,86400},
   {rotate_size,20971520},
   {disabled,[]},
   {enabled,[]},
   {disabled_users,[]},
   {sync,[]},
   {log_path,"/opt/couchbase/var/lib/couchbase/logs"}]},
 {{node,'ns_1@cb.local',isasl},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780091}}]},
   {path,"/opt/couchbase/var/lib/couchbase/isasl.pw"}]},
 {remote_clusters,[]},
 {scramsha_fallback_salt,<<"ïyñ-ÞÃ¼zÌÿ^\b">>},
 {client_cert_auth,[{state,"disable"},{prefixes,[]}]},
 {rest_creds,null},
 {{metakv,<<"/analytics/settings/config">>},
  <<"{\"analytics.settings.num_replicas\":0}">>},
 {{metakv,<<"/query/settings/config">>},
  <<"{\"timeout\":0,\"n1ql-feat-ctrl\":12,\"query.settings.curl_whitelist\":{\"all_access\":false,\"allowed_urls\":[],\"disallowed_urls\":[]},\"max-parallelism\":1,\"query.settings.tmp_space_dir\":\"/opt/couchbase/var/lib/couchbase/tmp\",\"completed-limit\":4000,\"prepared-limit\":16384,\"pipeline-batch\":16,\"pipeline-cap\":512,\"scan-cap\":512,\"loglevel\":\"info\",\"completed-threshold\":1000,\"query.settings.tmp_space_size\":5120}">>},
 {{metakv,<<"/eventing/settings/config">>},<<"{\"ram_quota\":256}">>},
 {{metakv,<<"/indexing/settings/config">>},
  <<"{\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.rebalance.redistribute_indexes\":false,\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.log_level\":\"info\",\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.compaction.min_frag\":30,\"indexer.settings.inmemory_snapshot.interval\":200,\"indexer.settings.enable_page_bloom_filter\":false,\"indexer.settings.max_cpu_percent\":0,\"indexer.settings.storage_mode\":\"\",\"indexer.settings.recovery.max_rollbacks\":2,\"indexer.settings.num_replica\":0,\"indexer.settings.memory_quota\":536870912,\"indexer.settings.compaction.abort_exceed_interval\":false}">>},
 {{couchdb,max_parallel_replica_indexers},2},
 {{couchdb,max_parallel_indexers},4},
 {{node,'ns_1@cb.local',membership},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780091}}]}|
   active]},
 {server_groups,
  [[{uuid,<<"0">>},{name,<<"Group 1">>},{nodes,['ns_1@cb.local']}]]},
 {quorum_nodes,['ns_1@cb.local']},
 {nodes_wanted,['ns_1@cb.local']},
 {{node,'ns_1@cb.local',compaction_daemon},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780091}}]},
   {check_interval,30},
   {min_db_file_size,131072},
   {min_view_file_size,20971520}]},
 {set_view_update_daemon,
  [{update_interval,5000},
   {update_min_changes,5000},
   {replica_update_min_changes,5000}]},
 {autocompaction,
  [{database_fragmentation_threshold,{30,undefined}},
   {view_fragmentation_threshold,{30,undefined}}]},
 {max_bucket_count,30},
 {index_aware_rebalance_disabled,false},
 {{node,'ns_1@cb.local',saslauthd_enabled},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780091}}]}|
   true]},
 {{node,'ns_1@cb.local',is_enterprise},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780091}}]}|
   true]},
 {{node,'ns_1@cb.local',config_version},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780091}}]}|
   {7,1}]},
 {{node,'ns_1@cb.local',uuid},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780091}}]}|
   <<"1678dfae96c38e07dff43c49b9f6967b">>]}]
[error_logger:info,2022-09-07T14:28:11.779Z,ns_1@cb.local:ns_config_sup<0.256.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_config_sup}
    started: [{pid,<0.260.0>},
              {id,ns_config},
              {mfargs,{ns_config,start_link,
                                 ["/opt/couchbase/etc/couchbase/config",
                                  ns_config_default]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2022-09-07T14:28:11.780Z,ns_1@cb.local:ns_config_sup<0.256.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_config_sup}
    started: [{pid,<0.263.0>},
              {id,ns_config_remote},
              {mfargs,{ns_config_replica,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2022-09-07T14:28:11.780Z,ns_1@cb.local:ns_config_sup<0.256.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_config_sup}
    started: [{pid,<0.264.0>},
              {id,ns_config_log},
              {mfargs,{ns_config_log,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2022-09-07T14:28:11.781Z,ns_1@cb.local:ns_server_cluster_sup<0.218.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_cluster_sup}
    started: [{pid,<0.256.0>},
              {id,ns_config_sup},
              {mfargs,{ns_config_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2022-09-07T14:28:11.783Z,ns_1@cb.local:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@cb.local',erl_external_listeners} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780091}}]},
 {inet,false}]
[ns_server:debug,2022-09-07T14:28:11.783Z,ns_1@cb.local:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@cb.local',node_encryption} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780091}}]}|false]
[ns_server:debug,2022-09-07T14:28:11.783Z,ns_1@cb.local:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@cb.local',address_family} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780091}}]}|inet]
[ns_server:debug,2022-09-07T14:28:11.783Z,ns_1@cb.local:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{local_changes_count,<<"1678dfae96c38e07dff43c49b9f6967b">>} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780091}}]}]
[error_logger:info,2022-09-07T14:28:11.783Z,ns_1@cb.local:ns_server_cluster_sup<0.218.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_cluster_sup}
    started: [{pid,<0.266.0>},
              {id,netconfig_updater},
              {mfargs,{netconfig_updater,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2022-09-07T14:28:11.786Z,ns_1@cb.local:ns_server_cluster_sup<0.218.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_cluster_sup}
    started: [{pid,<0.269.0>},
              {id,json_rpc_connection_sup},
              {mfargs,{json_rpc_connection_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[chronicle:debug,2022-09-07T14:28:11.793Z,ns_1@cb.local:chronicle_leader<0.239.0>:chronicle_leader:handle_state_timeout:608]State timeout when state is: {observer,true,false}
[chronicle:info,2022-09-07T14:28:11.793Z,ns_1@cb.local:<0.271.0>:chronicle_leader:do_election_worker:892]Starting election.
History ID: <<"1933459bb8a062c61b619e931c483c80">>
Log position: {{1,'ns_1@cb.local'},1}
Peers: ['ns_1@cb.local']
Required quorum: {majority,{set,1,16,16,8,80,48,
                                {[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],
                                 []},
                                {{[],[],[],[],[],[],[],[],[],[],[],[],
                                  ['ns_1@cb.local'],
                                  [],[],[]}}}}
[chronicle:info,2022-09-07T14:28:11.793Z,ns_1@cb.local:<0.271.0>:chronicle_leader:do_election_worker:901]I'm the only peer, so I'm the leader.
[chronicle:info,2022-09-07T14:28:11.793Z,ns_1@cb.local:chronicle_leader<0.239.0>:chronicle_leader:handle_election_result:691]Going to become a leader in term {2,'ns_1@cb.local'} (history id <<"1933459bb8a062c61b619e931c483c80">>)
[chronicle:debug,2022-09-07T14:28:11.818Z,ns_1@cb.local:chronicle_agent<0.235.0>:chronicle_agent:handle_establish_term:1529]Accepted term {2,'ns_1@cb.local'} in history <<"1933459bb8a062c61b619e931c483c80">>
[chronicle:debug,2022-09-07T14:28:11.818Z,ns_1@cb.local:chronicle_proposer<0.272.0>:chronicle_proposer:establish_term_init:367]Going to establish term {2,'ns_1@cb.local'} (history id <<"1933459bb8a062c61b619e931c483c80">>).
Quorum peers: ['ns_1@cb.local']
Metadata:
{metadata,'ns_1@cb.local',<<"fb54a3882f4ab57cf6f0be39357bb948">>,
          <<"1933459bb8a062c61b619e931c483c80">>,
          {1,'ns_1@cb.local'},
          {1,'ns_1@cb.local'},
          1,1,
          {log_entry,<<"1933459bb8a062c61b619e931c483c80">>,
                     {1,'ns_1@cb.local'},
                     1,
                     {config,undefined,0,undefined,
                             #{'ns_1@cb.local' =>
                                   #{id =>
                                         <<"fb54a3882f4ab57cf6f0be39357bb948">>,
                                     role => voter}},
                             undefined,
                             #{chronicle_config_rsm =>
                                   {rsm_config,chronicle_config_rsm,[]},
                               kv => {rsm_config,chronicle_kv,[]}},
                             #{},undefined,
                             [{<<"1933459bb8a062c61b619e931c483c80">>,0}]}},
          {log_entry,<<"1933459bb8a062c61b619e931c483c80">>,
                     {1,'ns_1@cb.local'},
                     1,
                     {config,undefined,0,undefined,
                             #{'ns_1@cb.local' =>
                                   #{id =>
                                         <<"fb54a3882f4ab57cf6f0be39357bb948">>,
                                     role => voter}},
                             undefined,
                             #{chronicle_config_rsm =>
                                   {rsm_config,chronicle_config_rsm,[]},
                               kv => {rsm_config,chronicle_kv,[]}},
                             #{},undefined,
                             [{<<"1933459bb8a062c61b619e931c483c80">>,0}]}},
          undefined}
[chronicle:debug,2022-09-07T14:28:11.819Z,ns_1@cb.local:chronicle_proposer<0.272.0>:chronicle_proposer:establish_term_maybe_transition:686]Established term {2,'ns_1@cb.local'} (history id <<"1933459bb8a062c61b619e931c483c80">>) successfully.
Votes: ['ns_1@cb.local']
[chronicle:debug,2022-09-07T14:28:11.819Z,ns_1@cb.local:chronicle_proposer<0.272.0>:chronicle_proposer:handle_state_enter:284]Starting recovery for term {2,'ns_1@cb.local'} in history <<"1933459bb8a062c61b619e931c483c80">>
[error_logger:info,2022-09-07T14:28:11.831Z,ns_1@cb.local:ns_server_nodes_sup<0.273.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.275.0>},
              {name,chronicle_compat_events},
              {mfargs,{chronicle_compat_events,start_link,[]}},
              {restart_type,permanent},
              {shutdown,5000},
              {child_type,worker}]
[chronicle:debug,2022-09-07T14:28:11.834Z,ns_1@cb.local:chronicle_proposer<0.272.0>:chronicle_proposer:handle_state_enter:296]Proposer for term {2,'ns_1@cb.local'} in history <<"1933459bb8a062c61b619e931c483c80">> is ready. Committed seqno: 2
[chronicle:info,2022-09-07T14:28:11.835Z,ns_1@cb.local:chronicle_leader<0.239.0>:chronicle_leader:handle_note_term_status:596]Term {2,'ns_1@cb.local'} established.
[error_logger:info,2022-09-07T14:28:11.835Z,ns_1@cb.local:ns_server_nodes_sup<0.273.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.280.0>},
              {name,remote_monitors},
              {mfargs,{remote_monitors,start_link,[]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[ns_server:debug,2022-09-07T14:28:11.841Z,ns_1@cb.local:menelaus_barrier<0.281.0>:one_shot_barrier:barrier_body:52]Barrier menelaus_barrier has started
[error_logger:info,2022-09-07T14:28:11.841Z,ns_1@cb.local:ns_server_nodes_sup<0.273.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.281.0>},
              {name,menelaus_barrier},
              {mfargs,{menelaus_sup,barrier_start_link,[]}},
              {restart_type,temporary},
              {shutdown,1000},
              {child_type,worker}]
[error_logger:info,2022-09-07T14:28:11.841Z,ns_1@cb.local:ns_server_nodes_sup<0.273.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.282.0>},
              {name,rest_lhttpc_pool},
              {mfargs,{lhttpc_manager,start_link,
                                      [[{name,rest_lhttpc_pool},
                                        {connection_timeout,120000},
                                        {pool_size,20}]]}},
              {restart_type,{permanent,1}},
              {shutdown,1000},
              {child_type,worker}]
[error_logger:info,2022-09-07T14:28:11.889Z,ns_1@cb.local:ns_server_nodes_sup<0.273.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.283.0>},
              {name,memcached_refresh},
              {mfargs,{memcached_refresh,start_link,[]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[error_logger:info,2022-09-07T14:28:11.916Z,ns_1@cb.local:ns_server_nodes_sup<0.273.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.284.0>},
              {name,ns_secrets},
              {mfargs,{ns_secrets,start_link,[]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[error_logger:info,2022-09-07T14:28:11.918Z,ns_1@cb.local:ns_ssl_services_sup<0.285.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_ssl_services_sup}
    started: [{pid,<0.286.0>},
              {id,ssl_service_events},
              {mfargs,{gen_event,start_link,[{local,ssl_service_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2022-09-07T14:28:11.924Z,ns_1@cb.local:ns_ssl_services_setup<0.287.0>:ns_ssl_services_setup:maybe_store_ca_certs:672]Considering to store CA certs
[ns_server:debug,2022-09-07T14:28:12.188Z,ns_1@cb.local:<0.291.0>:goport:handle_eof:585]Stream 'stderr' closed
[ns_server:debug,2022-09-07T14:28:12.188Z,ns_1@cb.local:<0.291.0>:goport:handle_eof:585]Stream 'stdout' closed
[ns_server:info,2022-09-07T14:28:12.193Z,ns_1@cb.local:<0.291.0>:goport:handle_process_exit:566]Port exited with status 0.
[ns_server:debug,2022-09-07T14:28:12.235Z,ns_1@cb.local:ns_ssl_services_setup<0.287.0>:ns_server_cert:generate_cert_and_pkey:181]Generated certificate and private key in 307364 us
[ns_server:debug,2022-09-07T14:28:12.236Z,ns_1@cb.local:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
cert_and_pkey ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780092}}]}|
 {<<"-----BEGIN CERTIFICATE-----\nMIIDITCCAgmgAwIBAgIIFxKaUqQBFTwwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciA5NGQ2NGQ0NTAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgOTRkNjRk\nNDUwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQDCrzpi1QtF4PyrbI2Y\nQB0augORV0Ro18YRvmfImOFnIZpNAmUYZRU68E39LZL794EOVdu05NB0tXNOfVrG\nB6gQNweW7UL1RiCL3OhoZ/VBsR9qSG5"...>>,
  {sanitized,<<"rUyVs4dBK1Fcwmw1hpHxsD1qPAlzmuJ/tb4d6NhlhWU=">>}}]
[ns_server:debug,2022-09-07T14:28:12.236Z,ns_1@cb.local:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{local_changes_count,<<"1678dfae96c38e07dff43c49b9f6967b">>} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780092}}]}]
[ns_server:debug,2022-09-07T14:28:12.401Z,ns_1@cb.local:ns_ssl_services_setup<0.287.0>:ns_ssl_services_setup:maybe_store_ca_certs:687]Updating CA file with 1 certificates
[ns_server:info,2022-09-07T14:28:12.423Z,ns_1@cb.local:ns_ssl_services_setup<0.287.0>:ns_ssl_services_setup:maybe_store_ca_certs:690]CA file updated: 1 cert(s) written
[ns_server:info,2022-09-07T14:28:12.424Z,ns_1@cb.local:ns_ssl_services_setup<0.287.0>:ns_ssl_services_setup:maybe_generate_node_certs:737]Regenerating certs because there are no certs on this node
[ns_server:debug,2022-09-07T14:28:12.827Z,ns_1@cb.local:<0.297.0>:goport:handle_eof:585]Stream 'stderr' closed
[ns_server:debug,2022-09-07T14:28:12.827Z,ns_1@cb.local:<0.297.0>:goport:handle_eof:585]Stream 'stdout' closed
[ns_server:info,2022-09-07T14:28:12.828Z,ns_1@cb.local:<0.297.0>:goport:handle_process_exit:566]Port exited with status 0.
[ns_server:info,2022-09-07T14:28:12.885Z,ns_1@cb.local:ns_ssl_services_setup<0.287.0>:ns_ssl_services_setup:save_node_certs:797]New node cert and pkey are written to tmp file
[ns_server:info,2022-09-07T14:28:12.929Z,ns_1@cb.local:ns_ssl_services_setup<0.287.0>:ns_ssl_services_setup:save_node_certs_phase2:820]Node cert and pkey files updated
[ns_server:debug,2022-09-07T14:28:12.930Z,ns_1@cb.local:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{local_changes_count,<<"1678dfae96c38e07dff43c49b9f6967b">>} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{3,63829780092}}]}]
[ns_server:debug,2022-09-07T14:28:12.930Z,ns_1@cb.local:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@cb.local',node_cert} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780092}}]},
 {subject,<<"CN=Couchbase Server Node (127.0.0.1)">>},
 {not_after,64691827199},
 {verified_with,<<48,185,21,78,241,90,231,242,183,201,180,2,229,131,239,69>>},
 {type,generated},
 {load_timestamp,63829780092},
 {ca,<<"-----BEGIN CERTIFICATE-----\nMIIDITCCAgmgAwIBAgIIFxKaUqQBFTwwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciA5NGQ2NGQ0NTAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgOTRkNjRk\nNDUwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQDCrzpi1QtF4PyrbI2Y\nQB0augORV0Ro18YRvmfImOFnIZpNAmUYZRU68E39LZL794EOVdu05NB0tXNOfVrG\nB6gQNwe"...>>},
 {pem,<<"-----BEGIN CERTIFICATE-----\nMIIDOTCCAiGgAwIBAgIIFxKaUsnSLBwwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciA5NGQ2NGQ0NTAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCwxKjAoBgNVBAMTIUNvdWNoYmFzZSBTZXJ2ZXIgTm9kZSAo\nMTI3LjAuMC4xKTCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBALESQvmu\nbrV8JXPnQh3zLduahBDrsIVoTPN0LJWCDoH6bNJOn7QdE6oSpVYqgG/rDVmX67GV\nEl3"...>>},
 {pkey_passphrase_settings,[]},
 {certs_epoch,0},
 {hostname,"127.0.0.1"}]
[ns_server:info,2022-09-07T14:28:12.972Z,ns_1@cb.local:ns_ssl_services_setup<0.287.0>:ns_ssl_services_setup:handle_info:602]cert_and_pkey changed
[error_logger:info,2022-09-07T14:28:12.972Z,ns_1@cb.local:ns_ssl_services_sup<0.285.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_ssl_services_sup}
    started: [{pid,<0.287.0>},
              {id,ns_ssl_services_setup},
              {mfargs,{ns_ssl_services_setup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2022-09-07T14:28:12.983Z,ns_1@cb.local:ns_ssl_services_setup<0.287.0>:ns_ssl_services_setup:maybe_store_ca_certs:672]Considering to store CA certs
[ns_server:info,2022-09-07T14:28:12.991Z,ns_1@cb.local:ns_ssl_services_setup<0.287.0>:ns_ssl_services_setup:validate_pkey:856]Private key passphrase validation suceeded
[ns_server:debug,2022-09-07T14:28:12.991Z,ns_1@cb.local:ns_ssl_services_setup<0.287.0>:ns_ssl_services_setup:notify_services:963]Going to notify following services: [cb_dist_tls,capi_ssl_service,memcached,
                                     event]
[ns_server:debug,2022-09-07T14:28:12.991Z,ns_1@cb.local:cb_dist<0.209.0>:cb_dist:info_msg:872]cb_dist: Restarting tls distribution protocols (if any)
[ns_server:info,2022-09-07T14:28:12.991Z,ns_1@cb.local:<0.311.0>:ns_ssl_services_setup:notify_service:999]Successfully notified service event
[ns_server:info,2022-09-07T14:28:12.993Z,ns_1@cb.local:<0.309.0>:ns_ssl_services_setup:notify_service:999]Successfully notified service cb_dist_tls
[ns_server:warn,2022-09-07T14:28:12.999Z,ns_1@cb.local:<0.310.0>:ns_ssl_services_setup:notify_service:1001]Failed to notify service memcached: {error,no_proccess}
[error_logger:info,2022-09-07T14:28:13.006Z,ns_1@cb.local:net_kernel<0.212.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {1,#Ref<0.2209594722.1843789828.36730>}}}
[ns_server:debug,2022-09-07T14:28:13.006Z,ns_1@cb.local:net_kernel<0.212.0>:cb_dist:info_msg:872]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2022-09-07T14:28:13.006Z,ns_1@cb.local:cb_dist<0.209.0>:cb_dist:info_msg:872]cb_dist: Added connection {con,#Ref<0.2209594722.1843658756.36735>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2022-09-07T14:28:13.006Z,ns_1@cb.local:cb_dist<0.209.0>:cb_dist:info_msg:872]cb_dist: Updated connection: {con,#Ref<0.2209594722.1843658756.36735>,
                                  inet_tcp_dist,<0.313.0>,
                                  #Ref<0.2209594722.1843658756.36737>}
[error_logger:info,2022-09-07T14:28:13.007Z,ns_1@cb.local:net_kernel<0.212.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.313.0>,shutdown}}
[error_logger:info,2022-09-07T14:28:13.007Z,ns_1@cb.local:net_kernel<0.212.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1157,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2022-09-07T14:28:13.007Z,ns_1@cb.local:cb_dist<0.209.0>:cb_dist:info_msg:872]cb_dist: Connection down: {con,#Ref<0.2209594722.1843658756.36735>,
                               inet_tcp_dist,<0.313.0>,
                               #Ref<0.2209594722.1843658756.36737>}
[ns_server:debug,2022-09-07T14:28:13.007Z,ns_1@cb.local:<0.312.0>:ns_couchdb_api:rpc_couchdb_node:162]RPC to couchdb node failed for restart_capi_ssl_service with {badrpc,nodedown}
Stack: [{ns_couchdb_api,rpc_couchdb_node,4,
                        [{file,"src/ns_couchdb_api.erl"},{line,160}]},
        {ns_ssl_services_setup,do_notify_service,1,
                               [{file,"src/ns_ssl_services_setup.erl"},
                                {line,1020}]},
        {ns_ssl_services_setup,notify_service,1,
                               [{file,"src/ns_ssl_services_setup.erl"},
                                {line,996}]},
        {async,'-async_init/4-fun-1-',3,[{file,"src/async.erl"},{line,191}]}]
[ns_server:warn,2022-09-07T14:28:13.007Z,ns_1@cb.local:<0.312.0>:ns_ssl_services_setup:notify_service:1001]Failed to notify service capi_ssl_service: {'EXIT',{error,{badrpc,nodedown}}}
[ns_server:info,2022-09-07T14:28:13.007Z,ns_1@cb.local:ns_ssl_services_setup<0.287.0>:ns_ssl_services_setup:notify_services:979]Succesfully notified services [event,cb_dist_tls]
[ns_server:info,2022-09-07T14:28:13.009Z,ns_1@cb.local:<0.302.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:129]Loaded pluggable UI specification for backup from "/opt/couchbase/etc/couchbase/pluggable-ui-backup.json"
[ns_server:info,2022-09-07T14:28:13.010Z,ns_1@cb.local:<0.302.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:129]Loaded pluggable UI specification for cbas from "/opt/couchbase/etc/couchbase/pluggable-ui-cbas.json"
[ns_server:info,2022-09-07T14:28:13.011Z,ns_1@cb.local:<0.302.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:129]Loaded pluggable UI specification for eventing from "/opt/couchbase/etc/couchbase/pluggable-ui-eventing.json"
[ns_server:info,2022-09-07T14:28:13.011Z,ns_1@cb.local:<0.302.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:129]Loaded pluggable UI specification for fts from "/opt/couchbase/etc/couchbase/pluggable-ui-fts.json"
[ns_server:info,2022-09-07T14:28:13.011Z,ns_1@cb.local:<0.302.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:129]Loaded pluggable UI specification for n1ql from "/opt/couchbase/etc/couchbase/pluggable-ui-query.json"
[ns_server:info,2022-09-07T14:28:13.024Z,ns_1@cb.local:ns_ssl_services_setup<0.287.0>:ns_ssl_services_setup:notify_services:990]Failed to notify some services. Will retry in 5 sec, [{{error,no_proccess},
                                                       memcached},
                                                      {{'EXIT',
                                                        {error,
                                                         {badrpc,nodedown}}},
                                                       capi_ssl_service}]
[ns_server:info,2022-09-07T14:28:13.046Z,ns_1@cb.local:<0.302.0>:menelaus_web:maybe_start_http_server:126]Started web service with options:
[{ip,"0.0.0.0"},
 [{keyfile,"/opt/couchbase/var/lib/couchbase/config/certs/pkey.pem"},
  {certfile,"/opt/couchbase/var/lib/couchbase/config/certs/chain.pem"},
  {versions,['tlsv1.2','tlsv1.3']},
  {cacerts,[<<48,130,3,33,48,130,2,9,160,3,2,1,2,2,8,23,18,154,82,164,1,21,
              60,48,13,6,9,42,134,72,134,247,13,1,1,11,5,0,48,36,49,34,48,32,
              6,3,85,4,3,19,25,67,111,117,99,104,98,97,115,101,32,83,101,114,
              118,101,114,32,57,52,100,54,52,100,52,53,48,30,23,13,49,51,48,
              49,48,49,48,48,48,48,48,48,90,23,13,52,57,49,50,51,49,50,51,53,
              57,53,57,90,48,36,49,34,48,32,6,3,85,4,3,19,25,67,111,117,99,
              104,98,97,115,101,32,83,101,114,118,101,114,32,57,52,100,54,52,
              100,52,53,48,130,1,34,48,13,6,9,42,134,72,134,247,13,1,1,1,5,0,
              3,130,1,15,0,48,130,1,10,2,130,1,1,0,194,175,58,98,213,11,69,
              224,252,171,108,141,152,64,29,26,186,3,145,87,68,104,215,198,
              17,190,103,200,152,225,103,33,154,77,2,101,24,101,21,58,240,77,
              253,45,146,251,247,129,14,85,219,180,228,208,116,181,115,78,
              125,90,198,7,168,16,55,7,150,237,66,245,70,32,139,220,232,104,
              103,245,65,177,31,106,72,110,120,151,250,197,73,215,91,175,13,
              220,52,194,168,103,38,26,60,90,13,132,154,32,60,227,248,208,
              130,20,102,21,35,42,0,59,200,160,63,47,64,42,42,126,117,233,54,
              223,184,15,104,95,253,242,228,249,181,15,39,32,210,132,102,13,
              23,167,42,95,220,164,228,82,191,6,139,169,190,0,223,155,219,
              188,13,245,152,27,48,187,120,139,42,26,103,232,190,238,187,241,
              79,160,250,145,59,20,219,5,142,169,29,251,185,220,133,142,155,
              49,155,191,115,197,132,112,209,160,210,144,35,126,25,150,32,
              139,244,211,244,232,126,190,241,155,155,47,160,187,39,229,223,
              27,236,217,144,127,217,93,214,184,72,80,152,220,99,162,102,147,
              233,116,67,156,133,32,112,198,46,105,2,3,1,0,1,163,87,48,85,48,
              14,6,3,85,29,15,1,1,255,4,4,3,2,2,164,48,19,6,3,85,29,37,4,12,
              48,10,6,8,43,6,1,5,5,7,3,1,48,15,6,3,85,29,19,1,1,255,4,5,48,3,
              1,1,255,48,29,6,3,85,29,14,4,22,4,20,26,62,215,165,145,243,120,
              59,142,217,59,224,230,37,149,26,147,152,18,136,48,13,6,9,42,
              134,72,134,247,13,1,1,11,5,0,3,130,1,1,0,75,201,225,9,128,150,
              65,117,74,201,239,161,49,117,149,221,111,121,254,46,56,20,44,
              215,53,72,21,97,155,106,187,190,52,189,22,217,122,255,61,75,17,
              115,57,144,77,227,111,247,253,175,36,226,26,104,126,16,85,98,0,
              83,54,183,38,69,236,89,157,164,52,166,213,142,23,103,2,116,161,
              20,86,33,226,146,49,227,200,165,194,227,135,194,150,0,12,241,
              208,44,8,56,108,176,101,158,239,144,179,231,21,58,226,99,160,
              31,227,96,122,178,108,38,165,81,36,60,232,52,211,27,63,188,131,
              246,50,242,200,250,91,139,169,137,255,138,88,27,7,158,108,179,
              241,87,134,231,130,126,229,203,90,0,116,187,60,128,244,113,169,
              146,19,38,91,169,55,122,218,31,17,122,237,2,4,182,252,217,173,
              164,24,141,61,235,200,42,49,168,94,140,32,130,90,168,71,189,
              146,149,28,198,91,62,48,122,221,237,203,241,28,223,158,236,176,
              99,91,183,59,184,31,73,192,215,55,210,146,64,72,11,232,58,130,
              57,242,47,185,118,10,199,112,44,101,57,194,120,18,234,209,13,
              216,190,53,243,209,52>>]},
  {dh,<<48,130,1,8,2,130,1,1,0,152,202,99,248,92,201,35,238,246,5,77,93,120,
        10,118,129,36,52,111,193,167,220,49,229,106,105,152,133,121,157,73,
        158,232,153,197,197,21,171,140,30,207,52,165,45,8,221,162,21,199,183,
        66,211,247,51,224,102,214,190,130,96,253,218,193,35,43,139,145,89,200,
        250,145,92,50,80,134,135,188,205,254,148,122,136,237,220,186,147,187,
        104,159,36,147,217,117,74,35,163,145,249,175,242,18,221,124,54,140,16,
        246,169,84,252,45,47,99,136,30,60,189,203,61,86,225,117,255,4,91,46,
        110,167,173,106,51,65,10,248,94,225,223,73,40,232,140,26,11,67,170,
        118,190,67,31,127,233,39,68,88,132,171,224,62,187,207,160,189,209,101,
        74,8,205,174,146,173,80,105,144,246,25,153,86,36,24,178,163,64,202,
        221,95,184,110,244,32,226,217,34,55,188,230,55,16,216,247,173,246,139,
        76,187,66,211,159,17,46,20,18,48,80,27,250,96,189,29,214,234,241,34,
        69,254,147,103,220,133,40,164,84,8,44,241,61,164,151,9,135,41,60,75,4,
        202,133,173,72,6,69,167,89,112,174,40,229,171,2,1,2>>},
  {ciphers,[{any,aes_256_gcm,aead,sha384},
            {any,aes_128_gcm,aead,sha256},
            {any,chacha20_poly1305,aead,sha256},
            {any,aes_128_ccm,aead,sha256},
            {any,aes_128_ccm_8,aead,sha256},
            {ecdhe_ecdsa,aes_256_gcm,aead,sha384},
            {ecdhe_rsa,aes_256_gcm,aead,sha384},
            {ecdhe_ecdsa,aes_256_ccm,aead},
            {ecdhe_ecdsa,aes_256_ccm_8,aead},
            {ecdhe_ecdsa,chacha20_poly1305,aead,sha256},
            {ecdhe_rsa,chacha20_poly1305,aead,sha256},
            {ecdhe_ecdsa,aes_128_gcm,aead,sha256},
            {ecdhe_rsa,aes_128_gcm,aead,sha256},
            {ecdhe_ecdsa,aes_128_ccm,aead},
            {ecdhe_ecdsa,aes_128_ccm_8,aead},
            {ecdh_ecdsa,aes_256_gcm,aead,sha384},
            {ecdh_rsa,aes_256_gcm,aead,sha384},
            {ecdh_ecdsa,aes_128_gcm,aead,sha256},
            {ecdh_rsa,aes_128_gcm,aead,sha256},
            {dhe_rsa,aes_256_gcm,aead,sha384},
            {dhe_dss,aes_256_gcm,aead,sha384},
            {dhe_rsa,aes_128_gcm,aead,sha256},
            {dhe_dss,aes_128_gcm,aead,sha256},
            {dhe_rsa,chacha20_poly1305,aead,sha256},
            {rsa_psk,aes_256_gcm,aead,sha384},
            {rsa_psk,aes_128_gcm,aead,sha256},
            {rsa,aes_256_gcm,aead,sha384},
            {rsa,aes_128_gcm,aead,sha256}]},
  {honor_cipher_order,true},
  {secure_renegotiate,true},
  {client_renegotiation,false},
  {password,"********"}],
 {ssl,true},
 {port,18091}]
[error_logger:info,2022-09-07T14:28:13.047Z,ns_1@cb.local:<0.302.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.302.0>,menelaus_web}
    started: [{pid,<0.316.0>},
              {id,menelaus_web_ipv4},
              {mfargs,
                  {menelaus_web,http_server,
                      [[{afamily,inet},
                        {ssl,true},
                        {name,menelaus_web_ssl},
                        {ssl_opts_fun,#Fun<ns_ssl_services_setup.6.133565730>},
                        {port,18091},
                        {nodelay,true},
                        {approot,
                            "/opt/couchbase/lib/ns_server/erlang/lib/ns_server/priv/public"}]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[ns_server:info,2022-09-07T14:28:13.050Z,ns_1@cb.local:<0.302.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:129]Loaded pluggable UI specification for backup from "/opt/couchbase/etc/couchbase/pluggable-ui-backup.json"
[ns_server:info,2022-09-07T14:28:13.050Z,ns_1@cb.local:<0.302.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:129]Loaded pluggable UI specification for cbas from "/opt/couchbase/etc/couchbase/pluggable-ui-cbas.json"
[ns_server:info,2022-09-07T14:28:13.052Z,ns_1@cb.local:<0.302.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:129]Loaded pluggable UI specification for eventing from "/opt/couchbase/etc/couchbase/pluggable-ui-eventing.json"
[ns_server:info,2022-09-07T14:28:13.053Z,ns_1@cb.local:<0.302.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:129]Loaded pluggable UI specification for fts from "/opt/couchbase/etc/couchbase/pluggable-ui-fts.json"
[ns_server:info,2022-09-07T14:28:13.053Z,ns_1@cb.local:<0.302.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:129]Loaded pluggable UI specification for n1ql from "/opt/couchbase/etc/couchbase/pluggable-ui-query.json"
[ns_server:info,2022-09-07T14:28:13.056Z,ns_1@cb.local:<0.302.0>:menelaus_web:maybe_start_http_server:126]Started web service with options:
[{ip,"::"},
 [{keyfile,"/opt/couchbase/var/lib/couchbase/config/certs/pkey.pem"},
  {certfile,"/opt/couchbase/var/lib/couchbase/config/certs/chain.pem"},
  {versions,['tlsv1.2','tlsv1.3']},
  {cacerts,[<<48,130,3,33,48,130,2,9,160,3,2,1,2,2,8,23,18,154,82,164,1,21,
              60,48,13,6,9,42,134,72,134,247,13,1,1,11,5,0,48,36,49,34,48,32,
              6,3,85,4,3,19,25,67,111,117,99,104,98,97,115,101,32,83,101,114,
              118,101,114,32,57,52,100,54,52,100,52,53,48,30,23,13,49,51,48,
              49,48,49,48,48,48,48,48,48,90,23,13,52,57,49,50,51,49,50,51,53,
              57,53,57,90,48,36,49,34,48,32,6,3,85,4,3,19,25,67,111,117,99,
              104,98,97,115,101,32,83,101,114,118,101,114,32,57,52,100,54,52,
              100,52,53,48,130,1,34,48,13,6,9,42,134,72,134,247,13,1,1,1,5,0,
              3,130,1,15,0,48,130,1,10,2,130,1,1,0,194,175,58,98,213,11,69,
              224,252,171,108,141,152,64,29,26,186,3,145,87,68,104,215,198,
              17,190,103,200,152,225,103,33,154,77,2,101,24,101,21,58,240,77,
              253,45,146,251,247,129,14,85,219,180,228,208,116,181,115,78,
              125,90,198,7,168,16,55,7,150,237,66,245,70,32,139,220,232,104,
              103,245,65,177,31,106,72,110,120,151,250,197,73,215,91,175,13,
              220,52,194,168,103,38,26,60,90,13,132,154,32,60,227,248,208,
              130,20,102,21,35,42,0,59,200,160,63,47,64,42,42,126,117,233,54,
              223,184,15,104,95,253,242,228,249,181,15,39,32,210,132,102,13,
              23,167,42,95,220,164,228,82,191,6,139,169,190,0,223,155,219,
              188,13,245,152,27,48,187,120,139,42,26,103,232,190,238,187,241,
              79,160,250,145,59,20,219,5,142,169,29,251,185,220,133,142,155,
              49,155,191,115,197,132,112,209,160,210,144,35,126,25,150,32,
              139,244,211,244,232,126,190,241,155,155,47,160,187,39,229,223,
              27,236,217,144,127,217,93,214,184,72,80,152,220,99,162,102,147,
              233,116,67,156,133,32,112,198,46,105,2,3,1,0,1,163,87,48,85,48,
              14,6,3,85,29,15,1,1,255,4,4,3,2,2,164,48,19,6,3,85,29,37,4,12,
              48,10,6,8,43,6,1,5,5,7,3,1,48,15,6,3,85,29,19,1,1,255,4,5,48,3,
              1,1,255,48,29,6,3,85,29,14,4,22,4,20,26,62,215,165,145,243,120,
              59,142,217,59,224,230,37,149,26,147,152,18,136,48,13,6,9,42,
              134,72,134,247,13,1,1,11,5,0,3,130,1,1,0,75,201,225,9,128,150,
              65,117,74,201,239,161,49,117,149,221,111,121,254,46,56,20,44,
              215,53,72,21,97,155,106,187,190,52,189,22,217,122,255,61,75,17,
              115,57,144,77,227,111,247,253,175,36,226,26,104,126,16,85,98,0,
              83,54,183,38,69,236,89,157,164,52,166,213,142,23,103,2,116,161,
              20,86,33,226,146,49,227,200,165,194,227,135,194,150,0,12,241,
              208,44,8,56,108,176,101,158,239,144,179,231,21,58,226,99,160,
              31,227,96,122,178,108,38,165,81,36,60,232,52,211,27,63,188,131,
              246,50,242,200,250,91,139,169,137,255,138,88,27,7,158,108,179,
              241,87,134,231,130,126,229,203,90,0,116,187,60,128,244,113,169,
              146,19,38,91,169,55,122,218,31,17,122,237,2,4,182,252,217,173,
              164,24,141,61,235,200,42,49,168,94,140,32,130,90,168,71,189,
              146,149,28,198,91,62,48,122,221,237,203,241,28,223,158,236,176,
              99,91,183,59,184,31,73,192,215,55,210,146,64,72,11,232,58,130,
              57,242,47,185,118,10,199,112,44,101,57,194,120,18,234,209,13,
              216,190,53,243,209,52>>]},
  {dh,<<48,130,1,8,2,130,1,1,0,152,202,99,248,92,201,35,238,246,5,77,93,120,
        10,118,129,36,52,111,193,167,220,49,229,106,105,152,133,121,157,73,
        158,232,153,197,197,21,171,140,30,207,52,165,45,8,221,162,21,199,183,
        66,211,247,51,224,102,214,190,130,96,253,218,193,35,43,139,145,89,200,
        250,145,92,50,80,134,135,188,205,254,148,122,136,237,220,186,147,187,
        104,159,36,147,217,117,74,35,163,145,249,175,242,18,221,124,54,140,16,
        246,169,84,252,45,47,99,136,30,60,189,203,61,86,225,117,255,4,91,46,
        110,167,173,106,51,65,10,248,94,225,223,73,40,232,140,26,11,67,170,
        118,190,67,31,127,233,39,68,88,132,171,224,62,187,207,160,189,209,101,
        74,8,205,174,146,173,80,105,144,246,25,153,86,36,24,178,163,64,202,
        221,95,184,110,244,32,226,217,34,55,188,230,55,16,216,247,173,246,139,
        76,187,66,211,159,17,46,20,18,48,80,27,250,96,189,29,214,234,241,34,
        69,254,147,103,220,133,40,164,84,8,44,241,61,164,151,9,135,41,60,75,4,
        202,133,173,72,6,69,167,89,112,174,40,229,171,2,1,2>>},
  {ciphers,[{any,aes_256_gcm,aead,sha384},
            {any,aes_128_gcm,aead,sha256},
            {any,chacha20_poly1305,aead,sha256},
            {any,aes_128_ccm,aead,sha256},
            {any,aes_128_ccm_8,aead,sha256},
            {ecdhe_ecdsa,aes_256_gcm,aead,sha384},
            {ecdhe_rsa,aes_256_gcm,aead,sha384},
            {ecdhe_ecdsa,aes_256_ccm,aead},
            {ecdhe_ecdsa,aes_256_ccm_8,aead},
            {ecdhe_ecdsa,chacha20_poly1305,aead,sha256},
            {ecdhe_rsa,chacha20_poly1305,aead,sha256},
            {ecdhe_ecdsa,aes_128_gcm,aead,sha256},
            {ecdhe_rsa,aes_128_gcm,aead,sha256},
            {ecdhe_ecdsa,aes_128_ccm,aead},
            {ecdhe_ecdsa,aes_128_ccm_8,aead},
            {ecdh_ecdsa,aes_256_gcm,aead,sha384},
            {ecdh_rsa,aes_256_gcm,aead,sha384},
            {ecdh_ecdsa,aes_128_gcm,aead,sha256},
            {ecdh_rsa,aes_128_gcm,aead,sha256},
            {dhe_rsa,aes_256_gcm,aead,sha384},
            {dhe_dss,aes_256_gcm,aead,sha384},
            {dhe_rsa,aes_128_gcm,aead,sha256},
            {dhe_dss,aes_128_gcm,aead,sha256},
            {dhe_rsa,chacha20_poly1305,aead,sha256},
            {rsa_psk,aes_256_gcm,aead,sha384},
            {rsa_psk,aes_128_gcm,aead,sha256},
            {rsa,aes_256_gcm,aead,sha384},
            {rsa,aes_128_gcm,aead,sha256}]},
  {honor_cipher_order,true},
  {secure_renegotiate,true},
  {client_renegotiation,false},
  {password,"********"}],
 {ssl,true},
 {port,18091}]
[error_logger:info,2022-09-07T14:28:13.057Z,ns_1@cb.local:<0.302.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.302.0>,menelaus_web}
    started: [{pid,<0.335.0>},
              {id,menelaus_web_ipv6},
              {mfargs,
                  {menelaus_web,http_server,
                      [[{afamily,inet6},
                        {ssl,true},
                        {name,menelaus_web_ssl},
                        {ssl_opts_fun,#Fun<ns_ssl_services_setup.6.133565730>},
                        {port,18091},
                        {nodelay,true},
                        {approot,
                            "/opt/couchbase/lib/ns_server/erlang/lib/ns_server/priv/public"}]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[ns_server:debug,2022-09-07T14:28:13.057Z,ns_1@cb.local:<0.300.0>:restartable:start_child:92]Started child process <0.302.0>
  MFA: {ns_ssl_services_setup,start_link_rest_service,[]}
[error_logger:info,2022-09-07T14:28:13.058Z,ns_1@cb.local:ns_ssl_services_sup<0.285.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_ssl_services_sup}
    started: [{pid,<0.300.0>},
              {id,ns_rest_ssl_service},
              {mfargs,
                  {restartable,start_link,
                      [{ns_ssl_services_setup,start_link_rest_service,[]},
                       1000]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,worker}]

[error_logger:info,2022-09-07T14:28:13.058Z,ns_1@cb.local:ns_server_nodes_sup<0.273.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.285.0>},
              {name,ns_ssl_services_sup},
              {mfargs,{ns_ssl_services_sup,start_link,[]}},
              {restart_type,permanent},
              {shutdown,infinity},
              {child_type,supervisor}]
[error_logger:info,2022-09-07T14:28:13.067Z,ns_1@cb.local:ns_server_nodes_sup<0.273.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.354.0>},
              {name,ldap_auth_cache},
              {mfargs,{ldap_auth_cache,start_link,[]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[error_logger:info,2022-09-07T14:28:13.069Z,ns_1@cb.local:users_sup<0.356.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,users_sup}
    started: [{pid,<0.357.0>},
              {id,user_storage_events},
              {mfargs,{gen_event,start_link,[{local,user_storage_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2022-09-07T14:28:13.076Z,ns_1@cb.local:users_storage_sup<0.358.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,users_storage_sup}
    started: [{pid,<0.359.0>},
              {id,users_replicator},
              {mfargs,{menelaus_users,start_replicator,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2022-09-07T14:28:13.078Z,ns_1@cb.local:users_replicator<0.359.0>:replicated_storage:wait_for_startup:46]Start waiting for startup
[ns_server:debug,2022-09-07T14:28:13.082Z,ns_1@cb.local:users_storage<0.360.0>:replicated_storage:announce_startup:60]Announce my startup to <0.359.0>
[ns_server:debug,2022-09-07T14:28:13.082Z,ns_1@cb.local:users_storage<0.360.0>:replicated_dets:open:148]Opening file "/opt/couchbase/var/lib/couchbase/config/users.dets"
[ns_server:debug,2022-09-07T14:28:13.082Z,ns_1@cb.local:users_replicator<0.359.0>:replicated_storage:wait_for_startup:49]Received replicated storage registration from <0.360.0>
[error_logger:info,2022-09-07T14:28:13.082Z,ns_1@cb.local:users_storage_sup<0.358.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,users_storage_sup}
    started: [{pid,<0.360.0>},
              {id,users_storage},
              {mfargs,{menelaus_users,start_storage,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2022-09-07T14:28:13.082Z,ns_1@cb.local:users_sup<0.356.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,users_sup}
    started: [{pid,<0.358.0>},
              {id,users_storage_sup},
              {mfargs,{users_storage_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2022-09-07T14:28:13.090Z,ns_1@cb.local:compiled_roles_cache<0.362.0>:versioned_cache:init:41]Starting versioned cache compiled_roles_cache
[error_logger:info,2022-09-07T14:28:13.090Z,ns_1@cb.local:users_sup<0.356.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,users_sup}
    started: [{pid,<0.362.0>},
              {id,compiled_roles_cache},
              {mfargs,{menelaus_roles,start_compiled_roles_cache,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2022-09-07T14:28:13.096Z,ns_1@cb.local:users_sup<0.356.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,users_sup}
    started: [{pid,<0.365.0>},
              {id,roles_cache},
              {mfargs,{roles_cache,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2022-09-07T14:28:13.096Z,ns_1@cb.local:ns_server_nodes_sup<0.273.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.356.0>},
              {name,users_sup},
              {mfargs,{users_sup,start_link,[]}},
              {restart_type,permanent},
              {shutdown,infinity},
              {child_type,supervisor}]
[error_logger:info,2022-09-07T14:28:13.097Z,ns_1@cb.local:kernel_safe_sup<0.67.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,kernel_safe_sup}
    started: [{pid,<0.369.0>},
              {id,dets_sup},
              {mfargs,{dets_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,supervisor}]

[error_logger:info,2022-09-07T14:28:13.097Z,ns_1@cb.local:kernel_safe_sup<0.67.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,kernel_safe_sup}
    started: [{pid,<0.370.0>},
              {id,dets},
              {mfargs,{dets_server,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,2000},
              {child_type,worker}]

[error_logger:info,2022-09-07T14:28:13.128Z,ns_1@cb.local:ns_server_nodes_sup<0.273.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.373.0>},
              {name,start_couchdb_node},
              {mfargs,{ns_server_nodes_sup,start_couchdb_node,[]}},
              {restart_type,{permanent,5}},
              {shutdown,86400000},
              {child_type,worker}]
[ns_server:debug,2022-09-07T14:28:13.128Z,ns_1@cb.local:wait_link_to_couchdb_node<0.374.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:160]Waiting for ns_couchdb node to start
[error_logger:info,2022-09-07T14:28:13.128Z,ns_1@cb.local:net_kernel<0.212.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {2,#Ref<0.2209594722.1843789828.36730>}}}
[ns_server:debug,2022-09-07T14:28:13.128Z,ns_1@cb.local:net_kernel<0.212.0>:cb_dist:info_msg:872]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2022-09-07T14:28:13.128Z,ns_1@cb.local:cb_dist<0.209.0>:cb_dist:info_msg:872]cb_dist: Added connection {con,#Ref<0.2209594722.1843658756.36833>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2022-09-07T14:28:13.128Z,ns_1@cb.local:cb_dist<0.209.0>:cb_dist:info_msg:872]cb_dist: Updated connection: {con,#Ref<0.2209594722.1843658756.36833>,
                                  inet_tcp_dist,<0.376.0>,
                                  #Ref<0.2209594722.1843658756.36836>}
[ns_server:debug,2022-09-07T14:28:13.129Z,ns_1@cb.local:cb_dist<0.209.0>:cb_dist:info_msg:872]cb_dist: Connection down: {con,#Ref<0.2209594722.1843658756.36833>,
                               inet_tcp_dist,<0.376.0>,
                               #Ref<0.2209594722.1843658756.36836>}
[error_logger:info,2022-09-07T14:28:13.129Z,ns_1@cb.local:net_kernel<0.212.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.376.0>,shutdown}}
[ns_server:debug,2022-09-07T14:28:13.129Z,ns_1@cb.local:<0.375.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:174]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2022-09-07T14:28:13.129Z,ns_1@cb.local:net_kernel<0.212.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1157,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2022-09-07T14:28:13.137Z,ns_1@cb.local:users_storage<0.360.0>:replicated_dets:select_from_table:281][dets] Starting select with {users_storage,
                                [{{docv2,'_','_','_'},[],['$_']}],
                                100}
[ns_server:debug,2022-09-07T14:28:13.137Z,ns_1@cb.local:users_storage<0.360.0>:replicated_dets:init_after_ack:141]Loading 0 items, 305 words took 55ms
[ns_server:debug,2022-09-07T14:28:13.137Z,ns_1@cb.local:users_storage<0.360.0>:replicated_dets:select_from_table:281][ets] Starting select with {users_storage,
                               [{{docv2,{user,{'_',local}},'_','_'},
                                 [],
                                 ['$_']}],
                               100}
[ns_server:debug,2022-09-07T14:28:13.137Z,ns_1@cb.local:roles_cache<0.365.0>:active_cache:renew:238]Starting roles_cache cache renewal
[ns_server:debug,2022-09-07T14:28:13.137Z,ns_1@cb.local:roles_cache<0.365.0>:active_cache:renew:244]roles_cache cache renewal process originated 0 queries
[ns_server:debug,2022-09-07T14:28:13.152Z,ns_1@cb.local:compiled_roles_cache<0.362.0>:versioned_cache:handle_info:89]Flushing cache compiled_roles_cache due to version change from undefined to {undefined,
                                                                             {0,
                                                                              1415866166},
                                                                             {0,
                                                                              1415866166},
                                                                             false,
                                                                             []}
[error_logger:info,2022-09-07T14:28:13.330Z,ns_1@cb.local:net_kernel<0.212.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {3,#Ref<0.2209594722.1843789828.36730>}}}
[ns_server:debug,2022-09-07T14:28:13.330Z,ns_1@cb.local:net_kernel<0.212.0>:cb_dist:info_msg:872]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2022-09-07T14:28:13.330Z,ns_1@cb.local:cb_dist<0.209.0>:cb_dist:info_msg:872]cb_dist: Added connection {con,#Ref<0.2209594722.1843658755.36518>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2022-09-07T14:28:13.330Z,ns_1@cb.local:cb_dist<0.209.0>:cb_dist:info_msg:872]cb_dist: Updated connection: {con,#Ref<0.2209594722.1843658755.36518>,
                                  inet_tcp_dist,<0.378.0>,
                                  #Ref<0.2209594722.1843658755.36521>}
[ns_server:debug,2022-09-07T14:28:13.331Z,ns_1@cb.local:cb_dist<0.209.0>:cb_dist:info_msg:872]cb_dist: Connection down: {con,#Ref<0.2209594722.1843658755.36518>,
                               inet_tcp_dist,<0.378.0>,
                               #Ref<0.2209594722.1843658755.36521>}
[error_logger:info,2022-09-07T14:28:13.331Z,ns_1@cb.local:net_kernel<0.212.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.378.0>,shutdown}}
[error_logger:info,2022-09-07T14:28:13.354Z,ns_1@cb.local:net_kernel<0.212.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1157,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2022-09-07T14:28:13.354Z,ns_1@cb.local:<0.375.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:174]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2022-09-07T14:28:13.555Z,ns_1@cb.local:net_kernel<0.212.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {4,#Ref<0.2209594722.1843789828.36730>}}}
[ns_server:debug,2022-09-07T14:28:13.555Z,ns_1@cb.local:net_kernel<0.212.0>:cb_dist:info_msg:872]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2022-09-07T14:28:13.555Z,ns_1@cb.local:cb_dist<0.209.0>:cb_dist:info_msg:872]cb_dist: Added connection {con,#Ref<0.2209594722.1843658756.36872>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2022-09-07T14:28:13.555Z,ns_1@cb.local:cb_dist<0.209.0>:cb_dist:info_msg:872]cb_dist: Updated connection: {con,#Ref<0.2209594722.1843658756.36872>,
                                  inet_tcp_dist,<0.380.0>,
                                  #Ref<0.2209594722.1843658755.36525>}
[ns_server:debug,2022-09-07T14:28:13.556Z,ns_1@cb.local:cb_dist<0.209.0>:cb_dist:info_msg:872]cb_dist: Connection down: {con,#Ref<0.2209594722.1843658756.36872>,
                               inet_tcp_dist,<0.380.0>,
                               #Ref<0.2209594722.1843658755.36525>}
[error_logger:info,2022-09-07T14:28:13.556Z,ns_1@cb.local:net_kernel<0.212.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.380.0>,shutdown}}
[error_logger:info,2022-09-07T14:28:13.556Z,ns_1@cb.local:net_kernel<0.212.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1157,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2022-09-07T14:28:13.556Z,ns_1@cb.local:<0.375.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:174]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2022-09-07T14:28:13.757Z,ns_1@cb.local:net_kernel<0.212.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {5,#Ref<0.2209594722.1843789828.36730>}}}
[ns_server:debug,2022-09-07T14:28:13.757Z,ns_1@cb.local:net_kernel<0.212.0>:cb_dist:info_msg:872]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2022-09-07T14:28:13.757Z,ns_1@cb.local:cb_dist<0.209.0>:cb_dist:info_msg:872]cb_dist: Added connection {con,#Ref<0.2209594722.1843658755.36531>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2022-09-07T14:28:13.757Z,ns_1@cb.local:cb_dist<0.209.0>:cb_dist:info_msg:872]cb_dist: Updated connection: {con,#Ref<0.2209594722.1843658755.36531>,
                                  inet_tcp_dist,<0.382.0>,
                                  #Ref<0.2209594722.1843658755.36534>}
[error_logger:info,2022-09-07T14:28:13.757Z,ns_1@cb.local:net_kernel<0.212.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.382.0>,shutdown}}
[ns_server:debug,2022-09-07T14:28:13.757Z,ns_1@cb.local:cb_dist<0.209.0>:cb_dist:info_msg:872]cb_dist: Connection down: {con,#Ref<0.2209594722.1843658755.36531>,
                               inet_tcp_dist,<0.382.0>,
                               #Ref<0.2209594722.1843658755.36534>}
[error_logger:info,2022-09-07T14:28:13.757Z,ns_1@cb.local:net_kernel<0.212.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1157,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2022-09-07T14:28:13.757Z,ns_1@cb.local:<0.375.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:174]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2022-09-07T14:28:13.959Z,ns_1@cb.local:net_kernel<0.212.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {6,#Ref<0.2209594722.1843789828.36730>}}}
[ns_server:debug,2022-09-07T14:28:13.959Z,ns_1@cb.local:net_kernel<0.212.0>:cb_dist:info_msg:872]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2022-09-07T14:28:13.959Z,ns_1@cb.local:cb_dist<0.209.0>:cb_dist:info_msg:872]cb_dist: Added connection {con,#Ref<0.2209594722.1843658755.36537>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2022-09-07T14:28:13.959Z,ns_1@cb.local:cb_dist<0.209.0>:cb_dist:info_msg:872]cb_dist: Updated connection: {con,#Ref<0.2209594722.1843658755.36537>,
                                  inet_tcp_dist,<0.384.0>,
                                  #Ref<0.2209594722.1843658755.36540>}
[ns_server:debug,2022-09-07T14:28:13.991Z,ns_1@cb.local:cb_dist<0.209.0>:cb_dist:info_msg:872]cb_dist: Connection down: {con,#Ref<0.2209594722.1843658755.36537>,
                               inet_tcp_dist,<0.384.0>,
                               #Ref<0.2209594722.1843658755.36540>}
[error_logger:info,2022-09-07T14:28:13.991Z,ns_1@cb.local:net_kernel<0.212.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.384.0>,{recv_challenge_ack_failed,{error,closed}}}}
[error_logger:info,2022-09-07T14:28:13.991Z,ns_1@cb.local:net_kernel<0.212.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1157,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2022-09-07T14:28:13.991Z,ns_1@cb.local:<0.375.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:174]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2022-09-07T14:28:14.192Z,ns_1@cb.local:net_kernel<0.212.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {7,#Ref<0.2209594722.1843789828.36730>}}}
[ns_server:debug,2022-09-07T14:28:14.192Z,ns_1@cb.local:net_kernel<0.212.0>:cb_dist:info_msg:872]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2022-09-07T14:28:14.192Z,ns_1@cb.local:cb_dist<0.209.0>:cb_dist:info_msg:872]cb_dist: Added connection {con,#Ref<0.2209594722.1843658754.36801>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2022-09-07T14:28:14.192Z,ns_1@cb.local:cb_dist<0.209.0>:cb_dist:info_msg:872]cb_dist: Updated connection: {con,#Ref<0.2209594722.1843658754.36801>,
                                  inet_tcp_dist,<0.386.0>,
                                  #Ref<0.2209594722.1843658755.36548>}
[ns_server:info,2022-09-07T14:28:14.195Z,ns_1@cb.local:ns_couchdb_port<0.373.0>:ns_port_server:log:226]ns_couchdb<0.373.0>: =ERROR REPORT==== 7-Sep-2022::14:28:13.990973 ===
ns_couchdb<0.373.0>: ** Connection attempt from node 'ns_1@cb.local' rejected. Invalid challenge reply. **
ns_couchdb<0.373.0>: 

[ns_server:debug,2022-09-07T14:28:14.203Z,ns_1@cb.local:cb_dist<0.209.0>:cb_dist:info_msg:872]cb_dist: Connection down: {con,#Ref<0.2209594722.1843658754.36801>,
                               inet_tcp_dist,<0.386.0>,
                               #Ref<0.2209594722.1843658755.36548>}
[error_logger:info,2022-09-07T14:28:14.203Z,ns_1@cb.local:net_kernel<0.212.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.386.0>,{recv_challenge_ack_failed,{error,closed}}}}
[ns_server:debug,2022-09-07T14:28:14.205Z,ns_1@cb.local:<0.375.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:174]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2022-09-07T14:28:14.204Z,ns_1@cb.local:net_kernel<0.212.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1157,nodedown,'couchdb_ns_1@cb.local'}}
[error_logger:info,2022-09-07T14:28:14.406Z,ns_1@cb.local:net_kernel<0.212.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {8,#Ref<0.2209594722.1843789828.36730>}}}
[ns_server:debug,2022-09-07T14:28:14.406Z,ns_1@cb.local:net_kernel<0.212.0>:cb_dist:info_msg:872]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2022-09-07T14:28:14.406Z,ns_1@cb.local:cb_dist<0.209.0>:cb_dist:info_msg:872]cb_dist: Added connection {con,#Ref<0.2209594722.1843658755.36558>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2022-09-07T14:28:14.406Z,ns_1@cb.local:cb_dist<0.209.0>:cb_dist:info_msg:872]cb_dist: Updated connection: {con,#Ref<0.2209594722.1843658755.36558>,
                                  inet_tcp_dist,<0.388.0>,
                                  #Ref<0.2209594722.1843658754.36810>}
[ns_server:debug,2022-09-07T14:28:14.413Z,ns_1@cb.local:cb_dist<0.209.0>:cb_dist:info_msg:872]cb_dist: Connection down: {con,#Ref<0.2209594722.1843658755.36558>,
                               inet_tcp_dist,<0.388.0>,
                               #Ref<0.2209594722.1843658754.36810>}
[error_logger:info,2022-09-07T14:28:14.413Z,ns_1@cb.local:net_kernel<0.212.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.388.0>,{recv_challenge_ack_failed,{error,closed}}}}
[error_logger:info,2022-09-07T14:28:14.413Z,ns_1@cb.local:net_kernel<0.212.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1157,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2022-09-07T14:28:14.413Z,ns_1@cb.local:<0.375.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:174]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2022-09-07T14:28:14.614Z,ns_1@cb.local:net_kernel<0.212.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {9,#Ref<0.2209594722.1843789828.36730>}}}
[ns_server:debug,2022-09-07T14:28:14.614Z,ns_1@cb.local:net_kernel<0.212.0>:cb_dist:info_msg:872]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2022-09-07T14:28:14.614Z,ns_1@cb.local:cb_dist<0.209.0>:cb_dist:info_msg:872]cb_dist: Added connection {con,#Ref<0.2209594722.1843658756.36897>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2022-09-07T14:28:14.614Z,ns_1@cb.local:cb_dist<0.209.0>:cb_dist:info_msg:872]cb_dist: Updated connection: {con,#Ref<0.2209594722.1843658756.36897>,
                                  inet_tcp_dist,<0.390.0>,
                                  #Ref<0.2209594722.1843658756.36900>}
[ns_server:debug,2022-09-07T14:28:14.616Z,ns_1@cb.local:cb_dist<0.209.0>:cb_dist:info_msg:872]cb_dist: Connection down: {con,#Ref<0.2209594722.1843658756.36897>,
                               inet_tcp_dist,<0.390.0>,
                               #Ref<0.2209594722.1843658756.36900>}
[error_logger:info,2022-09-07T14:28:14.616Z,ns_1@cb.local:net_kernel<0.212.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.390.0>,{recv_challenge_ack_failed,{error,closed}}}}
[error_logger:info,2022-09-07T14:28:14.616Z,ns_1@cb.local:net_kernel<0.212.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1157,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2022-09-07T14:28:14.616Z,ns_1@cb.local:<0.375.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:174]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2022-09-07T14:28:14.817Z,ns_1@cb.local:net_kernel<0.212.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {10,#Ref<0.2209594722.1843789828.36730>}}}
[ns_server:debug,2022-09-07T14:28:14.817Z,ns_1@cb.local:net_kernel<0.212.0>:cb_dist:info_msg:872]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2022-09-07T14:28:14.817Z,ns_1@cb.local:cb_dist<0.209.0>:cb_dist:info_msg:872]cb_dist: Added connection {con,#Ref<0.2209594722.1843658756.36909>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2022-09-07T14:28:14.817Z,ns_1@cb.local:cb_dist<0.209.0>:cb_dist:info_msg:872]cb_dist: Updated connection: {con,#Ref<0.2209594722.1843658756.36909>,
                                  inet_tcp_dist,<0.392.0>,
                                  #Ref<0.2209594722.1843658755.36576>}
[ns_server:debug,2022-09-07T14:28:14.820Z,ns_1@cb.local:cb_dist<0.209.0>:cb_dist:info_msg:872]cb_dist: Connection down: {con,#Ref<0.2209594722.1843658756.36909>,
                               inet_tcp_dist,<0.392.0>,
                               #Ref<0.2209594722.1843658755.36576>}
[error_logger:info,2022-09-07T14:28:14.820Z,ns_1@cb.local:net_kernel<0.212.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.392.0>,{recv_challenge_ack_failed,{error,closed}}}}
[error_logger:info,2022-09-07T14:28:14.820Z,ns_1@cb.local:net_kernel<0.212.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1157,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2022-09-07T14:28:14.820Z,ns_1@cb.local:<0.375.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:174]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2022-09-07T14:28:15.021Z,ns_1@cb.local:net_kernel<0.212.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {11,#Ref<0.2209594722.1843789828.36730>}}}
[ns_server:debug,2022-09-07T14:28:15.021Z,ns_1@cb.local:net_kernel<0.212.0>:cb_dist:info_msg:872]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2022-09-07T14:28:15.021Z,ns_1@cb.local:cb_dist<0.209.0>:cb_dist:info_msg:872]cb_dist: Added connection {con,#Ref<0.2209594722.1843658755.36581>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2022-09-07T14:28:15.021Z,ns_1@cb.local:cb_dist<0.209.0>:cb_dist:info_msg:872]cb_dist: Updated connection: {con,#Ref<0.2209594722.1843658755.36581>,
                                  inet_tcp_dist,<0.394.0>,
                                  #Ref<0.2209594722.1843658756.36922>}
[ns_server:debug,2022-09-07T14:28:15.029Z,ns_1@cb.local:<0.375.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:174]ns_couchdb is not ready: false
[ns_server:info,2022-09-07T14:28:15.618Z,ns_1@cb.local:ns_couchdb_port<0.373.0>:ns_port_server:log:226]ns_couchdb<0.373.0>: Apache CouchDB v4.5.1-290-g27d1470 (LogLevel=info) is starting.

[error_logger:info,2022-09-07T14:28:16.567Z,ns_1@cb.local:ns_server_nodes_sup<0.273.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.374.0>},
              {name,wait_for_couchdb_node},
              {mfargs,{erlang,apply,
                              [#Fun<ns_server_nodes_sup.0.18460070>,[]]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[ns_server:debug,2022-09-07T14:28:16.573Z,ns_1@cb.local:ns_server_nodes_sup<0.273.0>:ns_storage_conf:setup_db_and_ix_paths:59]Initialize db_and_ix_paths variable with [{db_path,
                                           "/opt/couchbase/var/lib/couchbase/data"},
                                          {index_path,
                                           "/opt/couchbase/var/lib/couchbase/data"}]
[ns_server:debug,2022-09-07T14:28:16.574Z,ns_1@cb.local:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{local_changes_count,<<"1678dfae96c38e07dff43c49b9f6967b">>} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{4,63829780096}}]}]
[ns_server:debug,2022-09-07T14:28:16.574Z,ns_1@cb.local:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@cb.local',cbas_dirs} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780096}}]},
 "/opt/couchbase/var/lib/couchbase/data"]
[ns_server:debug,2022-09-07T14:28:16.574Z,ns_1@cb.local:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{local_changes_count,<<"1678dfae96c38e07dff43c49b9f6967b">>} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{5,63829780096}}]}]
[ns_server:debug,2022-09-07T14:28:16.574Z,ns_1@cb.local:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@cb.local',eventing_dir} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780096}}]},
 47,111,112,116,47,99,111,117,99,104,98,97,115,101,47,118,97,114,47,108,105,
 98,47,99,111,117,99,104,98,97,115,101,47,100,97,116,97]
[ns_server:info,2022-09-07T14:28:16.579Z,ns_1@cb.local:ns_couchdb_port<0.373.0>:ns_port_server:log:226]ns_couchdb<0.373.0>: Apache CouchDB has started. Time to relax.
ns_couchdb<0.373.0>: 247: Booted. Waiting for shutdown request
ns_couchdb<0.373.0>: working as port

[error_logger:info,2022-09-07T14:28:16.585Z,ns_1@cb.local:ns_server_sup<0.399.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.400.0>},
              {name,ns_disksup},
              {mfargs,{ns_disksup,start_link,[]}},
              {restart_type,{permanent,4}},
              {shutdown,1000},
              {child_type,worker}]
[error_logger:info,2022-09-07T14:28:16.588Z,ns_1@cb.local:ns_server_sup<0.399.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.401.0>},
              {name,diag_handler_worker},
              {mfargs,{work_queue,start_link,[diag_handler_worker]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[ns_server:info,2022-09-07T14:28:16.590Z,ns_1@cb.local:ns_server_sup<0.399.0>:dir_size:start_link:33]Starting quick version of dir_size with program name: godu
[error_logger:info,2022-09-07T14:28:16.591Z,ns_1@cb.local:ns_server_sup<0.399.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.402.0>},
              {name,dir_size},
              {mfargs,{dir_size,start_link,[]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[error_logger:info,2022-09-07T14:28:16.594Z,ns_1@cb.local:ns_server_sup<0.399.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.403.0>},
              {name,request_tracker},
              {mfargs,{request_tracker,start_link,[]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[error_logger:info,2022-09-07T14:28:16.596Z,ns_1@cb.local:ns_server_sup<0.399.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.404.0>},
              {name,chronicle_kv_log},
              {mfargs,{chronicle_kv_log,start_link,[]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[ns_server:warn,2022-09-07T14:28:16.603Z,ns_1@cb.local:ns_log<0.406.0>:gossip_replicator:read_logs:244]Couldn't load logs from "/opt/couchbase/var/lib/couchbase/ns_log" (perhaps it's first
                         startup): {error,enoent}
[error_logger:info,2022-09-07T14:28:16.603Z,ns_1@cb.local:ns_server_sup<0.399.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.406.0>},
              {name,ns_log},
              {mfargs,{ns_log,start_link,[]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[error_logger:info,2022-09-07T14:28:16.604Z,ns_1@cb.local:ns_server_sup<0.399.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.408.0>},
              {name,event_log_events},
              {mfargs,{gen_event,start_link,[{local,event_log_events}]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[ns_server:warn,2022-09-07T14:28:16.608Z,ns_1@cb.local:event_log_server<0.409.0>:gossip_replicator:read_logs:244]Couldn't load logs from "/opt/couchbase/var/lib/couchbase/event_log" (perhaps it's first
                         startup): {error,enoent}
[error_logger:info,2022-09-07T14:28:16.608Z,ns_1@cb.local:ns_server_sup<0.399.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.409.0>},
              {name,event_log_server},
              {mfargs,{event_log_server,start_link,[]}},
              {restart_type,permanent},
              {shutdown,5000},
              {child_type,worker}]
[error_logger:info,2022-09-07T14:28:16.632Z,ns_1@cb.local:ns_server_sup<0.399.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.411.0>},
              {name,initargs_updater},
              {mfargs,{initargs_updater,start_link,[]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[error_logger:info,2022-09-07T14:28:16.634Z,ns_1@cb.local:ns_server_sup<0.399.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.413.0>},
              {name,timer_lag_recorder},
              {mfargs,{timer_lag_recorder,start_link,[]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[error_logger:info,2022-09-07T14:28:16.634Z,ns_1@cb.local:ns_server_sup<0.399.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.414.0>},
              {name,ns_babysitter_log_consumer},
              {mfargs,{ns_log,start_link_babysitter_log_consumer,[]}},
              {restart_type,{permanent,4}},
              {shutdown,1000},
              {child_type,worker}]
[ns_server:debug,2022-09-07T14:28:16.687Z,ns_1@cb.local:prometheus_cfg<0.415.0>:prometheus_cfg:ensure_prometheus_config:811]Updating prometheus config file: /opt/couchbase/var/lib/couchbase/config/prometheus.yml
[ns_server:debug,2022-09-07T14:28:16.699Z,ns_1@cb.local:prometheus_cfg<0.415.0>:prometheus_cfg:ensure_prometheus_config:811]Updating prometheus config file: /opt/couchbase/var/lib/couchbase/config/prometheus_rules.yml
[ns_server:debug,2022-09-07T14:28:16.773Z,ns_1@cb.local:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{local_changes_count,<<"1678dfae96c38e07dff43c49b9f6967b">>} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{6,63829780096}}]}]
[ns_server:debug,2022-09-07T14:28:16.773Z,ns_1@cb.local:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@cb.local',prometheus_auth_info} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780096}}]}|
 {"@prometheus",
  {auth,
   [{<<"plain">>,
     {sanitized,<<"M19CN2uT3IM7to60JvPLLFWe15QE+xqQj4ae4pILIQY=">>}},
    {<<"sha512">>,
     {[{<<"h">>,
        {sanitized,<<"ogcmvOcIDb3trVZUf8MQ924hUlHGPZQI7aFQbckkSIo=">>}},
       {<<"s">>,
        <<"XsQ6FZpbB9BqHVSzMNqrHDzDO0uulN8mxg267iifMDsUbKxifdxVg+Lc+pAhc3YVdmMhCop0SLTuJ+7LUuQAWw==">>},
       {<<"i">>,4000}]}},
    {<<"sha256">>,
     {[{<<"h">>,
        {sanitized,<<"Z1mj6nW/ZuHa02+s9poaXdfNdF/M/zUY/YJjIP5h0XQ=">>}},
       {<<"s">>,<<"ciebqiheHmvTVsOoEf7zCJiMybfXrUIx2RqHrbKMOF0=">>},
       {<<"i">>,4000}]}},
    {<<"sha1">>,
     {[{<<"h">>,
        {sanitized,<<"OrBaLmtLgYmmtpuHjerlFrjJQFp2/h97H+MvyWAZUtY=">>}},
       {<<"s">>,<<"0mqqcO1V739RDCcAsk84uGUFIGA=">>},
       {<<"i">>,4000}]}}]}}]
[ns_server:debug,2022-09-07T14:28:16.788Z,ns_1@cb.local:prometheus_cfg<0.415.0>:prometheus_cfg:apply_config:626]Restarting Prometheus as the start specs have changed
[error_logger:info,2022-09-07T14:28:16.801Z,ns_1@cb.local:ale_dynamic_sup<0.77.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ale_dynamic_sup}
    started: [{pid,<0.419.0>},
              {id,'sink-prometheus'},
              {mfargs,
                  {ale_dynamic_sup,delay_death,
                      [{ale_disk_sink,start_link,
                           ['sink-prometheus',
                            "/opt/couchbase/var/lib/couchbase/logs/prometheus.log",
                            [{rotation,
                                 [{compress,true},
                                  {size,41943040},
                                  {num_files,10},
                                  {buffer_size_max,52428800}]}]]},
                       1000]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2022-09-07T14:28:16.923Z,ns_1@cb.local:ns_server_sup<0.399.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.415.0>},
              {name,prometheus_cfg},
              {mfargs,{prometheus_cfg,start_link,[]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[ns_server:debug,2022-09-07T14:28:16.927Z,ns_1@cb.local:memcached_passwords<0.425.0>:memcached_cfg:init:58]Init config writer for memcached_passwords, "/opt/couchbase/var/lib/couchbase/isasl.pw"
[ns_server:debug,2022-09-07T14:28:16.930Z,ns_1@cb.local:memcached_passwords<0.425.0>:memcached_cfg:write_cfg:148]Writing config file for: "/opt/couchbase/var/lib/couchbase/isasl.pw"
[ns_server:debug,2022-09-07T14:28:16.977Z,ns_1@cb.local:memcached_passwords<0.425.0>:replicated_dets:select_from_table:281][ets] Starting select with {users_storage,
                               [{{docv2,{auth,{'_',local}},'_','_'},
                                 [],
                                 ['$_']}],
                               100}
[ns_server:debug,2022-09-07T14:28:16.978Z,ns_1@cb.local:memcached_refresh<0.283.0>:memcached_refresh:handle_call:49]File rename from "/opt/couchbase/var/lib/couchbase/isasl.pw.tmp" to "/opt/couchbase/var/lib/couchbase/isasl.pw" is requested
[ns_server:debug,2022-09-07T14:28:16.981Z,ns_1@cb.local:memcached_passwords<0.425.0>:memcached_cfg:rename_and_refresh:170]Successfully renamed "/opt/couchbase/var/lib/couchbase/isasl.pw.tmp" to "/opt/couchbase/var/lib/couchbase/isasl.pw"
[ns_server:debug,2022-09-07T14:28:16.982Z,ns_1@cb.local:memcached_refresh<0.283.0>:memcached_refresh:handle_cast:55]Refresh of isasl requested
[error_logger:info,2022-09-07T14:28:16.982Z,ns_1@cb.local:ns_server_sup<0.399.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.425.0>},
              {name,memcached_passwords},
              {mfargs,{memcached_passwords,start_link,[]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[ns_server:debug,2022-09-07T14:28:16.990Z,ns_1@cb.local:memcached_permissions<0.430.0>:memcached_cfg:init:58]Init config writer for memcached_permissions, "/opt/couchbase/var/lib/couchbase/config/memcached.rbac"
[error_logger:info,2022-09-07T14:28:16.990Z,ns_1@cb.local:inet_gethost_native_sup<0.428.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,inet_gethost_native_sup}
    started: [{pid,<0.429.0>},{mfa,{inet_gethost_native,init,[[]]}}]

[error_logger:info,2022-09-07T14:28:16.990Z,ns_1@cb.local:kernel_safe_sup<0.67.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,kernel_safe_sup}
    started: [{pid,<0.428.0>},
              {id,inet_gethost_native_sup},
              {mfargs,{inet_gethost_native,start_link,[]}},
              {restart_type,temporary},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2022-09-07T14:28:16.995Z,ns_1@cb.local:memcached_permissions<0.430.0>:memcached_cfg:write_cfg:148]Writing config file for: "/opt/couchbase/var/lib/couchbase/config/memcached.rbac"
[ns_server:debug,2022-09-07T14:28:16.997Z,ns_1@cb.local:memcached_permissions<0.430.0>:replicated_dets:select_from_table:281][ets] Starting select with {users_storage,
                               [{{docv2,{user,{'_',local}},'_','_'},
                                 [],
                                 ['$_']}],
                               100}
[ns_server:warn,2022-09-07T14:28:17.008Z,ns_1@cb.local:memcached_refresh<0.283.0>:ns_memcached:connect:1237]Unable to connect: {error,{badmatch,[{inet,{error,econnrefused}}]}}.
[ns_server:debug,2022-09-07T14:28:17.008Z,ns_1@cb.local:memcached_refresh<0.283.0>:memcached_refresh:handle_info:93]Refresh of [isasl] failed. Retry in 1000 ms.
[ns_server:debug,2022-09-07T14:28:17.008Z,ns_1@cb.local:memcached_refresh<0.283.0>:memcached_refresh:handle_call:49]File rename from "/opt/couchbase/var/lib/couchbase/config/memcached.rbac.tmp" to "/opt/couchbase/var/lib/couchbase/config/memcached.rbac" is requested
[ns_server:debug,2022-09-07T14:28:17.011Z,ns_1@cb.local:memcached_permissions<0.430.0>:memcached_cfg:rename_and_refresh:170]Successfully renamed "/opt/couchbase/var/lib/couchbase/config/memcached.rbac.tmp" to "/opt/couchbase/var/lib/couchbase/config/memcached.rbac"
[ns_server:debug,2022-09-07T14:28:17.012Z,ns_1@cb.local:memcached_refresh<0.283.0>:memcached_refresh:handle_cast:55]Refresh of rbac requested
[error_logger:info,2022-09-07T14:28:17.012Z,ns_1@cb.local:ns_server_sup<0.399.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.430.0>},
              {name,memcached_permissions},
              {mfargs,{memcached_permissions,start_link,[]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[ns_server:warn,2022-09-07T14:28:17.012Z,ns_1@cb.local:memcached_refresh<0.283.0>:ns_memcached:connect:1237]Unable to connect: {error,{badmatch,[{inet,{error,econnrefused}}]}}.
[ns_server:debug,2022-09-07T14:28:17.013Z,ns_1@cb.local:memcached_refresh<0.283.0>:memcached_refresh:handle_info:93]Refresh of [rbac,isasl] failed. Retry in 1000 ms.
[error_logger:info,2022-09-07T14:28:17.014Z,ns_1@cb.local:ns_server_sup<0.399.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.433.0>},
              {name,ns_email_alert},
              {mfargs,{ns_email_alert,start_link,[]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[error_logger:info,2022-09-07T14:28:17.016Z,ns_1@cb.local:ns_node_disco_sup<0.434.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_node_disco_sup}
    started: [{pid,<0.435.0>},
              {id,ns_node_disco_events},
              {mfargs,{gen_event,start_link,[{local,ns_node_disco_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2022-09-07T14:28:17.016Z,ns_1@cb.local:ns_node_disco<0.436.0>:ns_node_disco:init:112]Initting ns_node_disco with []
[ns_server:debug,2022-09-07T14:28:17.017Z,ns_1@cb.local:ns_cookie_manager<0.221.0>:ns_cookie_manager:do_cookie_sync:101]ns_cookie_manager do_cookie_sync
[error_logger:info,2022-09-07T14:28:17.017Z,ns_1@cb.local:ns_node_disco_sup<0.434.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_node_disco_sup}
    started: [{pid,<0.436.0>},
              {id,ns_node_disco},
              {mfargs,{ns_node_disco,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2022-09-07T14:28:17.017Z,ns_1@cb.local:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{local_changes_count,<<"1678dfae96c38e07dff43c49b9f6967b">>} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{7,63829780097}}]}]
[user:info,2022-09-07T14:28:17.017Z,ns_1@cb.local:ns_cookie_manager<0.221.0>:ns_cookie_manager:do_cookie_init:78]Initial otp cookie generated: {sanitized,
                                  <<"6jw2YfUfDxiQCs0fCwoUSmHbOdRC4E/A03fIDwJPR3g=">>}
[ns_server:debug,2022-09-07T14:28:17.017Z,ns_1@cb.local:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
otp ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780097}}]},
 {cookie,{sanitized,<<"6jw2YfUfDxiQCs0fCwoUSmHbOdRC4E/A03fIDwJPR3g=">>}}]
[ns_server:debug,2022-09-07T14:28:17.017Z,ns_1@cb.local:ns_cookie_manager<0.221.0>:ns_cookie_manager:do_cookie_sync:101]ns_cookie_manager do_cookie_sync
[ns_server:debug,2022-09-07T14:28:17.017Z,ns_1@cb.local:<0.438.0>:ns_node_disco:do_nodes_wanted_updated_fun:224]ns_node_disco: nodes_wanted updated: ['ns_1@cb.local'], with cookie: {sanitized,
                                                                      <<"6jw2YfUfDxiQCs0fCwoUSmHbOdRC4E/A03fIDwJPR3g=">>}
[ns_server:debug,2022-09-07T14:28:17.017Z,ns_1@cb.local:<0.440.0>:ns_node_disco:do_nodes_wanted_updated_fun:224]ns_node_disco: nodes_wanted updated: ['ns_1@cb.local'], with cookie: {sanitized,
                                                                      <<"6jw2YfUfDxiQCs0fCwoUSmHbOdRC4E/A03fIDwJPR3g=">>}
[error_logger:info,2022-09-07T14:28:17.019Z,ns_1@cb.local:ns_node_disco_sup<0.434.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_node_disco_sup}
    started: [{pid,<0.441.0>},
              {id,ns_node_disco_log},
              {mfargs,{ns_node_disco_log,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2022-09-07T14:28:17.021Z,ns_1@cb.local:<0.440.0>:ns_node_disco:do_nodes_wanted_updated_fun:227]ns_node_disco: nodes_wanted pong: ['ns_1@cb.local'], with cookie: {sanitized,
                                                                   <<"6jw2YfUfDxiQCs0fCwoUSmHbOdRC4E/A03fIDwJPR3g=">>}
[ns_server:debug,2022-09-07T14:28:17.021Z,ns_1@cb.local:<0.438.0>:ns_node_disco:do_nodes_wanted_updated_fun:227]ns_node_disco: nodes_wanted pong: ['ns_1@cb.local'], with cookie: {sanitized,
                                                                   <<"6jw2YfUfDxiQCs0fCwoUSmHbOdRC4E/A03fIDwJPR3g=">>}
[error_logger:info,2022-09-07T14:28:17.026Z,ns_1@cb.local:ns_config_rep_sup<0.442.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_config_rep_sup}
    started: [{pid,<0.443.0>},
              {id,ns_config_rep_merger},
              {mfargs,{ns_config_rep,start_link_merger,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[ns_server:debug,2022-09-07T14:28:17.026Z,ns_1@cb.local:ns_config_rep<0.444.0>:ns_config_rep:init:80]init pulling
[ns_server:debug,2022-09-07T14:28:17.026Z,ns_1@cb.local:ns_config_rep<0.444.0>:ns_config_rep:init:82]init pushing
[error_logger:info,2022-09-07T14:28:17.028Z,ns_1@cb.local:ns_config_rep_sup<0.442.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_config_rep_sup}
    started: [{pid,<0.444.0>},
              {id,ns_config_rep},
              {mfargs,{ns_config_rep,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2022-09-07T14:28:17.028Z,ns_1@cb.local:ns_node_disco_sup<0.434.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_node_disco_sup}
    started: [{pid,<0.442.0>},
              {id,ns_config_rep_sup},
              {mfargs,{ns_config_rep_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2022-09-07T14:28:17.028Z,ns_1@cb.local:ns_server_sup<0.399.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.434.0>},
              {name,ns_node_disco_sup},
              {mfargs,{ns_node_disco_sup,start_link,[]}},
              {restart_type,permanent},
              {shutdown,infinity},
              {child_type,supervisor}]
[error_logger:info,2022-09-07T14:28:17.029Z,ns_1@cb.local:ns_server_sup<0.399.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.454.0>},
              {name,tombstone_agent},
              {mfargs,{tombstone_agent,start_link,[]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[error_logger:info,2022-09-07T14:28:17.032Z,ns_1@cb.local:ns_server_sup<0.399.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.456.0>},
              {name,vbucket_map_mirror},
              {mfargs,{vbucket_map_mirror,start_link,[]}},
              {restart_type,permanent},
              {shutdown,brutal_kill},
              {child_type,worker}]
[error_logger:info,2022-09-07T14:28:17.034Z,ns_1@cb.local:ns_server_sup<0.399.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.458.0>},
              {name,capi_url_cache},
              {mfargs,{capi_url_cache,start_link,[]}},
              {restart_type,permanent},
              {shutdown,brutal_kill},
              {child_type,worker}]
[error_logger:info,2022-09-07T14:28:17.039Z,ns_1@cb.local:ns_server_sup<0.399.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.460.0>},
              {name,bucket_info_cache},
              {mfargs,{bucket_info_cache,start_link,[]}},
              {restart_type,permanent},
              {shutdown,brutal_kill},
              {child_type,worker}]
[error_logger:info,2022-09-07T14:28:17.040Z,ns_1@cb.local:ns_server_sup<0.399.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.463.0>},
              {name,ns_tick_event},
              {mfargs,{gen_event,start_link,[{local,ns_tick_event}]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[error_logger:info,2022-09-07T14:28:17.040Z,ns_1@cb.local:ns_server_sup<0.399.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.464.0>},
              {name,buckets_events},
              {mfargs,{gen_event,start_link,[{local,buckets_events}]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[error_logger:info,2022-09-07T14:28:17.040Z,ns_1@cb.local:ns_server_sup<0.399.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.465.0>},
              {name,ns_stats_event},
              {mfargs,{gen_event,start_link,[{local,ns_stats_event}]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[error_logger:info,2022-09-07T14:28:17.042Z,ns_1@cb.local:ns_server_sup<0.399.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.466.0>},
              {name,samples_loader_tasks},
              {mfargs,{samples_loader_tasks,start_link,[]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[error_logger:info,2022-09-07T14:28:17.050Z,ns_1@cb.local:ns_heart_sup<0.467.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_heart_sup}
    started: [{pid,<0.468.0>},
              {id,ns_heart},
              {mfargs,{ns_heart,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2022-09-07T14:28:17.051Z,ns_1@cb.local:ns_heart_sup<0.467.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_heart_sup}
    started: [{pid,<0.470.0>},
              {id,ns_heart_slow_updater},
              {mfargs,{ns_heart,start_link_slow_updater,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2022-09-07T14:28:17.051Z,ns_1@cb.local:ns_server_sup<0.399.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.467.0>},
              {name,ns_heart_sup},
              {mfargs,{ns_heart_sup,start_link,[]}},
              {restart_type,permanent},
              {shutdown,infinity},
              {child_type,supervisor}]
[error_logger:info,2022-09-07T14:28:17.053Z,ns_1@cb.local:ns_doctor_sup<0.474.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_doctor_sup}
    started: [{pid,<0.475.0>},
              {id,ns_doctor_events},
              {mfargs,{gen_event,start_link,[{local,ns_doctor_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2022-09-07T14:28:17.059Z,ns_1@cb.local:ns_doctor_sup<0.474.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_doctor_sup}
    started: [{pid,<0.476.0>},
              {id,ns_doctor},
              {mfargs,{ns_doctor,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2022-09-07T14:28:17.059Z,ns_1@cb.local:<0.471.0>:restartable:start_child:92]Started child process <0.474.0>
  MFA: {ns_doctor_sup,start_link,[]}
[error_logger:info,2022-09-07T14:28:17.059Z,ns_1@cb.local:ns_server_sup<0.399.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.471.0>},
              {name,ns_doctor_sup},
              {mfargs,{restartable,start_link,
                                   [{ns_doctor_sup,start_link,[]},infinity]}},
              {restart_type,permanent},
              {shutdown,infinity},
              {child_type,supervisor}]
[error_logger:info,2022-09-07T14:28:17.060Z,ns_1@cb.local:ns_server_sup<0.399.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.479.0>},
              {name,master_activity_events},
              {mfargs,{gen_event,start_link,[{local,master_activity_events}]}},
              {restart_type,permanent},
              {shutdown,brutal_kill},
              {child_type,worker}]
[error_logger:info,2022-09-07T14:28:17.064Z,ns_1@cb.local:ns_server_sup<0.399.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.482.0>},
              {name,xdcr_ckpt_store},
              {mfargs,{simple_store,start_link,[xdcr_ckpt_data]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[error_logger:info,2022-09-07T14:28:17.064Z,ns_1@cb.local:ns_server_sup<0.399.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.483.0>},
              {name,metakv_worker},
              {mfargs,{work_queue,start_link,[metakv_worker]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[error_logger:info,2022-09-07T14:28:17.065Z,ns_1@cb.local:ns_server_sup<0.399.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.484.0>},
              {name,index_events},
              {mfargs,{gen_event,start_link,[{local,index_events}]}},
              {restart_type,permanent},
              {shutdown,brutal_kill},
              {child_type,worker}]
[error_logger:info,2022-09-07T14:28:17.065Z,ns_1@cb.local:ns_server_sup<0.399.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.485.0>},
              {name,index_settings_manager},
              {mfargs,{index_settings_manager,start_link,[]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[error_logger:info,2022-09-07T14:28:17.065Z,ns_1@cb.local:ns_server_sup<0.399.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.487.0>},
              {name,query_settings_manager},
              {mfargs,{query_settings_manager,start_link,[]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[error_logger:info,2022-09-07T14:28:17.065Z,ns_1@cb.local:ns_server_sup<0.399.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.489.0>},
              {name,eventing_settings_manager},
              {mfargs,{eventing_settings_manager,start_link,[]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[error_logger:info,2022-09-07T14:28:17.065Z,ns_1@cb.local:ns_server_sup<0.399.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.491.0>},
              {name,analytics_settings_manager},
              {mfargs,{analytics_settings_manager,start_link,[]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[error_logger:info,2022-09-07T14:28:17.065Z,ns_1@cb.local:ns_server_sup<0.399.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.493.0>},
              {name,audit_events},
              {mfargs,{gen_event,start_link,[{local,audit_events}]}},
              {restart_type,permanent},
              {shutdown,brutal_kill},
              {child_type,worker}]
[ns_server:debug,2022-09-07T14:28:17.071Z,ns_1@cb.local:user_uuid_limits<0.494.0>:versioned_cache:init:41]Starting versioned cache user_uuid_limits
[error_logger:info,2022-09-07T14:28:17.071Z,ns_1@cb.local:ns_server_sup<0.399.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.494.0>},
              {name,user_limits_cache},
              {mfargs,{user_request_throttler,start_limits_cache,[]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[ns_server:debug,2022-09-07T14:28:17.071Z,ns_1@cb.local:user_request_throttler<0.496.0>:user_request_throttler:handle_info:286]Clearing all user stats {<0.498.0>,#Ref<0.2209594722.1843658756.37052>}
[error_logger:info,2022-09-07T14:28:17.071Z,ns_1@cb.local:ns_server_sup<0.399.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.496.0>},
              {name,user_request_throttler},
              {mfargs,{user_request_throttler,start_link,[]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[error_logger:info,2022-09-07T14:28:17.082Z,ns_1@cb.local:menelaus_sup<0.499.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_sup}
    started: [{pid,<0.500.0>},
              {id,menelaus_ui_auth},
              {mfargs,{menelaus_ui_auth,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2022-09-07T14:28:17.082Z,ns_1@cb.local:menelaus_sup<0.499.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_sup}
    started: [{pid,<0.502.0>},
              {id,scram_sha},
              {mfargs,{scram_sha,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2022-09-07T14:28:17.088Z,ns_1@cb.local:menelaus_sup<0.499.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_sup}
    started: [{pid,<0.503.0>},
              {id,menelaus_local_auth},
              {mfargs,{menelaus_local_auth,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2022-09-07T14:28:17.096Z,ns_1@cb.local:menelaus_sup<0.499.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_sup}
    started: [{pid,<0.504.0>},
              {id,menelaus_web_cache},
              {mfargs,{menelaus_web_cache,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2022-09-07T14:28:17.107Z,ns_1@cb.local:menelaus_sup<0.499.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_sup}
    started: [{pid,<0.506.0>},
              {id,menelaus_stats_gatherer},
              {mfargs,{menelaus_stats_gatherer,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2022-09-07T14:28:17.107Z,ns_1@cb.local:menelaus_sup<0.499.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_sup}
    started: [{pid,<0.507.0>},
              {id,json_rpc_events},
              {mfargs,{gen_event,start_link,[{local,json_rpc_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:error,2022-09-07T14:28:17.119Z,ns_1@cb.local:<0.505.0>:prometheus:post_async:192]Prometheus http request failed:
URL: http://127.0.0.1:9123/api/v1/query
Body: query=%7Bname%3D~%60kv_curr_items%7Ckv_curr_items_tot%7Ckv_mem_used_bytes%7Ccouch_docs_actual_disk_size%7Ccouch_views_actual_disk_size%7Ckv_ep_db_data_size_bytes%7Ckv_ep_bg_fetched%60%7D+or+kv_vb_curr_items%7Bstate%3D%27replica%27%7D+or+kv_vb_num_non_resident%7Bstate%3D%27active%27%7D+or+label_replace%28sum+by+%28bucket%2C+name%29+%28irate%28kv_ops%7Bop%3D%60get%60%7D%5B1m%5D%29%29%2C+%60name%60%2C%60cmd_get%60%2C+%60%60%2C+%60%60%29+or+label_replace%28irate%28kv_ops%7Bop%3D%60get%60%2Cresult%3D%60hit%60%7D%5B1m%5D%29%2C%60name%60%2C%60get_hits%60%2C%60%60%2C%60%60%29+or+label_replace%28sum+by+%28bucket%29+%28irate%28kv_cmd_lookup%5B1m%5D%29+or+irate%28kv_ops%7Bop%3D~%60set%7Cincr%7Cdecr%7Cdelete%7Cdel_meta%7Cget_meta%7Cset_meta%7Cset_ret_meta%7Cdel_ret_meta%60%7D%5B1m%5D%29%29%2C+%60name%60%2C+%60ops%60%2C+%60%60%2C+%60%60%29+or+sum+by+%28bucket%2C+name%29+%28%7Bname%3D~%60index_data_size%7Cindex_disk_size%7Ccouch_spatial_data_size%7Ccouch_spatial_disk_size%7Ccouch_views_data_size%60%7D%29&timeout=5s
Reason: {failed_connect,[{to_address,{"127.0.0.1",9123}},
                         {inet,[inet],econnrefused}]}
[ns_server:error,2022-09-07T14:28:17.120Z,ns_1@cb.local:<0.511.0>:prometheus:post_async:192]Prometheus http request failed:
URL: http://127.0.0.1:9123/api/v1/query
Body: query=%7Bcategory%3D%60system-processes%60%2Cname%3D~%60sysproc_mem_resident%7Csysproc_mem_size%7Csysproc_cpu_utilization%7Csysproc_major_faults_raw%60%7D&timeout=5s
Reason: {failed_connect,[{to_address,{"127.0.0.1",9123}},
                         {inet,[inet],econnrefused}]}
[ns_server:error,2022-09-07T14:28:17.121Z,ns_1@cb.local:<0.514.0>:prometheus:post_async:192]Prometheus http request failed:
URL: http://127.0.0.1:9123/api/v1/query
Body: query=%7Bcategory%3D%60system%60%2Cname%3D~%60sys_cpu_utilization_rate%7Csys_cpu_stolen_rate%7Csys_swap_total%7Csys_swap_used%7Csys_mem_total%7Csys_mem_free%7Csys_mem_limit%7Csys_cpu_cores_available%7Csys_allocstall%60%7D&timeout=5s
Reason: {failed_connect,[{to_address,{"127.0.0.1",9123}},
                         {inet,[inet],econnrefused}]}
[error_logger:info,2022-09-07T14:28:17.121Z,ns_1@cb.local:menelaus_web_sup<0.508.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_web_sup}
    started: [{pid,<0.515.0>},
              {id,menelaus_event},
              {mfargs,{menelaus_event,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[ns_server:info,2022-09-07T14:28:17.122Z,ns_1@cb.local:<0.517.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:129]Loaded pluggable UI specification for backup from "/opt/couchbase/etc/couchbase/pluggable-ui-backup.json"
[ns_server:info,2022-09-07T14:28:17.122Z,ns_1@cb.local:<0.517.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:129]Loaded pluggable UI specification for cbas from "/opt/couchbase/etc/couchbase/pluggable-ui-cbas.json"
[ns_server:info,2022-09-07T14:28:17.122Z,ns_1@cb.local:<0.517.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:129]Loaded pluggable UI specification for eventing from "/opt/couchbase/etc/couchbase/pluggable-ui-eventing.json"
[ns_server:info,2022-09-07T14:28:17.123Z,ns_1@cb.local:<0.517.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:129]Loaded pluggable UI specification for fts from "/opt/couchbase/etc/couchbase/pluggable-ui-fts.json"
[ns_server:info,2022-09-07T14:28:17.123Z,ns_1@cb.local:<0.517.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:129]Loaded pluggable UI specification for n1ql from "/opt/couchbase/etc/couchbase/pluggable-ui-query.json"
[ns_server:info,2022-09-07T14:28:17.124Z,ns_1@cb.local:<0.517.0>:menelaus_web:maybe_start_http_server:126]Started web service with options:
[{ip,"0.0.0.0"},{port,8091}]
[error_logger:info,2022-09-07T14:28:17.125Z,ns_1@cb.local:<0.517.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.517.0>,menelaus_web}
    started: [{pid,<0.518.0>},
              {id,menelaus_web_ipv4},
              {mfargs,
                  {menelaus_web,http_server,
                      [[{afamily,inet},
                        {name,menelaus_web},
                        {port,8091},
                        {nodelay,true},
                        {approot,
                            "/opt/couchbase/lib/ns_server/erlang/lib/ns_server/priv/public"}]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[ns_server:info,2022-09-07T14:28:17.125Z,ns_1@cb.local:<0.517.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:129]Loaded pluggable UI specification for backup from "/opt/couchbase/etc/couchbase/pluggable-ui-backup.json"
[ns_server:info,2022-09-07T14:28:17.126Z,ns_1@cb.local:<0.517.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:129]Loaded pluggable UI specification for cbas from "/opt/couchbase/etc/couchbase/pluggable-ui-cbas.json"
[ns_server:info,2022-09-07T14:28:17.126Z,ns_1@cb.local:<0.517.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:129]Loaded pluggable UI specification for eventing from "/opt/couchbase/etc/couchbase/pluggable-ui-eventing.json"
[ns_server:info,2022-09-07T14:28:17.126Z,ns_1@cb.local:<0.517.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:129]Loaded pluggable UI specification for fts from "/opt/couchbase/etc/couchbase/pluggable-ui-fts.json"
[ns_server:info,2022-09-07T14:28:17.127Z,ns_1@cb.local:<0.517.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:129]Loaded pluggable UI specification for n1ql from "/opt/couchbase/etc/couchbase/pluggable-ui-query.json"
[ns_server:info,2022-09-07T14:28:17.127Z,ns_1@cb.local:<0.517.0>:menelaus_web:maybe_start_http_server:126]Started web service with options:
[{ip,"::"},{port,8091}]
[error_logger:info,2022-09-07T14:28:17.128Z,ns_1@cb.local:<0.517.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.517.0>,menelaus_web}
    started: [{pid,<0.535.0>},
              {id,menelaus_web_ipv6},
              {mfargs,
                  {menelaus_web,http_server,
                      [[{afamily,inet6},
                        {name,menelaus_web},
                        {port,8091},
                        {nodelay,true},
                        {approot,
                            "/opt/couchbase/lib/ns_server/erlang/lib/ns_server/priv/public"}]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[ns_server:debug,2022-09-07T14:28:17.128Z,ns_1@cb.local:<0.516.0>:restartable:start_child:92]Started child process <0.517.0>
  MFA: {menelaus_web,start_link,[]}
[error_logger:info,2022-09-07T14:28:17.128Z,ns_1@cb.local:menelaus_web_sup<0.508.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_web_sup}
    started: [{pid,<0.516.0>},
              {id,menelaus_web},
              {mfargs,{restartable,start_link,
                                   [{menelaus_web,start_link,[]},infinity]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[user:info,2022-09-07T14:28:17.132Z,ns_1@cb.local:menelaus_sup<0.499.0>:menelaus_web_sup:start_link:38]Couchbase Server has started on web port 8091 on node 'ns_1@cb.local'. Version: "7.1.1-3175-enterprise".
[error_logger:info,2022-09-07T14:28:17.132Z,ns_1@cb.local:menelaus_sup<0.499.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_sup}
    started: [{pid,<0.508.0>},
              {id,menelaus_web_sup},
              {mfargs,{menelaus_web_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2022-09-07T14:28:17.138Z,ns_1@cb.local:menelaus_sup<0.499.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_sup}
    started: [{pid,<0.554.0>},
              {id,hot_keys_keeper},
              {mfargs,{hot_keys_keeper,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2022-09-07T14:28:17.139Z,ns_1@cb.local:menelaus_sup<0.499.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_sup}
    started: [{pid,<0.555.0>},
              {id,menelaus_web_alerts_srv},
              {mfargs,{menelaus_web_alerts_srv,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[ns_server:debug,2022-09-07T14:28:17.146Z,ns_1@cb.local:ns_heart<0.468.0>:goxdcr_rest:get_from_goxdcr:137]Goxdcr is temporary not available. Return empty list.
[error_logger:info,2022-09-07T14:28:17.149Z,ns_1@cb.local:menelaus_sup<0.499.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_sup}
    started: [{pid,<0.558.0>},
              {id,menelaus_cbauth},
              {mfargs,{menelaus_cbauth,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2022-09-07T14:28:17.149Z,ns_1@cb.local:ns_server_sup<0.399.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.499.0>},
              {name,menelaus},
              {mfargs,{menelaus_sup,start_link,[]}},
              {restart_type,permanent},
              {shutdown,infinity},
              {child_type,supervisor}]
[error_logger:info,2022-09-07T14:28:17.150Z,ns_1@cb.local:ns_server_sup<0.399.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.565.0>},
              {name,ns_ports_setup},
              {mfargs,{ns_ports_setup,start,[]}},
              {restart_type,{permanent,4}},
              {shutdown,brutal_kill},
              {child_type,worker}]
[ns_server:debug,2022-09-07T14:28:17.153Z,ns_1@cb.local:ns_heart<0.468.0>:cluster_logs_collection_task:maybe_build_cluster_logs_task:43]Ignoring exception trying to read cluster_logs_collection_task_status table: error:badarg
[ns_server:debug,2022-09-07T14:28:17.155Z,ns_1@cb.local:ns_ports_setup<0.565.0>:ns_ports_manager:set_dynamic_children:48]Setting children [memcached,saslauthd_port,goxdcr]
[error_logger:info,2022-09-07T14:28:17.156Z,ns_1@cb.local:service_agent_sup<0.570.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,service_agent_sup}
    started: [{pid,<0.571.0>},
              {id,service_agent_children_sup},
              {mfargs,{supervisor,start_link,
                                  [{local,service_agent_children_sup},
                                   service_agent_sup,child]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2022-09-07T14:28:17.157Z,ns_1@cb.local:service_agent_sup<0.570.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,service_agent_sup}
    started: [{pid,<0.572.0>},
              {id,service_agent_worker},
              {mfargs,{erlang,apply,[#Fun<service_agent_sup.0.32483565>,[]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2022-09-07T14:28:17.157Z,ns_1@cb.local:ns_server_sup<0.399.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.570.0>},
              {name,service_agent_sup},
              {mfargs,{service_agent_sup,start_link,[]}},
              {restart_type,permanent},
              {shutdown,infinity},
              {child_type,supervisor}]
[error_logger:info,2022-09-07T14:28:17.163Z,ns_1@cb.local:ns_server_sup<0.399.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.574.0>},
              {name,ns_memcached_sockets_pool},
              {mfargs,{ns_memcached_sockets_pool,start_link,[]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[ns_server:debug,2022-09-07T14:28:17.174Z,ns_1@cb.local:memcached_auth_server<0.575.0>:memcached_auth_server:reconnect:234]Skipping creation of 'Auth provider' connection because external users are disabled
[error_logger:info,2022-09-07T14:28:17.174Z,ns_1@cb.local:ns_server_sup<0.399.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.575.0>},
              {name,memcached_auth_server},
              {mfargs,{memcached_auth_server,start_link,[]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[ns_server:debug,2022-09-07T14:28:17.175Z,ns_1@cb.local:ns_audit_cfg<0.577.0>:ns_audit_cfg:write_audit_json:237]Writing new content to "/opt/couchbase/var/lib/couchbase/config/audit.json", Params [{descriptors_path,
                                                                                      "/opt/couchbase/etc/security"},
                                                                                     {version,
                                                                                      2},
                                                                                     {uuid,
                                                                                      "55497325"},
                                                                                     {event_states,
                                                                                      {[]}},
                                                                                     {filtering_enabled,
                                                                                      true},
                                                                                     {disabled_userids,
                                                                                      []},
                                                                                     {auditd_enabled,
                                                                                      false},
                                                                                     {log_path,
                                                                                      "/opt/couchbase/var/lib/couchbase/logs"},
                                                                                     {rotate_interval,
                                                                                      86400},
                                                                                     {rotate_size,
                                                                                      20971520},
                                                                                     {sync,
                                                                                      []}]
[ns_server:debug,2022-09-07T14:28:17.202Z,ns_1@cb.local:ns_audit_cfg<0.577.0>:ns_audit_cfg:notify_memcached:151]Instruct memcached to reload audit config
[error_logger:info,2022-09-07T14:28:17.202Z,ns_1@cb.local:ns_server_sup<0.399.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.577.0>},
              {name,ns_audit_cfg},
              {mfargs,{ns_audit_cfg,start_link,[]}},
              {restart_type,{permanent,4}},
              {shutdown,1000},
              {child_type,worker}]
[ns_server:warn,2022-09-07T14:28:17.213Z,ns_1@cb.local:<0.594.0>:ns_memcached:connect:1240]Unable to connect: {error,{badmatch,[{inet,{error,econnrefused}}]}}, retrying.
[error_logger:info,2022-09-07T14:28:17.213Z,ns_1@cb.local:ns_server_sup<0.399.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.597.0>},
              {name,ns_audit},
              {mfargs,{ns_audit,start_link,[]}},
              {restart_type,{permanent,4}},
              {shutdown,1000},
              {child_type,worker}]
[ns_server:debug,2022-09-07T14:28:17.213Z,ns_1@cb.local:memcached_config_mgr<0.598.0>:memcached_config_mgr:init:54]waiting for completion of initial ns_ports_setup round
[error_logger:info,2022-09-07T14:28:17.214Z,ns_1@cb.local:ns_server_sup<0.399.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.598.0>},
              {name,memcached_config_mgr},
              {mfargs,{memcached_config_mgr,start_link,[]}},
              {restart_type,{permanent,4}},
              {shutdown,1000},
              {child_type,worker}]
[ns_server:info,2022-09-07T14:28:17.221Z,ns_1@cb.local:<0.602.0>:ns_memcached_log_rotator:init:36]Starting log rotator on "/opt/couchbase/var/lib/couchbase/logs"/"memcached.log"* with an initial period of 39003ms
[error_logger:info,2022-09-07T14:28:17.222Z,ns_1@cb.local:ns_server_sup<0.399.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.602.0>},
              {name,ns_memcached_log_rotator},
              {mfargs,{ns_memcached_log_rotator,start_link,[]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[error_logger:info,2022-09-07T14:28:17.225Z,ns_1@cb.local:ns_server_sup<0.399.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.603.0>},
              {name,testconditions_store},
              {mfargs,{simple_store,start_link,[testconditions]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[ns_server:error,2022-09-07T14:28:17.230Z,ns_1@cb.local:<0.592.0>:prometheus:post_async:192]Prometheus http request failed:
URL: http://127.0.0.1:9123/api/v1/query
Body: query=%7Bname%3D~%60kv_curr_items%7Ckv_curr_items_tot%7Ckv_mem_used_bytes%7Ccouch_docs_actual_disk_size%7Ccouch_views_actual_disk_size%7Ckv_ep_db_data_size_bytes%7Ckv_ep_bg_fetched%60%7D+or+kv_vb_curr_items%7Bstate%3D%27replica%27%7D+or+kv_vb_num_non_resident%7Bstate%3D%27active%27%7D+or+label_replace%28sum+by+%28bucket%2C+name%29+%28irate%28kv_ops%7Bop%3D%60get%60%7D%5B1m%5D%29%29%2C+%60name%60%2C%60cmd_get%60%2C+%60%60%2C+%60%60%29+or+label_replace%28irate%28kv_ops%7Bop%3D%60get%60%2Cresult%3D%60hit%60%7D%5B1m%5D%29%2C%60name%60%2C%60get_hits%60%2C%60%60%2C%60%60%29+or+label_replace%28sum+by+%28bucket%29+%28irate%28kv_cmd_lookup%5B1m%5D%29+or+irate%28kv_ops%7Bop%3D~%60set%7Cincr%7Cdecr%7Cdelete%7Cdel_meta%7Cget_meta%7Cset_meta%7Cset_ret_meta%7Cdel_ret_meta%60%7D%5B1m%5D%29%29%2C+%60name%60%2C+%60ops%60%2C+%60%60%2C+%60%60%29+or+sum+by+%28bucket%2C+name%29+%28%7Bname%3D~%60index_data_size%7Cindex_disk_size%7Ccouch_spatial_data_size%7Ccouch_spatial_disk_size%7Ccouch_views_data_size%60%7D%29&timeout=5s
Reason: <<"Service Unavailable">>
[error_logger:info,2022-09-07T14:28:17.232Z,ns_1@cb.local:ns_server_sup<0.399.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.613.0>},
              {name,terse_cluster_info_uploader},
              {mfargs,{terse_cluster_info_uploader,start_link,[]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[ns_server:error,2022-09-07T14:28:17.233Z,ns_1@cb.local:<0.611.0>:prometheus:post_async:192]Prometheus http request failed:
URL: http://127.0.0.1:9123/api/v1/query
Body: query=%7Bcategory%3D%60system-processes%60%2Cname%3D~%60sysproc_mem_resident%7Csysproc_mem_size%7Csysproc_cpu_utilization%7Csysproc_major_faults_raw%60%7D&timeout=5s
Reason: <<"Service Unavailable">>
[ns_server:error,2022-09-07T14:28:17.236Z,ns_1@cb.local:<0.619.0>:prometheus:post_async:192]Prometheus http request failed:
URL: http://127.0.0.1:9123/api/v1/query
Body: query=%7Bcategory%3D%60system%60%2Cname%3D~%60sys_cpu_utilization_rate%7Csys_cpu_stolen_rate%7Csys_swap_total%7Csys_swap_used%7Csys_mem_total%7Csys_mem_free%7Csys_mem_limit%7Csys_cpu_cores_available%7Csys_allocstall%60%7D&timeout=5s
Reason: <<"Service Unavailable">>
[ns_server:debug,2022-09-07T14:28:17.238Z,ns_1@cb.local:ns_heart_slow_status_updater<0.470.0>:goxdcr_rest:get_from_goxdcr:137]Goxdcr is temporary not available. Return empty list.
[ns_server:debug,2022-09-07T14:28:17.238Z,ns_1@cb.local:ns_heart_slow_status_updater<0.470.0>:cluster_logs_collection_task:maybe_build_cluster_logs_task:43]Ignoring exception trying to read cluster_logs_collection_task_status table: error:badarg
[ns_server:debug,2022-09-07T14:28:17.252Z,ns_1@cb.local:terse_cluster_info_uploader<0.613.0>:terse_cluster_info_uploader:handle_info:42]Refreshing terse cluster info with <<"{\"rev\":7,\"nodesExt\":[{\"services\":{\"capi\":8092,\"capiSSL\":18092,\"kv\":11210,\"kvSSL\":11207,\"mgmt\":8091,\"mgmtSSL\":18091,\"projector\":9999},\"thisNode\":true}],\"clusterCapabilitiesVer\":[1,0],\"clusterCapabilities\":{\"n1ql\":[\"enhancedPreparedStatements\"]}}">>
[ns_server:warn,2022-09-07T14:28:17.253Z,ns_1@cb.local:<0.634.0>:ns_memcached:connect:1240]Unable to connect: {error,{badmatch,[{inet,{error,econnrefused}}]}}, retrying.
[error_logger:info,2022-09-07T14:28:17.257Z,ns_1@cb.local:ns_bucket_worker_sup<0.625.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_bucket_worker_sup}
    started: [{pid,<0.635.0>},
              {id,ns_bucket_sup},
              {mfargs,{ns_bucket_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2022-09-07T14:28:17.268Z,ns_1@cb.local:ns_bucket_worker_sup<0.625.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_bucket_worker_sup}
    started: [{pid,<0.636.0>},
              {id,ns_bucket_worker},
              {mfargs,{ns_bucket_worker,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2022-09-07T14:28:17.268Z,ns_1@cb.local:ns_server_sup<0.399.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.625.0>},
              {name,ns_bucket_worker_sup},
              {mfargs,{ns_bucket_worker_sup,start_link,[]}},
              {restart_type,permanent},
              {shutdown,infinity},
              {child_type,supervisor}]
[error_logger:info,2022-09-07T14:28:17.269Z,ns_1@cb.local:ns_server_sup<0.399.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.638.0>},
              {name,ns_server_stats},
              {mfargs,{ns_server_stats,start_link,[]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[error_logger:info,2022-09-07T14:28:17.289Z,ns_1@cb.local:ns_server_sup<0.399.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.641.0>},
              {name,{stats_reader,"@system"}},
              {mfargs,{stats_reader,start_link,["@system"]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[error_logger:info,2022-09-07T14:28:17.289Z,ns_1@cb.local:ns_server_sup<0.399.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.643.0>},
              {name,{stats_reader,"@system-processes"}},
              {mfargs,{stats_reader,start_link,["@system-processes"]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[error_logger:info,2022-09-07T14:28:17.290Z,ns_1@cb.local:ns_server_sup<0.399.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.645.0>},
              {name,{stats_reader,"@query"}},
              {mfargs,{stats_reader,start_link,["@query"]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[error_logger:info,2022-09-07T14:28:17.290Z,ns_1@cb.local:ns_server_sup<0.399.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.647.0>},
              {name,{stats_reader,"@global"}},
              {mfargs,{stats_reader,start_link,["@global"]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[error_logger:info,2022-09-07T14:28:17.292Z,ns_1@cb.local:ns_server_sup<0.399.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.649.0>},
              {name,goxdcr_status_keeper},
              {mfargs,{goxdcr_status_keeper,start_link,[]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[ns_server:debug,2022-09-07T14:28:17.293Z,ns_1@cb.local:goxdcr_status_keeper<0.649.0>:goxdcr_rest:get_from_goxdcr:137]Goxdcr is temporary not available. Return empty list.
[ns_server:debug,2022-09-07T14:28:17.294Z,ns_1@cb.local:goxdcr_status_keeper<0.649.0>:goxdcr_rest:get_from_goxdcr:137]Goxdcr is temporary not available. Return empty list.
[error_logger:info,2022-09-07T14:28:17.295Z,ns_1@cb.local:services_stats_sup<0.652.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,services_stats_sup}
    started: [{pid,<0.653.0>},
              {id,service_stats_children_sup},
              {mfargs,{supervisor,start_link,
                                  [{local,service_stats_children_sup},
                                   services_stats_sup,child]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2022-09-07T14:28:17.297Z,ns_1@cb.local:service_status_keeper_sup<0.654.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,service_status_keeper_sup}
    started: [{pid,<0.655.0>},
              {id,service_status_keeper_worker},
              {mfargs,{work_queue,start_link,[service_status_keeper_worker]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2022-09-07T14:28:17.303Z,ns_1@cb.local:service_status_keeper_sup<0.654.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,service_status_keeper_sup}
    started: [{pid,<0.656.0>},
              {id,service_status_keeper_index},
              {mfargs,{service_index,start_keeper,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2022-09-07T14:28:17.306Z,ns_1@cb.local:service_status_keeper_sup<0.654.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,service_status_keeper_sup}
    started: [{pid,<0.659.0>},
              {id,service_status_keeper_fts},
              {mfargs,{service_fts,start_keeper,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2022-09-07T14:28:17.308Z,ns_1@cb.local:service_status_keeper_sup<0.654.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,service_status_keeper_sup}
    started: [{pid,<0.662.0>},
              {id,service_status_keeper_eventing},
              {mfargs,{service_eventing,start_keeper,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2022-09-07T14:28:17.309Z,ns_1@cb.local:services_stats_sup<0.652.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,services_stats_sup}
    started: [{pid,<0.654.0>},
              {id,service_status_keeper_sup},
              {mfargs,{service_status_keeper_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2022-09-07T14:28:17.311Z,ns_1@cb.local:services_stats_sup<0.652.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,services_stats_sup}
    started: [{pid,<0.665.0>},
              {id,service_stats_worker},
              {mfargs,{erlang,apply,
                              [#Fun<services_stats_sup.0.114823200>,[]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2022-09-07T14:28:17.311Z,ns_1@cb.local:ns_server_sup<0.399.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.652.0>},
              {name,services_stats_sup},
              {mfargs,{services_stats_sup,start_link,[]}},
              {restart_type,permanent},
              {shutdown,infinity},
              {child_type,supervisor}]
[ns_server:debug,2022-09-07T14:28:17.320Z,ns_1@cb.local:<0.669.0>:new_concurrency_throttle:init:109]init concurrent throttle process, pid: <0.669.0>, type: kv_throttle# of available token: 1
[ns_server:debug,2022-09-07T14:28:17.322Z,ns_1@cb.local:compaction_daemon<0.667.0>:compaction_daemon:process_scheduler_message:1359]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2022-09-07T14:28:17.322Z,ns_1@cb.local:compaction_daemon<0.667.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-07T14:28:17.322Z,ns_1@cb.local:compaction_daemon<0.667.0>:compaction_daemon:process_scheduler_message:1359]No buckets to compact for compact_views. Rescheduling compaction.
[error_logger:info,2022-09-07T14:28:17.322Z,ns_1@cb.local:ns_server_sup<0.399.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.667.0>},
              {name,compaction_daemon},
              {mfargs,{compaction_daemon,start_link,[]}},
              {restart_type,{permanent,4}},
              {shutdown,86400000},
              {child_type,worker}]
[ns_server:debug,2022-09-07T14:28:17.323Z,ns_1@cb.local:compaction_daemon<0.667.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-07T14:28:17.323Z,ns_1@cb.local:compaction_daemon<0.667.0>:compaction_daemon:process_scheduler_message:1359]No buckets to compact for compact_master. Rescheduling compaction.
[ns_server:debug,2022-09-07T14:28:17.323Z,ns_1@cb.local:compaction_daemon<0.667.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_master too soon. Next run will be in 3600s
[error_logger:info,2022-09-07T14:28:17.326Z,ns_1@cb.local:cluster_logs_sup<0.670.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,cluster_logs_sup}
    started: [{pid,<0.671.0>},
              {id,ets_holder},
              {mfargs,{cluster_logs_collection_task,start_link_ets_holder,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2022-09-07T14:28:17.326Z,ns_1@cb.local:ns_server_sup<0.399.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.670.0>},
              {name,cluster_logs_sup},
              {mfargs,{cluster_logs_sup,start_link,[]}},
              {restart_type,permanent},
              {shutdown,infinity},
              {child_type,supervisor}]
[error_logger:info,2022-09-07T14:28:17.326Z,ns_1@cb.local:ns_server_sup<0.399.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.672.0>},
              {name,collections},
              {mfargs,{collections,start_link,[]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[error_logger:info,2022-09-07T14:28:17.327Z,ns_1@cb.local:ns_server_sup<0.399.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.674.0>},
              {name,leader_events},
              {mfargs,{gen_event,start_link,[{local,leader_events}]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[error_logger:info,2022-09-07T14:28:17.335Z,ns_1@cb.local:leader_leases_sup<0.677.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,leader_leases_sup}
    started: [{pid,<0.678.0>},
              {id,leader_activities},
              {mfargs,{leader_activities,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,10000},
              {child_type,worker}]

[error_logger:info,2022-09-07T14:28:17.339Z,ns_1@cb.local:leader_leases_sup<0.677.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,leader_leases_sup}
    started: [{pid,<0.679.0>},
              {id,leader_lease_agent},
              {mfargs,{leader_lease_agent,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2022-09-07T14:28:17.339Z,ns_1@cb.local:leader_services_sup<0.676.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,leader_services_sup}
    started: [{pid,<0.677.0>},
              {id,leader_leases_sup},
              {mfargs,{leader_leases_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2022-09-07T14:28:17.343Z,ns_1@cb.local:leader_registry_sup<0.680.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,leader_registry_sup}
    started: [{pid,<0.681.0>},
              {id,leader_registry},
              {mfargs,{leader_registry,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2022-09-07T14:28:17.348Z,ns_1@cb.local:leader_registry_sup<0.680.0>:mb_master:check_master_takeover_needed:254]Sending master node question to the following nodes: []
[ns_server:debug,2022-09-07T14:28:17.348Z,ns_1@cb.local:leader_registry_sup<0.680.0>:mb_master:check_master_takeover_needed:256]Got replies: []
[ns_server:debug,2022-09-07T14:28:17.348Z,ns_1@cb.local:leader_registry_sup<0.680.0>:mb_master:check_master_takeover_needed:262]Was unable to discover master, not going to force mastership takeover
[ns_server:debug,2022-09-07T14:28:17.348Z,ns_1@cb.local:mb_master<0.683.0>:mb_master:init:80]Heartbeat interval is 2000
[user:info,2022-09-07T14:28:17.349Z,ns_1@cb.local:mb_master<0.683.0>:mb_master:init:85]I'm the only node, so I'm the master.
[ns_server:debug,2022-09-07T14:28:17.349Z,ns_1@cb.local:leader_registry<0.681.0>:leader_registry:handle_new_leader:275]New leader is 'ns_1@cb.local'. Invalidating name cache.
[ns_server:debug,2022-09-07T14:28:17.358Z,ns_1@cb.local:mb_master<0.683.0>:master_activity_events:submit_cast:76]Failed to send master activity event {became_master,'ns_1@cb.local'}: {error,
                                                                       badarg}
[error_logger:info,2022-09-07T14:28:17.362Z,ns_1@cb.local:mb_master_sup<0.685.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,mb_master_sup}
    started: [{pid,<0.686.0>},
              {id,leader_lease_acquirer},
              {mfargs,{leader_lease_acquirer,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,10000},
              {child_type,worker}]

[ns_server:debug,2022-09-07T14:28:17.365Z,ns_1@cb.local:leader_quorum_nodes_manager<0.688.0>:leader_quorum_nodes_manager:pull_config:99]Attempting to pull config from nodes:
[]
[ns_server:debug,2022-09-07T14:28:17.365Z,ns_1@cb.local:leader_quorum_nodes_manager<0.688.0>:leader_quorum_nodes_manager:pull_config:104]Pulled config successfully.
[error_logger:info,2022-09-07T14:28:17.365Z,ns_1@cb.local:mb_master_sup<0.685.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,mb_master_sup}
    started: [{pid,<0.688.0>},
              {id,leader_quorum_nodes_manager},
              {mfargs,{leader_quorum_nodes_manager,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:info,2022-09-07T14:28:17.369Z,ns_1@cb.local:mb_master_sup<0.685.0>:misc:start_singleton:901]start_singleton(gen_server, start_link, [{via,leader_registry,ns_tick},
                                         ns_tick,[],[]]): started as <0.693.0> on 'ns_1@cb.local'

[error_logger:info,2022-09-07T14:28:17.369Z,ns_1@cb.local:mb_master_sup<0.685.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,mb_master_sup}
    started: [{pid,<0.693.0>},
              {id,ns_tick},
              {mfargs,{ns_tick,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,10},
              {child_type,worker}]

[ns_server:debug,2022-09-07T14:28:17.374Z,ns_1@cb.local:leader_lease_agent<0.679.0>:leader_lease_agent:do_handle_acquire_lease:140]Granting lease to {lease_holder,<<"fa814c54dc8b348114b4d9a3567953c9">>,
                                'ns_1@cb.local'} for 15000ms
[ns_server:debug,2022-09-07T14:28:17.374Z,ns_1@cb.local:<0.696.0>:chronicle_master:do_init:141]Starting with SelfRef = #Ref<0.2209594722.1843658753.37602>
[ns_server:info,2022-09-07T14:28:17.374Z,ns_1@cb.local:mb_master_sup<0.685.0>:misc:start_singleton:901]start_singleton(gen_server2, start_link, [{via,leader_registry,
                                           chronicle_master},
                                          chronicle_master,[],[]]): started as <0.696.0> on 'ns_1@cb.local'

[error_logger:info,2022-09-07T14:28:17.375Z,ns_1@cb.local:mb_master_sup<0.685.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,mb_master_sup}
    started: [{pid,<0.696.0>},
              {id,chronicle_master},
              {mfargs,{chronicle_master,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2022-09-07T14:28:17.379Z,ns_1@cb.local:ns_orchestrator_sup<0.699.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_orchestrator_sup}
    started: [{pid,<0.700.0>},
              {id,compat_mode_events},
              {mfargs,{gen_event,start_link,[{local,compat_mode_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2022-09-07T14:28:17.384Z,ns_1@cb.local:ns_ports_setup<0.565.0>:ns_ports_setup:set_children:60]Monitor ns_child_ports_sup <15902.129.0>
[ns_server:debug,2022-09-07T14:28:17.385Z,ns_1@cb.local:memcached_config_mgr<0.598.0>:memcached_config_mgr:init:56]ns_ports_setup seems to be ready
[ns_server:debug,2022-09-07T14:28:17.385Z,ns_1@cb.local:ns_config<0.260.0>:ns_config:do_upgrade_config:728]Upgrading config by changes:
[{set,cluster_compat_version,[6,5]}]

[ns_server:info,2022-09-07T14:28:17.385Z,ns_1@cb.local:ns_config<0.260.0>:ns_online_config_upgrader:do_upgrade_config:66]Performing online config upgrade to [6,6]
[ns_server:debug,2022-09-07T14:28:17.386Z,ns_1@cb.local:ns_config<0.260.0>:ns_config:do_upgrade_config:728]Upgrading config by changes:
[{set,cluster_compat_version,[6,6]},
 {delete,rbac_upgrade},
 {set,buckets,[{configs,[]}]}]

[ns_server:info,2022-09-07T14:28:17.394Z,ns_1@cb.local:<0.692.0>:leader_lease_acquire_worker:handle_fresh_lease_acquired:296]Acquired lease from node 'ns_1@cb.local' (lease uuid: <<"fa814c54dc8b348114b4d9a3567953c9">>)
[ns_server:debug,2022-09-07T14:28:17.396Z,ns_1@cb.local:memcached_config_mgr<0.598.0>:memcached_config_mgr:find_port_pid_loop:156]Found memcached port <15902.135.0>
[ns_server:info,2022-09-07T14:28:17.417Z,ns_1@cb.local:ns_config<0.260.0>:chronicle_compat:upgrade:366]Keys are migrated to chronicle. Rev = {<<"1933459bb8a062c61b619e931c483c80">>,
                                       3}. Sets = [{set,counters,[]},
                                                   {set,auto_reprovision_cfg,
                                                    [{enabled,true},
                                                     {max_nodes,1},
                                                     {count,0}]},
                                                   {set,bucket_names,[]},
                                                   {set,nodes_wanted,
                                                    ['ns_1@cb.local']},
                                                   {set,server_groups,
                                                    [[{uuid,<<"0">>},
                                                      {name,<<"Group 1">>},
                                                      {nodes,
                                                       ['ns_1@cb.local']}]]},
                                                   {set,
                                                    {node,'ns_1@cb.local',
                                                     membership},
                                                    active}]
[ns_server:info,2022-09-07T14:28:17.417Z,ns_1@cb.local:ns_config<0.260.0>:ns_online_config_upgrader:do_upgrade_config:66]Performing online config upgrade to [7,0]
[ns_server:debug,2022-09-07T14:28:17.417Z,ns_1@cb.local:ns_config<0.260.0>:ns_config:do_upgrade_config:728]Upgrading config by changes:
[{set,cluster_compat_version,[7,0]},
 {delete,rbac_upgrade},
 {set,{metakv,<<"/indexing/settings/config">>},
      <<"{\"indexer.settings.rebalance.redistribute_indexes\":false,\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.log_level\":\"info\",\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.compaction.min_frag\":30,\"indexer.settings.enable_page_bloom_filter\":false,\"indexer.settings.inmemory_snapshot.interval\":200,\"indexer.settings.max_cpu_percent\":0,\"indexer.settings.storage_mode\":\"\",\"indexer.settings.recovery.max_rollbacks\":2,\"indexer.settings.memory_quota\":536870912,\"indexer.settings.num_replica\":0,\"indexer.settings.compaction.abort_exceed_interval\":false}">>},
 {set,{metakv,<<"/query/settings/config">>},
      <<"{\"timeout\":0,\"numatrs\":1024,\"n1ql-feat-ctrl\":76,\"max-parallelism\":1,\"query.settings.curl_whitelist\":{\"all_access\":false,\"allowed_urls\":[],\"disallowed_urls\":[]},\"query.settings.tmp_space_dir\":\"/opt/couchbase/var/lib/couchbase/tmp\",\"completed-limit\":4000,\"pipeline-batch\":16,\"prepared-limit\":16384,\"pipeline-cap\":512,\"memory-quota\":0,\"cleanupwindow\":\"60s\",\"use-cbo\":true,\"scan-cap\":512,\"query.settings.tmp_space_size\":5120,\"completed-threshold\":1000,\"loglevel\":\"info\",\"cleanuplostattempts\":true,\"cleanupclientattempts\":true,\"txtimeout\":\"0ms\"}">>},
 {set,after_upgrade_cleanup,
      [{counters,'_deleted'},
       {{node,'ns_1@cb.local',membership},'_deleted'},
       {server_groups,'_deleted'},
       {nodes_wanted,[]},
       {buckets,[{configs,[]}]},
       {auto_reprovision_cfg,'_deleted'},
       {auto_reprovision_cfg,'_deleted'},
       {buckets,[{configs,[]}]},
       {{node,'ns_1@cb.local',membership},'_deleted'},
       {server_groups,'_deleted'},
       {nodes_wanted,[]}]}]

[ns_server:debug,2022-09-07T14:28:17.419Z,ns_1@cb.local:roles_cache<0.365.0>:active_cache:clean:181]Clearing the roles_cache cache
[ns_server:debug,2022-09-07T14:28:17.419Z,ns_1@cb.local:ns_cookie_manager<0.221.0>:ns_cookie_manager:do_cookie_sync:101]ns_cookie_manager do_cookie_sync
[ns_server:debug,2022-09-07T14:28:17.419Z,ns_1@cb.local:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
after_upgrade_cleanup ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780097}}]},
 {counters,'_deleted'},
 {{node,'ns_1@cb.local',membership},'_deleted'},
 {server_groups,'_deleted'},
 {nodes_wanted,[]},
 {buckets,[{configs,[]}]},
 {auto_reprovision_cfg,'_deleted'},
 {auto_reprovision_cfg,'_deleted'},
 {buckets,[{configs,[]}]},
 {{node,'ns_1@cb.local',membership},'_deleted'},
 {server_groups,'_deleted'},
 {nodes_wanted,[]}]
[ns_server:debug,2022-09-07T14:28:17.419Z,ns_1@cb.local:prometheus_cfg<0.415.0>:prometheus_cfg:maybe_apply_new_settings:608]Settings didn't change, ignoring update
[ns_server:debug,2022-09-07T14:28:17.419Z,ns_1@cb.local:ns_ssl_services_setup<0.287.0>:ns_ssl_services_setup:maybe_store_ca_certs:672]Considering to store CA certs
[ns_server:debug,2022-09-07T14:28:17.419Z,ns_1@cb.local:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
cluster_compat_version ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{3,63829780097}}]},7,0]
[ns_server:debug,2022-09-07T14:28:17.419Z,ns_1@cb.local:roles_cache<0.365.0>:active_cache:clean:181]Clearing the roles_cache cache
[ns_server:debug,2022-09-07T14:28:17.419Z,ns_1@cb.local:compiled_roles_cache<0.362.0>:versioned_cache:handle_info:89]Flushing cache compiled_roles_cache due to version change from {undefined,
                                                                {0,1415866166},
                                                                {0,1415866166},
                                                                false,[]} to {[7,
                                                                               0],
                                                                              {0,
                                                                               1415866166},
                                                                              {0,
                                                                               1415866166},
                                                                              false,
                                                                              []}
[ns_server:debug,2022-09-07T14:28:17.419Z,ns_1@cb.local:ns_config_rep<0.444.0>:ns_config_rep:do_push_keys:383]Replicating some config keys ([after_upgrade_cleanup,buckets,
                               cluster_compat_version,
                               {local_changes_count,
                                   <<"1678dfae96c38e07dff43c49b9f6967b">>},
                               {metakv,<<"/indexing/settings/config">>},
                               {metakv,<<"/query/settings/config">>}]..)
[ns_server:debug,2022-09-07T14:28:17.420Z,ns_1@cb.local:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
buckets ->
[{0,[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780097}}]},{configs,[]}]
[ns_server:debug,2022-09-07T14:28:17.419Z,ns_1@cb.local:<0.706.0>:ns_node_disco:do_nodes_wanted_updated_fun:224]ns_node_disco: nodes_wanted updated: ['ns_1@cb.local'], with cookie: {sanitized,
                                                                      <<"6jw2YfUfDxiQCs0fCwoUSmHbOdRC4E/A03fIDwJPR3g=">>}
[ns_server:debug,2022-09-07T14:28:17.420Z,ns_1@cb.local:<0.706.0>:ns_node_disco:do_nodes_wanted_updated_fun:227]ns_node_disco: nodes_wanted pong: ['ns_1@cb.local'], with cookie: {sanitized,
                                                                   <<"6jw2YfUfDxiQCs0fCwoUSmHbOdRC4E/A03fIDwJPR3g=">>}
[ns_server:debug,2022-09-07T14:28:17.420Z,ns_1@cb.local:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{metakv,<<"/indexing/settings/config">>} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780097}}]}|
 <<"{\"indexer.settings.rebalance.redistribute_indexes\":false,\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.log_level\":\"info\",\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.compaction.mi"...>>]
[ns_server:debug,2022-09-07T14:28:17.420Z,ns_1@cb.local:ns_config_rep<0.444.0>:ns_config_rep:handle_cast:173]Synchronized with merger in 6 us
[ns_server:debug,2022-09-07T14:28:17.421Z,ns_1@cb.local:ns_config_rep<0.444.0>:ns_config_rep:handle_call:149]Got full synchronization request from 'ns_1@cb.local'
[ns_server:debug,2022-09-07T14:28:17.421Z,ns_1@cb.local:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{metakv,<<"/query/settings/config">>} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780097}}]}|
 <<"{\"timeout\":0,\"numatrs\":1024,\"n1ql-feat-ctrl\":76,\"max-parallelism\":1,\"query.settings.curl_whitelist\":{\"all_access\":false,\"allowed_urls\":[],\"disallowed_urls\":[]},\"query.settings.tmp_space_dir\":\"/opt/couchbase/var/lib/couchbase/tmp\",\"completed-limit\":4000,\"pipeline-batch\":16,\"prepared-limit\":16384,\"pipeline-cap\":512,\"memory-quota\":0,\"cleanupwindow\":\"60s\",\"use-cbo\":true,\"scan-cap\":512,\"que"...>>]
[ns_server:debug,2022-09-07T14:28:17.421Z,ns_1@cb.local:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{local_changes_count,<<"1678dfae96c38e07dff43c49b9f6967b">>} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{8,63829780097}}]}]
[ns_server:debug,2022-09-07T14:28:17.421Z,ns_1@cb.local:ns_config_rep<0.444.0>:ns_config_rep:handle_cast:173]Synchronized with merger in 52 us
[ns_server:debug,2022-09-07T14:28:17.422Z,ns_1@cb.local:chronicle_kv_log<0.404.0>:chronicle_kv_log:log:59]update (key: {node,'ns_1@cb.local',membership}, rev: {<<"1933459bb8a062c61b619e931c483c80">>,
                                                      3})
active
[ns_server:debug,2022-09-07T14:28:17.422Z,ns_1@cb.local:ns_cookie_manager<0.221.0>:ns_cookie_manager:do_cookie_sync:101]ns_cookie_manager do_cookie_sync
[ns_server:debug,2022-09-07T14:28:17.422Z,ns_1@cb.local:chronicle_kv_log<0.404.0>:chronicle_kv_log:log:59]update (key: server_groups, rev: {<<"1933459bb8a062c61b619e931c483c80">>,3})
[[{uuid,<<"0">>},{name,<<"Group 1">>},{nodes,['ns_1@cb.local']}]]
[ns_server:debug,2022-09-07T14:28:17.422Z,ns_1@cb.local:<0.719.0>:ns_node_disco:do_nodes_wanted_updated_fun:224]ns_node_disco: nodes_wanted updated: ['ns_1@cb.local'], with cookie: {sanitized,
                                                                      <<"6jw2YfUfDxiQCs0fCwoUSmHbOdRC4E/A03fIDwJPR3g=">>}
[ns_server:debug,2022-09-07T14:28:17.422Z,ns_1@cb.local:<0.719.0>:ns_node_disco:do_nodes_wanted_updated_fun:227]ns_node_disco: nodes_wanted pong: ['ns_1@cb.local'], with cookie: {sanitized,
                                                                   <<"6jw2YfUfDxiQCs0fCwoUSmHbOdRC4E/A03fIDwJPR3g=">>}
[ns_server:debug,2022-09-07T14:28:17.422Z,ns_1@cb.local:chronicle_kv_log<0.404.0>:chronicle_kv_log:log:59]update (key: nodes_wanted, rev: {<<"1933459bb8a062c61b619e931c483c80">>,3})
['ns_1@cb.local']
[ns_server:debug,2022-09-07T14:28:17.423Z,ns_1@cb.local:chronicle_kv_log<0.404.0>:chronicle_kv_log:log:59]update (key: bucket_names, rev: {<<"1933459bb8a062c61b619e931c483c80">>,3})
[]
[ns_server:debug,2022-09-07T14:28:17.423Z,ns_1@cb.local:chronicle_kv_log<0.404.0>:chronicle_kv_log:log:59]update (key: auto_reprovision_cfg, rev: {<<"1933459bb8a062c61b619e931c483c80">>,
                                         3})
[{enabled,true},{max_nodes,1},{count,0}]
[ns_server:debug,2022-09-07T14:28:17.423Z,ns_1@cb.local:chronicle_kv_log<0.404.0>:chronicle_kv_log:log:59]update (key: counters, rev: {<<"1933459bb8a062c61b619e931c483c80">>,3})
[]
[ns_server:debug,2022-09-07T14:28:17.424Z,ns_1@cb.local:memcached_passwords<0.425.0>:memcached_cfg:write_cfg:148]Writing config file for: "/opt/couchbase/var/lib/couchbase/isasl.pw"
[ns_server:debug,2022-09-07T14:28:17.425Z,ns_1@cb.local:memcached_permissions<0.430.0>:memcached_cfg:write_cfg:148]Writing config file for: "/opt/couchbase/var/lib/couchbase/config/memcached.rbac"
[ns_server:debug,2022-09-07T14:28:17.430Z,ns_1@cb.local:memcached_permissions<0.430.0>:replicated_dets:select_from_table:281][ets] Starting select with {users_storage,
                               [{{docv2,{user,{'_',local}},'_','_'},
                                 [],
                                 ['$_']}],
                               100}
[ns_server:debug,2022-09-07T14:28:17.431Z,ns_1@cb.local:memcached_refresh<0.283.0>:memcached_refresh:handle_call:49]File rename from "/opt/couchbase/var/lib/couchbase/config/memcached.rbac.tmp" to "/opt/couchbase/var/lib/couchbase/config/memcached.rbac" is requested
[ns_server:debug,2022-09-07T14:28:17.442Z,ns_1@cb.local:kv<0.251.0>:chronicle_upgrade:upgrade_loop:80]Upgading chronicle from [7,0]. Final version = [7,1]
[ns_server:info,2022-09-07T14:28:17.443Z,ns_1@cb.local:kv<0.251.0>:ns_ssl_services_setup:chronicle_upgrade_to_71:1182]Upgrading CA certs to 7.1: setting ca_certificates to the following props:
 [[{id,0},
   {subject,<<"CN=Couchbase Server 94d64d45">>},
   {not_before,63524217600},
   {not_after,64691827199},
   {type,generated},
   {pem,<<"-----BEGIN CERTIFICATE-----\nMIIDITCCAgmgAwIBAgIIFxKaUqQBFTwwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciA5NGQ2NGQ0NTAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgOTRkNjRk\nNDUwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQDCrzpi1QtF4PyrbI2Y\nQB0augORV0Ro18YRvmfImOFnIZpNAmUYZRU68E39LZL794EOVdu05NB0tXNOfVrG\nB6gQNweW7UL1RiCL3OhoZ/VBsR9qSG54l/rFSddbrw3cNMKoZyYaPFoNhJogPOP4\n0IIUZhUjKgA7yKA/L0AqKn516TbfuA9oX/3y5Pm1Dycg0oRmDRenKl/cpORSvwaL\nqb4A35vbvA31mBswu3iLKhpn6L7uu/FPoPqROxTbBY6pHfu53IWOmzGbv3PFhHDR\noNKQI34ZliCL9NP06H6+8ZubL6C7J+XfG+zZkH/ZXda4SFCY3GOiZpPpdEOchSBw\nxi5pAgMBAAGjVzBVMA4GA1UdDwEB/wQEAwICpDATBgNVHSUEDDAKBggrBgEFBQcD\nATAPBgNVHRMBAf8EBTADAQH/MB0GA1UdDgQWBBQaPtelkfN4O47ZO+DmJZUak5gS\niDANBgkqhkiG9w0BAQsFAAOCAQEAS8nhCYCWQXVKye+hMXWV3W95/i44FCzXNUgV\nYZtqu740vRbZev89SxFzOZBN42/3/a8k4hpofhBVYgBTNrcmRexZnaQ0ptWOF2cC\ndKEUViHikjHjyKXC44fClgAM8dAsCDhssGWe75Cz5xU64mOgH+NgerJsJqVRJDzo\nNNMbP7yD9jLyyPpbi6mJ/4pYGweebLPxV4bngn7ly1oAdLs8gPRxqZITJlupN3ra\nHxF67QIEtvzZraQYjT3ryCoxqF6MIIJaqEe9kpUcxls+MHrd7cvxHN+e7LBjW7c7\nuB9JwNc30pJASAvoOoI58i+5dgrHcCxlOcJ4EurRDdi+NfPRNA==\n-----END CERTIFICATE-----\n\n">>},
   {origin,upgrade}]]
[ns_server:debug,2022-09-07T14:28:17.444Z,ns_1@cb.local:memcached_permissions<0.430.0>:memcached_cfg:rename_and_refresh:170]Successfully renamed "/opt/couchbase/var/lib/couchbase/config/memcached.rbac.tmp" to "/opt/couchbase/var/lib/couchbase/config/memcached.rbac"
[ns_server:debug,2022-09-07T14:28:17.444Z,ns_1@cb.local:memcached_refresh<0.283.0>:memcached_refresh:handle_cast:55]Refresh of rbac requested
[ns_server:info,2022-09-07T14:28:17.444Z,ns_1@cb.local:kv<0.251.0>:compaction_daemon:chronicle_upgrade_to_71:189]Upgrading autocompaction to 7.1: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:warn,2022-09-07T14:28:17.445Z,ns_1@cb.local:memcached_refresh<0.283.0>:ns_memcached:connect:1237]Unable to connect: {error,{badmatch,[{inet,{error,econnrefused}}]}}.
[ns_server:debug,2022-09-07T14:28:17.446Z,ns_1@cb.local:memcached_refresh<0.283.0>:memcached_refresh:handle_info:93]Refresh of [rbac,isasl] failed. Retry in 1000 ms.
[ns_server:debug,2022-09-07T14:28:17.453Z,ns_1@cb.local:memcached_config_mgr<0.598.0>:memcached_config_mgr:init:94]wrote memcached config to /opt/couchbase/var/lib/couchbase/config/memcached.json. Will activate memcached port server
[ns_server:debug,2022-09-07T14:28:17.455Z,ns_1@cb.local:memcached_config_mgr<0.598.0>:memcached_config_mgr:init:98]activated memcached port server
[ns_server:info,2022-09-07T14:28:17.457Z,ns_1@cb.local:memcached_config_mgr<0.598.0>:memcached_config_mgr:push_tls_config:225]Pushing TLS config to memcached:
{[{<<"private key">>,
   <<"/opt/couchbase/var/lib/couchbase/config/certs/pkey.pem">>},
  {<<"certificate chain">>,
   <<"/opt/couchbase/var/lib/couchbase/config/certs/chain.pem">>},
  {<<"CA file">>,<<"/opt/couchbase/var/lib/couchbase/config/certs/ca.pem">>},
  {<<"minimum version">>,<<"TLS 1.2">>},
  {<<"cipher list">>,
   {[{<<"TLS 1.2">>,<<"HIGH">>},
     {<<"TLS 1.3">>,
      <<"TLS_AES_256_GCM_SHA384:TLS_AES_128_GCM_SHA256:TLS_CHACHA20_POLY1305_SHA256">>}]}},
  {<<"cipher order">>,true},
  {<<"client cert auth">>,<<"disabled">>}]}
[ns_server:warn,2022-09-07T14:28:17.458Z,ns_1@cb.local:<0.722.0>:ns_memcached:connect:1240]Unable to connect: {error,{badmatch,[{inet,{error,econnrefused}}]}}, retrying.
[ns_server:info,2022-09-07T14:28:17.475Z,ns_1@cb.local:ns_ssl_services_setup<0.287.0>:ns_ssl_services_setup:handle_info:602]cert_and_pkey changed
[ns_server:info,2022-09-07T14:28:17.475Z,ns_1@cb.local:ns_config<0.260.0>:ns_online_config_upgrader:do_upgrade_config:66]Performing online config upgrade to [7,1]
[ns_server:debug,2022-09-07T14:28:17.475Z,ns_1@cb.local:chronicle_kv_log<0.404.0>:chronicle_kv_log:log:59]update (key: root_cert_and_pkey, rev: {<<"1933459bb8a062c61b619e931c483c80">>,
                                       4})
{sanitized,<<"9THFoecgd/EZtdO3ks6WT9peW0BuubE+bewKtNRZP44=">>}
[ns_server:debug,2022-09-07T14:28:17.475Z,ns_1@cb.local:roles_cache<0.365.0>:active_cache:clean:181]Clearing the roles_cache cache
[ns_server:debug,2022-09-07T14:28:17.475Z,ns_1@cb.local:chronicle_kv_log<0.404.0>:chronicle_kv_log:log:59]update (key: cluster_compat_version, rev: {<<"1933459bb8a062c61b619e931c483c80">>,
                                           4})
[7,1]
[ns_server:debug,2022-09-07T14:28:17.475Z,ns_1@cb.local:ns_cookie_manager<0.221.0>:ns_cookie_manager:do_cookie_sync:101]ns_cookie_manager do_cookie_sync
[ns_server:debug,2022-09-07T14:28:17.475Z,ns_1@cb.local:chronicle_kv_log<0.404.0>:chronicle_kv_log:log:59]update (key: ca_certificates, rev: {<<"1933459bb8a062c61b619e931c483c80">>,4})
[[{id,0},
  {subject,<<"CN=Couchbase Server 94d64d45">>},
  {not_before,63524217600},
  {not_after,64691827199},
  {type,generated},
  {pem,<<"-----BEGIN CERTIFICATE-----\nMIIDITCCAgmgAwIBAgIIFxKaUqQBFTwwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciA5NGQ2NGQ0NTAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgOTRkNjRk\nNDUwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQDCrzpi1QtF4PyrbI2Y\nQB0augORV0Ro18YRvmfImOFnIZpNAmUYZRU68E39LZL794EOVdu05NB0tXNOfVrG\nB6gQNwe"...>>},
  {origin,upgrade}]]
[ns_server:debug,2022-09-07T14:28:17.475Z,ns_1@cb.local:<0.723.0>:ns_node_disco:do_nodes_wanted_updated_fun:224]ns_node_disco: nodes_wanted updated: ['ns_1@cb.local'], with cookie: {sanitized,
                                                                      <<"6jw2YfUfDxiQCs0fCwoUSmHbOdRC4E/A03fIDwJPR3g=">>}
[ns_server:debug,2022-09-07T14:28:17.475Z,ns_1@cb.local:chronicle_kv_log<0.404.0>:chronicle_kv_log:log:59]update (key: autocompaction, rev: {<<"1933459bb8a062c61b619e931c483c80">>,4})
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-07T14:28:17.476Z,ns_1@cb.local:<0.723.0>:ns_node_disco:do_nodes_wanted_updated_fun:227]ns_node_disco: nodes_wanted pong: ['ns_1@cb.local'], with cookie: {sanitized,
                                                                   <<"6jw2YfUfDxiQCs0fCwoUSmHbOdRC4E/A03fIDwJPR3g=">>}
[ns_server:debug,2022-09-07T14:28:17.491Z,ns_1@cb.local:ns_config<0.260.0>:ns_config:do_upgrade_config:728]Upgrading config by changes:
[{set,cluster_compat_version,[7,1]},
 {set,audit_decriptors,
      [{8243,
        [{name,<<"mutate document">>},
         {description,<<"Document was mutated via the REST API">>},
         {enabled,true},
         {module,ns_server}]},
       {8255,
        [{name,<<"read document">>},
         {description,<<"Document was read via the REST API">>},
         {enabled,false},
         {module,ns_server}]},
       {8257,
        [{name,<<"alert email sent">>},
         {description,<<"An alert email was successfully sent">>},
         {enabled,true},
         {module,ns_server}]},
       {8265,
        [{name,<<"RBAC information retrieved">>},
         {description,<<"RBAC information was retrieved">>},
         {enabled,true},
         {module,ns_server}]},
       {20480,
        [{name,<<"opened DCP connection">>},
         {description,<<"opened DCP connection">>},
         {enabled,true},
         {module,memcached}]},
       {20482,
        [{name,<<"external memcached bucket flush">>},
         {description,<<"External user flushed the content of a memcached bucket">>},
         {enabled,true},
         {module,memcached}]},
       {20483,
        [{name,<<"invalid packet">>},
         {description,<<"Rejected an invalid packet">>},
         {enabled,true},
         {module,memcached}]},
       {20485,
        [{name,<<"authentication succeeded">>},
         {description,<<"Authentication to the cluster succeeded">>},
         {enabled,false},
         {module,memcached}]},
       {20488,
        [{name,<<"document read">>},
         {description,<<"Document was read">>},
         {enabled,false},
         {module,memcached}]},
       {20489,
        [{name,<<"document locked">>},
         {description,<<"Document was locked">>},
         {enabled,false},
         {module,memcached}]},
       {20490,
        [{name,<<"document modify">>},
         {description,<<"Document was modified">>},
         {enabled,false},
         {module,memcached}]},
       {20491,
        [{name,<<"document delete">>},
         {description,<<"Document was deleted">>},
         {enabled,false},
         {module,memcached}]},
       {20492,
        [{name,<<"select bucket">>},
         {description,<<"The specified bucket was selected">>},
         {enabled,true},
         {module,memcached}]},
       {20493,
        [{name,<<"session terminated">>},
         {description,<<"Session to the cluster has terminated">>},
         {enabled,false},
         {module,memcached}]},
       {20494,
        [{name,<<"tenant rate limited">>},
         {description,<<"The given tenant was rate limited">>},
         {enabled,true},
         {module,memcached}]},
       {28672,
        [{name,<<"SELECT statement">>},
         {description,<<"A N1QL SELECT statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28673,
        [{name,<<"EXPLAIN statement">>},
         {description,<<"A N1QL EXPLAIN statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28674,
        [{name,<<"PREPARE statement">>},
         {description,<<"A N1QL PREPARE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28675,
        [{name,<<"INFER statement">>},
         {description,<<"A N1QL INFER statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28676,
        [{name,<<"INSERT statement">>},
         {description,<<"A N1QL INSERT statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28677,
        [{name,<<"UPSERT statement">>},
         {description,<<"A N1QL UPSERT statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28678,
        [{name,<<"DELETE statement">>},
         {description,<<"A N1QL DELETE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28679,
        [{name,<<"UPDATE statement">>},
         {description,<<"A N1QL UPDATE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28680,
        [{name,<<"MERGE statement">>},
         {description,<<"A N1QL MERGE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28681,
        [{name,<<"CREATE INDEX statement">>},
         {description,<<"A N1QL CREATE INDEX statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28682,
        [{name,<<"DROP INDEX statement">>},
         {description,<<"A N1QL DROP INDEX statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28683,
        [{name,<<"ALTER INDEX statement">>},
         {description,<<"A N1QL ALTER INDEX statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28684,
        [{name,<<"BUILD INDEX statement">>},
         {description,<<"A N1QL BUILD INDEX statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28685,
        [{name,<<"GRANT ROLE statement">>},
         {description,<<"A N1QL GRANT ROLE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28686,
        [{name,<<"REVOKE ROLE statement">>},
         {description,<<"A N1QL REVOKE ROLE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28687,
        [{name,<<"UNRECOGNIZED statement">>},
         {description,<<"An unrecognized statement was received by the N1QL query engine">>},
         {enabled,false},
         {module,n1ql}]},
       {28688,
        [{name,<<"CREATE PRIMARY INDEX statement">>},
         {description,<<"A N1QL CREATE PRIMARY INDEX statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28689,
        [{name,<<"/admin/stats API request">>},
         {description,<<"An HTTP request was made to the API at /admin/stats.">>},
         {enabled,false},
         {module,n1ql}]},
       {28690,
        [{name,<<"/admin/vitals API request">>},
         {description,<<"An HTTP request was made to the API at /admin/vitals.">>},
         {enabled,false},
         {module,n1ql}]},
       {28691,
        [{name,<<"/admin/prepareds API request">>},
         {description,<<"An HTTP request was made to the API at /admin/prepareds.">>},
         {enabled,false},
         {module,n1ql}]},
       {28692,
        [{name,<<"/admin/active_requests API request">>},
         {description,<<"An HTTP request was made to the API at /admin/active_requests.">>},
         {enabled,false},
         {module,n1ql}]},
       {28693,
        [{name,<<"/admin/indexes/prepareds API request">>},
         {description,<<"An HTTP request was made to the API at /admin/indexes/prepareds.">>},
         {enabled,false},
         {module,n1ql}]},
       {28694,
        [{name,<<"/admin/indexes/active_requests API request">>},
         {description,<<"An HTTP request was made to the API at /admin/indexes/active_requests.">>},
         {enabled,false},
         {module,n1ql}]},
       {28695,
        [{name,<<"/admin/indexes/completed_requests API request">>},
         {description,<<"An HTTP request was made to the API at /admin/indexes/completed_requests.">>},
         {enabled,false},
         {module,n1ql}]},
       {28697,
        [{name,<<"/admin/ping API request">>},
         {description,<<"An HTTP request was made to the API at /admin/ping.">>},
         {enabled,false},
         {module,n1ql}]},
       {28698,
        [{name,<<"/admin/config API request">>},
         {description,<<"An HTTP request was made to the API at /admin/config.">>},
         {enabled,false},
         {module,n1ql}]},
       {28699,
        [{name,<<"/admin/ssl_cert API request">>},
         {description,<<"An HTTP request was made to the API at /admin/ssl_cert.">>},
         {enabled,false},
         {module,n1ql}]},
       {28700,
        [{name,<<"/admin/settings API request">>},
         {description,<<"An HTTP request was made to the API at /admin/settings.">>},
         {enabled,false},
         {module,n1ql}]},
       {28701,
        [{name,<<"/admin/clusters API request">>},
         {description,<<"An HTTP request was made to the API at /admin/clusters.">>},
         {enabled,false},
         {module,n1ql}]},
       {28702,
        [{name,<<"/admin/completed_requests API request">>},
         {description,<<"An HTTP request was made to the API at /admin/completed_requests.">>},
         {enabled,false},
         {module,n1ql}]},
       {28704,
        [{name,<<"/admin/functions API request">>},
         {description,<<"An HTTP request was made to the API at /admin/functions.">>},
         {enabled,false},
         {module,n1ql}]},
       {28705,
        [{name,<<"/admin/indexes/functions API request">>},
         {description,<<"An HTTP request was made to the API at /admin/indexes/functions.">>},
         {enabled,false},
         {module,n1ql}]},
       {28706,
        [{name,<<"CREATE FUNCTION statement">>},
         {description,<<"A N1QL CREATE FUNCTION statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28707,
        [{name,<<"DROP FUNCTION statement">>},
         {description,<<"A N1QL DROP FUNCTION statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28708,
        [{name,<<"EXECUTE FUNCTION statement">>},
         {description,<<"A N1QL EXECUTE FUNCTION statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28709,
        [{name,<<"/admin/tasks API request">>},
         {description,<<"An HTTP request was made to the API at /admin/tasks.">>},
         {enabled,false},
         {module,n1ql}]},
       {28710,
        [{name,<<"/admin/indexes/tasks API request">>},
         {description,<<"An HTTP request was made to the API at /admin/indexes/tasks.">>},
         {enabled,false},
         {module,n1ql}]},
       {28711,
        [{name,<<"/admin/dictionary_cache API request">>},
         {description,<<"An HTTP request was made to the API at /admin/dictionary_cache.">>},
         {enabled,false},
         {module,n1ql}]},
       {28712,
        [{name,<<"/admin/indexes/dictionary_cache API request">>},
         {description,<<"An HTTP request was made to the API at /admin/indexes/dictionary_cache.">>},
         {enabled,false},
         {module,n1ql}]},
       {28713,
        [{name,<<"CREATE SCOPE statement">>},
         {description,<<"A N1QL CREATE SCOPE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28714,
        [{name,<<"DROP SCOPE statement">>},
         {description,<<"A N1QL DROP SCOPE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28715,
        [{name,<<"CREATE COLLECTION statement">>},
         {description,<<"A N1QL CREATE COLLECTION statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28716,
        [{name,<<"DROP COLLECTION statement">>},
         {description,<<"A N1QL DROP COLLECTION statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28717,
        [{name,<<"FLUSH COLLECTION statement">>},
         {description,<<"A N1QL FLUSH COLLECTION statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28718,
        [{name,<<"UPDATE STATISTICS statement">>},
         {description,<<"A N1QL UPDATE STATISTICS statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28719,
        [{name,<<"ADVISE statement">>},
         {description,<<"A N1QL ADVISE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28720,
        [{name,<<"START TRANSACTION statement">>},
         {description,<<"A N1QL START TRANSACTION statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28721,
        [{name,<<"COMMIT TRANSACTION statement">>},
         {description,<<"A N1QL COMMIT TRANSACTION statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28722,
        [{name,<<"ROLLBACK TRANSACTION statement">>},
         {description,<<"A N1QL ROLLBACK TRANSACTION statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28723,
        [{name,<<"ROLLBACK TRANSACTION TO SAVEPOINT statement">>},
         {description,<<"A N1QL ROLLBACK TRANSACTION TO SAVEPOINT statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28724,
        [{name,<<"SET TRANSACTION ISOLATION statement">>},
         {description,<<"A N1QL SET TRANSACTION ISOLATION statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28725,
        [{name,<<"SAVEPOINT statement">>},
         {description,<<"A N1QL SAVEPOINT statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28726,
        [{name,<<"/admin/transactions API request">>},
         {description,<<"An HTTP request was made to the API at /admin/transactions.">>},
         {enabled,false},
         {module,n1ql}]},
       {28727,
        [{name,<<"/admin/indexes/transactions API request">>},
         {description,<<"An HTTP request was made to the API at /admin/indexes/transactions.">>},
         {enabled,false},
         {module,n1ql}]},
       {28728,
        [{name,<<"N1QL backup / restore API request">>},
         {description,<<"An HTTP request was made to archive or restore N1QL metadata ">>},
         {enabled,false},
         {module,n1ql}]},
       {28729,
        [{name,<<"/admin/shutdown API request">>},
         {description,<<"An HTTP request was made to initate graceful shutdown">>},
         {enabled,false},
         {module,n1ql}]},
       {32768,
        [{name,<<"Create Function">>},
         {description,<<"Request to create or update eventing function definition">>},
         {enabled,true},
         {module,eventing}]},
       {32769,
        [{name,<<"Delete Function">>},
         {description,<<"Request to delete eventing function definition">>},
         {enabled,true},
         {module,eventing}]},
       {32770,
        [{name,<<"Fetch Functions">>},
         {description,<<"Request to fetch eventing function definition">>},
         {enabled,false},
         {module,eventing}]},
       {32771,
        [{name,<<"List Deployed">>},
         {description,<<"Request to fetch eventing deployed functions list">>},
         {enabled,false},
         {module,eventing}]},
       {32772,
        [{name,<<"Fetch Drafts">>},
         {description,<<"Request to fetch eventing function draft definitions">>},
         {enabled,false},
         {module,eventing}]},
       {32773,
        [{name,<<"Delete Drafts">>},
         {description,<<"Request to delete eventing function draft definitions">>},
         {enabled,true},
         {module,eventing}]},
       {32774,
        [{name,<<"Save Draft">>},
         {description,<<"Request to save a draft definition">>},
         {enabled,true},
         {module,eventing}]},
       {32775,
        [{name,<<"Start Debug">>},
         {description,<<"Request to start eventing function debugger">>},
         {enabled,true},
         {module,eventing}]},
       {32776,
        [{name,<<"Stop Debug">>},
         {description,<<"Request to stop eventing function debugger">>},
         {enabled,true},
         {module,eventing}]},
       {32777,
        [{name,<<"Start Tracing">>},
         {description,<<"Request to start tracing eventing function execution">>},
         {enabled,true},
         {module,eventing}]},
       {32778,
        [{name,<<"Stop Tracing">>},
         {description,<<"Request to stop tracing eventing function execution">>},
         {enabled,true},
         {module,eventing}]},
       {32779,
        [{name,<<"Set Settings">>},
         {description,<<"Request to save settings for an eventing function">>},
         {enabled,true},
         {module,eventing}]},
       {32780,
        [{name,<<"Fetch Config">>},
         {description,<<"Request to fetch eventing config">>},
         {enabled,false},
         {module,eventing}]},
       {32781,
        [{name,<<"Save Config">>},
         {description,<<"Request to save eventing config">>},
         {enabled,true},
         {module,eventing}]},
       {32783,
        [{name,<<"Get Settings">>},
         {description,<<"Request to fetch eventing function settings">>},
         {enabled,false},
         {module,eventing}]},
       {32784,
        [{name,<<"Import Functions">>},
         {description,<<"Request to import one or more eventing functions">>},
         {enabled,true},
         {module,eventing}]},
       {32785,
        [{name,<<"Export Functions">>},
         {description,<<"Request to export all eventing functions">>},
         {enabled,false},
         {module,eventing}]},
       {32786,
        [{name,<<"List Running">>},
         {description,<<"Request to fetch eventing running function list">>},
         {enabled,false},
         {module,eventing}]},
       {32789,
        [{name,<<"Deploy Function">>},
         {description,<<"Request to deploy eventing function">>},
         {enabled,true},
         {module,eventing}]},
       {32790,
        [{name,<<"Undeploy Function">>},
         {description,<<"Request to undeploy eventing function">>},
         {enabled,true},
         {module,eventing}]},
       {32791,
        [{name,<<"Pause Function">>},
         {description,<<"Request to pause eventing function">>},
         {enabled,true},
         {module,eventing}]},
       {32792,
        [{name,<<"Resume Function">>},
         {description,<<"Request to resume eventing function">>},
         {enabled,true},
         {module,eventing}]},
       {32793,
        [{name,<<"Backup Functions">>},
         {description,<<"Request to backup one or more eventing functions">>},
         {enabled,false},
         {module,eventing}]},
       {32794,
        [{name,<<"Restore Functions">>},
         {description,<<"Request to restore one or more eventing functions from a backup">>},
         {enabled,true},
         {module,eventing}]},
       {32795,
        [{name,<<"List Function">>},
         {description,<<"Request to fetch eventing functions">>},
         {enabled,false},
         {module,eventing}]},
       {32796,
        [{name,<<"Function Status">>},
         {description,<<"Request to fetch eventing function status">>},
         {enabled,false},
         {module,eventing}]},
       {32797,
        [{name,<<"Clear Stats">>},
         {description,<<"Request to reset eventing function stats">>},
         {enabled,true},
         {module,eventing}]},
       {32798,
        [{name,<<"Fetch Stats">>},
         {description,<<"Request to fetch eventing function stats">>},
         {enabled,false},
         {module,eventing}]},
       {32799,
        [{name,<<"Eventing Cluster Stats">>},
         {description,<<"Request to fetch eventing cluster stats">>},
         {enabled,false},
         {module,eventing}]},
       {32801,
        [{name,<<"Eventing System Event">>},
         {description,<<"Request to execute eventing node related functions">>},
         {enabled,false},
         {module,eventing}]},
       {32802,
        [{name,<<"Get User Info">>},
         {description,<<"Request to get user eventing permissions">>},
         {enabled,false},
         {module,eventing}]},
       {36865,
        [{name,<<"Service configuration change">>},
         {description,<<"A successful service configuration change was made.">>},
         {enabled,true},
         {module,analytics}]},
       {36866,
        [{name,<<"Node configuration change">>},
         {description,<<"A successful node configuration change was made.">>},
         {enabled,true},
         {module,analytics}]},
       {36867,
        [{name,<<"SELECT statement">>},
         {description,<<"A N1QL SELECT statement was executed">>},
         {enabled,false},
         {module,analytics}]},
       {36868,
        [{name,<<"CREATE DATAVERSE statement">>},
         {description,<<"A N1QL CREATE DATAVERSE statement was executed">>},
         {enabled,false},
         {module,analytics}]},
       {36869,
        [{name,<<"DROP DATAVERSE statement">>},
         {description,<<"A N1QL DROP DATAVERSE statement was executed">>},
         {enabled,false},
         {module,analytics}]},
       {36870,
        [{name,<<"CREATE DATASET statement">>},
         {description,<<"A N1QL CREATE DATASET statement was executed">>},
         {enabled,false},
         {module,analytics}]},
       {36871,
        [{name,<<"DROP DATASET statement">>},
         {description,<<"A N1QL DROP DATASET statement was executed">>},
         {enabled,false},
         {module,analytics}]},
       {36872,
        [{name,<<"CREATE INDEX statement">>},
         {description,<<"A N1QL CREATE INDEX statement was executed">>},
         {enabled,false},
         {module,analytics}]},
       {36873,
        [{name,<<"DROP INDEX statement">>},
         {description,<<"A N1QL DROP INDEX statement was executed">>},
         {enabled,false},
         {module,analytics}]},
       {36877,
        [{name,<<"CONNECT LINK statement">>},
         {description,<<"A N1QL CONNECT LINK statement was executed">>},
         {enabled,false},
         {module,analytics}]},
       {36878,
        [{name,<<"DISCONNECT LINK statement">>},
         {description,<<"A N1QL DISCONNECT LINK statement was executed">>},
         {enabled,false},
         {module,analytics}]},
       {36879,
        [{name,<<"UNRECOGNIZED statement">>},
         {description,<<"An UNRECOGNIZED N1QL statement was encountered">>},
         {enabled,false},
         {module,analytics}]},
       {36880,
        [{name,<<"ALTER COLLECTION statement">>},
         {description,<<"A N1QL ALTER COLLECTION statement was executed">>},
         {enabled,false},
         {module,analytics}]},
       {40960,
        [{name,<<"Create Design Doc">>},
         {description,<<"Design Doc is Created">>},
         {enabled,true},
         {module,view_engine}]},
       {40961,
        [{name,<<"Delete Design Doc">>},
         {description,<<"Design Doc is Deleted">>},
         {enabled,true},
         {module,view_engine}]},
       {40962,
        [{name,<<"Query DDoc Meta Data">>},
         {description,<<"Design Doc Meta Data Query Request">>},
         {enabled,true},
         {module,view_engine}]},
       {40963,
        [{name,<<"View Query">>},
         {description,<<"View Query Request">>},
         {enabled,false},
         {module,view_engine}]},
       {40964,
        [{name,<<"Update Design Doc">>},
         {description,<<"Design Doc is Updated">>},
         {enabled,true},
         {module,view_engine}]},
       {40966,
        [{name,<<"Access denied">>},
         {description,<<"Access denied to the REST API due to invalid permissions or credentials">>},
         {enabled,true},
         {module,view_engine}]},
       {45056,
        [{name,<<"Modify configuration">>},
         {description,<<"Backup service configuration was modified">>},
         {enabled,true},
         {module,backup}]},
       {45057,
        [{name,<<"Fetch configuration">>},
         {description,<<"Backup service configuration was retrieved">>},
         {enabled,false},
         {module,backup}]},
       {45058,
        [{name,<<"Add plan">>},
         {description,<<"A new backup plan was added">>},
         {enabled,true},
         {module,backup}]},
       {45059,
        [{name,<<"Modify plan">>},
         {description,<<"Existing backup plan was modified">>},
         {enabled,true},
         {module,backup}]},
       {45060,
        [{name,<<"Delete plan">>},
         {description,<<"A backup plan was removed">>},
         {enabled,true},
         {module,backup}]},
       {45061,
        [{name,<<"Fetch plan">>},
         {description,<<"One or more backup plans where fetched">>},
         {enabled,false},
         {module,backup}]},
       {45062,
        [{name,<<"Add repository">>},
         {description,<<"A new active backup repository was added">>},
         {enabled,true},
         {module,backup}]},
       {45063,
        [{name,<<"Archive repository">>},
         {description,<<"An active repository was archived">>},
         {enabled,true},
         {module,backup}]},
       {45064,
        [{name,<<"Pause repository">>},
         {description,<<"An active repository was paused">>},
         {enabled,true},
         {module,backup}]},
       {45065,
        [{name,<<"Resume repository">>},
         {description,<<"An active repository was resumed">>},
         {enabled,true},
         {module,backup}]},
       {45066,
        [{name,<<"Fetch repository">>},
         {description,<<"A repository was fetched">>},
         {enabled,false},
         {module,backup}]},
       {45067,
        [{name,<<"Restore repository">>},
         {description,<<"The repository data was restored">>},
         {enabled,true},
         {module,backup}]},
       {45068,
        [{name,<<"Backup repository">>},
         {description,<<"A manual backup was triggered on an active repository">>},
         {enabled,true},
         {module,backup}]},
       {45069,
        [{name,<<"Merge repository">>},
         {description,<<"A manual merge was triggered on an active repository">>},
         {enabled,true},
         {module,backup}]},
       {45070,
        [{name,<<"Info repository">>},
         {description,<<"Information about the structure and contents of the backup repository was fetched.">>},
         {enabled,false},
         {module,backup}]},
       {45071,
        [{name,<<"Examine repository">>},
         {description,<<"A document was retrieved from the repository backups">>},
         {enabled,true},
         {module,backup}]},
       {45072,
        [{name,<<"Delete repository">>},
         {description,<<"A repository was deleted">>},
         {enabled,true},
         {module,backup}]},
       {45073,
        [{name,<<"Delete backup">>},
         {description,<<"An active repository backup was deleted">>},
         {enabled,true},
         {module,backup}]},
       {45074,
        [{name,<<"Access denied">>},
         {description,<<"A user has been denied access to the REST API due to invalid permissions or credentials">>},
         {enabled,true},
         {module,backup}]}]},
 {delete,rbac_upgrade},
 {set,{metakv,<<"/indexing/settings/config">>},
      <<"{\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.rebalance.redistribute_indexes\":false,\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.log_level\":\"info\",\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.compaction.min_frag\":30,\"indexer.settings.inmemory_snapshot.interval\":200,\"indexer.settings.enable_page_bloom_filter\":false,\"indexer.settings.max_cpu_percent\":0,\"indexer.settings.storage_mode\":\"\",\"indexer.settings.recovery.max_rollbacks\":2,\"indexer.settings.num_replica\":0,\"indexer.settings.memory_quota\":536870912,\"indexer.settings.compaction.abort_exceed_interval\":false}">>}]

[ns_server:debug,2022-09-07T14:28:17.499Z,ns_1@cb.local:ns_cookie_manager<0.221.0>:ns_cookie_manager:do_cookie_sync:101]ns_cookie_manager do_cookie_sync
[ns_server:debug,2022-09-07T14:28:17.499Z,ns_1@cb.local:roles_cache<0.365.0>:active_cache:clean:181]Clearing the roles_cache cache
[ns_server:debug,2022-09-07T14:28:17.499Z,ns_1@cb.local:<0.727.0>:ns_node_disco:do_nodes_wanted_updated_fun:224]ns_node_disco: nodes_wanted updated: ['ns_1@cb.local'], with cookie: {sanitized,
                                                                      <<"6jw2YfUfDxiQCs0fCwoUSmHbOdRC4E/A03fIDwJPR3g=">>}
[ns_server:debug,2022-09-07T14:28:17.499Z,ns_1@cb.local:compiled_roles_cache<0.362.0>:versioned_cache:handle_info:89]Flushing cache compiled_roles_cache due to version change from {[7,0],
                                                                {0,1415866166},
                                                                {0,1415866166},
                                                                false,[]} to {[7,
                                                                               1],
                                                                              {0,
                                                                               1415866166},
                                                                              {0,
                                                                               1415866166},
                                                                              false,
                                                                              []}
[ns_server:debug,2022-09-07T14:28:17.499Z,ns_1@cb.local:<0.727.0>:ns_node_disco:do_nodes_wanted_updated_fun:227]ns_node_disco: nodes_wanted pong: ['ns_1@cb.local'], with cookie: {sanitized,
                                                                   <<"6jw2YfUfDxiQCs0fCwoUSmHbOdRC4E/A03fIDwJPR3g=">>}
[ns_server:debug,2022-09-07T14:28:17.502Z,ns_1@cb.local:ns_config_rep<0.444.0>:ns_config_rep:do_push_keys:383]Replicating some config keys ([audit_decriptors,cluster_compat_version,
                               {local_changes_count,
                                   <<"1678dfae96c38e07dff43c49b9f6967b">>},
                               {metakv,<<"/indexing/settings/config">>}]..)
[ns_server:debug,2022-09-07T14:28:17.502Z,ns_1@cb.local:prometheus_cfg<0.415.0>:prometheus_cfg:maybe_apply_new_settings:608]Settings didn't change, ignoring update
[ns_server:debug,2022-09-07T14:28:17.502Z,ns_1@cb.local:prometheus_cfg<0.415.0>:prometheus_cfg:maybe_apply_new_settings:608]Settings didn't change, ignoring update
[ns_server:debug,2022-09-07T14:28:17.503Z,ns_1@cb.local:ns_config_rep<0.444.0>:ns_config_rep:handle_cast:173]Synchronized with merger in 7 us
[ns_server:debug,2022-09-07T14:28:17.504Z,ns_1@cb.local:ns_config_rep<0.444.0>:ns_config_rep:handle_call:149]Got full synchronization request from 'ns_1@cb.local'
[ns_server:debug,2022-09-07T14:28:17.504Z,ns_1@cb.local:ns_config_rep<0.444.0>:ns_config_rep:handle_cast:173]Synchronized with merger in 10 us
[user:warn,2022-09-07T14:28:17.504Z,ns_1@cb.local:compat_mode_manager<0.701.0>:compat_mode_manager:handle_consider_switching_compat_mode:43]Changed cluster compat mode from undefined to [7,1]
[error_logger:info,2022-09-07T14:28:17.504Z,ns_1@cb.local:ns_orchestrator_sup<0.699.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_orchestrator_sup}
    started: [{pid,<0.701.0>},
              {id,compat_mode_manager},
              {mfargs,{compat_mode_manager,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2022-09-07T14:28:17.507Z,ns_1@cb.local:memcached_passwords<0.425.0>:replicated_dets:select_from_table:281][ets] Starting select with {users_storage,
                               [{{docv2,{auth,{'_',local}},'_','_'},
                                 [],
                                 ['$_']}],
                               100}
[ns_server:debug,2022-09-07T14:28:17.507Z,ns_1@cb.local:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
audit_decriptors ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780097}}]},
 {8243,
  [{name,<<"mutate document">>},
   {description,<<"Document was mutated via the REST API">>},
   {enabled,true},
   {module,ns_server}]},
 {8255,
  [{name,<<"read document">>},
   {description,<<"Document was read via the REST API">>},
   {enabled,false},
   {module,ns_server}]},
 {8257,
  [{name,<<"alert email sent">>},
   {description,<<"An alert email was successfully sent">>},
   {enabled,true},
   {module,ns_server}]},
 {8265,
  [{name,<<"RBAC information retrieved">>},
   {description,<<"RBAC information was retrieved">>},
   {enabled,true},
   {module,ns_server}]},
 {20480,
  [{name,<<"opened DCP connection">>},
   {description,<<"opened DCP connection">>},
   {enabled,true},
   {module,memcached}]},
 {20482,
  [{name,<<"external memcached bucket flush">>},
   {description,<<"External user flushed the content of a memcached bucket">>},
   {enabled,true},
   {module,memcached}]},
 {20483,
  [{name,<<"invalid packet">>},
   {description,<<"Rejected an invalid packet">>},
   {enabled,true},
   {module,memcached}]},
 {20485,
  [{name,<<"authentication succeeded">>},
   {description,<<"Authentication to the cluster succeeded">>},
   {enabled,false},
   {module,memcached}]},
 {20488,
  [{name,<<"document read">>},
   {description,<<"Document was read">>},
   {enabled,false},
   {module,memcached}]},
 {20489,
  [{name,<<"document locked">>},
   {description,<<"Document was locked">>},
   {enabled,false},
   {module,memcached}]},
 {20490,
  [{name,<<"document modify">>},
   {description,<<"Document was modified">>},
   {enabled,false},
   {module,memcached}]},
 {20491,
  [{name,<<"document delete">>},
   {description,<<"Document was deleted">>},
   {enabled,false},
   {module,memcached}]},
 {20492,
  [{name,<<"select bucket">>},
   {description,<<"The specified bucket was selected">>},
   {enabled,true},
   {module,memcached}]},
 {20493,
  [{name,<<"session terminated">>},
   {description,<<"Session to the cluster has terminated">>},
   {enabled,false},
   {module,memcached}]},
 {20494,
  [{name,<<"tenant rate limited">>},
   {description,<<"The given tenant was rate limited">>},
   {enabled,true},
   {module,memcached}]},
 {28672,
  [{name,<<"SELECT statement">>},
   {description,<<"A N1QL SELECT statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28673,
  [{name,<<"EXPLAIN statement">>},
   {description,<<"A N1QL EXPLAIN statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28674,
  [{name,<<"PREPARE statement">>},
   {description,<<"A N1QL PREPARE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28675,
  [{name,<<"INFER statement">>},
   {description,<<"A N1QL INFER statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28676,
  [{name,<<"INSERT statement">>},
   {description,<<"A N1QL INSERT statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28677,
  [{name,<<"UPSERT statement">>},
   {description,<<"A N1QL UPSERT statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28678,
  [{name,<<"DELETE statement">>},
   {description,<<"A N1QL DELETE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28679,
  [{name,<<"UPDATE statement">>},
   {description,<<"A N1QL UPDATE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28680,
  [{name,<<"MERGE statement">>},
   {description,<<"A N1QL MERGE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28681,
  [{name,<<"CREATE INDEX statement">>},
   {description,<<"A N1QL CREATE INDEX statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28682,
  [{name,<<"DROP INDEX statement">>},
   {description,<<"A N1QL DROP INDEX statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28683,
  [{name,<<"ALTER INDEX statement">>},
   {description,<<"A N1QL ALTER INDEX statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28684,
  [{name,<<"BUILD INDEX statement">>},
   {description,<<"A N1QL BUILD INDEX statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28685,
  [{name,<<"GRANT ROLE statement">>},
   {description,<<"A N1QL GRANT ROLE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28686,
  [{name,<<"REVOKE ROLE statement">>},
   {description,<<"A N1QL REVOKE ROLE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28687,
  [{name,<<"UNRECOGNIZED statement">>},
   {description,<<"An unrecognized statement was received by the N1QL query engine">>},
   {enabled,false},
   {module,n1ql}]},
 {28688,
  [{name,<<"CREATE PRIMARY INDEX statement">>},
   {description,<<"A N1QL CREATE PRIMARY INDEX statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28689,
  [{name,<<"/admin/stats API request">>},
   {description,<<"An HTTP request was made to the API at /admin/stats.">>},
   {enabled,false},
   {module,n1ql}]},
 {28690,
  [{name,<<"/admin/vitals API request">>},
   {description,<<"An HTTP request was made to the API at /admin/vitals.">>},
   {enabled,false},
   {module,n1ql}]},
 {28691,
  [{name,<<"/admin/prepareds API request">>},
   {description,<<"An HTTP request was made to the API at /admin/prepareds.">>},
   {enabled,false},
   {module,n1ql}]},
 {28692,
  [{name,<<"/admin/active_requests API request">>},
   {description,<<"An HTTP request was made to the API at /admin/active_requests.">>},
   {enabled,false},
   {module,n1ql}]},
 {28693,
  [{name,<<"/admin/indexes/prepareds API request">>},
   {description,<<"An HTTP request was made to the API at /admin/indexes/prepareds.">>},
   {enabled,false},
   {module,n1ql}]},
 {28694,
  [{name,<<"/admin/indexes/active_requests API request">>},
   {description,<<"An HTTP request was made to the API at /admin/indexes/active_requests.">>},
   {enabled,false},
   {module,n1ql}]},
 {28695,
  [{name,<<"/admin/indexes/completed_requests API request">>},
   {description,<<"An HTTP request was made to the API at /admin/indexes/completed_requests.">>},
   {enabled,false},
   {module,n1ql}]},
 {28697,
  [{name,<<"/admin/ping API request">>},
   {description,<<"An HTTP request was made to the API at /admin/ping.">>},
   {enabled,false},
   {module,n1ql}]},
 {28698,
  [{name,<<"/admin/config API request">>},
   {description,<<"An HTTP request was made to the API at /admin/config.">>},
   {enabled,false},
   {module,n1ql}]},
 {28699,
  [{name,<<"/admin/ssl_cert API request">>},
   {description,<<"An HTTP request was made to the API at /admin/ssl_cert.">>},
   {enabled,false},
   {module,n1ql}]},
 {28700,
  [{name,<<"/admin/settings API request">>},
   {description,<<"An HTTP request was made to the API at /admin/settings.">>},
   {enabled,false},
   {module,n1ql}]},
 {28701,
  [{name,<<"/admin/clusters API request">>},
   {description,<<"An HTTP request was made to the API at /admin/clusters.">>},
   {enabled,false},
   {module,n1ql}]},
 {28702,
  [{name,<<"/admin/completed_requests API request">>},
   {description,<<"An HTTP request was made to the API at /admin/completed_requests.">>},
   {enabled,false},
   {module,n1ql}]},
 {28704,
  [{name,<<"/admin/functions API request">>},
   {description,<<"An HTTP request was made to the API at /admin/functions.">>},
   {enabled,false},
   {module,n1ql}]},
 {28705,
  [{name,<<"/admin/indexes/functions API request">>},
   {description,<<"An HTTP request was made to the API at /admin/indexes/functions.">>},
   {enabled,false},
   {module,n1ql}]},
 {28706,
  [{name,<<"CREATE FUNCTION statement">>},
   {description,<<"A N1QL CREATE FUNCTION statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28707,
  [{name,<<"DROP FUNCTION statement">>},
   {description,<<"A N1QL DROP FUNCTION statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28708,
  [{name,<<"EXECUTE FUNCTION statement">>},
   {description,<<"A N1QL EXECUTE FUNCTION statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28709,
  [{name,<<"/admin/tasks API request">>},
   {description,<<"An HTTP request was made to the API at /admin/tasks.">>},
   {enabled,false},
   {module,n1ql}]},
 {28710,
  [{name,<<"/admin/indexes/tasks API request">>},
   {description,<<"An HTTP request was made to the API at /admin/indexes/tasks.">>},
   {enabled,false},
   {module,n1ql}]},
 {28711,
  [{name,<<"/admin/dictionary_cache API request">>},
   {description,<<"An HTTP request was made to the API at /admin/dictionary_cache.">>},
   {enabled,false},
   {module,n1ql}]},
 {28712,
  [{name,<<"/admin/indexes/dictionary_cache API request">>},
   {description,<<"An HTTP request was made to the API at /admin/indexes/dictionary_cache.">>},
   {enabled,false},
   {module,n1ql}]},
 {28713,
  [{name,<<"CREATE SCOPE statement">>},
   {description,<<"A N1QL CREATE SCOPE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28714,
  [{name,<<"DROP SCOPE statement">>},
   {description,<<"A N1QL DROP SCOPE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28715,
  [{name,<<"CREATE COLLECTION statement">>},
   {description,<<"A N1QL CREATE COLLECTION statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28716,
  [{name,<<"DROP COLLECTION statement">>},
   {description,<<"A N1QL DROP COLLECTION statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28717,
  [{name,<<"FLUSH COLLECTION statement">>},
   {description,<<"A N1QL FLUSH COLLECTION statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28718,
  [{name,<<"UPDATE STATISTICS statement">>},
   {description,<<"A N1QL UPDATE STATISTICS statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28719,
  [{name,<<"ADVISE statement">>},
   {description,<<"A N1QL ADVISE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28720,
  [{name,<<"START TRANSACTION statement">>},
   {description,<<"A N1QL START TRANSACTION statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28721,
  [{name,<<"COMMIT TRANSACTION statement">>},
   {description,<<"A N1QL COMMIT TRANSACTION statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28722,
  [{name,<<"ROLLBACK TRANSACTION statement">>},
   {description,<<"A N1QL ROLLBACK TRANSACTION statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28723,
  [{name,<<"ROLLBACK TRANSACTION TO SAVEPOINT statement">>},
   {description,<<"A N1QL ROLLBACK TRANSACTION TO SAVEPOINT statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28724,
  [{name,<<"SET TRANSACTION ISOLATION statement">>},
   {description,<<"A N1QL SET TRANSACTION ISOLATION statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28725,
  [{name,<<"SAVEPOINT statement">>},
   {description,<<"A N1QL SAVEPOINT statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28726,
  [{name,<<"/admin/transactions API request">>},
   {description,<<"An HTTP request was made to the API at /admin/transactions.">>},
   {enabled,false},
   {module,n1ql}]},
 {28727,
  [{name,<<"/admin/indexes/transactions API request">>},
   {description,<<"An HTTP request was made to the API at /admin/indexes/transactions.">>},
   {enabled,false},
   {module,n1ql}]},
 {28728,
  [{name,<<"N1QL backup / restore API request">>},
   {description,<<"An HTTP request was made to archive or restore N1QL metadata ">>},
   {enabled,false},
   {module,n1ql}]},
 {28729,
  [{name,<<"/admin/shutdown API request">>},
   {description,<<"An HTTP request was made to initate graceful shutdown">>},
   {enabled,false},
   {module,n1ql}]},
 {32768,
  [{name,<<"Create Function">>},
   {description,<<"Request to create or update eventing function definition">>},
   {enabled,true},
   {module,eventing}]},
 {32769,
  [{name,<<"Delete Function">>},
   {description,<<"Request to delete eventing function definition">>},
   {enabled,true},
   {module,eventing}]},
 {32770,
  [{name,<<"Fetch Functions">>},
   {description,<<"Request to fetch eventing function definition">>},
   {enabled,false},
   {module,eventing}]},
 {32771,
  [{name,<<"List Deployed">>},
   {description,<<"Request to fetch eventing deployed functions list">>},
   {enabled,false},
   {module,eventing}]},
 {32772,
  [{name,<<"Fetch Drafts">>},
   {description,<<"Request to fetch eventing function draft definitions">>},
   {enabled,false},
   {module,eventing}]},
 {32773,
  [{name,<<"Delete Drafts">>},
   {description,<<"Request to delete eventing function draft definitions">>},
   {enabled,true},
   {module,eventing}]},
 {32774,
  [{name,<<"Save Draft">>},
   {description,<<"Request to save a draft definition">>},
   {enabled,true},
   {module,eventing}]},
 {32775,
  [{name,<<"Start Debug">>},
   {description,<<"Request to start eventing function debugger">>},
   {enabled,true},
   {module,eventing}]},
 {32776,
  [{name,<<"Stop Debug">>},
   {description,<<"Request to stop eventing function debugger">>},
   {enabled,true},
   {module,eventing}]},
 {32777,
  [{name,<<"Start Tracing">>},
   {description,<<"Request to start tracing eventing function e"...>>},
   {enabled,true},
   {module,eventing}]},
 {32778,
  [{name,<<"Stop Tracing">>},
   {description,<<"Request to stop tracing eventing functio"...>>},
   {enabled,true},
   {module,eventing}]},
 {32779,
  [{name,<<"Set Settings">>},
   {description,<<"Request to save settings for an even"...>>},
   {enabled,true},
   {module,eventing}]},
 {32780,
  [{name,<<"Fetch Config">>},
   {description,<<"Request to fetch eventing config">>},
   {enabled,false},
   {module,eventing}]},
 {32781,
  [{name,<<"Save Config">>},
   {description,<<"Request to save eventing con"...>>},
   {enabled,true},
   {module,eventing}]},
 {32783,
  [{name,<<"Get Settings">>},
   {description,<<"Request to fetch eventin"...>>},
   {enabled,false},
   {module,eventing}]},
 {32784,
  [{name,<<"Import Functions">>},
   {description,<<"Request to import on"...>>},
   {enabled,true},
   {module,eventing}]},
 {32785,
  [{name,<<"Export Functions">>},
   {description,<<"Request to expor"...>>},
   {enabled,false},
   {module,eventing}]},
 {32786,
  [{name,<<"List Running">>},
   {description,<<"Request to f"...>>},
   {enabled,false},
   {module,eventing}]},
 {32789,
  [{name,<<"Deploy Funct"...>>},
   {description,<<"Request "...>>},
   {enabled,true},
   {module,eventing}]},
 {32790,
  [{name,<<"Undeploy"...>>},
   {description,<<"Requ"...>>},
   {enabled,true},
   {module,...}]},
 {32791,[{name,<<"Paus"...>>},{description,<<...>>},{enabled,...},{...}]},
 {32792,[{name,<<...>>},{description,...},{...}|...]},
 {32793,[{name,...},{...}|...]},
 {32794,[{...}|...]},
 {32795,[...]},
 {32796,...},
 {...}|...]
[ns_server:debug,2022-09-07T14:28:17.508Z,ns_1@cb.local:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
cluster_compat_version ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{4,63829780097}}]},7,1]
[ns_server:debug,2022-09-07T14:28:17.508Z,ns_1@cb.local:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{metakv,<<"/indexing/settings/config">>} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780097}}]}|
 <<"{\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.rebalance.redistribute_indexes\":false,\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.log_level\":\"info\",\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.compaction.mi"...>>]
[ns_server:debug,2022-09-07T14:28:17.508Z,ns_1@cb.local:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{local_changes_count,<<"1678dfae96c38e07dff43c49b9f6967b">>} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{9,63829780097}}]}]
[ns_server:debug,2022-09-07T14:28:17.509Z,ns_1@cb.local:memcached_refresh<0.283.0>:memcached_refresh:handle_call:49]File rename from "/opt/couchbase/var/lib/couchbase/isasl.pw.tmp" to "/opt/couchbase/var/lib/couchbase/isasl.pw" is requested
[error_logger:info,2022-09-07T14:28:17.512Z,ns_1@cb.local:ns_orchestrator_child_sup<0.740.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_orchestrator_child_sup}
    started: [{pid,<0.743.0>},
              {id,ns_janitor_server},
              {mfargs,{ns_janitor_server,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2022-09-07T14:28:17.514Z,ns_1@cb.local:ns_ssl_services_setup<0.287.0>:ns_ssl_services_setup:maybe_store_ca_certs:672]Considering to store CA certs
[ns_server:debug,2022-09-07T14:28:17.517Z,ns_1@cb.local:memcached_permissions<0.430.0>:memcached_cfg:write_cfg:148]Writing config file for: "/opt/couchbase/var/lib/couchbase/config/memcached.rbac"
[ns_server:info,2022-09-07T14:28:17.517Z,ns_1@cb.local:ns_orchestrator_child_sup<0.740.0>:misc:start_singleton:901]start_singleton(gen_server, start_link, [{via,leader_registry,
                                          auto_reprovision},
                                         auto_reprovision,[],[]]): started as <0.744.0> on 'ns_1@cb.local'

[error_logger:info,2022-09-07T14:28:17.518Z,ns_1@cb.local:ns_orchestrator_child_sup<0.740.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_orchestrator_child_sup}
    started: [{pid,<0.744.0>},
              {id,auto_reprovision},
              {mfargs,{auto_reprovision,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2022-09-07T14:28:17.520Z,ns_1@cb.local:memcached_permissions<0.430.0>:replicated_dets:select_from_table:281][ets] Starting select with {users_storage,
                               [{{docv2,{user,{'_',local}},'_','_'},
                                 [],
                                 ['$_']}],
                               100}
[ns_server:debug,2022-09-07T14:28:17.522Z,ns_1@cb.local:memcached_refresh<0.283.0>:memcached_refresh:handle_call:49]File rename from "/opt/couchbase/var/lib/couchbase/config/memcached.rbac.tmp" to "/opt/couchbase/var/lib/couchbase/config/memcached.rbac" is requested
[ns_server:debug,2022-09-07T14:28:17.522Z,ns_1@cb.local:memcached_passwords<0.425.0>:memcached_cfg:rename_and_refresh:170]Successfully renamed "/opt/couchbase/var/lib/couchbase/isasl.pw.tmp" to "/opt/couchbase/var/lib/couchbase/isasl.pw"
[ns_server:info,2022-09-07T14:28:17.526Z,ns_1@cb.local:ns_orchestrator_child_sup<0.740.0>:misc:start_singleton:901]start_singleton(gen_server, start_link, [{via,leader_registry,auto_rebalance},
                                         auto_rebalance,[],[]]): started as <0.746.0> on 'ns_1@cb.local'

[error_logger:info,2022-09-07T14:28:17.526Z,ns_1@cb.local:ns_orchestrator_child_sup<0.740.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_orchestrator_child_sup}
    started: [{pid,<0.746.0>},
              {id,auto_rebalance},
              {mfargs,{auto_rebalance,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:info,2022-09-07T14:28:17.526Z,ns_1@cb.local:ns_orchestrator_child_sup<0.740.0>:misc:start_singleton:901]start_singleton(gen_statem, start_link, [{via,leader_registry,ns_orchestrator},
                                         ns_orchestrator,[],[]]): started as <0.747.0> on 'ns_1@cb.local'

[error_logger:info,2022-09-07T14:28:17.526Z,ns_1@cb.local:ns_orchestrator_child_sup<0.740.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_orchestrator_child_sup}
    started: [{pid,<0.747.0>},
              {id,ns_orchestrator},
              {mfargs,{ns_orchestrator,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2022-09-07T14:28:17.527Z,ns_1@cb.local:ns_orchestrator_sup<0.699.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_orchestrator_sup}
    started: [{pid,<0.740.0>},
              {id,ns_orchestrator_child_sup},
              {mfargs,{ns_orchestrator_child_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2022-09-07T14:28:17.527Z,ns_1@cb.local:<0.749.0>:auto_failover:init:192]init auto_failover.
[user:info,2022-09-07T14:28:17.527Z,ns_1@cb.local:<0.749.0>:auto_failover:handle_call:223]Enabled auto-failover with timeout 120 and max count 1
[ns_server:debug,2022-09-07T14:28:17.533Z,ns_1@cb.local:memcached_refresh<0.283.0>:memcached_refresh:handle_cast:55]Refresh of isasl requested
[ns_server:debug,2022-09-07T14:28:17.533Z,ns_1@cb.local:memcached_permissions<0.430.0>:memcached_cfg:rename_and_refresh:170]Successfully renamed "/opt/couchbase/var/lib/couchbase/config/memcached.rbac.tmp" to "/opt/couchbase/var/lib/couchbase/config/memcached.rbac"
[ns_server:debug,2022-09-07T14:28:17.533Z,ns_1@cb.local:memcached_refresh<0.283.0>:memcached_refresh:handle_cast:55]Refresh of rbac requested
[ns_server:debug,2022-09-07T14:28:17.533Z,ns_1@cb.local:memcached_passwords<0.425.0>:memcached_cfg:write_cfg:148]Writing config file for: "/opt/couchbase/var/lib/couchbase/isasl.pw"
[ns_server:warn,2022-09-07T14:28:17.537Z,ns_1@cb.local:memcached_refresh<0.283.0>:ns_memcached:connect:1237]Unable to connect: {error,{badmatch,[{inet,{error,econnrefused}}]}}.
[ns_server:debug,2022-09-07T14:28:17.537Z,ns_1@cb.local:memcached_permissions<0.430.0>:memcached_cfg:write_cfg:148]Writing config file for: "/opt/couchbase/var/lib/couchbase/config/memcached.rbac"
[ns_server:debug,2022-09-07T14:28:17.537Z,ns_1@cb.local:memcached_refresh<0.283.0>:memcached_refresh:handle_info:93]Refresh of [rbac,isasl] failed. Retry in 1000 ms.
[ns_server:warn,2022-09-07T14:28:17.539Z,ns_1@cb.local:memcached_refresh<0.283.0>:ns_memcached:connect:1237]Unable to connect: {error,{badmatch,[{inet,{error,econnrefused}}]}}.
[ns_server:debug,2022-09-07T14:28:17.539Z,ns_1@cb.local:memcached_refresh<0.283.0>:memcached_refresh:handle_info:93]Refresh of [rbac,isasl] failed. Retry in 1000 ms.
[ns_server:info,2022-09-07T14:28:17.540Z,ns_1@cb.local:ns_orchestrator_sup<0.699.0>:misc:start_singleton:901]start_singleton(gen_server, start_link, [{via,leader_registry,auto_failover},
                                         auto_failover,[],[]]): started as <0.749.0> on 'ns_1@cb.local'

[ns_server:debug,2022-09-07T14:28:17.540Z,ns_1@cb.local:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{local_changes_count,<<"1678dfae96c38e07dff43c49b9f6967b">>} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{10,63829780097}}]}]
[error_logger:info,2022-09-07T14:28:17.540Z,ns_1@cb.local:ns_orchestrator_sup<0.699.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_orchestrator_sup}
    started: [{pid,<0.749.0>},
              {id,auto_failover},
              {mfargs,{auto_failover,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2022-09-07T14:28:17.541Z,ns_1@cb.local:memcached_permissions<0.430.0>:replicated_dets:select_from_table:281][ets] Starting select with {users_storage,
                               [{{docv2,{user,{'_',local}},'_','_'},
                                 [],
                                 ['$_']}],
                               100}
[error_logger:info,2022-09-07T14:28:17.541Z,ns_1@cb.local:mb_master_sup<0.685.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,mb_master_sup}
    started: [{pid,<0.699.0>},
              {id,ns_orchestrator_sup},
              {mfargs,{ns_orchestrator_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2022-09-07T14:28:17.541Z,ns_1@cb.local:ns_config_rep<0.444.0>:ns_config_rep:do_push_keys:383]Replicating some config keys ([auto_failover_cfg,
                               {local_changes_count,
                                   <<"1678dfae96c38e07dff43c49b9f6967b">>}]..)
[ns_server:debug,2022-09-07T14:28:17.541Z,ns_1@cb.local:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
auto_failover_cfg ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780097}}]},
 {enabled,true},
 {timeout,120},
 {count,0},
 {max_count,1},
 {failed_over_server_groups,[]},
 {failover_on_data_disk_issues,[{enabled,false},{timePeriod,120}]},
 {failover_server_group,false},
 {can_abort_rebalance,true}]
[ns_server:debug,2022-09-07T14:28:17.544Z,ns_1@cb.local:memcached_refresh<0.283.0>:memcached_refresh:handle_call:49]File rename from "/opt/couchbase/var/lib/couchbase/config/memcached.rbac.tmp" to "/opt/couchbase/var/lib/couchbase/config/memcached.rbac" is requested
[ns_server:info,2022-09-07T14:28:17.548Z,ns_1@cb.local:mb_master_sup<0.685.0>:misc:start_singleton:901]start_singleton(gen_server, start_link, [{via,leader_registry,
                                          tombstone_purger},
                                         tombstone_purger,[],[]]): started as <0.757.0> on 'ns_1@cb.local'

[error_logger:info,2022-09-07T14:28:17.549Z,ns_1@cb.local:mb_master_sup<0.685.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,mb_master_sup}
    started: [{pid,<0.757.0>},
              {id,tombstone_purger},
              {mfargs,{tombstone_purger,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2022-09-07T14:28:17.549Z,ns_1@cb.local:memcached_permissions<0.430.0>:memcached_cfg:rename_and_refresh:170]Successfully renamed "/opt/couchbase/var/lib/couchbase/config/memcached.rbac.tmp" to "/opt/couchbase/var/lib/couchbase/config/memcached.rbac"
[ns_server:debug,2022-09-07T14:28:17.549Z,ns_1@cb.local:memcached_refresh<0.283.0>:memcached_refresh:handle_cast:55]Refresh of rbac requested
[ns_server:debug,2022-09-07T14:28:17.554Z,ns_1@cb.local:<0.759.0>:license_reporting:init:60]Starting license_reporting server
[ns_server:info,2022-09-07T14:28:17.555Z,ns_1@cb.local:mb_master_sup<0.685.0>:misc:start_singleton:901]start_singleton(gen_server, start_link, [{via,leader_registry,
                                          license_reporting},
                                         license_reporting,[],[]]): started as <0.759.0> on 'ns_1@cb.local'

[error_logger:info,2022-09-07T14:28:17.555Z,ns_1@cb.local:mb_master_sup<0.685.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,mb_master_sup}
    started: [{pid,<0.759.0>},
              {id,license_reporting},
              {mfargs,{license_reporting,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2022-09-07T14:28:17.555Z,ns_1@cb.local:leader_registry_sup<0.680.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,leader_registry_sup}
    started: [{pid,<0.683.0>},
              {id,mb_master},
              {mfargs,{mb_master,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2022-09-07T14:28:17.555Z,ns_1@cb.local:leader_services_sup<0.676.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,leader_services_sup}
    started: [{pid,<0.680.0>},
              {id,leader_registry_sup},
              {mfargs,{leader_registry_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2022-09-07T14:28:17.555Z,ns_1@cb.local:<0.675.0>:restartable:start_child:92]Started child process <0.676.0>
  MFA: {leader_services_sup,start_link,[]}
[error_logger:info,2022-09-07T14:28:17.556Z,ns_1@cb.local:ns_server_sup<0.399.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.675.0>},
              {name,leader_services_sup},
              {mfargs,{restartable,start_link,
                                   [{leader_services_sup,start_link,[]},
                                    infinity]}},
              {restart_type,permanent},
              {shutdown,infinity},
              {child_type,supervisor}]
[error_logger:info,2022-09-07T14:28:17.560Z,ns_1@cb.local:ns_server_sup<0.399.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.761.0>},
              {name,ns_tick_agent},
              {mfargs,{ns_tick_agent,start_link,[]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[error_logger:info,2022-09-07T14:28:17.561Z,ns_1@cb.local:ns_server_sup<0.399.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.763.0>},
              {name,master_activity_events_ingress},
              {mfargs,{gen_event,start_link,
                                 [{local,master_activity_events_ingress}]}},
              {restart_type,permanent},
              {shutdown,brutal_kill},
              {child_type,worker}]
[error_logger:info,2022-09-07T14:28:17.561Z,ns_1@cb.local:ns_server_sup<0.399.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.764.0>},
              {name,master_activity_events_timestamper},
              {mfargs,{master_activity_events,start_link_timestamper,[]}},
              {restart_type,permanent},
              {shutdown,brutal_kill},
              {child_type,worker}]
[ns_server:warn,2022-09-07T14:28:17.561Z,ns_1@cb.local:memcached_refresh<0.283.0>:ns_memcached:connect:1237]Unable to connect: {error,{badmatch,[{inet,{error,econnrefused}}]}}.
[ns_server:debug,2022-09-07T14:28:17.562Z,ns_1@cb.local:memcached_refresh<0.283.0>:memcached_refresh:handle_info:93]Refresh of [rbac,isasl] failed. Retry in 1000 ms.
[error_logger:info,2022-09-07T14:28:17.570Z,ns_1@cb.local:ns_server_sup<0.399.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.765.0>},
              {name,master_activity_events_pids_watcher},
              {mfargs,{master_activity_events_pids_watcher,start_link,[]}},
              {restart_type,permanent},
              {shutdown,brutal_kill},
              {child_type,worker}]
[error_logger:info,2022-09-07T14:28:17.578Z,ns_1@cb.local:ns_server_sup<0.399.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.768.0>},
              {name,master_activity_events_keeper},
              {mfargs,{master_activity_events_keeper,start_link,[]}},
              {restart_type,permanent},
              {shutdown,brutal_kill},
              {child_type,worker}]
[error_logger:info,2022-09-07T14:28:17.590Z,ns_1@cb.local:health_monitor_sup<0.770.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,health_monitor_sup}
    started: [{pid,<0.771.0>},
              {id,ns_server_monitor},
              {mfargs,{ns_server_monitor,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2022-09-07T14:28:17.590Z,ns_1@cb.local:health_monitor_sup<0.770.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,health_monitor_sup}
    started: [{pid,<0.773.0>},
              {id,service_monitor_children_sup},
              {mfargs,{supervisor,start_link,
                                  [{local,service_monitor_children_sup},
                                   health_monitor_sup,child]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2022-09-07T14:28:17.591Z,ns_1@cb.local:health_monitor_sup<0.770.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,health_monitor_sup}
    started: [{pid,<0.774.0>},
              {id,service_monitor_worker},
              {mfargs,{erlang,apply,[#Fun<health_monitor_sup.0.70530162>,[]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2022-09-07T14:28:17.603Z,ns_1@cb.local:health_monitor_sup<0.770.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,health_monitor_sup}
    started: [{pid,<0.780.0>},
              {id,node_monitor},
              {mfargs,{node_monitor,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2022-09-07T14:28:17.608Z,ns_1@cb.local:health_monitor_sup<0.770.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,health_monitor_sup}
    started: [{pid,<0.786.0>},
              {id,node_status_analyzer},
              {mfargs,{node_status_analyzer,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2022-09-07T14:28:17.609Z,ns_1@cb.local:ns_server_sup<0.399.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.770.0>},
              {name,health_monitor_sup},
              {mfargs,{health_monitor_sup,start_link,[]}},
              {restart_type,permanent},
              {shutdown,infinity},
              {child_type,supervisor}]
[ns_server:debug,2022-09-07T14:28:17.611Z,ns_1@cb.local:memcached_passwords<0.425.0>:replicated_dets:select_from_table:281][ets] Starting select with {users_storage,
                               [{{docv2,{auth,{'_',local}},'_','_'},
                                 [],
                                 ['$_']}],
                               100}
[ns_server:debug,2022-09-07T14:28:17.612Z,ns_1@cb.local:memcached_refresh<0.283.0>:memcached_refresh:handle_call:49]File rename from "/opt/couchbase/var/lib/couchbase/isasl.pw.tmp" to "/opt/couchbase/var/lib/couchbase/isasl.pw" is requested
[error_logger:info,2022-09-07T14:28:17.614Z,ns_1@cb.local:ns_server_sup<0.399.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.788.0>},
              {name,rebalance_agent},
              {mfargs,{rebalance_agent,start_link,[]}},
              {restart_type,permanent},
              {shutdown,5000},
              {child_type,worker}]
[ns_server:debug,2022-09-07T14:28:17.620Z,ns_1@cb.local:memcached_refresh<0.283.0>:memcached_refresh:handle_cast:55]Refresh of isasl requested
[ns_server:debug,2022-09-07T14:28:17.620Z,ns_1@cb.local:memcached_passwords<0.425.0>:memcached_cfg:rename_and_refresh:170]Successfully renamed "/opt/couchbase/var/lib/couchbase/isasl.pw.tmp" to "/opt/couchbase/var/lib/couchbase/isasl.pw"
[ns_server:debug,2022-09-07T14:28:17.621Z,ns_1@cb.local:memcached_passwords<0.425.0>:memcached_cfg:write_cfg:148]Writing config file for: "/opt/couchbase/var/lib/couchbase/isasl.pw"
[error_logger:info,2022-09-07T14:28:17.623Z,ns_1@cb.local:ns_server_sup<0.399.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.789.0>},
              {name,ns_rebalance_report_manager},
              {mfargs,{ns_rebalance_report_manager,start_link,[]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[error_logger:info,2022-09-07T14:28:17.624Z,ns_1@cb.local:ns_server_nodes_sup<0.273.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.399.0>},
              {name,ns_server_sup},
              {mfargs,{ns_server_sup,start_link,[]}},
              {restart_type,permanent},
              {shutdown,infinity},
              {child_type,supervisor}]
[ns_server:debug,2022-09-07T14:28:17.624Z,ns_1@cb.local:ns_server_nodes_sup<0.273.0>:one_shot_barrier:notify:21]Notifying on barrier menelaus_barrier
[ns_server:debug,2022-09-07T14:28:17.624Z,ns_1@cb.local:menelaus_barrier<0.281.0>:one_shot_barrier:barrier_body:56]Barrier menelaus_barrier got notification from <0.273.0>
[ns_server:debug,2022-09-07T14:28:17.624Z,ns_1@cb.local:ns_server_nodes_sup<0.273.0>:one_shot_barrier:notify:26]Successfuly notified on barrier menelaus_barrier
[ns_server:debug,2022-09-07T14:28:17.624Z,ns_1@cb.local:<0.270.0>:restartable:start_child:92]Started child process <0.273.0>
  MFA: {ns_server_nodes_sup,start_link,[]}
[error_logger:info,2022-09-07T14:28:17.624Z,ns_1@cb.local:ns_server_cluster_sup<0.218.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_cluster_sup}
    started: [{pid,<0.270.0>},
              {id,ns_server_nodes_sup},
              {mfargs,{restartable,start_link,
                                   [{ns_server_nodes_sup,start_link,[]},
                                    infinity]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2022-09-07T14:28:17.635Z,ns_1@cb.local:compiled_roles_cache<0.362.0>:menelaus_roles:build_compiled_roles:998]Compile roles for user {[],anonymous}
[ns_server:debug,2022-09-07T14:28:17.635Z,ns_1@cb.local:compiled_roles_cache<0.362.0>:menelaus_roles:build_compiled_roles:998]Compile roles for user {"@",admin}
[error_logger:info,2022-09-07T14:28:17.639Z,ns_1@cb.local:ns_server_cluster_sup<0.218.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_cluster_sup}
    started: [{pid,<0.791.0>},
              {id,remote_api},
              {mfargs,{remote_api,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2022-09-07T14:28:17.640Z,ns_1@cb.local:root_sup<0.196.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,root_sup}
    started: [{pid,<0.218.0>},
              {id,ns_server_cluster_sup},
              {mfargs,{ns_server_cluster_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2022-09-07T14:28:17.640Z,ns_1@cb.local:application_controller<0.44.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    application: ns_server
    started_at: 'ns_1@cb.local'

[ns_server:debug,2022-09-07T14:28:17.640Z,ns_1@cb.local:<0.9.0>:child_erlang:child_loop:128]160: Entered child_loop
[ns_server:debug,2022-09-07T14:28:17.671Z,ns_1@cb.local:json_rpc_connection-saslauthd-saslauthd-port<0.793.0>:json_rpc_connection:init:68]Observed revrpc connection: label "saslauthd-saslauthd-port", handling process <0.793.0>
[ns_server:debug,2022-09-07T14:28:17.671Z,ns_1@cb.local:json_rpc_connection-goxdcr-cbauth<0.792.0>:json_rpc_connection:init:68]Observed revrpc connection: label "goxdcr-cbauth", handling process <0.792.0>
[ns_server:debug,2022-09-07T14:28:17.671Z,ns_1@cb.local:menelaus_cbauth<0.558.0>:menelaus_cbauth:handle_cast:101]Observed json rpc process {"goxdcr-cbauth",<0.792.0>} started
[ns_server:debug,2022-09-07T14:28:17.687Z,ns_1@cb.local:memcached_passwords<0.425.0>:replicated_dets:select_from_table:281][ets] Starting select with {users_storage,
                               [{{docv2,{auth,{'_',local}},'_','_'},
                                 [],
                                 ['$_']}],
                               100}
[ns_server:debug,2022-09-07T14:28:17.732Z,ns_1@cb.local:compiled_roles_cache<0.362.0>:menelaus_roles:build_compiled_roles:998]Compile roles for user {"@goxdcr",admin}
[ns_server:debug,2022-09-07T14:28:17.813Z,ns_1@cb.local:memcached_refresh<0.283.0>:memcached_refresh:handle_info:89]Refresh of [rbac,isasl] succeeded
[ns_server:debug,2022-09-07T14:28:17.813Z,ns_1@cb.local:memcached_refresh<0.283.0>:memcached_refresh:handle_call:49]File rename from "/opt/couchbase/var/lib/couchbase/isasl.pw.tmp" to "/opt/couchbase/var/lib/couchbase/isasl.pw" is requested
[ns_server:debug,2022-09-07T14:28:17.818Z,ns_1@cb.local:memcached_refresh<0.283.0>:memcached_refresh:handle_cast:55]Refresh of isasl requested
[ns_server:debug,2022-09-07T14:28:17.818Z,ns_1@cb.local:memcached_passwords<0.425.0>:memcached_cfg:rename_and_refresh:170]Successfully renamed "/opt/couchbase/var/lib/couchbase/isasl.pw.tmp" to "/opt/couchbase/var/lib/couchbase/isasl.pw"
[ns_server:debug,2022-09-07T14:28:17.828Z,ns_1@cb.local:memcached_refresh<0.283.0>:memcached_refresh:handle_info:89]Refresh of [isasl] succeeded
[ns_server:debug,2022-09-07T14:28:18.025Z,ns_1@cb.local:ns_ssl_services_setup<0.287.0>:ns_ssl_services_setup:notify_services:963]Going to notify following services: [memcached,capi_ssl_service]
[ns_server:info,2022-09-07T14:28:18.025Z,ns_1@cb.local:<0.810.0>:ns_ssl_services_setup:notify_service:999]Successfully notified service memcached
[ns_server:info,2022-09-07T14:28:18.035Z,ns_1@cb.local:<0.811.0>:ns_ssl_services_setup:notify_service:999]Successfully notified service capi_ssl_service
[ns_server:info,2022-09-07T14:28:18.035Z,ns_1@cb.local:ns_ssl_services_setup<0.287.0>:ns_ssl_services_setup:notify_services:979]Succesfully notified services [capi_ssl_service,memcached]
[ns_server:debug,2022-09-07T14:28:18.256Z,ns_1@cb.local:terse_cluster_info_uploader<0.613.0>:terse_cluster_info_uploader:handle_info:42]Refreshing terse cluster info with <<"{\"rev\":14,\"nodesExt\":[{\"services\":{\"capi\":8092,\"capiSSL\":18092,\"kv\":11210,\"kvSSL\":11207,\"mgmt\":8091,\"mgmtSSL\":18091,\"projector\":9999},\"thisNode\":true}],\"clusterCapabilitiesVer\":[1,0],\"clusterCapabilities\":{\"n1ql\":[\"enhancedPreparedStatements\"]},\"revEpoch\":1}">>
[ns_server:info,2022-09-07T14:28:18.468Z,ns_1@cb.local:memcached_config_mgr<0.598.0>:memcached_config_mgr:push_tls_config:229]Successfully pushed TLS config to memcached
[ns_server:info,2022-09-07T14:28:18.470Z,ns_1@cb.local:memcached_config_mgr<0.598.0>:memcached_config_mgr:push_tls_config:225]Pushing TLS config to memcached:
{[{<<"private key">>,
   <<"/opt/couchbase/var/lib/couchbase/config/certs/pkey.pem">>},
  {<<"certificate chain">>,
   <<"/opt/couchbase/var/lib/couchbase/config/certs/chain.pem">>},
  {<<"CA file">>,<<"/opt/couchbase/var/lib/couchbase/config/certs/ca.pem">>},
  {<<"minimum version">>,<<"TLS 1.2">>},
  {<<"cipher list">>,
   {[{<<"TLS 1.2">>,<<"HIGH">>},
     {<<"TLS 1.3">>,
      <<"TLS_AES_256_GCM_SHA384:TLS_AES_128_GCM_SHA256:TLS_CHACHA20_POLY1305_SHA256">>}]}},
  {<<"cipher order">>,true},
  {<<"client cert auth">>,<<"disabled">>}]}
[ns_server:info,2022-09-07T14:28:18.477Z,ns_1@cb.local:memcached_config_mgr<0.598.0>:memcached_config_mgr:push_tls_config:229]Successfully pushed TLS config to memcached
[ns_server:debug,2022-09-07T14:28:18.528Z,ns_1@cb.local:<0.749.0>:auto_failover_logic:log_master_activity:141]Transitioned node {'ns_1@cb.local',<<"1678dfae96c38e07dff43c49b9f6967b">>} state new -> up
[ns_server:debug,2022-09-07T14:28:18.983Z,ns_1@cb.local:compiled_roles_cache<0.362.0>:menelaus_roles:build_compiled_roles:998]Compile roles for user {"<ud>Administrator</ud>",admin}
[ns_server:debug,2022-09-07T14:28:18.984Z,ns_1@cb.local:<0.529.0>:menelaus_web:check_bucket_uuid:1222]Attempt to access non existent bucket "todo"
[ns_server:debug,2022-09-07T14:28:19.504Z,ns_1@cb.local:<0.529.0>:menelaus_web:check_bucket_uuid:1222]Attempt to access non existent bucket "todo"
[ns_server:debug,2022-09-07T14:28:20.011Z,ns_1@cb.local:<0.529.0>:menelaus_web:check_bucket_uuid:1222]Attempt to access non existent bucket "todo"
[ns_server:debug,2022-09-07T14:28:20.521Z,ns_1@cb.local:<0.529.0>:menelaus_web:check_bucket_uuid:1222]Attempt to access non existent bucket "todo"
[ns_server:debug,2022-09-07T14:28:21.023Z,ns_1@cb.local:<0.529.0>:menelaus_web:check_bucket_uuid:1222]Attempt to access non existent bucket "todo"
[ns_server:debug,2022-09-07T14:28:21.541Z,ns_1@cb.local:<0.529.0>:menelaus_web:check_bucket_uuid:1222]Attempt to access non existent bucket "todo"
[ns_server:debug,2022-09-07T14:28:22.050Z,ns_1@cb.local:<0.529.0>:menelaus_web:check_bucket_uuid:1222]Attempt to access non existent bucket "todo"
[ns_server:debug,2022-09-07T14:28:22.560Z,ns_1@cb.local:<0.529.0>:menelaus_web:check_bucket_uuid:1222]Attempt to access non existent bucket "todo"
[ns_server:debug,2022-09-07T14:28:23.071Z,ns_1@cb.local:<0.529.0>:menelaus_web:check_bucket_uuid:1222]Attempt to access non existent bucket "todo"
[ns_server:debug,2022-09-07T14:28:23.574Z,ns_1@cb.local:<0.529.0>:menelaus_web:check_bucket_uuid:1222]Attempt to access non existent bucket "todo"
[ns_server:debug,2022-09-07T14:28:24.086Z,ns_1@cb.local:<0.529.0>:menelaus_web:check_bucket_uuid:1222]Attempt to access non existent bucket "todo"
[ns_server:debug,2022-09-07T14:28:24.593Z,ns_1@cb.local:<0.529.0>:menelaus_web:check_bucket_uuid:1222]Attempt to access non existent bucket "todo"
[ns_server:debug,2022-09-07T14:28:25.110Z,ns_1@cb.local:<0.529.0>:menelaus_web:check_bucket_uuid:1222]Attempt to access non existent bucket "todo"
[ns_server:debug,2022-09-07T14:28:25.613Z,ns_1@cb.local:<0.529.0>:menelaus_web:check_bucket_uuid:1222]Attempt to access non existent bucket "todo"
[ns_server:debug,2022-09-07T14:28:26.117Z,ns_1@cb.local:<0.529.0>:menelaus_web:check_bucket_uuid:1222]Attempt to access non existent bucket "todo"
[ns_server:debug,2022-09-07T14:28:26.639Z,ns_1@cb.local:<0.529.0>:menelaus_web:check_bucket_uuid:1222]Attempt to access non existent bucket "todo"
[ns_server:debug,2022-09-07T14:28:27.141Z,ns_1@cb.local:<0.529.0>:menelaus_web:check_bucket_uuid:1222]Attempt to access non existent bucket "todo"
[ns_server:debug,2022-09-07T14:28:27.651Z,ns_1@cb.local:<0.529.0>:menelaus_web:check_bucket_uuid:1222]Attempt to access non existent bucket "todo"
[ns_server:debug,2022-09-07T14:28:28.161Z,ns_1@cb.local:<0.529.0>:menelaus_web:check_bucket_uuid:1222]Attempt to access non existent bucket "todo"
[ns_server:debug,2022-09-07T14:28:28.670Z,ns_1@cb.local:<0.529.0>:menelaus_web:check_bucket_uuid:1222]Attempt to access non existent bucket "todo"
[ns_server:debug,2022-09-07T14:28:28.992Z,ns_1@cb.local:compiled_roles_cache<0.362.0>:menelaus_roles:build_compiled_roles:998]Compile roles for user {"@prometheus",stats_reader}
[ns_server:debug,2022-09-07T14:28:29.187Z,ns_1@cb.local:<0.529.0>:menelaus_web:check_bucket_uuid:1222]Attempt to access non existent bucket "todo"
[ns_server:debug,2022-09-07T14:28:29.690Z,ns_1@cb.local:<0.529.0>:menelaus_web:check_bucket_uuid:1222]Attempt to access non existent bucket "todo"
[ns_server:debug,2022-09-07T14:28:30.201Z,ns_1@cb.local:<0.529.0>:menelaus_web:check_bucket_uuid:1222]Attempt to access non existent bucket "todo"
[ns_server:debug,2022-09-07T14:28:30.710Z,ns_1@cb.local:<0.529.0>:menelaus_web:check_bucket_uuid:1222]Attempt to access non existent bucket "todo"
[ns_server:debug,2022-09-07T14:28:31.214Z,ns_1@cb.local:<0.529.0>:menelaus_web:check_bucket_uuid:1222]Attempt to access non existent bucket "todo"
[ns_server:debug,2022-09-07T14:28:31.726Z,ns_1@cb.local:<0.529.0>:menelaus_web:check_bucket_uuid:1222]Attempt to access non existent bucket "todo"
[ns_server:debug,2022-09-07T14:28:32.235Z,ns_1@cb.local:<0.529.0>:menelaus_web:check_bucket_uuid:1222]Attempt to access non existent bucket "todo"
[ns_server:debug,2022-09-07T14:28:47.324Z,ns_1@cb.local:compaction_daemon<0.667.0>:compaction_daemon:process_scheduler_message:1359]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2022-09-07T14:28:47.324Z,ns_1@cb.local:compaction_daemon<0.667.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-07T14:28:47.324Z,ns_1@cb.local:compaction_daemon<0.667.0>:compaction_daemon:process_scheduler_message:1359]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2022-09-07T14:28:47.324Z,ns_1@cb.local:compaction_daemon<0.667.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-07T14:28:50.305Z,ns_1@cb.local:compiled_roles_cache<0.362.0>:menelaus_roles:build_compiled_roles:998]Compile roles for user {[],wrong_token}
[ns_server:debug,2022-09-07T14:29:13.561Z,ns_1@cb.local:compiled_roles_cache<0.362.0>:menelaus_roles:build_compiled_roles:998]Compile roles for user {"<ud>admin</ud>",admin}
[ns_server:debug,2022-09-07T14:29:13.561Z,ns_1@cb.local:<0.2619.0>:menelaus_web:check_bucket_uuid:1222]Attempt to access non existent bucket "todo"
[ns_server:debug,2022-09-07T14:29:14.076Z,ns_1@cb.local:<0.2619.0>:menelaus_web:check_bucket_uuid:1222]Attempt to access non existent bucket "todo"
[ns_server:debug,2022-09-07T14:29:14.581Z,ns_1@cb.local:<0.2619.0>:menelaus_web:check_bucket_uuid:1222]Attempt to access non existent bucket "todo"
[ns_server:debug,2022-09-07T14:29:15.094Z,ns_1@cb.local:<0.2619.0>:menelaus_web:check_bucket_uuid:1222]Attempt to access non existent bucket "todo"
[ns_server:debug,2022-09-07T14:29:15.604Z,ns_1@cb.local:<0.2619.0>:menelaus_web:check_bucket_uuid:1222]Attempt to access non existent bucket "todo"
[ns_server:debug,2022-09-07T14:29:16.108Z,ns_1@cb.local:<0.2619.0>:menelaus_web:check_bucket_uuid:1222]Attempt to access non existent bucket "todo"
[ns_server:debug,2022-09-07T14:29:16.612Z,ns_1@cb.local:<0.2619.0>:menelaus_web:check_bucket_uuid:1222]Attempt to access non existent bucket "todo"
[ns_server:debug,2022-09-07T14:29:17.119Z,ns_1@cb.local:<0.2619.0>:menelaus_web:check_bucket_uuid:1222]Attempt to access non existent bucket "todo"
[ns_server:debug,2022-09-07T14:29:17.325Z,ns_1@cb.local:compaction_daemon<0.667.0>:compaction_daemon:process_scheduler_message:1359]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2022-09-07T14:29:17.325Z,ns_1@cb.local:compaction_daemon<0.667.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-07T14:29:17.325Z,ns_1@cb.local:compaction_daemon<0.667.0>:compaction_daemon:process_scheduler_message:1359]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2022-09-07T14:29:17.325Z,ns_1@cb.local:compaction_daemon<0.667.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-07T14:29:17.419Z,ns_1@cb.local:<0.696.0>:chronicle_master:do_handle_info:296]Successful after upgrade cleanup
[ns_server:debug,2022-09-07T14:29:17.419Z,ns_1@cb.local:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
nodes_wanted ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780157}}]}]
[ns_server:debug,2022-09-07T14:29:17.419Z,ns_1@cb.local:ns_cookie_manager<0.221.0>:ns_cookie_manager:do_cookie_sync:101]ns_cookie_manager do_cookie_sync
[ns_server:debug,2022-09-07T14:29:17.419Z,ns_1@cb.local:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
server_groups ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780157}}]}|
 '_deleted']
[ns_server:debug,2022-09-07T14:29:17.419Z,ns_1@cb.local:ns_config_rep<0.444.0>:ns_config_rep:do_push_keys:383]Replicating some config keys ([after_upgrade_cleanup,auto_reprovision_cfg,
                               counters,nodes_wanted,server_groups,
                               {local_changes_count,
                                   <<"1678dfae96c38e07dff43c49b9f6967b">>},
                               {node,'ns_1@cb.local',membership}]..)
[ns_server:debug,2022-09-07T14:29:17.419Z,ns_1@cb.local:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@cb.local',membership} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780157}}]}|
 '_deleted']
[ns_server:debug,2022-09-07T14:29:17.419Z,ns_1@cb.local:<0.2839.0>:ns_node_disco:do_nodes_wanted_updated_fun:224]ns_node_disco: nodes_wanted updated: ['ns_1@cb.local'], with cookie: {sanitized,
                                                                      <<"6jw2YfUfDxiQCs0fCwoUSmHbOdRC4E/A03fIDwJPR3g=">>}
[ns_server:debug,2022-09-07T14:29:17.420Z,ns_1@cb.local:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
auto_reprovision_cfg ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780157}}]}|
 '_deleted']
[ns_server:debug,2022-09-07T14:29:17.420Z,ns_1@cb.local:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
counters ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780157}}]}|
 '_deleted']
[ns_server:debug,2022-09-07T14:29:17.420Z,ns_1@cb.local:<0.2839.0>:ns_node_disco:do_nodes_wanted_updated_fun:227]ns_node_disco: nodes_wanted pong: ['ns_1@cb.local'], with cookie: {sanitized,
                                                                   <<"6jw2YfUfDxiQCs0fCwoUSmHbOdRC4E/A03fIDwJPR3g=">>}
[ns_server:debug,2022-09-07T14:29:17.420Z,ns_1@cb.local:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
after_upgrade_cleanup ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780157}}]}|
 '_deleted']
[ns_server:debug,2022-09-07T14:29:17.420Z,ns_1@cb.local:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{local_changes_count,<<"1678dfae96c38e07dff43c49b9f6967b">>} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{11,63829780157}}]}]
[ns_server:debug,2022-09-07T14:29:17.420Z,ns_1@cb.local:terse_cluster_info_uploader<0.613.0>:terse_cluster_info_uploader:handle_info:42]Refreshing terse cluster info with <<"{\"rev\":18,\"nodesExt\":[{\"services\":{\"capi\":8092,\"capiSSL\":18092,\"kv\":11210,\"kvSSL\":11207,\"mgmt\":8091,\"mgmtSSL\":18091,\"projector\":9999},\"thisNode\":true}],\"clusterCapabilitiesVer\":[1,0],\"clusterCapabilities\":{\"n1ql\":[\"enhancedPreparedStatements\"]},\"revEpoch\":1}">>
[ns_server:debug,2022-09-07T14:29:17.634Z,ns_1@cb.local:<0.2619.0>:menelaus_web:check_bucket_uuid:1222]Attempt to access non existent bucket "todo"
[ns_server:debug,2022-09-07T14:29:18.144Z,ns_1@cb.local:<0.2619.0>:menelaus_web:check_bucket_uuid:1222]Attempt to access non existent bucket "todo"
[ns_server:debug,2022-09-07T14:29:18.653Z,ns_1@cb.local:<0.2619.0>:menelaus_web:check_bucket_uuid:1222]Attempt to access non existent bucket "todo"
[ns_server:debug,2022-09-07T14:29:19.160Z,ns_1@cb.local:<0.2619.0>:menelaus_web:check_bucket_uuid:1222]Attempt to access non existent bucket "todo"
[ns_server:debug,2022-09-07T14:29:19.672Z,ns_1@cb.local:<0.2619.0>:menelaus_web:check_bucket_uuid:1222]Attempt to access non existent bucket "todo"
[ns_server:debug,2022-09-07T14:29:28.068Z,ns_1@cb.local:ldap_auth_cache<0.354.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2022-09-07T14:29:47.326Z,ns_1@cb.local:compaction_daemon<0.667.0>:compaction_daemon:process_scheduler_message:1359]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2022-09-07T14:29:47.326Z,ns_1@cb.local:compaction_daemon<0.667.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-07T14:29:47.326Z,ns_1@cb.local:compaction_daemon<0.667.0>:compaction_daemon:process_scheduler_message:1359]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2022-09-07T14:29:47.326Z,ns_1@cb.local:compaction_daemon<0.667.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-07T14:30:17.327Z,ns_1@cb.local:compaction_daemon<0.667.0>:compaction_daemon:process_scheduler_message:1359]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2022-09-07T14:30:17.327Z,ns_1@cb.local:compaction_daemon<0.667.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-07T14:30:17.327Z,ns_1@cb.local:compaction_daemon<0.667.0>:compaction_daemon:process_scheduler_message:1359]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2022-09-07T14:30:17.327Z,ns_1@cb.local:compaction_daemon<0.667.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-07T14:30:43.069Z,ns_1@cb.local:ldap_auth_cache<0.354.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2022-09-07T14:30:47.328Z,ns_1@cb.local:compaction_daemon<0.667.0>:compaction_daemon:process_scheduler_message:1359]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2022-09-07T14:30:47.328Z,ns_1@cb.local:compaction_daemon<0.667.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-07T14:30:47.328Z,ns_1@cb.local:compaction_daemon<0.667.0>:compaction_daemon:process_scheduler_message:1359]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2022-09-07T14:30:47.328Z,ns_1@cb.local:compaction_daemon<0.667.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-07T14:31:17.329Z,ns_1@cb.local:compaction_daemon<0.667.0>:compaction_daemon:process_scheduler_message:1359]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2022-09-07T14:31:17.329Z,ns_1@cb.local:compaction_daemon<0.667.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-07T14:31:17.329Z,ns_1@cb.local:compaction_daemon<0.667.0>:compaction_daemon:process_scheduler_message:1359]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2022-09-07T14:31:17.329Z,ns_1@cb.local:compaction_daemon<0.667.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-07T14:31:47.330Z,ns_1@cb.local:compaction_daemon<0.667.0>:compaction_daemon:process_scheduler_message:1359]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2022-09-07T14:31:47.330Z,ns_1@cb.local:compaction_daemon<0.667.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-07T14:31:47.330Z,ns_1@cb.local:compaction_daemon<0.667.0>:compaction_daemon:process_scheduler_message:1359]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2022-09-07T14:31:47.330Z,ns_1@cb.local:compaction_daemon<0.667.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-07T14:31:58.070Z,ns_1@cb.local:ldap_auth_cache<0.354.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[cluster:info,2022-09-07T14:32:08.878Z,ns_1@cb.local:ns_cluster<0.253.0>:ns_cluster:handle_call:442]Changing address to "127.0.0.1" due to client request
[cluster:info,2022-09-07T14:32:08.878Z,ns_1@cb.local:ns_cluster<0.253.0>:ns_cluster:do_change_address:725]Change of address to "127.0.0.1" is requested.
[ns_server:debug,2022-09-07T14:32:08.878Z,ns_1@cb.local:ns_node_disco<0.436.0>:ns_node_disco:maybe_monitor_rename_txn:215]Monitor node renaming transaction. Pid = <0.8733.0>, MRef = #Ref<0.2209594722.1843658754.48702>
[ns_server:debug,2022-09-07T14:32:08.879Z,ns_1@cb.local:remote_monitors<0.280.0>:remote_monitors:maybe_monitor_rename_txn:159]Monitor node renaming transaction. Pid = <0.8733.0>, MRef = #Ref<0.2209594722.1843658755.40293>
[ns_server:debug,2022-09-07T14:32:08.879Z,ns_1@cb.local:cb_dist<0.209.0>:cb_dist:info_msg:872]cb_dist: Closing listener {external,inet_tcp_dist}
[ns_server:debug,2022-09-07T14:32:08.879Z,ns_1@cb.local:cb_dist<0.209.0>:cb_dist:info_msg:872]cb_dist: Full list of processes expected to stop: [<0.214.0>]
[ns_server:debug,2022-09-07T14:32:08.879Z,ns_1@cb.local:cb_dist<0.209.0>:cb_dist:info_msg:872]cb_dist: Down from <0.214.0>
[error_logger:info,2022-09-07T14:32:08.879Z,ns_1@cb.local:net_kernel<0.212.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,744,nodedown,'ns_1@cb.local'}}
[ns_server:debug,2022-09-07T14:32:08.879Z,ns_1@cb.local:cb_dist<0.209.0>:cb_dist:info_msg:872]cb_dist: Connection down: {con,#Ref<0.2209594722.1843658755.36581>,
                               inet_tcp_dist,<0.394.0>,
                               #Ref<0.2209594722.1843658756.36922>}
[user:warn,2022-09-07T14:32:08.880Z,nonode@nohost:ns_node_disco<0.436.0>:ns_node_disco:handle_info:183]Node nonode@nohost saw that node 'ns_1@cb.local' went down. Details: [{nodedown_reason,
                                                                       net_kernel_terminated}]
[chronicle:info,2022-09-07T14:32:08.880Z,nonode@nohost:chronicle_proposer<0.272.0>:chronicle_proposer:handle_nodedown:1135]Peer 'ns_1@cb.local' went down: [{nodedown_reason,net_kernel_terminated}]
[ns_server:debug,2022-09-07T14:32:08.880Z,nonode@nohost:<0.414.0>:misc:delaying_crash:1736]Delaying crash exit:{{nodedown,'babysitter_of_ns_1@cb.local'},
                     {gen_server,call,
                         [{ns_babysitter_log,'babysitter_of_ns_1@cb.local'},
                          consume,infinity]}} by 1000ms
Stacktrace: [{gen_server,call,3,[{file,"gen_server.erl"},{line,247}]},
             {ns_log,babysitter_log_consumption_loop,0,
                     [{file,"src/ns_log.erl"},{line,66}]},
             {misc,delaying_crash,2,[{file,"src/misc.erl"},{line,1734}]},
             {proc_lib,init_p,3,[{file,"proc_lib.erl"},{line,211}]}]
[ns_server:debug,2022-09-07T14:32:08.880Z,nonode@nohost:cb_dist<0.209.0>:cb_dist:info_msg:872]cb_dist: Connection down: {con,#Ref<0.2209594722.1843658754.36455>,
                               inet_tcp_dist,<0.216.0>,
                               #Ref<0.2209594722.1843658754.36458>}
[error_logger:error,2022-09-07T14:32:08.880Z,nonode@nohost:cb_dist<0.209.0>:ale_error_logger_handler:do_log:101]cb_dist: terminating with reason: shutdown
[ns_server:debug,2022-09-07T14:32:08.881Z,nonode@nohost:<0.8735.0>:dist_manager:teardown:303]Got nodedown msg {nodedown,'ns_1@cb.local',
                           [{nodedown_reason,net_kernel_terminated}]} after terminating net kernel
[ns_server:info,2022-09-07T14:32:08.881Z,nonode@nohost:<0.8733.0>:dist_manager:do_adjust_address:345]Adjusted IP to "127.0.0.1"
[ns_server:info,2022-09-07T14:32:08.882Z,nonode@nohost:<0.8733.0>:dist_manager:bringup:244]Attempting to bring up net_kernel with name 'ns_1@127.0.0.1'
[error_logger:info,2022-09-07T14:32:08.883Z,nonode@nohost:ssl_dist_admin_sup<0.8738.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ssl_dist_admin_sup}
    started: [{pid,<0.8739.0>},
              {id,ssl_pem_cache_dist},
              {mfargs,{ssl_pem_cache,start_link_dist,[[]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,4000},
              {child_type,worker}]

[error_logger:info,2022-09-07T14:32:08.884Z,nonode@nohost:ssl_dist_admin_sup<0.8738.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ssl_dist_admin_sup}
    started: [{pid,<0.8740.0>},
              {id,ssl_dist_manager},
              {mfargs,{ssl_manager,start_link_dist,[[]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,4000},
              {child_type,worker}]

[error_logger:info,2022-09-07T14:32:08.884Z,nonode@nohost:ssl_dist_sup<0.8737.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ssl_dist_sup}
    started: [{pid,<0.8738.0>},
              {id,ssl_dist_admin_sup},
              {mfargs,{ssl_dist_admin_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,4000},
              {child_type,supervisor}]

[error_logger:info,2022-09-07T14:32:08.884Z,nonode@nohost:tls_dist_sup<0.8741.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,tls_dist_sup}
    started: [{pid,<0.8742.0>},
              {id,dist_tls_connection},
              {mfargs,{tls_connection_sup,start_link_dist,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,4000},
              {child_type,supervisor}]

[error_logger:info,2022-09-07T14:32:08.884Z,nonode@nohost:tls_dist_server_sup<0.8743.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,tls_dist_server_sup}
    started: [{pid,<0.8744.0>},
              {id,dist_tls_socket},
              {mfargs,{ssl_listen_tracker_sup,start_link_dist,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,4000},
              {child_type,supervisor}]

[error_logger:info,2022-09-07T14:32:08.885Z,nonode@nohost:tls_dist_server_sup<0.8743.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,tls_dist_server_sup}
    started: [{pid,<0.8745.0>},
              {id,dist_tls_server_session_ticket},
              {mfargs,{tls_server_session_ticket_sup,start_link_dist,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,4000},
              {child_type,supervisor}]

[error_logger:info,2022-09-07T14:32:08.885Z,nonode@nohost:tls_dist_server_sup<0.8743.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,tls_dist_server_sup}
    started: [{pid,<0.8746.0>},
              {id,dist_ssl_server_session_cache_sup},
              {mfargs,
                  {ssl_upgrade_server_session_cache_sup,start_link_dist,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,4000},
              {child_type,supervisor}]

[error_logger:info,2022-09-07T14:32:08.885Z,nonode@nohost:tls_dist_sup<0.8741.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,tls_dist_sup}
    started: [{pid,<0.8743.0>},
              {id,dist_tls_server_sup},
              {mfargs,{tls_dist_server_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,4000},
              {child_type,supervisor}]

[error_logger:info,2022-09-07T14:32:08.885Z,nonode@nohost:ssl_dist_sup<0.8737.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ssl_dist_sup}
    started: [{pid,<0.8741.0>},
              {id,tls_dist_sup},
              {mfargs,{tls_dist_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,4000},
              {child_type,supervisor}]

[error_logger:info,2022-09-07T14:32:08.885Z,nonode@nohost:net_sup<0.8736.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,net_sup}
    started: [{pid,<0.8737.0>},
              {id,ssl_dist_sup},
              {mfargs,{ssl_dist_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2022-09-07T14:32:08.885Z,nonode@nohost:cb_dist<0.8747.0>:cb_dist:info_msg:872]cb_dist: Starting cb_dist with config []
[error_logger:info,2022-09-07T14:32:08.887Z,nonode@nohost:net_sup<0.8736.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,net_sup}
    started: [{pid,<0.8747.0>},
              {id,cb_dist},
              {mfargs,{cb_dist,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,worker}]

[error_logger:info,2022-09-07T14:32:08.887Z,nonode@nohost:net_sup<0.8736.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,net_sup}
    started: [{pid,<0.8748.0>},
              {id,cb_epmd},
              {mfargs,{cb_epmd,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,2000},
              {child_type,worker}]

[error_logger:info,2022-09-07T14:32:08.888Z,nonode@nohost:net_sup<0.8736.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,net_sup}
    started: [{pid,<0.8749.0>},
              {id,auth},
              {mfargs,{auth,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,2000},
              {child_type,worker}]

[ns_server:debug,2022-09-07T14:32:08.888Z,nonode@nohost:cb_dist<0.8747.0>:cb_dist:info_msg:872]cb_dist: Initial protos: [{external,inet_tcp_dist}], required protos: [{external,
                                                                        inet_tcp_dist}]
[ns_server:debug,2022-09-07T14:32:08.888Z,nonode@nohost:cb_dist<0.8747.0>:cb_dist:info_msg:872]cb_dist: Starting {external,inet_tcp_dist} listener on 21100...
[ns_server:debug,2022-09-07T14:32:08.888Z,nonode@nohost:cb_dist<0.8747.0>:cb_dist:info_msg:872]cb_dist: Started listener: inet_tcp_dist
[chronicle:info,2022-09-07T14:32:08.889Z,ns_1@127.0.0.1:chronicle_proposer<0.272.0>:chronicle_proposer:handle_nodeup:1093]Peer 'ns_1@127.0.0.1' came up
[ns_server:debug,2022-09-07T14:32:08.889Z,ns_1@127.0.0.1:<0.361.0>:doc_replicator:nodeup_monitoring_loop:136]got nodeup event. Considering ddocs replication
[user:info,2022-09-07T14:32:08.889Z,ns_1@127.0.0.1:ns_node_disco<0.436.0>:ns_node_disco:handle_info:177]Node 'ns_1@127.0.0.1' saw that node 'ns_1@127.0.0.1' came up. Tags: []
[ns_server:debug,2022-09-07T14:32:08.889Z,ns_1@127.0.0.1:cb_dist<0.8747.0>:cb_dist:info_msg:872]cb_dist: Started acceptor inet_tcp_dist: <0.8751.0>
[chronicle:info,2022-09-07T14:32:08.889Z,ns_1@127.0.0.1:chronicle_proposer<0.272.0>:chronicle_proposer:handle_nodeup:1127]Peer 'ns_1@127.0.0.1' is not in peers: ['ns_1@cb.local']
[chronicle:debug,2022-09-07T14:32:08.889Z,ns_1@127.0.0.1:chronicle_leader<0.239.0>:chronicle_leader:handle_note_term_status:603]Ignoring stale term status {<<"1933459bb8a062c61b619e931c483c80">>,
                            {2,'ns_1@cb.local'},
                            finished}: {error,{not_a_leader,follower}}
[ns_server:debug,2022-09-07T14:32:08.889Z,ns_1@127.0.0.1:net_kernel<0.8750.0>:cb_dist:info_msg:872]cb_dist: Setting up new connection to 'ns_1@cb.local' using inet_tcp_dist
[chronicle:info,2022-09-07T14:32:08.889Z,ns_1@127.0.0.1:chronicle_proposer<0.272.0>:chronicle_proposer:handle_stop:1269]Proposer for term {2,'ns_1@cb.local'} in history <<"1933459bb8a062c61b619e931c483c80">> is terminating.
[error_logger:info,2022-09-07T14:32:08.889Z,ns_1@127.0.0.1:net_sup<0.8736.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,net_sup}
    started: [{pid,<0.8750.0>},
              {id,net_kernel},
              {mfargs,{net_kernel,start_link,
                                  [['ns_1@127.0.0.1',longnames],
                                   false,net_sup_dynamic]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,2000},
              {child_type,worker}]

[ns_server:debug,2022-09-07T14:32:08.889Z,ns_1@127.0.0.1:cb_dist<0.8747.0>:cb_dist:info_msg:872]cb_dist: Added connection {con,#Ref<0.2209594722.1843658754.48723>,
                               inet_tcp_dist,undefined,undefined}
[chronicle:debug,2022-09-07T14:32:08.889Z,ns_1@127.0.0.1:chronicle_agent<0.235.0>:chronicle_agent:handle_local_mark_committed:1997]Marked seqno 8 committed
[error_logger:info,2022-09-07T14:32:08.889Z,ns_1@127.0.0.1:kernel_sup<0.49.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,kernel_sup}
    started: [{pid,<0.8736.0>},
              {id,net_sup_dynamic},
              {mfargs,
                  {erl_distribution,start_link,
                      [['ns_1@127.0.0.1',longnames],false,net_sup_dynamic]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,supervisor}]

[ns_server:debug,2022-09-07T14:32:08.889Z,ns_1@127.0.0.1:cb_dist<0.8747.0>:cb_dist:info_msg:872]cb_dist: Updated connection: {con,#Ref<0.2209594722.1843658754.48723>,
                                  inet_tcp_dist,<0.8755.0>,
                                  #Ref<0.2209594722.1843658754.48725>}
[ns_server:debug,2022-09-07T14:32:08.889Z,ns_1@127.0.0.1:<0.8733.0>:dist_manager:configure_net_kernel:290]Set net_kernel vebosity to 10 -> 0
[chronicle:info,2022-09-07T14:32:08.889Z,ns_1@127.0.0.1:chronicle_server<0.243.0>:chronicle_proposer:stop:136]Proposer <0.272.0> stopped: {shutdown,stop}
[ns_server:debug,2022-09-07T14:32:08.890Z,ns_1@127.0.0.1:cb_dist<0.8747.0>:cb_dist:info_msg:872]cb_dist: Accepted new connection from <0.8751.0> DistCtrl #Port<0.241>: {con,
                                                                         #Ref<0.2209594722.1843658758.38710>,
                                                                         inet_tcp_dist,
                                                                         undefined,
                                                                         undefined}
[ns_server:debug,2022-09-07T14:32:08.890Z,ns_1@127.0.0.1:net_kernel<0.8750.0>:cb_dist:info_msg:872]cb_dist: Accepting connection from <0.8751.0> using module inet_tcp_dist
[ns_server:debug,2022-09-07T14:32:08.890Z,ns_1@127.0.0.1:cb_dist<0.8747.0>:cb_dist:info_msg:872]cb_dist: Updated connection: {con,#Ref<0.2209594722.1843658758.38710>,
                                  inet_tcp_dist,<0.8757.0>,
                                  #Ref<0.2209594722.1843658753.44055>}
[error_logger:error,2022-09-07T14:32:08.890Z,ns_1@127.0.0.1:net_kernel<0.8750.0>:ale_error_logger_handler:do_log:101]
=========================ERROR REPORT=========================

** Cannot get connection id for node 'ns_1@127.0.0.1'

[ns_server:debug,2022-09-07T14:32:08.890Z,ns_1@127.0.0.1:cb_dist<0.8747.0>:cb_dist:info_msg:872]cb_dist: Connection down: {con,#Ref<0.2209594722.1843658758.38710>,
                               inet_tcp_dist,<0.8757.0>,
                               #Ref<0.2209594722.1843658753.44055>}
[error_logger:info,2022-09-07T14:32:08.890Z,ns_1@127.0.0.1:net_kernel<0.8750.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.8757.0>,shutdown}}
[ns_server:info,2022-09-07T14:32:08.890Z,ns_1@127.0.0.1:<0.8733.0>:dist_manager:save_node:158]saving node to "/opt/couchbase/var/lib/couchbase/couchbase-server.node"
[ns_server:debug,2022-09-07T14:32:08.902Z,ns_1@127.0.0.1:<0.8733.0>:dist_manager:bringup:260]Attempted to save node name to disk: ok
[ns_server:debug,2022-09-07T14:32:08.903Z,ns_1@127.0.0.1:<0.8733.0>:dist_manager:wait_for_node:267]Waiting for connection to node 'babysitter_of_ns_1@cb.local' to be established
[error_logger:info,2022-09-07T14:32:08.903Z,ns_1@127.0.0.1:net_kernel<0.8750.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{connect,normal,'babysitter_of_ns_1@cb.local'}}
[ns_server:debug,2022-09-07T14:32:08.903Z,ns_1@127.0.0.1:net_kernel<0.8750.0>:cb_dist:info_msg:872]cb_dist: Setting up new connection to 'babysitter_of_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2022-09-07T14:32:08.904Z,ns_1@127.0.0.1:cb_dist<0.8747.0>:cb_dist:info_msg:872]cb_dist: Added connection {con,#Ref<0.2209594722.1843658754.48743>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2022-09-07T14:32:08.904Z,ns_1@127.0.0.1:cb_dist<0.8747.0>:cb_dist:info_msg:872]cb_dist: Updated connection: {con,#Ref<0.2209594722.1843658754.48743>,
                                  inet_tcp_dist,<0.8759.0>,
                                  #Ref<0.2209594722.1843658754.48746>}
[ns_server:debug,2022-09-07T14:32:08.905Z,ns_1@127.0.0.1:<0.8733.0>:dist_manager:wait_for_node:279]Observed node 'babysitter_of_ns_1@cb.local' to come up
[ns_server:info,2022-09-07T14:32:08.906Z,ns_1@127.0.0.1:<0.8733.0>:dist_manager:do_adjust_address:349]Re-setting cookie {{sanitized,<<"6jw2YfUfDxiQCs0fCwoUSmHbOdRC4E/A03fIDwJPR3g=">>},
                   'ns_1@127.0.0.1'}
[ns_server:info,2022-09-07T14:32:08.915Z,ns_1@127.0.0.1:<0.8733.0>:dist_manager:save_address_config:145]Deleting irrelevant ip file "/opt/couchbase/var/lib/couchbase/ip_start": {error,
                                                                          enoent}
[ns_server:info,2022-09-07T14:32:08.915Z,ns_1@127.0.0.1:<0.8733.0>:dist_manager:save_address_config:146]saving ip config to "/opt/couchbase/var/lib/couchbase/ip"
[ns_server:info,2022-09-07T14:32:08.925Z,ns_1@127.0.0.1:<0.8733.0>:dist_manager:save_address_config:149]Persisted the address successfully
[ns_server:debug,2022-09-07T14:32:08.925Z,ns_1@127.0.0.1:<0.8733.0>:dist_manager:rename_node_in_configs:430]Renaming node from 'ns_1@cb.local' to 'ns_1@127.0.0.1' in config
[ns_server:debug,2022-09-07T14:32:08.925Z,ns_1@127.0.0.1:chronicle_local<0.222.0>:chronicle_local:handle_rename:149]Handle renaming from 'ns_1@cb.local' to 'ns_1@127.0.0.1'
[chronicle:debug,2022-09-07T14:32:08.925Z,ns_1@127.0.0.1:chronicle_agent<0.235.0>:chronicle_agent:handle_reprovision:1140]Reprovisioning peer with config:
{log_entry,<<"1933459bb8a062c61b619e931c483c80">>,
           {3,'ns_1@127.0.0.1'},
           9,
           {config,undefined,0,<<"a7105b68650d59694ca7ecccf647946f">>,
                   #{'ns_1@127.0.0.1' =>
                         #{id => <<"fb54a3882f4ab57cf6f0be39357bb948">>,
                           role => voter}},
                   undefined,
                   #{chronicle_config_rsm =>
                         {rsm_config,chronicle_config_rsm,[]},
                     kv => {rsm_config,chronicle_kv,[]}},
                   #{},undefined,
                   [{<<"1933459bb8a062c61b619e931c483c80">>,0}]}}
[chronicle:info,2022-09-07T14:32:08.929Z,ns_1@127.0.0.1:chronicle_leader<0.239.0>:chronicle_leader:handle_reprovisioned:498]System reprovisioned.
[chronicle:info,2022-09-07T14:32:08.929Z,ns_1@127.0.0.1:<0.8762.0>:chronicle_leader:do_election_worker:892]Starting election.
History ID: <<"1933459bb8a062c61b619e931c483c80">>
Log position: {{3,'ns_1@127.0.0.1'},9}
Peers: ['ns_1@127.0.0.1']
Required quorum: {majority,{set,1,16,16,8,80,48,
                                {[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],
                                 []},
                                {{[],
                                  ['ns_1@127.0.0.1'],
                                  [],[],[],[],[],[],[],[],[],[],[],[],[],
                                  []}}}}
[chronicle:info,2022-09-07T14:32:08.929Z,ns_1@127.0.0.1:<0.8762.0>:chronicle_leader:do_election_worker:901]I'm the only peer, so I'm the leader.
[chronicle:info,2022-09-07T14:32:08.929Z,ns_1@127.0.0.1:chronicle_leader<0.239.0>:chronicle_leader:handle_election_result:691]Going to become a leader in term {4,'ns_1@127.0.0.1'} (history id <<"1933459bb8a062c61b619e931c483c80">>)
[chronicle:debug,2022-09-07T14:32:08.933Z,ns_1@127.0.0.1:chronicle_agent<0.235.0>:chronicle_agent:handle_establish_term:1529]Accepted term {4,'ns_1@127.0.0.1'} in history <<"1933459bb8a062c61b619e931c483c80">>
[chronicle:debug,2022-09-07T14:32:08.933Z,ns_1@127.0.0.1:chronicle_proposer<0.8763.0>:chronicle_proposer:establish_term_init:367]Going to establish term {4,'ns_1@127.0.0.1'} (history id <<"1933459bb8a062c61b619e931c483c80">>).
Quorum peers: ['ns_1@127.0.0.1']
Metadata:
{metadata,'ns_1@127.0.0.1',<<"fb54a3882f4ab57cf6f0be39357bb948">>,
          <<"1933459bb8a062c61b619e931c483c80">>,
          {3,'ns_1@127.0.0.1'},
          {3,'ns_1@127.0.0.1'},
          9,9,
          {log_entry,<<"1933459bb8a062c61b619e931c483c80">>,
                     {3,'ns_1@127.0.0.1'},
                     9,
                     {config,undefined,0,
                             <<"a7105b68650d59694ca7ecccf647946f">>,
                             #{'ns_1@127.0.0.1' =>
                                   #{id =>
                                         <<"fb54a3882f4ab57cf6f0be39357bb948">>,
                                     role => voter}},
                             undefined,
                             #{chronicle_config_rsm =>
                                   {rsm_config,chronicle_config_rsm,[]},
                               kv => {rsm_config,chronicle_kv,[]}},
                             #{},undefined,
                             [{<<"1933459bb8a062c61b619e931c483c80">>,0}]}},
          {log_entry,<<"1933459bb8a062c61b619e931c483c80">>,
                     {3,'ns_1@127.0.0.1'},
                     9,
                     {config,undefined,0,
                             <<"a7105b68650d59694ca7ecccf647946f">>,
                             #{'ns_1@127.0.0.1' =>
                                   #{id =>
                                         <<"fb54a3882f4ab57cf6f0be39357bb948">>,
                                     role => voter}},
                             undefined,
                             #{chronicle_config_rsm =>
                                   {rsm_config,chronicle_config_rsm,[]},
                               kv => {rsm_config,chronicle_kv,[]}},
                             #{},undefined,
                             [{<<"1933459bb8a062c61b619e931c483c80">>,0}]}},
          undefined}
[chronicle:debug,2022-09-07T14:32:08.933Z,ns_1@127.0.0.1:chronicle_proposer<0.8763.0>:chronicle_proposer:establish_term_maybe_transition:686]Established term {4,'ns_1@127.0.0.1'} (history id <<"1933459bb8a062c61b619e931c483c80">>) successfully.
Votes: ['ns_1@127.0.0.1']
[chronicle:debug,2022-09-07T14:32:08.933Z,ns_1@127.0.0.1:chronicle_proposer<0.8763.0>:chronicle_proposer:handle_state_enter:284]Starting recovery for term {4,'ns_1@127.0.0.1'} in history <<"1933459bb8a062c61b619e931c483c80">>
[chronicle:debug,2022-09-07T14:32:08.936Z,ns_1@127.0.0.1:chronicle_proposer<0.8763.0>:chronicle_proposer:handle_state_enter:296]Proposer for term {4,'ns_1@127.0.0.1'} in history <<"1933459bb8a062c61b619e931c483c80">> is ready. Committed seqno: 10
[chronicle:info,2022-09-07T14:32:08.936Z,ns_1@127.0.0.1:chronicle_leader<0.239.0>:chronicle_leader:handle_note_term_status:596]Term {4,'ns_1@127.0.0.1'} established.
[ns_server:debug,2022-09-07T14:32:08.963Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:439]renaming node conf {node,'ns_1@cb.local',membership} -> {node,
                                                         'ns_1@127.0.0.1',
                                                         membership}:
  '_deleted' ->
  '_deleted'
[ns_server:debug,2022-09-07T14:32:08.963Z,ns_1@127.0.0.1:chronicle_kv_log<0.404.0>:chronicle_kv_log:handle_info:42]delete (key: {node,'ns_1@cb.local',membership}, rev: {<<"1933459bb8a062c61b619e931c483c80">>,
                                                      11})
[ns_server:debug,2022-09-07T14:32:08.963Z,ns_1@127.0.0.1:chronicle_kv_log<0.404.0>:chronicle_kv_log:log:59]update (key: {node,'ns_1@127.0.0.1',membership}, rev: {<<"1933459bb8a062c61b619e931c483c80">>,
                                                       11})
active
[ns_server:debug,2022-09-07T14:32:08.963Z,ns_1@127.0.0.1:chronicle_kv_log<0.404.0>:chronicle_kv_log:log:59]update (key: server_groups, rev: {<<"1933459bb8a062c61b619e931c483c80">>,11})
[[{uuid,<<"0">>},{name,<<"Group 1">>},{nodes,['ns_1@127.0.0.1']}]]
[ns_server:debug,2022-09-07T14:32:08.963Z,ns_1@127.0.0.1:mb_master<0.683.0>:mb_master:update_peers:562]List of peers has changed from ['ns_1@cb.local'] to ['ns_1@127.0.0.1']
[ns_server:debug,2022-09-07T14:32:08.963Z,ns_1@127.0.0.1:chronicle_kv_log<0.404.0>:chronicle_kv_log:log:59]update (key: nodes_wanted, rev: {<<"1933459bb8a062c61b619e931c483c80">>,11})
['ns_1@127.0.0.1']
[ns_server:debug,2022-09-07T14:32:08.965Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:439]renaming node conf {node,'ns_1@cb.local',prometheus_auth_info} -> {node,
                                                                   'ns_1@127.0.0.1',
                                                                   prometheus_auth_info}:
  {"@prometheus",
   {auth,[{<<"plain">>,
           {sanitized,<<"M19CN2uT3IM7to60JvPLLFWe15QE+xqQj4ae4pILIQY=">>}},
          {<<"sha512">>,
           {[{<<"h">>,
              {sanitized,<<"ogcmvOcIDb3trVZUf8MQ924hUlHGPZQI7aFQbckkSIo=">>}},
             {<<"s">>,
              <<"XsQ6FZpbB9BqHVSzMNqrHDzDO0uulN8mxg267iifMDsUbKxifdxVg+Lc+pAhc3YVdmMhCop0SLTuJ+7LUuQAWw==">>},
             {<<"i">>,4000}]}},
          {<<"sha256">>,
           {[{<<"h">>,
              {sanitized,<<"Z1mj6nW/ZuHa02+s9poaXdfNdF/M/zUY/YJjIP5h0XQ=">>}},
             {<<"s">>,<<"ciebqiheHmvTVsOoEf7zCJiMybfXrUIx2RqHrbKMOF0=">>},
             {<<"i">>,4000}]}},
          {<<"sha1">>,
           {[{<<"h">>,
              {sanitized,<<"OrBaLmtLgYmmtpuHjerlFrjJQFp2/h97H+MvyWAZUtY=">>}},
             {<<"s">>,<<"0mqqcO1V739RDCcAsk84uGUFIGA=">>},
             {<<"i">>,4000}]}}]}} ->
  {"@prometheus",
   {auth,[{<<"plain">>,
           {sanitized,<<"M19CN2uT3IM7to60JvPLLFWe15QE+xqQj4ae4pILIQY=">>}},
          {<<"sha512">>,
           {[{<<"h">>,
              {sanitized,<<"ogcmvOcIDb3trVZUf8MQ924hUlHGPZQI7aFQbckkSIo=">>}},
             {<<"s">>,
              <<"XsQ6FZpbB9BqHVSzMNqrHDzDO0uulN8mxg267iifMDsUbKxifdxVg+Lc+pAhc3YVdmMhCop0SLTuJ+7LUuQAWw==">>},
             {<<"i">>,4000}]}},
          {<<"sha256">>,
           {[{<<"h">>,
              {sanitized,<<"Z1mj6nW/ZuHa02+s9poaXdfNdF/M/zUY/YJjIP5h0XQ=">>}},
             {<<"s">>,<<"ciebqiheHmvTVsOoEf7zCJiMybfXrUIx2RqHrbKMOF0=">>},
             {<<"i">>,4000}]}},
          {<<"sha1">>,
           {[{<<"h">>,
              {sanitized,<<"OrBaLmtLgYmmtpuHjerlFrjJQFp2/h97H+MvyWAZUtY=">>}},
             {<<"s">>,<<"0mqqcO1V739RDCcAsk84uGUFIGA=">>},
             {<<"i">>,4000}]}}]}}
[ns_server:debug,2022-09-07T14:32:08.965Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:439]renaming node conf {node,'ns_1@cb.local',eventing_dir} -> {node,
                                                           'ns_1@127.0.0.1',
                                                           eventing_dir}:
  "/opt/couchbase/var/lib/couchbase/data" ->
  "/opt/couchbase/var/lib/couchbase/data"
[ns_server:debug,2022-09-07T14:32:08.965Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:439]renaming node conf {node,'ns_1@cb.local',cbas_dirs} -> {node,
                                                        'ns_1@127.0.0.1',
                                                        cbas_dirs}:
  ["/opt/couchbase/var/lib/couchbase/data"] ->
  ["/opt/couchbase/var/lib/couchbase/data"]
[ns_server:debug,2022-09-07T14:32:08.965Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:439]renaming node conf {node,'ns_1@cb.local',node_cert} -> {node,
                                                        'ns_1@127.0.0.1',
                                                        node_cert}:
  [{subject,<<"CN=Couchbase Server Node (127.0.0.1)">>},
   {not_after,64691827199},
   {verified_with,<<48,185,21,78,241,90,231,242,183,201,180,2,229,131,239,69>>},
   {type,generated},
   {load_timestamp,63829780092},
   {ca,<<"-----BEGIN CERTIFICATE-----\nMIIDITCCAgmgAwIBAgIIFxKaUqQBFTwwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciA5NGQ2NGQ0NTAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgOTRkNjRk\nNDUwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQDCrzpi1QtF4PyrbI2Y\nQB0augORV0Ro18YRvmfImOFnIZpNAmUYZRU68E39LZL794EOVdu05NB0tXNOfVrG\nB6gQNweW7UL1RiCL3OhoZ/VBsR9qSG54l/rFSddbrw3cNMKoZyYaPFoNhJogPOP4\n0IIUZhUjKgA7yKA/L0AqKn516TbfuA9oX/3y5Pm1Dycg0oRmDRenKl/cpORSvwaL\nqb4A35vbvA31mBswu3iLKhpn6L7uu/FPoPqROxTbBY6pHfu53IWOmzGbv3PFhHDR\noNKQI34ZliCL9NP06H6+8ZubL6C7J+XfG+zZkH/ZXda4SFCY3GOiZpPpdEOchSBw\nxi5pAgMBAAGjVzBVMA4GA1UdDwEB/wQEAwICpDATBgNVHSUEDDAKBggrBgEFBQcD\nATAPBgNVHRMBAf8EBTADAQH/MB0GA1UdDgQWBBQaPtelkfN4O47ZO+DmJZUak5gS\niDANBgkqhkiG9w0BAQsFAAOCAQEAS8nhCYCWQXVKye+hMXWV3W95/i44FCzXNUgV\nYZtqu740vRbZev89SxFzOZBN42/3/a8k4hpofhBVYgBTNrcmRexZnaQ0ptWOF2cC\ndKEUViHikjHjyKXC44fClgAM8dAsCDhssGWe75Cz5xU64mOgH+NgerJsJqVRJDzo\nNNMbP7yD9jLyyPpbi6mJ/4pYGweebLPxV4bngn7ly1oAdLs8gPRxqZITJlupN3ra\nHxF67QIEtvzZraQYjT3ryCoxqF6MIIJaqEe9kpUcxls+MHrd7cvxHN+e7LBjW7c7\nuB9JwNc30pJASAvoOoI58i+5dgrHcCxlOcJ4EurRDdi+NfPRNA==\n-----END CERTIFICATE-----\n">>},
   {pem,<<"-----BEGIN CERTIFICATE-----\nMIIDOTCCAiGgAwIBAgIIFxKaUsnSLBwwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciA5NGQ2NGQ0NTAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCwxKjAoBgNVBAMTIUNvdWNoYmFzZSBTZXJ2ZXIgTm9kZSAo\nMTI3LjAuMC4xKTCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBALESQvmu\nbrV8JXPnQh3zLduahBDrsIVoTPN0LJWCDoH6bNJOn7QdE6oSpVYqgG/rDVmX67GV\nEl3Cbf4bLB4oK3aYh+v5xlz4ZR4EHtdlBQs7KXT26b8fjtVyCizWAJUXL0iFDrd5\niwmcnK237ql+zfGqCU6/WdaKM0CX083pGiAkURuv9pK8pbmyiBkNaGxAmezfSHLL\n/qo2+rAlvLk11j96RcRcf6dCXCkbNi7JNRTRomjwyp0+ZjYcmSMCrTDoE6yQ6tEo\nbbDEqb3uXMolkTEL+af4QYkzCWC+3CaPaYzg70sThu1F58b4AUoUyoOW+7CKDTAM\n8G8G85m+KnzVmK8CAwEAAaNnMGUwDgYDVR0PAQH/BAQDAgWgMBMGA1UdJQQMMAoG\nCCsGAQUFBwMBMAwGA1UdEwEB/wQCMAAwHwYDVR0jBBgwFoAUGj7XpZHzeDuO2Tvg\n5iWVGpOYEogwDwYDVR0RBAgwBocEfwAAATANBgkqhkiG9w0BAQsFAAOCAQEATva9\nDD9A/yvfrypvdr+sjaMyeChgSbHWoZyyS7hpdx+6elTWi4NWxif1+rSYs9brnKgx\nkJBiqizAHLICaZ3myXJmzWJDUJrQLV/+3CF3drvzMjS4Au0FiyscWGu5hRY1dMl5\nsNpR7qQwQmW8//bUE2f7aV0ZKlwd9LNLBTfCacQU73ts1mCRNZFc1cavu7G0y3GM\nJPXa8HqzTWQKBHPlfdkp3AUm1fMn2P/PVwAdCueIrCjkULsbVHngbUwsC4IH7f5E\nmc0HE1NhiabzYia7J+yHbT9Ya5wEN34EUqtMoOYaIcCf+gczsJ5i3xYMeaL/FXe5\nmMRwK4iRqr6hLdfIAQ==\n-----END CERTIFICATE-----\n">>},
   {pkey_passphrase_settings,[]},
   {certs_epoch,0},
   {hostname,"127.0.0.1"}] ->
  [{subject,<<"CN=Couchbase Server Node (127.0.0.1)">>},
   {not_after,64691827199},
   {verified_with,<<48,185,21,78,241,90,231,242,183,201,180,2,229,131,239,69>>},
   {type,generated},
   {load_timestamp,63829780092},
   {ca,<<"-----BEGIN CERTIFICATE-----\nMIIDITCCAgmgAwIBAgIIFxKaUqQBFTwwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciA5NGQ2NGQ0NTAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgOTRkNjRk\nNDUwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQDCrzpi1QtF4PyrbI2Y\nQB0augORV0Ro18YRvmfImOFnIZpNAmUYZRU68E39LZL794EOVdu05NB0tXNOfVrG\nB6gQNweW7UL1RiCL3OhoZ/VBsR9qSG54l/rFSddbrw3cNMKoZyYaPFoNhJogPOP4\n0IIUZhUjKgA7yKA/L0AqKn516TbfuA9oX/3y5Pm1Dycg0oRmDRenKl/cpORSvwaL\nqb4A35vbvA31mBswu3iLKhpn6L7uu/FPoPqROxTbBY6pHfu53IWOmzGbv3PFhHDR\noNKQI34ZliCL9NP06H6+8ZubL6C7J+XfG+zZkH/ZXda4SFCY3GOiZpPpdEOchSBw\nxi5pAgMBAAGjVzBVMA4GA1UdDwEB/wQEAwICpDATBgNVHSUEDDAKBggrBgEFBQcD\nATAPBgNVHRMBAf8EBTADAQH/MB0GA1UdDgQWBBQaPtelkfN4O47ZO+DmJZUak5gS\niDANBgkqhkiG9w0BAQsFAAOCAQEAS8nhCYCWQXVKye+hMXWV3W95/i44FCzXNUgV\nYZtqu740vRbZev89SxFzOZBN42/3/a8k4hpofhBVYgBTNrcmRexZnaQ0ptWOF2cC\ndKEUViHikjHjyKXC44fClgAM8dAsCDhssGWe75Cz5xU64mOgH+NgerJsJqVRJDzo\nNNMbP7yD9jLyyPpbi6mJ/4pYGweebLPxV4bngn7ly1oAdLs8gPRxqZITJlupN3ra\nHxF67QIEtvzZraQYjT3ryCoxqF6MIIJaqEe9kpUcxls+MHrd7cvxHN+e7LBjW7c7\nuB9JwNc30pJASAvoOoI58i+5dgrHcCxlOcJ4EurRDdi+NfPRNA==\n-----END CERTIFICATE-----\n">>},
   {pem,<<"-----BEGIN CERTIFICATE-----\nMIIDOTCCAiGgAwIBAgIIFxKaUsnSLBwwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciA5NGQ2NGQ0NTAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCwxKjAoBgNVBAMTIUNvdWNoYmFzZSBTZXJ2ZXIgTm9kZSAo\nMTI3LjAuMC4xKTCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBALESQvmu\nbrV8JXPnQh3zLduahBDrsIVoTPN0LJWCDoH6bNJOn7QdE6oSpVYqgG/rDVmX67GV\nEl3Cbf4bLB4oK3aYh+v5xlz4ZR4EHtdlBQs7KXT26b8fjtVyCizWAJUXL0iFDrd5\niwmcnK237ql+zfGqCU6/WdaKM0CX083pGiAkURuv9pK8pbmyiBkNaGxAmezfSHLL\n/qo2+rAlvLk11j96RcRcf6dCXCkbNi7JNRTRomjwyp0+ZjYcmSMCrTDoE6yQ6tEo\nbbDEqb3uXMolkTEL+af4QYkzCWC+3CaPaYzg70sThu1F58b4AUoUyoOW+7CKDTAM\n8G8G85m+KnzVmK8CAwEAAaNnMGUwDgYDVR0PAQH/BAQDAgWgMBMGA1UdJQQMMAoG\nCCsGAQUFBwMBMAwGA1UdEwEB/wQCMAAwHwYDVR0jBBgwFoAUGj7XpZHzeDuO2Tvg\n5iWVGpOYEogwDwYDVR0RBAgwBocEfwAAATANBgkqhkiG9w0BAQsFAAOCAQEATva9\nDD9A/yvfrypvdr+sjaMyeChgSbHWoZyyS7hpdx+6elTWi4NWxif1+rSYs9brnKgx\nkJBiqizAHLICaZ3myXJmzWJDUJrQLV/+3CF3drvzMjS4Au0FiyscWGu5hRY1dMl5\nsNpR7qQwQmW8//bUE2f7aV0ZKlwd9LNLBTfCacQU73ts1mCRNZFc1cavu7G0y3GM\nJPXa8HqzTWQKBHPlfdkp3AUm1fMn2P/PVwAdCueIrCjkULsbVHngbUwsC4IH7f5E\nmc0HE1NhiabzYia7J+yHbT9Ya5wEN34EUqtMoOYaIcCf+gczsJ5i3xYMeaL/FXe5\nmMRwK4iRqr6hLdfIAQ==\n-----END CERTIFICATE-----\n">>},
   {pkey_passphrase_settings,[]},
   {certs_epoch,0},
   {hostname,"127.0.0.1"}]
[ns_server:debug,2022-09-07T14:32:08.967Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:439]renaming node conf {node,'ns_1@cb.local',erl_external_listeners} -> {node,
                                                                     'ns_1@127.0.0.1',
                                                                     erl_external_listeners}:
  [{inet,false}] ->
  [{inet,false}]
[ns_server:debug,2022-09-07T14:32:08.967Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:439]renaming node conf {node,'ns_1@cb.local',node_encryption} -> {node,
                                                              'ns_1@127.0.0.1',
                                                              node_encryption}:
  false ->
  false
[ns_server:debug,2022-09-07T14:32:08.967Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:439]renaming node conf {node,'ns_1@cb.local',address_family} -> {node,
                                                             'ns_1@127.0.0.1',
                                                             address_family}:
  inet ->
  inet
[ns_server:debug,2022-09-07T14:32:08.967Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:439]renaming node conf quorum_nodes -> quorum_nodes:
  ['ns_1@cb.local'] ->
  ['ns_1@127.0.0.1']
[ns_server:debug,2022-09-07T14:32:08.967Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:439]renaming node conf {node,'ns_1@cb.local',audit} -> {node,'ns_1@127.0.0.1',
                                                    audit}:
  [] ->
  []
[ns_server:debug,2022-09-07T14:32:08.967Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:439]renaming node conf {node,'ns_1@cb.local',backup_grpc_port} -> {node,
                                                               'ns_1@127.0.0.1',
                                                               backup_grpc_port}:
  9124 ->
  9124
[ns_server:debug,2022-09-07T14:32:08.967Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:439]renaming node conf {node,'ns_1@cb.local',backup_http_port} -> {node,
                                                               'ns_1@127.0.0.1',
                                                               backup_http_port}:
  8097 ->
  8097
[ns_server:debug,2022-09-07T14:32:08.967Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:439]renaming node conf {node,'ns_1@cb.local',backup_https_port} -> {node,
                                                                'ns_1@127.0.0.1',
                                                                backup_https_port}:
  18097 ->
  18097
[ns_server:debug,2022-09-07T14:32:08.968Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:439]renaming node conf {node,'ns_1@cb.local',capi_port} -> {node,
                                                        'ns_1@127.0.0.1',
                                                        capi_port}:
  8092 ->
  8092
[ns_server:debug,2022-09-07T14:32:08.968Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:439]renaming node conf {node,'ns_1@cb.local',cbas_admin_port} -> {node,
                                                              'ns_1@127.0.0.1',
                                                              cbas_admin_port}:
  9110 ->
  9110
[ns_server:debug,2022-09-07T14:32:08.968Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:439]renaming node conf {node,'ns_1@cb.local',cbas_cc_client_port} -> {node,
                                                                  'ns_1@127.0.0.1',
                                                                  cbas_cc_client_port}:
  9113 ->
  9113
[ns_server:debug,2022-09-07T14:32:08.968Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:439]renaming node conf {node,'ns_1@cb.local',cbas_cc_cluster_port} -> {node,
                                                                   'ns_1@127.0.0.1',
                                                                   cbas_cc_cluster_port}:
  9112 ->
  9112
[ns_server:debug,2022-09-07T14:32:08.968Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:439]renaming node conf {node,'ns_1@cb.local',cbas_cc_http_port} -> {node,
                                                                'ns_1@127.0.0.1',
                                                                cbas_cc_http_port}:
  9111 ->
  9111
[ns_server:debug,2022-09-07T14:32:08.968Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:439]renaming node conf {node,'ns_1@cb.local',cbas_cluster_port} -> {node,
                                                                'ns_1@127.0.0.1',
                                                                cbas_cluster_port}:
  9115 ->
  9115
[ns_server:debug,2022-09-07T14:32:08.968Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:439]renaming node conf {node,'ns_1@cb.local',cbas_console_port} -> {node,
                                                                'ns_1@127.0.0.1',
                                                                cbas_console_port}:
  9114 ->
  9114
[ns_server:debug,2022-09-07T14:32:08.968Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:439]renaming node conf {node,'ns_1@cb.local',cbas_data_port} -> {node,
                                                             'ns_1@127.0.0.1',
                                                             cbas_data_port}:
  9116 ->
  9116
[ns_server:debug,2022-09-07T14:32:08.968Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:439]renaming node conf {node,'ns_1@cb.local',cbas_debug_port} -> {node,
                                                              'ns_1@127.0.0.1',
                                                              cbas_debug_port}:
  -1 ->
  -1
[ns_server:debug,2022-09-07T14:32:08.968Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:439]renaming node conf {node,'ns_1@cb.local',cbas_http_port} -> {node,
                                                             'ns_1@127.0.0.1',
                                                             cbas_http_port}:
  8095 ->
  8095
[ns_server:debug,2022-09-07T14:32:08.969Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:439]renaming node conf {node,'ns_1@cb.local',cbas_messaging_port} -> {node,
                                                                  'ns_1@127.0.0.1',
                                                                  cbas_messaging_port}:
  9118 ->
  9118
[ns_server:debug,2022-09-07T14:32:08.969Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:439]renaming node conf {node,'ns_1@cb.local',cbas_metadata_callback_port} -> {node,
                                                                          'ns_1@127.0.0.1',
                                                                          cbas_metadata_callback_port}:
  9119 ->
  9119
[ns_server:debug,2022-09-07T14:32:08.969Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:439]renaming node conf {node,'ns_1@cb.local',cbas_metadata_port} -> {node,
                                                                 'ns_1@127.0.0.1',
                                                                 cbas_metadata_port}:
  9121 ->
  9121
[ns_server:debug,2022-09-07T14:32:08.969Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:439]renaming node conf {node,'ns_1@cb.local',cbas_parent_port} -> {node,
                                                               'ns_1@127.0.0.1',
                                                               cbas_parent_port}:
  9122 ->
  9122
[ns_server:debug,2022-09-07T14:32:08.969Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:439]renaming node conf {node,'ns_1@cb.local',cbas_replication_port} -> {node,
                                                                    'ns_1@127.0.0.1',
                                                                    cbas_replication_port}:
  9120 ->
  9120
[ns_server:debug,2022-09-07T14:32:08.969Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:439]renaming node conf {node,'ns_1@cb.local',cbas_result_port} -> {node,
                                                               'ns_1@127.0.0.1',
                                                               cbas_result_port}:
  9117 ->
  9117
[ns_server:debug,2022-09-07T14:32:08.969Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:439]renaming node conf {node,'ns_1@cb.local',cbas_ssl_port} -> {node,
                                                            'ns_1@127.0.0.1',
                                                            cbas_ssl_port}:
  18095 ->
  18095
[ns_server:debug,2022-09-07T14:32:08.969Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:439]renaming node conf {node,'ns_1@cb.local',compaction_daemon} -> {node,
                                                                'ns_1@127.0.0.1',
                                                                compaction_daemon}:
  [{check_interval,30},
   {min_db_file_size,131072},
   {min_view_file_size,20971520}] ->
  [{check_interval,30},
   {min_db_file_size,131072},
   {min_view_file_size,20971520}]
[ns_server:debug,2022-09-07T14:32:08.969Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:439]renaming node conf {node,'ns_1@cb.local',config_version} -> {node,
                                                             'ns_1@127.0.0.1',
                                                             config_version}:
  {7,1} ->
  {7,1}
[ns_server:debug,2022-09-07T14:32:08.970Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:439]renaming node conf {node,'ns_1@cb.local',event_log} -> {node,
                                                        'ns_1@127.0.0.1',
                                                        event_log}:
  [{filename,"/opt/couchbase/var/lib/couchbase/event_log"}] ->
  [{filename,"/opt/couchbase/var/lib/couchbase/event_log"}]
[ns_server:debug,2022-09-07T14:32:08.970Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:439]renaming node conf {node,'ns_1@cb.local',eventing_debug_port} -> {node,
                                                                  'ns_1@127.0.0.1',
                                                                  eventing_debug_port}:
  9140 ->
  9140
[ns_server:debug,2022-09-07T14:32:08.970Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:439]renaming node conf {node,'ns_1@cb.local',eventing_http_port} -> {node,
                                                                 'ns_1@127.0.0.1',
                                                                 eventing_http_port}:
  8096 ->
  8096
[ns_server:debug,2022-09-07T14:32:08.970Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:439]renaming node conf {node,'ns_1@cb.local',eventing_https_port} -> {node,
                                                                  'ns_1@127.0.0.1',
                                                                  eventing_https_port}:
  18096 ->
  18096
[ns_server:debug,2022-09-07T14:32:08.970Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:439]renaming node conf {node,'ns_1@cb.local',fts_grpc_port} -> {node,
                                                            'ns_1@127.0.0.1',
                                                            fts_grpc_port}:
  9130 ->
  9130
[ns_server:debug,2022-09-07T14:32:08.970Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:439]renaming node conf {node,'ns_1@cb.local',fts_grpc_ssl_port} -> {node,
                                                                'ns_1@127.0.0.1',
                                                                fts_grpc_ssl_port}:
  19130 ->
  19130
[ns_server:debug,2022-09-07T14:32:08.971Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:439]renaming node conf {node,'ns_1@cb.local',fts_http_port} -> {node,
                                                            'ns_1@127.0.0.1',
                                                            fts_http_port}:
  8094 ->
  8094
[ns_server:debug,2022-09-07T14:32:08.971Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:439]renaming node conf {node,'ns_1@cb.local',fts_ssl_port} -> {node,
                                                           'ns_1@127.0.0.1',
                                                           fts_ssl_port}:
  18094 ->
  18094
[ns_server:debug,2022-09-07T14:32:08.971Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:439]renaming node conf {node,'ns_1@cb.local',indexer_admin_port} -> {node,
                                                                 'ns_1@127.0.0.1',
                                                                 indexer_admin_port}:
  9100 ->
  9100
[ns_server:debug,2022-09-07T14:32:08.971Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:439]renaming node conf {node,'ns_1@cb.local',indexer_http_port} -> {node,
                                                                'ns_1@127.0.0.1',
                                                                indexer_http_port}:
  9102 ->
  9102
[ns_server:debug,2022-09-07T14:32:08.971Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:439]renaming node conf {node,'ns_1@cb.local',indexer_https_port} -> {node,
                                                                 'ns_1@127.0.0.1',
                                                                 indexer_https_port}:
  19102 ->
  19102
[ns_server:debug,2022-09-07T14:32:08.971Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:439]renaming node conf {node,'ns_1@cb.local',indexer_scan_port} -> {node,
                                                                'ns_1@127.0.0.1',
                                                                indexer_scan_port}:
  9101 ->
  9101
[ns_server:debug,2022-09-07T14:32:08.971Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:439]renaming node conf {node,'ns_1@cb.local',indexer_stcatchup_port} -> {node,
                                                                     'ns_1@127.0.0.1',
                                                                     indexer_stcatchup_port}:
  9104 ->
  9104
[ns_server:debug,2022-09-07T14:32:08.971Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:439]renaming node conf {node,'ns_1@cb.local',indexer_stinit_port} -> {node,
                                                                  'ns_1@127.0.0.1',
                                                                  indexer_stinit_port}:
  9103 ->
  9103
[ns_server:debug,2022-09-07T14:32:08.971Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:439]renaming node conf {node,'ns_1@cb.local',indexer_stmaint_port} -> {node,
                                                                   'ns_1@127.0.0.1',
                                                                   indexer_stmaint_port}:
  9105 ->
  9105
[ns_server:debug,2022-09-07T14:32:08.971Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:439]renaming node conf {node,'ns_1@cb.local',is_enterprise} -> {node,
                                                            'ns_1@127.0.0.1',
                                                            is_enterprise}:
  true ->
  true
[ns_server:debug,2022-09-07T14:32:08.971Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:439]renaming node conf {node,'ns_1@cb.local',isasl} -> {node,'ns_1@127.0.0.1',
                                                    isasl}:
  [{path,"/opt/couchbase/var/lib/couchbase/isasl.pw"}] ->
  [{path,"/opt/couchbase/var/lib/couchbase/isasl.pw"}]
[ns_server:debug,2022-09-07T14:32:08.974Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:439]renaming node conf {node,'ns_1@cb.local',memcached} -> {node,
                                                        'ns_1@127.0.0.1',
                                                        memcached}:
  [{port,11210},
   {dedicated_port,11209},
   {dedicated_ssl_port,11206},
   {ssl_port,11207},
   {admin_user,"@ns_server"},
   {other_users,["@cbq-engine","@projector","@goxdcr","@index","@fts",
                 "@eventing","@cbas","@backup"]},
   {admin_pass,"*****"},
   {engines,[{membase,[{engine,"/opt/couchbase/lib/memcached/ep.so"},
                       {static_config_string,"failpartialwarmup=false"}]},
             {memcached,[{engine,"/opt/couchbase/lib/memcached/default_engine.so"},
                         {static_config_string,"vb0=true"}]}]},
   {config_path,"/opt/couchbase/var/lib/couchbase/config/memcached.json"},
   {audit_file,"/opt/couchbase/var/lib/couchbase/config/audit.json"},
   {rbac_file,"/opt/couchbase/var/lib/couchbase/config/memcached.rbac"},
   {log_path,"/opt/couchbase/var/lib/couchbase/logs"},
   {log_prefix,"memcached.log"},
   {log_generations,20},
   {log_cyclesize,10485760},
   {log_rotation_period,39003}] ->
  [{port,11210},
   {dedicated_port,11209},
   {dedicated_ssl_port,11206},
   {ssl_port,11207},
   {admin_user,"@ns_server"},
   {other_users,["@cbq-engine","@projector","@goxdcr","@index","@fts",
                 "@eventing","@cbas","@backup"]},
   {admin_pass,"*****"},
   {engines,[{membase,[{engine,"/opt/couchbase/lib/memcached/ep.so"},
                       {static_config_string,"failpartialwarmup=false"}]},
             {memcached,[{engine,"/opt/couchbase/lib/memcached/default_engine.so"},
                         {static_config_string,"vb0=true"}]}]},
   {config_path,"/opt/couchbase/var/lib/couchbase/config/memcached.json"},
   {audit_file,"/opt/couchbase/var/lib/couchbase/config/audit.json"},
   {rbac_file,"/opt/couchbase/var/lib/couchbase/config/memcached.rbac"},
   {log_path,"/opt/couchbase/var/lib/couchbase/logs"},
   {log_prefix,"memcached.log"},
   {log_generations,20},
   {log_cyclesize,10485760},
   {log_rotation_period,39003}]
[ns_server:debug,2022-09-07T14:32:08.974Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:439]renaming node conf {node,'ns_1@cb.local',memcached_config} -> {node,
                                                               'ns_1@127.0.0.1',
                                                               memcached_config}:
  {[{interfaces,{memcached_config_mgr,get_interfaces,[]}},
    {client_cert_auth,{memcached_config_mgr,client_cert_auth,[]}},
    {connection_idle_time,connection_idle_time},
    {privilege_debug,privilege_debug},
    {breakpad,
        {[{enabled,breakpad_enabled},
          {minidump_dir,{memcached_config_mgr,get_minidump_dir,[]}}]}},
    {admin,{"~s",[admin_user]}},
    {verbosity,verbosity},
    {audit_file,{"~s",[audit_file]}},
    {rbac_file,{"~s",[rbac_file]}},
    {dedupe_nmvb_maps,dedupe_nmvb_maps},
    {tracing_enabled,tracing_enabled},
    {datatype_snappy,{memcached_config_mgr,is_snappy_enabled,[]}},
    {xattr_enabled,true},
    {scramsha_fallback_salt,{memcached_config_mgr,get_fallback_salt,[]}},
    {collections_enabled,{memcached_config_mgr,collections_enabled,[]}},
    {enforce_tenant_limits_enabled,
        {memcached_config_mgr,should_enforce_limits,[]}},
    {max_connections,max_connections},
    {system_connections,system_connections},
    {num_reader_threads,num_reader_threads},
    {num_writer_threads,num_writer_threads},
    {num_auxio_threads,num_auxio_threads},
    {num_nonio_threads,num_nonio_threads},
    {num_storage_threads,num_storage_threads},
    {logger,
        {[{filename,{"~s/~s",[log_path,log_prefix]}},
          {cyclesize,log_cyclesize}]}},
    {external_auth_service,
        {memcached_config_mgr,get_external_auth_service,[]}},
    {active_external_users_push_interval,
        {memcached_config_mgr,get_external_users_push_interval,[]}},
    {prometheus,{memcached_config_mgr,prometheus_cfg,[]}}]} ->
  {[{interfaces,{memcached_config_mgr,get_interfaces,[]}},
    {client_cert_auth,{memcached_config_mgr,client_cert_auth,[]}},
    {connection_idle_time,connection_idle_time},
    {privilege_debug,privilege_debug},
    {breakpad,
        {[{enabled,breakpad_enabled},
          {minidump_dir,{memcached_config_mgr,get_minidump_dir,[]}}]}},
    {admin,{"~s",[admin_user]}},
    {verbosity,verbosity},
    {audit_file,{"~s",[audit_file]}},
    {rbac_file,{"~s",[rbac_file]}},
    {dedupe_nmvb_maps,dedupe_nmvb_maps},
    {tracing_enabled,tracing_enabled},
    {datatype_snappy,{memcached_config_mgr,is_snappy_enabled,[]}},
    {xattr_enabled,true},
    {scramsha_fallback_salt,{memcached_config_mgr,get_fallback_salt,[]}},
    {collections_enabled,{memcached_config_mgr,collections_enabled,[]}},
    {enforce_tenant_limits_enabled,
        {memcached_config_mgr,should_enforce_limits,[]}},
    {max_connections,max_connections},
    {system_connections,system_connections},
    {num_reader_threads,num_reader_threads},
    {num_writer_threads,num_writer_threads},
    {num_auxio_threads,num_auxio_threads},
    {num_nonio_threads,num_nonio_threads},
    {num_storage_threads,num_storage_threads},
    {logger,
        {[{filename,{"~s/~s",[log_path,log_prefix]}},
          {cyclesize,log_cyclesize}]}},
    {external_auth_service,
        {memcached_config_mgr,get_external_auth_service,[]}},
    {active_external_users_push_interval,
        {memcached_config_mgr,get_external_users_push_interval,[]}},
    {prometheus,{memcached_config_mgr,prometheus_cfg,[]}}]}
[ns_server:debug,2022-09-07T14:32:08.975Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:439]renaming node conf {node,'ns_1@cb.local',memcached_dedicated_ssl_port} -> {node,
                                                                           'ns_1@127.0.0.1',
                                                                           memcached_dedicated_ssl_port}:
  11206 ->
  11206
[ns_server:debug,2022-09-07T14:32:08.975Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:439]renaming node conf {node,'ns_1@cb.local',memcached_defaults} -> {node,
                                                                 'ns_1@127.0.0.1',
                                                                 memcached_defaults}:
  [{max_connections,65000},
   {system_connections,5000},
   {connection_idle_time,0},
   {verbosity,0},
   {privilege_debug,false},
   {breakpad_enabled,true},
   {breakpad_minidump_dir_path,"/opt/couchbase/var/lib/couchbase/crash"},
   {dedupe_nmvb_maps,false},
   {je_malloc_conf,undefined},
   {tracing_enabled,true},
   {datatype_snappy,true},
   {num_reader_threads,<<"default">>},
   {num_writer_threads,<<"default">>},
   {num_auxio_threads,<<"default">>},
   {num_nonio_threads,<<"default">>},
   {num_storage_threads,<<"default">>}] ->
  [{max_connections,65000},
   {system_connections,5000},
   {connection_idle_time,0},
   {verbosity,0},
   {privilege_debug,false},
   {breakpad_enabled,true},
   {breakpad_minidump_dir_path,"/opt/couchbase/var/lib/couchbase/crash"},
   {dedupe_nmvb_maps,false},
   {je_malloc_conf,undefined},
   {tracing_enabled,true},
   {datatype_snappy,true},
   {num_reader_threads,<<"default">>},
   {num_writer_threads,<<"default">>},
   {num_auxio_threads,<<"default">>},
   {num_nonio_threads,<<"default">>},
   {num_storage_threads,<<"default">>}]
[ns_server:debug,2022-09-07T14:32:08.975Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:439]renaming node conf {node,'ns_1@cb.local',memcached_prometheus} -> {node,
                                                                   'ns_1@127.0.0.1',
                                                                   memcached_prometheus}:
  11280 ->
  11280
[ns_server:debug,2022-09-07T14:32:08.976Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:439]renaming node conf {node,'ns_1@cb.local',ns_log} -> {node,'ns_1@127.0.0.1',
                                                     ns_log}:
  [{filename,"/opt/couchbase/var/lib/couchbase/ns_log"}] ->
  [{filename,"/opt/couchbase/var/lib/couchbase/ns_log"}]
[ns_server:debug,2022-09-07T14:32:08.976Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:439]renaming node conf {node,'ns_1@cb.local',port_servers} -> {node,
                                                           'ns_1@127.0.0.1',
                                                           port_servers}:
  [] ->
  []
[ns_server:debug,2022-09-07T14:32:08.976Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:439]renaming node conf {node,'ns_1@cb.local',projector_port} -> {node,
                                                             'ns_1@127.0.0.1',
                                                             projector_port}:
  9999 ->
  9999
[ns_server:debug,2022-09-07T14:32:08.976Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:439]renaming node conf {node,'ns_1@cb.local',projector_ssl_port} -> {node,
                                                                 'ns_1@127.0.0.1',
                                                                 projector_ssl_port}:
  9999 ->
  9999
[ns_server:debug,2022-09-07T14:32:08.976Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:439]renaming node conf {node,'ns_1@cb.local',prometheus_http_port} -> {node,
                                                                   'ns_1@127.0.0.1',
                                                                   prometheus_http_port}:
  9123 ->
  9123
[ns_server:debug,2022-09-07T14:32:08.976Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:439]renaming node conf {node,'ns_1@cb.local',query_port} -> {node,
                                                         'ns_1@127.0.0.1',
                                                         query_port}:
  8093 ->
  8093
[ns_server:debug,2022-09-07T14:32:08.976Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:439]renaming node conf {node,'ns_1@cb.local',rest} -> {node,'ns_1@127.0.0.1',rest}:
  [{port,8091},{port_meta,global}] ->
  [{port,8091},{port_meta,global}]
[ns_server:debug,2022-09-07T14:32:08.976Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:439]renaming node conf {node,'ns_1@cb.local',saslauthd_enabled} -> {node,
                                                                'ns_1@127.0.0.1',
                                                                saslauthd_enabled}:
  true ->
  true
[ns_server:debug,2022-09-07T14:32:08.976Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:439]renaming node conf {node,'ns_1@cb.local',ssl_capi_port} -> {node,
                                                            'ns_1@127.0.0.1',
                                                            ssl_capi_port}:
  18092 ->
  18092
[ns_server:debug,2022-09-07T14:32:08.976Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:439]renaming node conf {node,'ns_1@cb.local',ssl_query_port} -> {node,
                                                             'ns_1@127.0.0.1',
                                                             ssl_query_port}:
  18093 ->
  18093
[ns_server:debug,2022-09-07T14:32:08.976Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:439]renaming node conf {node,'ns_1@cb.local',ssl_rest_port} -> {node,
                                                            'ns_1@127.0.0.1',
                                                            ssl_rest_port}:
  18091 ->
  18091
[ns_server:debug,2022-09-07T14:32:08.976Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:439]renaming node conf {node,'ns_1@cb.local',uuid} -> {node,'ns_1@127.0.0.1',uuid}:
  <<"1678dfae96c38e07dff43c49b9f6967b">> ->
  <<"1678dfae96c38e07dff43c49b9f6967b">>
[ns_server:debug,2022-09-07T14:32:08.977Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:439]renaming node conf {node,'ns_1@cb.local',xdcr_rest_port} -> {node,
                                                             'ns_1@127.0.0.1',
                                                             xdcr_rest_port}:
  9998 ->
  9998
[ns_server:debug,2022-09-07T14:32:08.977Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:439]renaming node conf {node,'ns_1@cb.local',{project_intact,is_vulnerable}} -> {node,
                                                                             'ns_1@127.0.0.1',
                                                                             {project_intact,
                                                                              is_vulnerable}}:
  false ->
  false
[ns_server:debug,2022-09-07T14:32:08.977Z,ns_1@127.0.0.1:<0.8733.0>:dist_manager:wait_for_node:267]Waiting for connection to node 'couchdb_ns_1@cb.local' to be established
[error_logger:info,2022-09-07T14:32:08.977Z,ns_1@127.0.0.1:net_kernel<0.8750.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@cb.local'}}
[ns_server:warn,2022-09-07T14:32:08.977Z,ns_1@127.0.0.1:leader_quorum_nodes_manager<0.688.0>:leader_quorum_nodes_manager:handle_quorum_nodes_updated:155]Somebody else updated the quorum nodes when we are the master node.
Our quorum nodes: ['ns_1@cb.local']
Their quorum nodes: ['ns_1@127.0.0.1']
[ns_server:debug,2022-09-07T14:32:08.977Z,ns_1@127.0.0.1:net_kernel<0.8750.0>:cb_dist:info_msg:872]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2022-09-07T14:32:08.977Z,ns_1@127.0.0.1:leader_activities<0.678.0>:leader_activities:handle_internal_process_down:433]Process {quorum_nodes_manager,<0.688.0>} terminated with reason {shutdown,
                                                                 {quorum_nodes_update_conflict,
                                                                  ['ns_1@cb.local'],
                                                                  ['ns_1@127.0.0.1']}}
[ns_server:debug,2022-09-07T14:32:08.977Z,ns_1@127.0.0.1:<0.691.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_config_events,<0.688.0>} exited with reason {shutdown,
                                                                                {quorum_nodes_update_conflict,
                                                                                 ['ns_1@cb.local'],
                                                                                 ['ns_1@127.0.0.1']}}
[ns_server:debug,2022-09-07T14:32:08.977Z,ns_1@127.0.0.1:cb_dist<0.8747.0>:cb_dist:info_msg:872]cb_dist: Added connection {con,#Ref<0.2209594722.1843658758.38734>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2022-09-07T14:32:08.977Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{local_changes_count,<<"1678dfae96c38e07dff43c49b9f6967b">>} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{12,63829780328}}]}]
[ns_server:info,2022-09-07T14:32:08.978Z,ns_1@127.0.0.1:ns_ssl_services_setup<0.287.0>:ns_ssl_services_setup:handle_info:602]cert_and_pkey changed
[ns_server:debug,2022-09-07T14:32:08.978Z,ns_1@127.0.0.1:cb_dist<0.8747.0>:cb_dist:info_msg:872]cb_dist: Updated connection: {con,#Ref<0.2209594722.1843658758.38734>,
                                  inet_tcp_dist,<0.8766.0>,
                                  #Ref<0.2209594722.1843658755.40360>}
[error_logger:error,2022-09-07T14:32:08.977Z,ns_1@127.0.0.1:mb_master_sup<0.685.0>:ale_error_logger_handler:do_log:101]
=========================SUPERVISOR REPORT=========================
    supervisor: {local,mb_master_sup}
    errorContext: child_terminated
    reason: {shutdown,
                {quorum_nodes_update_conflict,
                    ['ns_1@cb.local'],
                    ['ns_1@127.0.0.1']}}
    offender: [{pid,<0.688.0>},
               {id,leader_quorum_nodes_manager},
               {mfargs,{leader_quorum_nodes_manager,start_link,[]}},
               {restart_type,permanent},
               {significant,false},
               {shutdown,1000},
               {child_type,worker}]

[ns_server:debug,2022-09-07T14:32:08.978Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',{project_intact,is_vulnerable}} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|false]
[ns_server:debug,2022-09-07T14:32:08.978Z,ns_1@127.0.0.1:prometheus_cfg<0.415.0>:prometheus_cfg:maybe_apply_new_settings:608]Settings didn't change, ignoring update
[ns_server:debug,2022-09-07T14:32:08.978Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',xdcr_rest_port} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|9998]
[ns_server:debug,2022-09-07T14:32:08.978Z,ns_1@127.0.0.1:leader_quorum_nodes_manager<0.8768.0>:leader_quorum_nodes_manager:pull_config:99]Attempting to pull config from nodes:
[]
[error_logger:info,2022-09-07T14:32:08.978Z,ns_1@127.0.0.1:mb_master_sup<0.685.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,mb_master_sup}
    started: [{pid,<0.8768.0>},
              {id,leader_quorum_nodes_manager},
              {mfargs,{leader_quorum_nodes_manager,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2022-09-07T14:32:08.978Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',uuid} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|
 <<"1678dfae96c38e07dff43c49b9f6967b">>]
[ns_server:debug,2022-09-07T14:32:08.978Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',ssl_rest_port} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|18091]
[ns_server:debug,2022-09-07T14:32:08.978Z,ns_1@127.0.0.1:terse_cluster_info_uploader<0.613.0>:terse_cluster_info_uploader:handle_info:42]Refreshing terse cluster info with <<"{\"rev\":23,\"nodesExt\":[{\"services\":{\"capi\":8092,\"capiSSL\":18092,\"kv\":11210,\"kvSSL\":11207,\"mgmt\":8091,\"mgmtSSL\":18091,\"projector\":9999},\"thisNode\":true}],\"clusterCapabilitiesVer\":[1,0],\"clusterCapabilities\":{\"n1ql\":[\"enhancedPreparedStatements\"]},\"revEpoch\":1}">>
[ns_server:debug,2022-09-07T14:32:08.978Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',ssl_query_port} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|18093]
[ns_server:debug,2022-09-07T14:32:08.978Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',ssl_capi_port} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|18092]
[ns_server:debug,2022-09-07T14:32:08.978Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',saslauthd_enabled} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|true]
[ns_server:debug,2022-09-07T14:32:08.978Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',rest} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]},
 {port,8091},
 {port_meta,global}]
[ns_server:debug,2022-09-07T14:32:08.978Z,ns_1@127.0.0.1:prometheus_cfg<0.415.0>:prometheus_cfg:maybe_apply_new_settings:608]Settings didn't change, ignoring update
[ns_server:debug,2022-09-07T14:32:08.979Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',query_port} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|8093]
[ns_server:debug,2022-09-07T14:32:08.978Z,ns_1@127.0.0.1:ns_config_rep<0.444.0>:ns_config_rep:do_push_keys:383]Replicating some config keys ([quorum_nodes,
                               {local_changes_count,
                                   <<"1678dfae96c38e07dff43c49b9f6967b">>},
                               {node,'ns_1@127.0.0.1',address_family},
                               {node,'ns_1@127.0.0.1',audit},
                               {node,'ns_1@127.0.0.1',backup_grpc_port},
                               {node,'ns_1@127.0.0.1',backup_http_port},
                               {node,'ns_1@127.0.0.1',backup_https_port},
                               {node,'ns_1@127.0.0.1',capi_port},
                               {node,'ns_1@127.0.0.1',cbas_admin_port},
                               {node,'ns_1@127.0.0.1',cbas_cc_client_port},
                               {node,'ns_1@127.0.0.1',cbas_cc_cluster_port},
                               {node,'ns_1@127.0.0.1',cbas_cc_http_port},
                               {node,'ns_1@127.0.0.1',cbas_cluster_port},
                               {node,'ns_1@127.0.0.1',cbas_console_port},
                               {node,'ns_1@127.0.0.1',cbas_data_port},
                               {node,'ns_1@127.0.0.1',cbas_debug_port},
                               {node,'ns_1@127.0.0.1',cbas_dirs},
                               {node,'ns_1@127.0.0.1',cbas_http_port},
                               {node,'ns_1@127.0.0.1',cbas_messaging_port},
                               {node,'ns_1@127.0.0.1',
                                   cbas_metadata_callback_port},
                               {node,'ns_1@127.0.0.1',cbas_metadata_port},
                               {node,'ns_1@127.0.0.1',cbas_parent_port},
                               {node,'ns_1@127.0.0.1',cbas_replication_port},
                               {node,'ns_1@127.0.0.1',cbas_result_port},
                               {node,'ns_1@127.0.0.1',cbas_ssl_port},
                               {node,'ns_1@127.0.0.1',compaction_daemon},
                               {node,'ns_1@127.0.0.1',config_version},
                               {node,'ns_1@127.0.0.1',erl_external_listeners},
                               {node,'ns_1@127.0.0.1',event_log},
                               {node,'ns_1@127.0.0.1',eventing_debug_port},
                               {node,'ns_1@127.0.0.1',eventing_dir},
                               {node,'ns_1@127.0.0.1',eventing_http_port},
                               {node,'ns_1@127.0.0.1',eventing_https_port},
                               {node,'ns_1@127.0.0.1',fts_grpc_port},
                               {node,'ns_1@127.0.0.1',fts_grpc_ssl_port},
                               {node,'ns_1@127.0.0.1',fts_http_port},
                               {node,'ns_1@127.0.0.1',fts_ssl_port},
                               {node,'ns_1@127.0.0.1',indexer_admin_port},
                               {node,'ns_1@127.0.0.1',indexer_http_port},
                               {node,'ns_1@127.0.0.1',indexer_https_port},
                               {node,'ns_1@127.0.0.1',indexer_scan_port},
                               {node,'ns_1@127.0.0.1',indexer_stcatchup_port},
                               {node,'ns_1@127.0.0.1',indexer_stinit_port},
                               {node,'ns_1@127.0.0.1',indexer_stmaint_port},
                               {node,'ns_1@127.0.0.1',is_enterprise},
                               {node,'ns_1@127.0.0.1',isasl},
                               {node,'ns_1@127.0.0.1',membership},
                               {node,'ns_1@127.0.0.1',memcached},
                               {node,'ns_1@127.0.0.1',memcached_config},
                               {node,'ns_1@127.0.0.1',
                                   memcached_dedicated_ssl_port},
                               {node,'ns_1@127.0.0.1',memcached_defaults},
                               {node,'ns_1@127.0.0.1',memcached_prometheus},
                               {node,'ns_1@127.0.0.1',node_cert},
                               {node,'ns_1@127.0.0.1',node_encryption},
                               {node,'ns_1@127.0.0.1',ns_log},
                               {node,'ns_1@127.0.0.1',port_servers},
                               {node,'ns_1@127.0.0.1',projector_port},
                               {node,'ns_1@127.0.0.1',projector_ssl_port},
                               {node,'ns_1@127.0.0.1',prometheus_auth_info},
                               {node,'ns_1@127.0.0.1',prometheus_http_port},
                               {node,'ns_1@127.0.0.1',query_port},
                               {node,'ns_1@127.0.0.1',rest},
                               {node,'ns_1@127.0.0.1',saslauthd_enabled},
                               {node,'ns_1@127.0.0.1',ssl_capi_port}]..)
[ns_server:debug,2022-09-07T14:32:08.979Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',prometheus_http_port} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|9123]
[ns_server:debug,2022-09-07T14:32:08.979Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',projector_ssl_port} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|9999]
[ns_server:debug,2022-09-07T14:32:08.979Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',projector_port} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|9999]
[ns_server:debug,2022-09-07T14:32:08.979Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',port_servers} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}]
[ns_server:debug,2022-09-07T14:32:08.979Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',ns_log} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]},
 {filename,"/opt/couchbase/var/lib/couchbase/ns_log"}]
[ns_server:debug,2022-09-07T14:32:08.980Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',memcached_prometheus} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|11280]
[ns_server:debug,2022-09-07T14:32:08.980Z,ns_1@127.0.0.1:prometheus_cfg<0.415.0>:prometheus_cfg:maybe_apply_new_settings:608]Settings didn't change, ignoring update
[ns_server:debug,2022-09-07T14:32:08.980Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',memcached_defaults} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]},
 {max_connections,65000},
 {system_connections,5000},
 {connection_idle_time,0},
 {verbosity,0},
 {privilege_debug,false},
 {breakpad_enabled,true},
 {breakpad_minidump_dir_path,"/opt/couchbase/var/lib/couchbase/crash"},
 {dedupe_nmvb_maps,false},
 {je_malloc_conf,undefined},
 {tracing_enabled,true},
 {datatype_snappy,true},
 {num_reader_threads,<<"default">>},
 {num_writer_threads,<<"default">>},
 {num_auxio_threads,<<"default">>},
 {num_nonio_threads,<<"default">>},
 {num_storage_threads,<<"default">>}]
[ns_server:debug,2022-09-07T14:32:08.980Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',memcached_dedicated_ssl_port} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|11206]
[ns_server:debug,2022-09-07T14:32:08.980Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',memcached_config} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|
 {[{interfaces,{memcached_config_mgr,get_interfaces,[]}},
   {client_cert_auth,{memcached_config_mgr,client_cert_auth,[]}},
   {connection_idle_time,connection_idle_time},
   {privilege_debug,privilege_debug},
   {breakpad,
    {[{enabled,breakpad_enabled},
      {minidump_dir,{memcached_config_mgr,get_minidump_dir,[]}}]}},
   {admin,{"~s",[admin_user]}},
   {verbosity,verbosity},
   {audit_file,{"~s",[audit_file]}},
   {rbac_file,{"~s",[rbac_file]}},
   {dedupe_nmvb_maps,dedupe_nmvb_maps},
   {tracing_enabled,tracing_enabled},
   {datatype_snappy,{memcached_config_mgr,is_snappy_enabled,[]}},
   {xattr_enabled,true},
   {scramsha_fallback_salt,{memcached_config_mgr,get_fallback_salt,[]}},
   {collections_enabled,{memcached_config_mgr,collections_enabled,[]}},
   {enforce_tenant_limits_enabled,
    {memcached_config_mgr,should_enforce_limits,[]}},
   {max_connections,max_connections},
   {system_connections,system_connections},
   {num_reader_threads,num_reader_threads},
   {num_writer_threads,num_writer_threads},
   {num_auxio_threads,num_auxio_threads},
   {num_nonio_threads,num_nonio_threads},
   {num_storage_threads,num_storage_threads},
   {logger,
    {[{filename,{"~s/~s",[log_path,log_prefix]}},{cyclesize,log_cyclesize}]}},
   {external_auth_service,{memcached_config_mgr,get_external_auth_service,[]}},
   {active_external_users_push_interval,
    {memcached_config_mgr,get_external_users_push_interval,[]}},
   {prometheus,{memcached_config_mgr,prometheus_cfg,[]}}]}]
[ns_server:debug,2022-09-07T14:32:08.981Z,ns_1@127.0.0.1:leader_quorum_nodes_manager<0.8768.0>:leader_quorum_nodes_manager:pull_config:104]Pulled config successfully.
[ns_server:debug,2022-09-07T14:32:08.981Z,ns_1@127.0.0.1:<0.8733.0>:dist_manager:wait_for_node:279]Observed node 'couchdb_ns_1@cb.local' to come up
[ns_server:debug,2022-09-07T14:32:08.981Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',memcached} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]},
 {port,11210},
 {dedicated_port,11209},
 {dedicated_ssl_port,11206},
 {ssl_port,11207},
 {admin_user,"@ns_server"},
 {other_users,["@cbq-engine","@projector","@goxdcr","@index","@fts",
               "@eventing","@cbas","@backup"]},
 {admin_pass,"*****"},
 {engines,[{membase,[{engine,"/opt/couchbase/lib/memcached/ep.so"},
                     {static_config_string,"failpartialwarmup=false"}]},
           {memcached,[{engine,"/opt/couchbase/lib/memcached/default_engine.so"},
                       {static_config_string,"vb0=true"}]}]},
 {config_path,"/opt/couchbase/var/lib/couchbase/config/memcached.json"},
 {audit_file,"/opt/couchbase/var/lib/couchbase/config/audit.json"},
 {rbac_file,"/opt/couchbase/var/lib/couchbase/config/memcached.rbac"},
 {log_path,"/opt/couchbase/var/lib/couchbase/logs"},
 {log_prefix,"memcached.log"},
 {log_generations,20},
 {log_cyclesize,10485760},
 {log_rotation_period,39003}]
[ns_server:debug,2022-09-07T14:32:08.981Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',isasl} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]},
 {path,"/opt/couchbase/var/lib/couchbase/isasl.pw"}]
[ns_server:debug,2022-09-07T14:32:08.981Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',is_enterprise} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|true]
[ns_server:debug,2022-09-07T14:32:08.981Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',indexer_stmaint_port} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|9105]
[ns_server:debug,2022-09-07T14:32:08.981Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',indexer_stinit_port} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|9103]
[ns_server:debug,2022-09-07T14:32:08.981Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',indexer_stcatchup_port} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|9104]
[ns_server:debug,2022-09-07T14:32:08.981Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',indexer_scan_port} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|9101]
[ns_server:debug,2022-09-07T14:32:08.981Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',indexer_https_port} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|19102]
[ns_server:debug,2022-09-07T14:32:08.981Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',indexer_http_port} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|9102]
[ns_server:debug,2022-09-07T14:32:08.982Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',indexer_admin_port} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|9100]
[ns_server:debug,2022-09-07T14:32:08.982Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',fts_ssl_port} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|18094]
[ns_server:debug,2022-09-07T14:32:08.982Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',fts_http_port} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|8094]
[ns_server:debug,2022-09-07T14:32:08.982Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',fts_grpc_ssl_port} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|19130]
[ns_server:debug,2022-09-07T14:32:08.982Z,ns_1@127.0.0.1:terse_cluster_info_uploader<0.613.0>:terse_cluster_info_uploader:handle_info:42]Refreshing terse cluster info with <<"{\"rev\":23,\"nodesExt\":[{\"services\":{\"capi\":8092,\"capiSSL\":18092,\"kv\":11210,\"kvSSL\":11207,\"mgmt\":8091,\"mgmtSSL\":18091,\"projector\":9999},\"thisNode\":true}],\"clusterCapabilitiesVer\":[1,0],\"clusterCapabilities\":{\"n1ql\":[\"enhancedPreparedStatements\"]},\"revEpoch\":1}">>
[ns_server:debug,2022-09-07T14:32:08.982Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',fts_grpc_port} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|9130]
[ns_server:debug,2022-09-07T14:32:08.982Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',eventing_https_port} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|18096]
[ns_server:debug,2022-09-07T14:32:08.982Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',eventing_http_port} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|8096]
[ns_server:debug,2022-09-07T14:32:08.982Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',eventing_debug_port} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|9140]
[ns_server:debug,2022-09-07T14:32:08.982Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',event_log} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]},
 {filename,"/opt/couchbase/var/lib/couchbase/event_log"}]
[ns_server:debug,2022-09-07T14:32:08.983Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',config_version} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|{7,1}]
[ns_server:debug,2022-09-07T14:32:08.983Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',compaction_daemon} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]},
 {check_interval,30},
 {min_db_file_size,131072},
 {min_view_file_size,20971520}]
[ns_server:debug,2022-09-07T14:32:08.983Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',cbas_ssl_port} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|18095]
[ns_server:debug,2022-09-07T14:32:08.983Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',cbas_result_port} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|9117]
[ns_server:debug,2022-09-07T14:32:08.983Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',cbas_replication_port} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|9120]
[ns_server:debug,2022-09-07T14:32:08.983Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',cbas_parent_port} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|9122]
[ns_server:debug,2022-09-07T14:32:08.983Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',cbas_metadata_port} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|9121]
[ns_server:debug,2022-09-07T14:32:08.983Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',cbas_metadata_callback_port} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|9119]
[ns_server:debug,2022-09-07T14:32:08.983Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',cbas_messaging_port} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|9118]
[ns_server:debug,2022-09-07T14:32:08.983Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',cbas_http_port} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|8095]
[ns_server:debug,2022-09-07T14:32:08.983Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',cbas_debug_port} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|-1]
[ns_server:debug,2022-09-07T14:32:08.984Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',cbas_data_port} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|9116]
[ns_server:debug,2022-09-07T14:32:08.984Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',cbas_console_port} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|9114]
[ns_server:debug,2022-09-07T14:32:08.984Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',cbas_cluster_port} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|9115]
[ns_server:debug,2022-09-07T14:32:08.984Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',cbas_cc_http_port} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|9111]
[ns_server:debug,2022-09-07T14:32:08.984Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',cbas_cc_cluster_port} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|9112]
[ns_server:debug,2022-09-07T14:32:08.984Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',cbas_cc_client_port} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|9113]
[ns_server:debug,2022-09-07T14:32:08.984Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',cbas_admin_port} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|9110]
[ns_server:debug,2022-09-07T14:32:08.984Z,ns_1@127.0.0.1:ns_config_rep<0.444.0>:ns_config_rep:handle_cast:173]Synchronized with merger in 188 us
[ns_server:debug,2022-09-07T14:32:08.984Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',capi_port} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|8092]
[ns_server:debug,2022-09-07T14:32:08.984Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',backup_https_port} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|18097]
[ns_server:debug,2022-09-07T14:32:08.985Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',backup_http_port} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|8097]
[ns_server:debug,2022-09-07T14:32:08.985Z,ns_1@127.0.0.1:<0.8733.0>:dist_manager:complete_rename:399]Node 'ns_1@cb.local' has been renamed to 'ns_1@127.0.0.1'.
[ns_server:debug,2022-09-07T14:32:08.985Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',backup_grpc_port} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|9124]
[ns_server:debug,2022-09-07T14:32:08.985Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',audit} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}]
[ns_server:debug,2022-09-07T14:32:08.985Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
quorum_nodes ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780328}}]},
 'ns_1@127.0.0.1']
[ns_server:debug,2022-09-07T14:32:08.985Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',address_family} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|inet]
[ns_server:debug,2022-09-07T14:32:08.985Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',node_encryption} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|false]
[ns_server:debug,2022-09-07T14:32:08.985Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',erl_external_listeners} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]},
 {inet,false}]
[ns_server:debug,2022-09-07T14:32:08.985Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',node_cert} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]},
 {subject,<<"CN=Couchbase Server Node (127.0.0.1)">>},
 {not_after,64691827199},
 {verified_with,<<48,185,21,78,241,90,231,242,183,201,180,2,229,131,239,69>>},
 {type,generated},
 {load_timestamp,63829780092},
 {ca,<<"-----BEGIN CERTIFICATE-----\nMIIDITCCAgmgAwIBAgIIFxKaUqQBFTwwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciA5NGQ2NGQ0NTAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgOTRkNjRk\nNDUwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQDCrzpi1QtF4PyrbI2Y\nQB0augORV0Ro18YRvmfImOFnIZpNAmUYZRU68E39LZL794EOVdu05NB0tXNOfVrG\nB6gQNwe"...>>},
 {pem,<<"-----BEGIN CERTIFICATE-----\nMIIDOTCCAiGgAwIBAgIIFxKaUsnSLBwwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciA5NGQ2NGQ0NTAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCwxKjAoBgNVBAMTIUNvdWNoYmFzZSBTZXJ2ZXIgTm9kZSAo\nMTI3LjAuMC4xKTCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBALESQvmu\nbrV8JXPnQh3zLduahBDrsIVoTPN0LJWCDoH6bNJOn7QdE6oSpVYqgG/rDVmX67GV\nEl3"...>>},
 {pkey_passphrase_settings,[]},
 {certs_epoch,0},
 {hostname,"127.0.0.1"}]
[ns_server:debug,2022-09-07T14:32:08.986Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',cbas_dirs} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]},
 "/opt/couchbase/var/lib/couchbase/data"]
[ns_server:debug,2022-09-07T14:32:08.986Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',eventing_dir} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]},
 47,111,112,116,47,99,111,117,99,104,98,97,115,101,47,118,97,114,47,108,105,
 98,47,99,111,117,99,104,98,97,115,101,47,100,97,116,97]
[ns_server:debug,2022-09-07T14:32:08.986Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',prometheus_auth_info} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|
 {"@prometheus",
  {auth,
   [{<<"plain">>,
     {sanitized,<<"M19CN2uT3IM7to60JvPLLFWe15QE+xqQj4ae4pILIQY=">>}},
    {<<"sha512">>,
     {[{<<"h">>,
        {sanitized,<<"ogcmvOcIDb3trVZUf8MQ924hUlHGPZQI7aFQbckkSIo=">>}},
       {<<"s">>,
        <<"XsQ6FZpbB9BqHVSzMNqrHDzDO0uulN8mxg267iifMDsUbKxifdxVg+Lc+pAhc3YVdmMhCop0SLTuJ+7LUuQAWw==">>},
       {<<"i">>,4000}]}},
    {<<"sha256">>,
     {[{<<"h">>,
        {sanitized,<<"Z1mj6nW/ZuHa02+s9poaXdfNdF/M/zUY/YJjIP5h0XQ=">>}},
       {<<"s">>,<<"ciebqiheHmvTVsOoEf7zCJiMybfXrUIx2RqHrbKMOF0=">>},
       {<<"i">>,4000}]}},
    {<<"sha1">>,
     {[{<<"h">>,
        {sanitized,<<"OrBaLmtLgYmmtpuHjerlFrjJQFp2/h97H+MvyWAZUtY=">>}},
       {<<"s">>,<<"0mqqcO1V739RDCcAsk84uGUFIGA=">>},
       {<<"i">>,4000}]}}]}}]
[ns_server:debug,2022-09-07T14:32:08.986Z,ns_1@127.0.0.1:<0.471.0>:restartable:loop:65]Restarting child <0.474.0>
  MFA: {ns_doctor_sup,start_link,[]}
  Shutdown policy: infinity
  Caller: {<0.8733.0>,#Ref<0.2209594722.1843658759.39495>}
[ns_server:debug,2022-09-07T14:32:08.986Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',membership} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{3,63829780328}}]}|
 '_deleted']
[ns_server:debug,2022-09-07T14:32:08.987Z,ns_1@127.0.0.1:<0.477.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.476.0>} exited with reason shutdown
[ns_server:debug,2022-09-07T14:32:08.987Z,ns_1@127.0.0.1:<0.471.0>:restartable:shutdown_child:114]Successfully terminated process <0.474.0>
[error_logger:info,2022-09-07T14:32:08.987Z,ns_1@127.0.0.1:ns_doctor_sup<0.8785.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_doctor_sup}
    started: [{pid,<0.8786.0>},
              {id,ns_doctor_events},
              {mfargs,{gen_event,start_link,[{local,ns_doctor_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2022-09-07T14:32:08.988Z,ns_1@127.0.0.1:ns_doctor_sup<0.8785.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_doctor_sup}
    started: [{pid,<0.8787.0>},
              {id,ns_doctor},
              {mfargs,{ns_doctor,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2022-09-07T14:32:08.988Z,ns_1@127.0.0.1:<0.471.0>:restartable:start_child:92]Started child process <0.8785.0>
  MFA: {ns_doctor_sup,start_link,[]}
[ns_server:debug,2022-09-07T14:32:08.988Z,ns_1@127.0.0.1:<0.675.0>:restartable:loop:65]Restarting child <0.676.0>
  MFA: {leader_services_sup,start_link,[]}
  Shutdown policy: infinity
  Caller: {<0.8733.0>,#Ref<0.2209594722.1843658754.48832>}
[ns_server:info,2022-09-07T14:32:08.988Z,ns_1@127.0.0.1:mb_master<0.683.0>:mb_master:terminate:298]Synchronously shutting down child mb_master_sup
[ns_server:debug,2022-09-07T14:32:08.988Z,ns_1@127.0.0.1:<0.750.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {compat_mode_events,<0.749.0>} exited with reason shutdown
[ns_server:debug,2022-09-07T14:32:08.988Z,ns_1@127.0.0.1:<0.758.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.757.0>} exited with reason shutdown
[ns_server:debug,2022-09-07T14:32:08.988Z,ns_1@127.0.0.1:<0.760.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_config_events,<0.759.0>} exited with reason shutdown
[ns_server:info,2022-09-07T14:32:08.988Z,ns_1@127.0.0.1:leader_registry<0.681.0>:leader_registry:handle_down:286]Process <0.759.0> registered as 'license_reporting' terminated.
[ns_server:info,2022-09-07T14:32:08.989Z,ns_1@127.0.0.1:leader_registry<0.681.0>:leader_registry:handle_down:286]Process <0.757.0> registered as 'tombstone_purger' terminated.
[ns_server:info,2022-09-07T14:32:08.989Z,ns_1@127.0.0.1:leader_registry<0.681.0>:leader_registry:handle_down:286]Process <0.749.0> registered as 'auto_failover' terminated.
[ns_server:debug,2022-09-07T14:32:08.989Z,ns_1@127.0.0.1:<0.698.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_config_events,<0.696.0>} exited with reason shutdown
[ns_server:info,2022-09-07T14:32:08.989Z,ns_1@127.0.0.1:leader_registry<0.681.0>:leader_registry:handle_down:286]Process <0.747.0> registered as 'ns_orchestrator' terminated.
[ns_server:info,2022-09-07T14:32:08.989Z,ns_1@127.0.0.1:leader_registry<0.681.0>:leader_registry:handle_down:286]Process <0.746.0> registered as 'auto_rebalance' terminated.
[ns_server:debug,2022-09-07T14:32:08.989Z,ns_1@127.0.0.1:leader_activities<0.678.0>:leader_activities:handle_internal_process_down:433]Process {quorum_nodes_manager,<0.8768.0>} terminated with reason shutdown
[ns_server:info,2022-09-07T14:32:08.989Z,ns_1@127.0.0.1:leader_registry<0.681.0>:leader_registry:handle_down:286]Process <0.744.0> registered as 'auto_reprovision' terminated.
[ns_server:debug,2022-09-07T14:32:08.989Z,ns_1@127.0.0.1:<0.8778.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_config_events,<0.8768.0>} exited with reason shutdown
[ns_server:info,2022-09-07T14:32:08.989Z,ns_1@127.0.0.1:leader_registry<0.681.0>:leader_registry:handle_down:286]Process <0.696.0> registered as 'chronicle_master' terminated.
[ns_server:debug,2022-09-07T14:32:08.989Z,ns_1@127.0.0.1:leader_activities<0.678.0>:leader_activities:handle_internal_process_down:433]Process {acquirer,<0.686.0>} terminated with reason shutdown
[ns_server:info,2022-09-07T14:32:08.989Z,ns_1@127.0.0.1:leader_registry<0.681.0>:leader_registry:handle_down:286]Process <0.693.0> registered as 'ns_tick' terminated.
[ns_server:debug,2022-09-07T14:32:08.989Z,ns_1@127.0.0.1:<0.687.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_node_disco_events,<0.686.0>} exited with reason shutdown
[ns_server:debug,2022-09-07T14:32:08.989Z,ns_1@127.0.0.1:<0.684.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.683.0>} exited with reason shutdown
[ns_server:debug,2022-09-07T14:32:08.989Z,ns_1@127.0.0.1:<0.682.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {leader_events,<0.681.0>} exited with reason shutdown
[ns_server:warn,2022-09-07T14:32:08.989Z,ns_1@127.0.0.1:leader_lease_agent<0.679.0>:leader_lease_agent:handle_terminate:302]Terminating with reason shutdown when we own an active lease:
{lease,
    {lease_holder,<<"fa814c54dc8b348114b4d9a3567953c9">>,'ns_1@cb.local'},
    -576460513808768184,-576460498808768184,
    {timer,#Ref<0.2209594722.1843658757.39342>,
        {lease_expired,
            {lease_holder,<<"fa814c54dc8b348114b4d9a3567953c9">>,
                'ns_1@cb.local'}}},
    active}
Persisting updated lease.
[ns_server:debug,2022-09-07T14:32:08.989Z,ns_1@127.0.0.1:<0.697.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {'kv-events',<0.696.0>} exited with reason shutdown
[ns_server:debug,2022-09-07T14:32:09.005Z,ns_1@127.0.0.1:leader_activities<0.678.0>:leader_activities:handle_internal_process_down:433]Process {agent,<0.679.0>} terminated with reason shutdown
[ns_server:debug,2022-09-07T14:32:09.005Z,ns_1@127.0.0.1:<0.675.0>:restartable:shutdown_child:114]Successfully terminated process <0.676.0>
[error_logger:info,2022-09-07T14:32:09.005Z,ns_1@127.0.0.1:leader_leases_sup<0.8804.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,leader_leases_sup}
    started: [{pid,<0.8805.0>},
              {id,leader_activities},
              {mfargs,{leader_activities,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,10000},
              {child_type,worker}]

[ns_server:warn,2022-09-07T14:32:09.008Z,ns_1@127.0.0.1:leader_lease_agent<0.8806.0>:leader_lease_agent:maybe_recover_persisted_lease:390]Found persisted lease [{node,'ns_1@cb.local'},
                       {uuid,<<"fa814c54dc8b348114b4d9a3567953c9">>},
                       {time_left,13497},
                       {status,active}]
[error_logger:info,2022-09-07T14:32:09.008Z,ns_1@127.0.0.1:leader_leases_sup<0.8804.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,leader_leases_sup}
    started: [{pid,<0.8806.0>},
              {id,leader_lease_agent},
              {mfargs,{leader_lease_agent,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2022-09-07T14:32:09.008Z,ns_1@127.0.0.1:leader_services_sup<0.8803.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,leader_services_sup}
    started: [{pid,<0.8804.0>},
              {id,leader_leases_sup},
              {mfargs,{leader_leases_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2022-09-07T14:32:09.008Z,ns_1@127.0.0.1:leader_registry_sup<0.8808.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,leader_registry_sup}
    started: [{pid,<0.8809.0>},
              {id,leader_registry},
              {mfargs,{leader_registry,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2022-09-07T14:32:09.008Z,ns_1@127.0.0.1:leader_registry_sup<0.8808.0>:mb_master:check_master_takeover_needed:254]Sending master node question to the following nodes: []
[ns_server:debug,2022-09-07T14:32:09.009Z,ns_1@127.0.0.1:leader_registry_sup<0.8808.0>:mb_master:check_master_takeover_needed:256]Got replies: []
[ns_server:debug,2022-09-07T14:32:09.009Z,ns_1@127.0.0.1:leader_registry_sup<0.8808.0>:mb_master:check_master_takeover_needed:262]Was unable to discover master, not going to force mastership takeover
[ns_server:debug,2022-09-07T14:32:09.009Z,ns_1@127.0.0.1:mb_master<0.8811.0>:mb_master:init:80]Heartbeat interval is 2000
[user:info,2022-09-07T14:32:09.009Z,ns_1@127.0.0.1:mb_master<0.8811.0>:mb_master:init:85]I'm the only node, so I'm the master.
[ns_server:debug,2022-09-07T14:32:09.009Z,ns_1@127.0.0.1:leader_registry<0.8809.0>:leader_registry:handle_new_leader:275]New leader is 'ns_1@127.0.0.1'. Invalidating name cache.
[error_logger:info,2022-09-07T14:32:09.009Z,ns_1@127.0.0.1:mb_master_sup<0.8813.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,mb_master_sup}
    started: [{pid,<0.8814.0>},
              {id,leader_lease_acquirer},
              {mfargs,{leader_lease_acquirer,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,10000},
              {child_type,worker}]

[ns_server:debug,2022-09-07T14:32:09.009Z,ns_1@127.0.0.1:leader_quorum_nodes_manager<0.8816.0>:leader_quorum_nodes_manager:pull_config:99]Attempting to pull config from nodes:
[]
[error_logger:info,2022-09-07T14:32:09.009Z,ns_1@127.0.0.1:mb_master_sup<0.8813.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,mb_master_sup}
    started: [{pid,<0.8816.0>},
              {id,leader_quorum_nodes_manager},
              {mfargs,{leader_quorum_nodes_manager,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:info,2022-09-07T14:32:09.009Z,ns_1@127.0.0.1:mb_master_sup<0.8813.0>:misc:start_singleton:901]start_singleton(gen_server, start_link, [{via,leader_registry,ns_tick},
                                         ns_tick,[],[]]): started as <0.8819.0> on 'ns_1@127.0.0.1'

[ns_server:debug,2022-09-07T14:32:09.010Z,ns_1@127.0.0.1:leader_quorum_nodes_manager<0.8816.0>:leader_quorum_nodes_manager:pull_config:104]Pulled config successfully.
[ns_server:warn,2022-09-07T14:32:09.010Z,ns_1@127.0.0.1:<0.8817.0>:leader_lease_acquire_worker:handle_lease_already_acquired:226]Failed to acquire lease from 'ns_1@127.0.0.1' because its already taken by {'ns_1@cb.local',
                                                                            <<"fa814c54dc8b348114b4d9a3567953c9">>} (valid for 13495ms)
[error_logger:info,2022-09-07T14:32:09.010Z,ns_1@127.0.0.1:mb_master_sup<0.8813.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,mb_master_sup}
    started: [{pid,<0.8819.0>},
              {id,ns_tick},
              {mfargs,{ns_tick,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,10},
              {child_type,worker}]

[ns_server:debug,2022-09-07T14:32:09.010Z,ns_1@127.0.0.1:<0.8824.0>:chronicle_master:do_init:141]Starting with SelfRef = #Ref<0.2209594722.1843658759.39559>
[ns_server:info,2022-09-07T14:32:09.010Z,ns_1@127.0.0.1:mb_master_sup<0.8813.0>:misc:start_singleton:901]start_singleton(gen_server2, start_link, [{via,leader_registry,
                                           chronicle_master},
                                          chronicle_master,[],[]]): started as <0.8824.0> on 'ns_1@127.0.0.1'

[error_logger:info,2022-09-07T14:32:09.010Z,ns_1@127.0.0.1:mb_master_sup<0.8813.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,mb_master_sup}
    started: [{pid,<0.8824.0>},
              {id,chronicle_master},
              {mfargs,{chronicle_master,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2022-09-07T14:32:09.010Z,ns_1@127.0.0.1:ns_orchestrator_sup<0.8827.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_orchestrator_sup}
    started: [{pid,<0.8828.0>},
              {id,compat_mode_events},
              {mfargs,{gen_event,start_link,[{local,compat_mode_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2022-09-07T14:32:09.011Z,ns_1@127.0.0.1:ns_orchestrator_sup<0.8827.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_orchestrator_sup}
    started: [{pid,<0.8829.0>},
              {id,compat_mode_manager},
              {mfargs,{compat_mode_manager,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2022-09-07T14:32:09.011Z,ns_1@127.0.0.1:ns_orchestrator_child_sup<0.8830.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_orchestrator_child_sup}
    started: [{pid,<0.8831.0>},
              {id,ns_janitor_server},
              {mfargs,{ns_janitor_server,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:info,2022-09-07T14:32:09.011Z,ns_1@127.0.0.1:ns_orchestrator_child_sup<0.8830.0>:misc:start_singleton:901]start_singleton(gen_server, start_link, [{via,leader_registry,
                                          auto_reprovision},
                                         auto_reprovision,[],[]]): started as <0.8833.0> on 'ns_1@127.0.0.1'

[error_logger:info,2022-09-07T14:32:09.012Z,ns_1@127.0.0.1:ns_orchestrator_child_sup<0.8830.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_orchestrator_child_sup}
    started: [{pid,<0.8833.0>},
              {id,auto_reprovision},
              {mfargs,{auto_reprovision,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:info,2022-09-07T14:32:09.012Z,ns_1@127.0.0.1:ns_orchestrator_child_sup<0.8830.0>:misc:start_singleton:901]start_singleton(gen_server, start_link, [{via,leader_registry,auto_rebalance},
                                         auto_rebalance,[],[]]): started as <0.8838.0> on 'ns_1@127.0.0.1'

[error_logger:info,2022-09-07T14:32:09.012Z,ns_1@127.0.0.1:ns_orchestrator_child_sup<0.8830.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_orchestrator_child_sup}
    started: [{pid,<0.8838.0>},
              {id,auto_rebalance},
              {mfargs,{auto_rebalance,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:info,2022-09-07T14:32:09.012Z,ns_1@127.0.0.1:ns_orchestrator_child_sup<0.8830.0>:misc:start_singleton:901]start_singleton(gen_statem, start_link, [{via,leader_registry,ns_orchestrator},
                                         ns_orchestrator,[],[]]): started as <0.8839.0> on 'ns_1@127.0.0.1'

[error_logger:info,2022-09-07T14:32:09.012Z,ns_1@127.0.0.1:ns_orchestrator_child_sup<0.8830.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_orchestrator_child_sup}
    started: [{pid,<0.8839.0>},
              {id,ns_orchestrator},
              {mfargs,{ns_orchestrator,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2022-09-07T14:32:09.012Z,ns_1@127.0.0.1:ns_orchestrator_sup<0.8827.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_orchestrator_sup}
    started: [{pid,<0.8830.0>},
              {id,ns_orchestrator_child_sup},
              {mfargs,{ns_orchestrator_child_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2022-09-07T14:32:09.012Z,ns_1@127.0.0.1:<0.8841.0>:auto_failover:init:192]init auto_failover.
[user:info,2022-09-07T14:32:09.012Z,ns_1@127.0.0.1:<0.8841.0>:auto_failover:handle_call:223]Enabled auto-failover with timeout 120 and max count 1
[ns_server:info,2022-09-07T14:32:09.013Z,ns_1@127.0.0.1:ns_orchestrator_sup<0.8827.0>:misc:start_singleton:901]start_singleton(gen_server, start_link, [{via,leader_registry,auto_failover},
                                         auto_failover,[],[]]): started as <0.8841.0> on 'ns_1@127.0.0.1'

[error_logger:info,2022-09-07T14:32:09.013Z,ns_1@127.0.0.1:ns_orchestrator_sup<0.8827.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_orchestrator_sup}
    started: [{pid,<0.8841.0>},
              {id,auto_failover},
              {mfargs,{auto_failover,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2022-09-07T14:32:09.013Z,ns_1@127.0.0.1:mb_master_sup<0.8813.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,mb_master_sup}
    started: [{pid,<0.8827.0>},
              {id,ns_orchestrator_sup},
              {mfargs,{ns_orchestrator_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:info,2022-09-07T14:32:09.013Z,ns_1@127.0.0.1:mb_master_sup<0.8813.0>:misc:start_singleton:901]start_singleton(gen_server, start_link, [{via,leader_registry,
                                          tombstone_purger},
                                         tombstone_purger,[],[]]): started as <0.8843.0> on 'ns_1@127.0.0.1'

[error_logger:info,2022-09-07T14:32:09.013Z,ns_1@127.0.0.1:mb_master_sup<0.8813.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,mb_master_sup}
    started: [{pid,<0.8843.0>},
              {id,tombstone_purger},
              {mfargs,{tombstone_purger,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2022-09-07T14:32:09.013Z,ns_1@127.0.0.1:<0.8845.0>:license_reporting:init:60]Starting license_reporting server
[ns_server:info,2022-09-07T14:32:09.014Z,ns_1@127.0.0.1:mb_master_sup<0.8813.0>:misc:start_singleton:901]start_singleton(gen_server, start_link, [{via,leader_registry,
                                          license_reporting},
                                         license_reporting,[],[]]): started as <0.8845.0> on 'ns_1@127.0.0.1'

[error_logger:info,2022-09-07T14:32:09.014Z,ns_1@127.0.0.1:mb_master_sup<0.8813.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,mb_master_sup}
    started: [{pid,<0.8845.0>},
              {id,license_reporting},
              {mfargs,{license_reporting,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2022-09-07T14:32:09.014Z,ns_1@127.0.0.1:leader_registry_sup<0.8808.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,leader_registry_sup}
    started: [{pid,<0.8811.0>},
              {id,mb_master},
              {mfargs,{mb_master,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2022-09-07T14:32:09.014Z,ns_1@127.0.0.1:leader_services_sup<0.8803.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,leader_services_sup}
    started: [{pid,<0.8808.0>},
              {id,leader_registry_sup},
              {mfargs,{leader_registry_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2022-09-07T14:32:09.014Z,ns_1@127.0.0.1:<0.675.0>:restartable:start_child:92]Started child process <0.8803.0>
  MFA: {leader_services_sup,start_link,[]}
[ns_server:debug,2022-09-07T14:32:09.015Z,ns_1@127.0.0.1:remote_monitors<0.280.0>:remote_monitors:handle_info:86]Node renaming transaction ended. MRef = #Ref<0.2209594722.1843658755.40293>
[ns_server:debug,2022-09-07T14:32:09.015Z,ns_1@127.0.0.1:ns_node_disco<0.436.0>:ns_node_disco:handle_info:174]Node renaming transaction ended. MRef = #Ref<0.2209594722.1843658754.48702>
[cluster:debug,2022-09-07T14:32:09.015Z,ns_1@127.0.0.1:ns_cluster<0.253.0>:ns_cluster:maybe_rename:761]Renamed node from 'ns_1@cb.local' to 'ns_1@127.0.0.1'.
[ns_server:debug,2022-09-07T14:32:09.015Z,ns_1@127.0.0.1:wait_link_to_couchdb_node<0.374.0>:ns_server_nodes_sup:wait_link_to_couchdb_node_loop:204]Link to couchdb node was unpaused.
[ns_server:debug,2022-09-07T14:32:09.015Z,ns_1@127.0.0.1:memcached_config_mgr<0.598.0>:memcached_config_mgr:handle_info:190]Got DOWN with reason: unpaused from memcached port server: <15902.135.0>. Shutting down
[ns_server:debug,2022-09-07T14:32:09.015Z,ns_1@127.0.0.1:ns_ports_setup<0.565.0>:ns_ports_setup:children_loop_continue:93]Remote monitor <15902.129.0> was unpaused after node name change. Restart loop.
[cluster:info,2022-09-07T14:32:09.015Z,ns_1@127.0.0.1:ns_cluster<0.253.0>:ns_cluster:do_change_address:730]Renamed node. New name is 'ns_1@127.0.0.1'.
[ns_server:debug,2022-09-07T14:32:09.015Z,ns_1@127.0.0.1:wait_link_to_couchdb_node<0.374.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:160]Waiting for ns_couchdb node to start
[ns_server:debug,2022-09-07T14:32:09.015Z,ns_1@127.0.0.1:ns_node_disco_events<0.435.0>:ns_config_rep:handle_node_disco_event:512]Detected new nodes (['ns_1@127.0.0.1']).  Moving config around.
[ns_server:info,2022-09-07T14:32:09.016Z,ns_1@127.0.0.1:ns_node_disco_events<0.435.0>:ns_node_disco_log:handle_event:40]ns_node_disco_log: nodes changed: ['ns_1@127.0.0.1']
[ns_server:debug,2022-09-07T14:32:09.015Z,ns_1@127.0.0.1:ns_cookie_manager<0.221.0>:ns_cookie_manager:do_cookie_sync:101]ns_cookie_manager do_cookie_sync
[ns_server:debug,2022-09-07T14:32:09.016Z,ns_1@127.0.0.1:<0.707.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_config_events,<0.598.0>} exited with reason {shutdown,
                                                                                {memcached_port_server_down,
                                                                                 <15902.135.0>,
                                                                                 unpaused}}
[ns_server:debug,2022-09-07T14:32:09.016Z,ns_1@127.0.0.1:<0.8847.0>:ns_node_disco:do_nodes_wanted_updated_fun:224]ns_node_disco: nodes_wanted updated: ['ns_1@127.0.0.1'], with cookie: {sanitized,
                                                                       <<"6jw2YfUfDxiQCs0fCwoUSmHbOdRC4E/A03fIDwJPR3g=">>}
[error_logger:error,2022-09-07T14:32:09.016Z,ns_1@127.0.0.1:ns_server_sup<0.399.0>:ale_error_logger_handler:do_log:101]
=========================SUPERVISOR REPORT=========================
    supervisor: {local,ns_server_sup}
    errorContext: child_terminated
    reason: {shutdown,{memcached_port_server_down,<15902.135.0>,unpaused}}
    offender: [{pid,<0.598.0>},
               {name,memcached_config_mgr},
               {mfargs,{memcached_config_mgr,start_link,[]}},
               {restart_type,{permanent,4}},
               {shutdown,1000},
               {child_type,worker}]
[ns_server:debug,2022-09-07T14:32:09.016Z,ns_1@127.0.0.1:<0.8847.0>:ns_node_disco:do_nodes_wanted_updated_fun:227]ns_node_disco: nodes_wanted pong: ['ns_1@127.0.0.1'], with cookie: {sanitized,
                                                                    <<"6jw2YfUfDxiQCs0fCwoUSmHbOdRC4E/A03fIDwJPR3g=">>}
[ns_server:debug,2022-09-07T14:32:09.016Z,ns_1@127.0.0.1:memcached_config_mgr<0.8850.0>:memcached_config_mgr:init:54]waiting for completion of initial ns_ports_setup round
[error_logger:info,2022-09-07T14:32:09.016Z,ns_1@127.0.0.1:ns_server_sup<0.399.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.8850.0>},
              {name,memcached_config_mgr},
              {mfargs,{memcached_config_mgr,start_link,[]}},
              {restart_type,{permanent,4}},
              {shutdown,1000},
              {child_type,worker}]
[ns_server:debug,2022-09-07T14:32:09.017Z,ns_1@127.0.0.1:ns_ports_setup<0.565.0>:ns_ports_manager:set_dynamic_children:48]Setting children [memcached,saslauthd_port,goxdcr]
[ns_server:debug,2022-09-07T14:32:09.018Z,ns_1@127.0.0.1:ns_audit<0.597.0>:ns_audit:handle_call:148]Audit rename_node: [{local,{[{ip,<<"172.18.0.2">>},{port,8091}]}},
                    {remote,{[{ip,<<"172.18.0.1">>},{port,34858}]}},
                    {sessionid,<<"b411389657bba637036e362c7412193dcf883673">>},
                    {real_userid,{[{domain,wrong_token},
                                   {user,<<"<ud></ud>">>}]}},
                    {timestamp,<<"2022-09-07T14:32:09.018Z">>},
                    {hostname,<<"127.0.0.1">>},
                    {node,'ns_1@127.0.0.1'}]
[ns_server:debug,2022-09-07T14:32:09.018Z,ns_1@127.0.0.1:ns_ports_setup<0.565.0>:ns_ports_setup:set_children:60]Monitor ns_child_ports_sup <15902.129.0>
[ns_server:debug,2022-09-07T14:32:09.018Z,ns_1@127.0.0.1:memcached_config_mgr<0.8850.0>:memcached_config_mgr:init:56]ns_ports_setup seems to be ready
[ns_server:debug,2022-09-07T14:32:09.019Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{local_changes_count,<<"1678dfae96c38e07dff43c49b9f6967b">>} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{13,63829780329}}]}]
[ns_server:debug,2022-09-07T14:32:09.019Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
cluster_name ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780329}}]},
 116,111,100,111]
[ns_server:debug,2022-09-07T14:32:09.019Z,ns_1@127.0.0.1:ns_audit<0.597.0>:ns_audit:handle_call:148]Audit cluster_settings: [{local,{[{ip,<<"172.18.0.2">>},{port,8091}]}},
                         {remote,{[{ip,<<"172.18.0.1">>},{port,34858}]}},
                         {sessionid,<<"b411389657bba637036e362c7412193dcf883673">>},
                         {real_userid,{[{domain,wrong_token},
                                        {user,<<"<ud></ud>">>}]}},
                         {timestamp,<<"2022-09-07T14:32:09.019Z">>},
                         {cluster_name,<<"todo">>},
                         {quotas,{[{kv,4046},
                                   {index,512},
                                   {fts,512},
                                   {cbas,1529},
                                   {eventing,256}]}}]
[ns_server:debug,2022-09-07T14:32:09.020Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{local_changes_count,<<"1678dfae96c38e07dff43c49b9f6967b">>} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{14,63829780329}}]}]
[ns_server:debug,2022-09-07T14:32:09.020Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
settings ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780329}}]},
 {stats,[{send_stats,true}]}]
[ns_server:debug,2022-09-07T14:32:09.020Z,ns_1@127.0.0.1:ns_config_rep<0.444.0>:ns_config_rep:do_push_keys:383]Replicating some config keys ([cluster_name,settings,
                               {local_changes_count,
                                   <<"1678dfae96c38e07dff43c49b9f6967b">>}]..)
[ns_server:debug,2022-09-07T14:32:09.021Z,ns_1@127.0.0.1:memcached_config_mgr<0.8850.0>:memcached_config_mgr:find_port_pid_loop:156]Found memcached port <15902.135.0>
[ns_server:debug,2022-09-07T14:32:09.021Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
memory_quota ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780329}}]}|3739]
[ns_server:debug,2022-09-07T14:32:09.021Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
cbas_memory_quota ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780329}}]}|1488]
[ns_server:debug,2022-09-07T14:32:09.021Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{metakv,<<"/indexing/settings/config">>} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{3,63829780329}}]}|
 <<"{\"indexer.settings.rebalance.redistribute_indexes\":false,\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.log_level\":\"info\",\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.compaction.mi"...>>]
[ns_server:debug,2022-09-07T14:32:09.021Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
fts_memory_quota ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780329}}]}|256]
[ns_server:debug,2022-09-07T14:32:09.021Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{local_changes_count,<<"1678dfae96c38e07dff43c49b9f6967b">>} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{15,63829780329}}]}]
[ns_server:debug,2022-09-07T14:32:09.021Z,ns_1@127.0.0.1:ns_config_rep<0.444.0>:ns_config_rep:do_push_keys:383]Replicating some config keys ([cbas_memory_quota,fts_memory_quota,
                               memory_quota,
                               {local_changes_count,
                                   <<"1678dfae96c38e07dff43c49b9f6967b">>},
                               {metakv,<<"/indexing/settings/config">>}]..)
[ns_server:debug,2022-09-07T14:32:09.024Z,ns_1@127.0.0.1:memcached_config_mgr<0.8850.0>:memcached_config_mgr:do_read_current_memcached_config:363]Got enoent while trying to read active memcached config from /opt/couchbase/var/lib/couchbase/config/memcached.json.prev
[ns_server:debug,2022-09-07T14:32:09.026Z,ns_1@127.0.0.1:memcached_config_mgr<0.8850.0>:memcached_config_mgr:init:101]found memcached port to be already active
[ns_server:info,2022-09-07T14:32:09.027Z,ns_1@127.0.0.1:memcached_config_mgr<0.8850.0>:memcached_config_mgr:push_tls_config:225]Pushing TLS config to memcached:
{[{<<"private key">>,
   <<"/opt/couchbase/var/lib/couchbase/config/certs/pkey.pem">>},
  {<<"certificate chain">>,
   <<"/opt/couchbase/var/lib/couchbase/config/certs/chain.pem">>},
  {<<"CA file">>,<<"/opt/couchbase/var/lib/couchbase/config/certs/ca.pem">>},
  {<<"minimum version">>,<<"TLS 1.2">>},
  {<<"cipher list">>,
   {[{<<"TLS 1.2">>,<<"HIGH">>},
     {<<"TLS 1.3">>,
      <<"TLS_AES_256_GCM_SHA384:TLS_AES_128_GCM_SHA256:TLS_CHACHA20_POLY1305_SHA256">>}]}},
  {<<"cipher order">>,true},
  {<<"client cert auth">>,<<"disabled">>}]}
[ns_server:info,2022-09-07T14:32:09.033Z,ns_1@127.0.0.1:memcached_config_mgr<0.8850.0>:memcached_config_mgr:push_tls_config:229]Successfully pushed TLS config to memcached
[ns_server:debug,2022-09-07T14:32:09.048Z,ns_1@127.0.0.1:chronicle_kv_log<0.404.0>:chronicle_kv_log:log:59]update (key: {node,'ns_1@127.0.0.1',services}, rev: {<<"1933459bb8a062c61b619e931c483c80">>,
                                                     12})
[backup,cbas,eventing,fts,index,kv,n1ql]
[ns_server:debug,2022-09-07T14:32:09.048Z,ns_1@127.0.0.1:ns_audit<0.597.0>:ns_audit:handle_call:148]Audit setup_node_services: [{local,{[{ip,<<"172.18.0.2">>},{port,8091}]}},
                            {remote,{[{ip,<<"172.18.0.1">>},{port,34858}]}},
                            {sessionid,
                                <<"b411389657bba637036e362c7412193dcf883673">>},
                            {real_userid,
                                {[{domain,wrong_token},
                                  {user,<<"<ud></ud>">>}]}},
                            {timestamp,<<"2022-09-07T14:32:09.048Z">>},
                            {services,
                                [backup,cbas,eventing,fts,index,kv,n1ql]},
                            {node,'ns_1@127.0.0.1'}]
[ns_server:debug,2022-09-07T14:32:09.049Z,ns_1@127.0.0.1:terse_cluster_info_uploader<0.613.0>:terse_cluster_info_uploader:handle_info:42]Refreshing terse cluster info with <<"{\"rev\":27,\"nodesExt\":[{\"services\":{\"capi\":8092,\"capiSSL\":18092,\"kv\":11210,\"kvSSL\":11207,\"mgmt\":8091,\"mgmtSSL\":18091,\"projector\":9999},\"thisNode\":true}],\"clusterCapabilitiesVer\":[1,0],\"clusterCapabilities\":{\"n1ql\":[\"enhancedPreparedStatements\"]},\"revEpoch\":1}">>
[ns_server:debug,2022-09-07T14:32:09.048Z,ns_1@127.0.0.1:prometheus_cfg<0.415.0>:prometheus_cfg:maybe_apply_new_settings:611]New settings received: [{enabled,true},
                        {retention_size,1024},
                        {retention_time,365},
                        {wal_compression,false},
                        {storage_path,"./stats_data"},
                        {config_file,"prometheus.yml"},
                        {log_file_name,"prometheus.log"},
                        {prometheus_auth_enabled,true},
                        {prometheus_auth_filename,
                            "/opt/couchbase/var/lib/couchbase/config/prometheus_auth"},
                        {log_level,"debug"},
                        {max_block_duration,25},
                        {scrape_interval,10},
                        {scrape_timeout,10},
                        {snapshot_timeout_msecs,30000},
                        {decimation_enabled,false},
                        {truncation_enabled,false},
                        {clean_tombstones_enabled,false},
                        {decimation_defs,
                            [{low,14400,skip},
                             {medium,14400,20},
                             {large,57600,60},
                             {large,31536000,600}]},
                        {pruning_interval,60000},
                        {truncate_max_age,259200},
                        {decimation_match_patterns,["{job=\"general\"}"]},
                        {truncation_match_patterns,
                            ["{job=~\".*_high_cardinality\"}"]},
                        {token_file,"prometheus_token"},
                        {query_max_samples,200000},
                        {intervals_calculation_period,600000},
                        {cbcollect_stats_dump_max_size,1073741824},
                        {cbcollect_stats_min_period,14},
                        {average_sample_size,3},
                        {services,
                            [{ns_server,
                                 [{high_cardinality_enabled,true},
                                  {high_cardinality_scrape_interval,60},
                                  {high_cardinality_scrape_timeout,-1}]},
                             {index,
                                 [{high_cardinality_enabled,true},
                                  {high_cardinality_scrape_interval,-1},
                                  {high_cardinality_scrape_timeout,-1}]},
                             {fts,
                                 [{high_cardinality_enabled,true},
                                  {high_cardinality_scrape_interval,-1},
                                  {high_cardinality_scrape_timeout,-1}]},
                             {kv,[{high_cardinality_enabled,true},
                                  {high_cardinality_scrape_interval,-1},
                                  {high_cardinality_scrape_timeout,-1}]},
                             {cbas,
                                 [{high_cardinality_enabled,true},
                                  {high_cardinality_scrape_interval,-1},
                                  {high_cardinality_scrape_timeout,-1}]},
                             {eventing,
                                 [{high_cardinality_enabled,true},
                                  {high_cardinality_scrape_interval,-1},
                                  {high_cardinality_scrape_timeout,-1}]}]},
                        {external_prometheus_services,
                            [{index,[{high_cardinality_enabled,true}]},
                             {fts,[{high_cardinality_enabled,true}]},
                             {kv,[{high_cardinality_enabled,true}]},
                             {cbas,[{high_cardinality_enabled,true}]},
                             {eventing,[{high_cardinality_enabled,true}]},
                             {ns_server,[{high_cardinality_enabled,true}]}]},
                        {prometheus_metrics_enabled,false},
                        {prometheus_metrics_scrape_interval,60},
                        {listen_addr_type,loopback},
                        {log_queries,false},
                        {derived_metrics_filter,all},
                        {derived_metrics_interval,-1},
                        {rules_config_file,"prometheus_rules.yml"},
                        {loopback_delta,600},
                        {listen_port,9123},
                        {addr,"127.0.0.1:9123"},
                        {prometheus_creds,{"ns_server","********"}},
                        {targets,
                            [{ns_server,"127.0.0.1:8091"},
                             {xdcr,"127.0.0.1:9998"},
                             {backup,"127.0.0.1:8097"},
                             {cbas,"127.0.0.1:8095"},
                             {eventing,"127.0.0.1:8096"},
                             {fts,"127.0.0.1:8094"},
                             {index,"127.0.0.1:9102"},
                             {kv,"127.0.0.1:11280"},
                             {n1ql,"127.0.0.1:8093"}]},
                        {afamily,inet},
                        {dynamic_scrape_intervals,[]}]
Old settings: [{enabled,true},
               {retention_size,1024},
               {retention_time,365},
               {wal_compression,false},
               {storage_path,"./stats_data"},
               {config_file,"prometheus.yml"},
               {log_file_name,"prometheus.log"},
               {prometheus_auth_enabled,true},
               {prometheus_auth_filename,
                   "/opt/couchbase/var/lib/couchbase/config/prometheus_auth"},
               {log_level,"debug"},
               {max_block_duration,25},
               {scrape_interval,10},
               {scrape_timeout,10},
               {snapshot_timeout_msecs,30000},
               {decimation_enabled,false},
               {truncation_enabled,false},
               {clean_tombstones_enabled,false},
               {decimation_defs,
                   [{low,14400,skip},
                    {medium,14400,20},
                    {large,57600,60},
                    {large,31536000,600}]},
               {pruning_interval,60000},
               {truncate_max_age,259200},
               {decimation_match_patterns,["{job=\"general\"}"]},
               {truncation_match_patterns,["{job=~\".*_high_cardinality\"}"]},
               {token_file,"prometheus_token"},
               {query_max_samples,200000},
               {intervals_calculation_period,600000},
               {cbcollect_stats_dump_max_size,1073741824},
               {cbcollect_stats_min_period,14},
               {average_sample_size,3},
               {services,
                   [{ns_server,
                        [{high_cardinality_enabled,true},
                         {high_cardinality_scrape_interval,60},
                         {high_cardinality_scrape_timeout,-1}]},
                    {index,
                        [{high_cardinality_enabled,true},
                         {high_cardinality_scrape_interval,-1},
                         {high_cardinality_scrape_timeout,-1}]},
                    {fts,
                        [{high_cardinality_enabled,true},
                         {high_cardinality_scrape_interval,-1},
                         {high_cardinality_scrape_timeout,-1}]},
                    {kv,[{high_cardinality_enabled,true},
                         {high_cardinality_scrape_interval,-1},
                         {high_cardinality_scrape_timeout,-1}]},
                    {cbas,
                        [{high_cardinality_enabled,true},
                         {high_cardinality_scrape_interval,-1},
                         {high_cardinality_scrape_timeout,-1}]},
                    {eventing,
                        [{high_cardinality_enabled,true},
                         {high_cardinality_scrape_interval,-1},
                         {high_cardinality_scrape_timeout,-1}]}]},
               {external_prometheus_services,
                   [{index,[{high_cardinality_enabled,true}]},
                    {fts,[{high_cardinality_enabled,true}]},
                    {kv,[{high_cardinality_enabled,true}]},
                    {cbas,[{high_cardinality_enabled,true}]},
                    {eventing,[{high_cardinality_enabled,true}]},
                    {ns_server,[{high_cardinality_enabled,true}]}]},
               {prometheus_metrics_enabled,false},
               {prometheus_metrics_scrape_interval,60},
               {listen_addr_type,loopback},
               {log_queries,false},
               {derived_metrics_filter,all},
               {derived_metrics_interval,-1},
               {rules_config_file,"prometheus_rules.yml"},
               {loopback_delta,600},
               {listen_port,9123},
               {addr,"127.0.0.1:9123"},
               {prometheus_creds,{"ns_server","********"}},
               {targets,
                   [{ns_server,"127.0.0.1:8091"},
                    {xdcr,"127.0.0.1:9998"},
                    {kv,"127.0.0.1:11280"}]},
               {afamily,inet},
               {dynamic_scrape_intervals,[]}]
[ns_server:debug,2022-09-07T14:32:09.049Z,ns_1@127.0.0.1:ns_config_rep<0.444.0>:ns_config_rep:do_push_keys:383]Replicating some config keys ([{local_changes_count,
                                   <<"1678dfae96c38e07dff43c49b9f6967b">>},
                               {metakv,<<"/indexing/settings/config">>}]..)
[ns_server:debug,2022-09-07T14:32:09.049Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{metakv,<<"/indexing/settings/config">>} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{4,63829780329}}]}|
 <<"{\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.rebalance.redistribute_indexes\":false,\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.log_level\":\"info\",\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.compaction.mi"...>>]
[ns_server:debug,2022-09-07T14:32:09.049Z,ns_1@127.0.0.1:ns_audit<0.597.0>:ns_audit:handle_call:148]Audit modify_index_storage_mode: [{local,
                                      {[{ip,<<"172.18.0.2">>},{port,8091}]}},
                                  {remote,
                                      {[{ip,<<"172.18.0.1">>},{port,34858}]}},
                                  {sessionid,
                                      <<"b411389657bba637036e362c7412193dcf883673">>},
                                  {real_userid,
                                      {[{domain,wrong_token},
                                        {user,<<"<ud></ud>">>}]}},
                                  {timestamp,<<"2022-09-07T14:32:09.049Z">>},
                                  {storageMode,<<"plasma">>}]
[ns_server:debug,2022-09-07T14:32:09.049Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{local_changes_count,<<"1678dfae96c38e07dff43c49b9f6967b">>} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{16,63829780329}}]}]
[ns_server:debug,2022-09-07T14:32:09.050Z,ns_1@127.0.0.1:prometheus_cfg<0.415.0>:prometheus_cfg:ensure_prometheus_config:811]Updating prometheus config file: /opt/couchbase/var/lib/couchbase/config/prometheus.yml
[ns_server:debug,2022-09-07T14:32:09.065Z,ns_1@127.0.0.1:prometheus_cfg<0.415.0>:prometheus_cfg:ensure_prometheus_config:811]Updating prometheus config file: /opt/couchbase/var/lib/couchbase/config/prometheus_rules.yml
[ns_server:debug,2022-09-07T14:32:09.069Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{local_changes_count,<<"1678dfae96c38e07dff43c49b9f6967b">>} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{17,63829780329}}]}]
[ns_server:debug,2022-09-07T14:32:09.069Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
rest ->
[{port,8091}]
[ns_server:debug,2022-09-07T14:32:09.069Z,ns_1@127.0.0.1:ns_config_rep<0.444.0>:ns_config_rep:do_push_keys:383]Replicating some config keys ([rest,
                               {local_changes_count,
                                   <<"1678dfae96c38e07dff43c49b9f6967b">>}]..)
[ns_server:debug,2022-09-07T14:32:09.069Z,ns_1@127.0.0.1:terse_cluster_info_uploader<0.613.0>:terse_cluster_info_uploader:handle_info:42]Refreshing terse cluster info with <<"{\"rev\":29,\"nodesExt\":[{\"services\":{\"capi\":8092,\"capiSSL\":18092,\"kv\":11210,\"kvSSL\":11207,\"mgmt\":8091,\"mgmtSSL\":18091,\"projector\":9999},\"thisNode\":true}],\"clusterCapabilitiesVer\":[1,0],\"clusterCapabilities\":{\"n1ql\":[\"enhancedPreparedStatements\"]},\"revEpoch\":1}">>
[ns_server:debug,2022-09-07T14:32:09.075Z,ns_1@127.0.0.1:prometheus_cfg<0.415.0>:prometheus_cfg:try_config_reload:700]Reloading prometheus config
[ns_server:debug,2022-09-07T14:32:09.089Z,ns_1@127.0.0.1:prometheus_cfg<0.415.0>:prometheus_cfg:try_config_reload:703]Config successfully reloaded
[ns_server:debug,2022-09-07T14:32:09.090Z,ns_1@127.0.0.1:prometheus_cfg<0.415.0>:prometheus_cfg:maybe_apply_new_settings:608]Settings didn't change, ignoring update
[ns_server:debug,2022-09-07T14:32:09.103Z,ns_1@127.0.0.1:menelaus_ui_auth<0.500.0>:token_server:handle_cast:235]Purge tokens []
[ns_server:debug,2022-09-07T14:32:09.103Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{local_changes_count,<<"1678dfae96c38e07dff43c49b9f6967b">>} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{18,63829780329}}]}]
[ns_server:debug,2022-09-07T14:32:09.103Z,ns_1@127.0.0.1:compiled_roles_cache<0.362.0>:versioned_cache:handle_info:89]Flushing cache compiled_roles_cache due to version change from {[7,1],
                                                                {0,1415866166},
                                                                {0,1415866166},
                                                                false,[]} to {[7,
                                                                               1],
                                                                              {0,
                                                                               1415866166},
                                                                              {0,
                                                                               1415866166},
                                                                              true,
                                                                              []}
[ns_server:debug,2022-09-07T14:32:09.103Z,ns_1@127.0.0.1:ns_audit<0.597.0>:ns_audit:handle_call:148]Audit password_change: [{local,{[{ip,<<"172.18.0.2">>},{port,8091}]}},
                        {remote,{[{ip,<<"172.18.0.1">>},{port,34858}]}},
                        {sessionid,<<"b411389657bba637036e362c7412193dcf883673">>},
                        {real_userid,{[{domain,wrong_token},
                                       {user,<<"<ud></ud>">>}]}},
                        {timestamp,<<"2022-09-07T14:32:09.103Z">>},
                        {identity,{[{domain,builtin},
                                    {user,<<"<ud>admin</ud>">>}]}}]
[ns_server:debug,2022-09-07T14:32:09.103Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
rest_creds ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780329}}]}|
 {"<ud>admin</ud>",
  {auth,
   [{<<"plain">>,
     {sanitized,<<"mcPCh2CwRLArgh5PGoubNf43Umyo0hRlCWrpgFQXOVg=">>}},
    {<<"sha512">>,
     {[{<<"h">>,
        {sanitized,<<"U0RGkBHSXAG5dAsOvTH+QGobrx6orS5bdCBqgDyQ+BM=">>}},
       {<<"s">>,
        <<"UpTtwmbTZhnFLOkKwRU9RDn1G6Nr3oVYbVNJ+oayaNMdPzcjvoYDY0p8VEQ1tbuSWKwz2RlQlPyc5/c0qdE8Uw==">>},
       {<<"i">>,4000}]}},
    {<<"sha256">>,
     {[{<<"h">>,
        {sanitized,<<"Ru0lDlLhCS2xLqGsf8+eFR2FOQD8CQBcz5oQzOiYq2E=">>}},
       {<<"s">>,<<"2E26bewdxfGnlaB61iTF3UGV91wy8MaGmypQGeyTb7E=">>},
       {<<"i">>,4000}]}},
    {<<"sha1">>,
     {[{<<"h">>,
        {sanitized,<<"32H/xGHX51I6hfs1h89rVNzUuANXbLudifWbeUP4weA=">>}},
       {<<"s">>,<<"lMYLrzOTT/BNkybU5FRBrBsBbI0=">>},
       {<<"i">>,4000}]}}]}}]
[ns_server:debug,2022-09-07T14:32:09.103Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{local_changes_count,<<"1678dfae96c38e07dff43c49b9f6967b">>} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{19,63829780329}}]}]
[ns_server:debug,2022-09-07T14:32:09.103Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
uuid ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780329}}]}|
 <<"f7a8495a9eb390597cfca9b1032b2584">>]
[ns_server:debug,2022-09-07T14:32:09.103Z,ns_1@127.0.0.1:ns_config_rep<0.444.0>:ns_config_rep:do_push_keys:383]Replicating some config keys ([rest_creds,uuid,
                               {local_changes_count,
                                   <<"1678dfae96c38e07dff43c49b9f6967b">>}]..)
[ns_server:debug,2022-09-07T14:32:09.105Z,ns_1@127.0.0.1:memcached_passwords<0.425.0>:memcached_cfg:write_cfg:148]Writing config file for: "/opt/couchbase/var/lib/couchbase/isasl.pw"
[ns_server:debug,2022-09-07T14:32:09.106Z,ns_1@127.0.0.1:memcached_permissions<0.430.0>:memcached_cfg:write_cfg:148]Writing config file for: "/opt/couchbase/var/lib/couchbase/config/memcached.rbac"
[error_logger:info,2022-09-07T14:32:09.106Z,ns_1@127.0.0.1:service_monitor_children_sup<0.773.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,service_monitor_children_sup}
    started: [{pid,<0.8920.0>},
              {id,{kv,dcp_traffic_monitor}},
              {mfargs,{dcp_traffic_monitor,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2022-09-07T14:32:09.108Z,ns_1@127.0.0.1:memcached_permissions<0.430.0>:replicated_dets:select_from_table:281][ets] Starting select with {users_storage,
                               [{{docv2,{user,{'_',local}},'_','_'},
                                 [],
                                 ['$_']}],
                               100}
[ns_server:debug,2022-09-07T14:32:09.109Z,ns_1@127.0.0.1:compiled_roles_cache<0.362.0>:menelaus_roles:build_compiled_roles:998]Compile roles for user {"<ud>admin</ud>",admin}
[ns_server:debug,2022-09-07T14:32:09.109Z,ns_1@127.0.0.1:memcached_refresh<0.283.0>:memcached_refresh:handle_call:49]File rename from "/opt/couchbase/var/lib/couchbase/config/memcached.rbac.tmp" to "/opt/couchbase/var/lib/couchbase/config/memcached.rbac" is requested
[ns_server:debug,2022-09-07T14:32:09.109Z,ns_1@127.0.0.1:ns_audit<0.597.0>:ns_audit:handle_call:148]Audit login_success: [{local,{[{ip,<<"172.18.0.2">>},{port,8091}]}},
                      {remote,{[{ip,<<"172.18.0.1">>},{port,34854}]}},
                      {sessionid,<<"b3740d8cf0080a8b89291f496feba96c33ca5f53">>},
                      {real_userid,{[{domain,builtin},
                                     {user,<<"<ud>admin</ud>">>}]}},
                      {timestamp,<<"2022-09-07T14:32:09.109Z">>},
                      {roles,[<<"admin">>]}]
[error_logger:info,2022-09-07T14:32:09.110Z,ns_1@127.0.0.1:service_stats_children_sup<0.653.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,service_stats_children_sup}
    started: [{pid,<0.8924.0>},
              {id,{service_cbas,stats_reader,"@cbas"}},
              {mfargs,{stats_reader,start_link,["@cbas"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2022-09-07T14:32:09.111Z,ns_1@127.0.0.1:service_stats_children_sup<0.653.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,service_stats_children_sup}
    started: [{pid,<0.8926.0>},
              {id,{service_eventing,stats_reader,"@eventing"}},
              {mfargs,{stats_reader,start_link,["@eventing"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2022-09-07T14:32:09.111Z,ns_1@127.0.0.1:service_stats_children_sup<0.653.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,service_stats_children_sup}
    started: [{pid,<0.8928.0>},
              {id,{service_fts,stats_reader,"@fts"}},
              {mfargs,{stats_reader,start_link,["@fts"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2022-09-07T14:32:09.111Z,ns_1@127.0.0.1:service_stats_children_sup<0.653.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,service_stats_children_sup}
    started: [{pid,<0.8930.0>},
              {id,{service_index,stats_reader,"@index"}},
              {mfargs,{stats_reader,start_link,["@index"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2022-09-07T14:32:09.112Z,ns_1@127.0.0.1:memcached_refresh<0.283.0>:memcached_refresh:handle_cast:55]Refresh of rbac requested
[ns_server:debug,2022-09-07T14:32:09.112Z,ns_1@127.0.0.1:memcached_permissions<0.430.0>:memcached_cfg:rename_and_refresh:170]Successfully renamed "/opt/couchbase/var/lib/couchbase/config/memcached.rbac.tmp" to "/opt/couchbase/var/lib/couchbase/config/memcached.rbac"
[ns_server:debug,2022-09-07T14:32:09.118Z,ns_1@127.0.0.1:memcached_refresh<0.283.0>:memcached_refresh:handle_info:89]Refresh of [rbac] succeeded
[ns_server:debug,2022-09-07T14:32:09.119Z,ns_1@127.0.0.1:menelaus_cbauth<0.558.0>:menelaus_cbauth:handle_cast:101]Observed json rpc process {"goxdcr-cbauth",<0.792.0>} needs_update
[error_logger:info,2022-09-07T14:32:09.119Z,ns_1@127.0.0.1:service_agent_children_sup<0.571.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,service_agent_children_sup}
    started: [{pid,<0.8932.0>},
              {id,{service_agent,backup}},
              {mfargs,{service_agent,start_link,[backup]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2022-09-07T14:32:09.120Z,ns_1@127.0.0.1:menelaus_cbauth<0.558.0>:menelaus_cbauth:handle_cast:101]Observed json rpc process {"goxdcr-cbauth",<0.792.0>} needs_update
[error_logger:info,2022-09-07T14:32:09.120Z,ns_1@127.0.0.1:service_agent_children_sup<0.571.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,service_agent_children_sup}
    started: [{pid,<0.8936.0>},
              {id,{service_agent,cbas}},
              {mfargs,{service_agent,start_link,[cbas]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2022-09-07T14:32:09.120Z,ns_1@127.0.0.1:service_agent_children_sup<0.571.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,service_agent_children_sup}
    started: [{pid,<0.8940.0>},
              {id,{service_agent,eventing}},
              {mfargs,{service_agent,start_link,[eventing]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2022-09-07T14:32:09.121Z,ns_1@127.0.0.1:menelaus_cbauth<0.558.0>:menelaus_cbauth:handle_cast:101]Observed json rpc process {"goxdcr-cbauth",<0.792.0>} needs_update
[error_logger:info,2022-09-07T14:32:09.121Z,ns_1@127.0.0.1:service_agent_children_sup<0.571.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,service_agent_children_sup}
    started: [{pid,<0.8944.0>},
              {id,{service_agent,fts}},
              {mfargs,{service_agent,start_link,[fts]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2022-09-07T14:32:09.121Z,ns_1@127.0.0.1:service_agent_children_sup<0.571.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,service_agent_children_sup}
    started: [{pid,<0.8948.0>},
              {id,{service_agent,index}},
              {mfargs,{service_agent,start_link,[index]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2022-09-07T14:32:09.122Z,ns_1@127.0.0.1:menelaus_cbauth<0.558.0>:menelaus_cbauth:handle_cast:101]Observed json rpc process {"goxdcr-cbauth",<0.792.0>} needs_update
[error_logger:info,2022-09-07T14:32:09.122Z,ns_1@127.0.0.1:service_agent_children_sup<0.571.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,service_agent_children_sup}
    started: [{pid,<0.8952.0>},
              {id,{service_agent,n1ql}},
              {mfargs,{service_agent,start_link,[n1ql]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2022-09-07T14:32:09.124Z,ns_1@127.0.0.1:service_monitor_children_sup<0.773.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,service_monitor_children_sup}
    started: [{pid,<0.8956.0>},
              {id,{kv,kv_stats_monitor}},
              {mfargs,{kv_stats_monitor,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2022-09-07T14:32:09.124Z,ns_1@127.0.0.1:menelaus_cbauth<0.558.0>:menelaus_cbauth:handle_cast:101]Observed json rpc process {"goxdcr-cbauth",<0.792.0>} needs_update
[ns_server:debug,2022-09-07T14:32:09.127Z,ns_1@127.0.0.1:menelaus_cbauth<0.558.0>:menelaus_cbauth:handle_cast:101]Observed json rpc process {"goxdcr-cbauth",<0.792.0>} needs_update
[error_logger:info,2022-09-07T14:32:09.133Z,ns_1@127.0.0.1:service_monitor_children_sup<0.773.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,service_monitor_children_sup}
    started: [{pid,<0.8959.0>},
              {id,{kv,kv_monitor}},
              {mfargs,{kv_monitor,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2022-09-07T14:32:09.150Z,ns_1@127.0.0.1:memcached_passwords<0.425.0>:replicated_dets:select_from_table:281][ets] Starting select with {users_storage,
                               [{{docv2,{auth,{'_',local}},'_','_'},
                                 [],
                                 ['$_']}],
                               100}
[ns_server:debug,2022-09-07T14:32:09.151Z,ns_1@127.0.0.1:memcached_refresh<0.283.0>:memcached_refresh:handle_call:49]File rename from "/opt/couchbase/var/lib/couchbase/isasl.pw.tmp" to "/opt/couchbase/var/lib/couchbase/isasl.pw" is requested
[ns_server:debug,2022-09-07T14:32:09.153Z,ns_1@127.0.0.1:memcached_passwords<0.425.0>:memcached_cfg:rename_and_refresh:170]Successfully renamed "/opt/couchbase/var/lib/couchbase/isasl.pw.tmp" to "/opt/couchbase/var/lib/couchbase/isasl.pw"
[ns_server:debug,2022-09-07T14:32:09.153Z,ns_1@127.0.0.1:memcached_refresh<0.283.0>:memcached_refresh:handle_cast:55]Refresh of isasl requested
[ns_server:debug,2022-09-07T14:32:09.158Z,ns_1@127.0.0.1:memcached_refresh<0.283.0>:memcached_refresh:handle_info:89]Refresh of [isasl] succeeded
[ns_server:debug,2022-09-07T14:32:09.163Z,ns_1@127.0.0.1:ns_ports_setup<0.565.0>:ns_ports_manager:set_dynamic_children:48]Setting children [memcached,saslauthd_port,backup,cbas,eventing,fts,goxdcr,
                  index,kv,n1ql]
[ns_server:debug,2022-09-07T14:32:09.229Z,ns_1@127.0.0.1:users_storage<0.360.0>:replicated_dets:select_from_table:281][ets] Starting select with {users_storage,
                               [{{docv2,{auth,'_'},'_','_'},[],['$_']}],
                               100}
[ns_server:debug,2022-09-07T14:32:09.292Z,ns_1@127.0.0.1:compiled_roles_cache<0.362.0>:menelaus_roles:build_compiled_roles:998]Compile roles for user {"@",admin}
[ns_server:debug,2022-09-07T14:32:09.293Z,ns_1@127.0.0.1:json_rpc_connection-backup-cbauth<0.8983.0>:json_rpc_connection:init:68]Observed revrpc connection: label "backup-cbauth", handling process <0.8983.0>
[ns_server:debug,2022-09-07T14:32:09.293Z,ns_1@127.0.0.1:menelaus_cbauth<0.558.0>:menelaus_cbauth:handle_cast:101]Observed json rpc process {"backup-cbauth",<0.8983.0>} started
[ns_server:debug,2022-09-07T14:32:09.341Z,ns_1@127.0.0.1:compiled_roles_cache<0.362.0>:menelaus_roles:build_compiled_roles:998]Compile roles for user {"@backup",admin}
[ns_server:debug,2022-09-07T14:32:09.355Z,ns_1@127.0.0.1:ns_config_rep<0.444.0>:ns_config_rep:do_push_keys:383]Replicating some config keys ([{local_changes_count,
                                   <<"1678dfae96c38e07dff43c49b9f6967b">>},
                               {metakv,
                                   <<"/cbbs/config/historyRotationSize">>}]..)
[ns_server:debug,2022-09-07T14:32:09.355Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{metakv,<<"/cbbs/config/historyRotationSize">>} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780329}}]}|
 <<"50">>]
[ns_server:debug,2022-09-07T14:32:09.355Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{local_changes_count,<<"1678dfae96c38e07dff43c49b9f6967b">>} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{20,63829780329}}]}]
[ns_server:debug,2022-09-07T14:32:09.379Z,ns_1@127.0.0.1:json_rpc_connection-backup-service_api<0.9002.0>:json_rpc_connection:init:68]Observed revrpc connection: label "backup-service_api", handling process <0.9002.0>
[ns_server:debug,2022-09-07T14:32:09.379Z,ns_1@127.0.0.1:service_agent-backup<0.8932.0>:service_agent:do_handle_connection:319]Observed new json rpc connection for backup: <0.9002.0>
[ns_server:debug,2022-09-07T14:32:09.379Z,ns_1@127.0.0.1:<0.8935.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {json_rpc_events,<0.8933.0>} exited with reason normal
[ns_server:debug,2022-09-07T14:32:09.419Z,ns_1@127.0.0.1:json_rpc_connection-cbas-cbauth<0.9008.0>:json_rpc_connection:init:68]Observed revrpc connection: label "cbas-cbauth", handling process <0.9008.0>
[ns_server:debug,2022-09-07T14:32:09.419Z,ns_1@127.0.0.1:menelaus_cbauth<0.558.0>:menelaus_cbauth:handle_cast:101]Observed json rpc process {"cbas-cbauth",<0.9008.0>} started
[ns_server:debug,2022-09-07T14:32:09.421Z,ns_1@127.0.0.1:compiled_roles_cache<0.362.0>:menelaus_roles:build_compiled_roles:998]Compile roles for user {"@cbas",admin}
[ns_server:debug,2022-09-07T14:32:09.531Z,ns_1@127.0.0.1:json_rpc_connection-eventing-cbauth<0.9013.0>:json_rpc_connection:init:68]Observed revrpc connection: label "eventing-cbauth", handling process <0.9013.0>
[ns_server:debug,2022-09-07T14:32:09.531Z,ns_1@127.0.0.1:menelaus_cbauth<0.558.0>:menelaus_cbauth:handle_cast:101]Observed json rpc process {"eventing-cbauth",<0.9013.0>} started
[ns_server:debug,2022-09-07T14:32:09.546Z,ns_1@127.0.0.1:compiled_roles_cache<0.362.0>:menelaus_roles:build_compiled_roles:998]Compile roles for user {"@eventing",admin}
[ns_server:debug,2022-09-07T14:32:09.564Z,ns_1@127.0.0.1:json_rpc_connection-eventing-service_api<0.9018.0>:json_rpc_connection:init:68]Observed revrpc connection: label "eventing-service_api", handling process <0.9018.0>
[ns_server:debug,2022-09-07T14:32:09.564Z,ns_1@127.0.0.1:service_agent-eventing<0.8940.0>:service_agent:do_handle_connection:319]Observed new json rpc connection for eventing: <0.9018.0>
[ns_server:debug,2022-09-07T14:32:09.564Z,ns_1@127.0.0.1:<0.8943.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {json_rpc_events,<0.8941.0>} exited with reason normal
[ns_server:debug,2022-09-07T14:32:09.567Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{local_changes_count,<<"1678dfae96c38e07dff43c49b9f6967b">>} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{21,63829780329}}]}]
[ns_server:debug,2022-09-07T14:32:09.567Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{metakv,<<"/eventing/settings/config">>} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780329}}]}|
 <<"{\n \"enable_debugger\": false,\n \"ram_quota\": 256\n}">>]
[ns_server:debug,2022-09-07T14:32:09.567Z,ns_1@127.0.0.1:ns_config_rep<0.444.0>:ns_config_rep:do_push_keys:383]Replicating some config keys ([{local_changes_count,
                                   <<"1678dfae96c38e07dff43c49b9f6967b">>},
                               {metakv,<<"/eventing/settings/config">>}]..)
[ns_server:debug,2022-09-07T14:32:09.604Z,ns_1@127.0.0.1:json_rpc_connection-cbas-service_api<0.9055.0>:json_rpc_connection:init:68]Observed revrpc connection: label "cbas-service_api", handling process <0.9055.0>
[ns_server:debug,2022-09-07T14:32:09.604Z,ns_1@127.0.0.1:service_agent-cbas<0.8936.0>:service_agent:do_handle_connection:319]Observed new json rpc connection for cbas: <0.9055.0>
[ns_server:debug,2022-09-07T14:32:09.604Z,ns_1@127.0.0.1:<0.8939.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {json_rpc_events,<0.8937.0>} exited with reason normal
[ns_server:debug,2022-09-07T14:32:09.644Z,ns_1@127.0.0.1:json_rpc_connection-fts-cbauth<0.9064.0>:json_rpc_connection:init:68]Observed revrpc connection: label "fts-cbauth", handling process <0.9064.0>
[ns_server:debug,2022-09-07T14:32:09.645Z,ns_1@127.0.0.1:menelaus_cbauth<0.558.0>:menelaus_cbauth:handle_cast:101]Observed json rpc process {"fts-cbauth",<0.9064.0>} started
[ns_server:debug,2022-09-07T14:32:09.655Z,ns_1@127.0.0.1:compiled_roles_cache<0.362.0>:menelaus_roles:build_compiled_roles:998]Compile roles for user {"@goxdcr",admin}
[ns_server:debug,2022-09-07T14:32:09.685Z,ns_1@127.0.0.1:compiled_roles_cache<0.362.0>:menelaus_roles:build_compiled_roles:998]Compile roles for user {"@fts",admin}
[ns_server:debug,2022-09-07T14:32:09.709Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{local_changes_count,<<"1678dfae96c38e07dff43c49b9f6967b">>} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{22,63829780329}}]}]
[ns_server:debug,2022-09-07T14:32:09.709Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{metakv,<<"/fts/cbgt/cfg/version">>} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780329}}]}|
 <<"5.6.0">>]
[ns_server:debug,2022-09-07T14:32:09.709Z,ns_1@127.0.0.1:ns_config_rep<0.444.0>:ns_config_rep:do_push_keys:383]Replicating some config keys ([{local_changes_count,
                                   <<"1678dfae96c38e07dff43c49b9f6967b">>},
                               {metakv,<<"/fts/cbgt/cfg/version">>}]..)
[ns_server:debug,2022-09-07T14:32:09.716Z,ns_1@127.0.0.1:ns_config_rep<0.444.0>:ns_config_rep:do_push_keys:383]Replicating some config keys ([{local_changes_count,
                                   <<"1678dfae96c38e07dff43c49b9f6967b">>},
                               {metakv,
                                   <<"/fts/cbgt/cfg/nodeDefs-known/1678dfae96c38e07dff43c49b9f6967b">>}]..)
[ns_server:debug,2022-09-07T14:32:09.717Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{local_changes_count,<<"1678dfae96c38e07dff43c49b9f6967b">>} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{23,63829780329}}]}]
[ns_server:debug,2022-09-07T14:32:09.717Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{metakv,<<"/fts/cbgt/cfg/nodeDefs-known/1678dfae96c38e07dff43c49b9f6967b">>} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780329}}]}|
 <<"{\"uuid\":\"558f7035dcaec8b2\",\"nodeDefs\":{\"1678dfae96c38e07dff43c49b9f6967b\":{\"hostPort\":\"127.0.0.1:8094\",\"uuid\":\"1678dfae96c38e07dff43c49b9f6967b\",\"implVersion\":\"5.6.0\",\"tags\":[\"feed\",\"janitor\",\"pindex\",\"queryer\",\"cbauth_service\"],\"container\":\"\",\"weight\":1,\"extras\":\"{\\\"bindGRPC\\\":\\\"127.0.0.1:9130\\\",\\\"bindGRPCSSL\\\":\\\"127.0.0.1:19130\\\",\\\"bindHTTPS\\\":\\\":18094\\\",\\\"features\\\":\\\"leanPlan,advMe"...>>]
[ns_server:debug,2022-09-07T14:32:09.729Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{local_changes_count,<<"1678dfae96c38e07dff43c49b9f6967b">>} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{24,63829780329}}]}]
[ns_server:debug,2022-09-07T14:32:09.729Z,ns_1@127.0.0.1:ns_config_rep<0.444.0>:ns_config_rep:do_push_keys:383]Replicating some config keys ([{local_changes_count,
                                   <<"1678dfae96c38e07dff43c49b9f6967b">>},
                               {metakv,
                                   <<"/fts/cbgt/cfg/nodeDefs-wanted/1678dfae96c38e07dff43c49b9f6967b">>}]..)
[ns_server:debug,2022-09-07T14:32:09.729Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{metakv,<<"/fts/cbgt/cfg/nodeDefs-wanted/1678dfae96c38e07dff43c49b9f6967b">>} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780329}}]}|
 <<"{\"uuid\":\"678555f91065570e\",\"nodeDefs\":{\"1678dfae96c38e07dff43c49b9f6967b\":{\"hostPort\":\"127.0.0.1:8094\",\"uuid\":\"1678dfae96c38e07dff43c49b9f6967b\",\"implVersion\":\"5.6.0\",\"tags\":[\"feed\",\"janitor\",\"pindex\",\"queryer\",\"cbauth_service\"],\"container\":\"\",\"weight\":1,\"extras\":\"{\\\"bindGRPC\\\":\\\"127.0.0.1:9130\\\",\\\"bindGRPCSSL\\\":\\\"127.0.0.1:19130\\\",\\\"bindHTTPS\\\":\\\":18094\\\",\\\"features\\\":\\\"leanPlan,advMe"...>>]
[ns_server:debug,2022-09-07T14:32:09.779Z,ns_1@127.0.0.1:json_rpc_connection-fts-service_api<0.9137.0>:json_rpc_connection:init:68]Observed revrpc connection: label "fts-service_api", handling process <0.9137.0>
[ns_server:debug,2022-09-07T14:32:09.779Z,ns_1@127.0.0.1:service_agent-fts<0.8944.0>:service_agent:do_handle_connection:319]Observed new json rpc connection for fts: <0.9137.0>
[ns_server:debug,2022-09-07T14:32:09.779Z,ns_1@127.0.0.1:<0.8947.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {json_rpc_events,<0.8945.0>} exited with reason normal
[ns_server:debug,2022-09-07T14:32:09.824Z,ns_1@127.0.0.1:json_rpc_connection-index-cbauth<0.9142.0>:json_rpc_connection:init:68]Observed revrpc connection: label "index-cbauth", handling process <0.9142.0>
[ns_server:debug,2022-09-07T14:32:09.824Z,ns_1@127.0.0.1:menelaus_cbauth<0.558.0>:menelaus_cbauth:handle_cast:101]Observed json rpc process {"index-cbauth",<0.9142.0>} started
[ns_server:debug,2022-09-07T14:32:09.852Z,ns_1@127.0.0.1:compiled_roles_cache<0.362.0>:menelaus_roles:build_compiled_roles:998]Compile roles for user {"@index",admin}
[error_logger:error,2022-09-07T14:32:09.885Z,ns_1@127.0.0.1:<0.414.0>:ale_error_logger_handler:do_log:101]
=========================CRASH REPORT=========================
  crasher:
    initial call: ns_log:'-fun.babysitter_log_consumption_loop_tramp/0-'/0
    pid: <0.414.0>
    registered_name: []
    exception exit: {{nodedown,'babysitter_of_ns_1@cb.local'},
                     {gen_server,call,
                         [{ns_babysitter_log,'babysitter_of_ns_1@cb.local'},
                          consume,infinity]}}
      in function  gen_server:call/3 (gen_server.erl, line 247)
      in call from ns_log:babysitter_log_consumption_loop/0 (src/ns_log.erl, line 66)
      in call from misc:delaying_crash/2 (src/misc.erl, line 1734)
    ancestors: [ns_server_sup,ns_server_nodes_sup,<0.270.0>,
                  ns_server_cluster_sup,root_sup,<0.145.0>]
    message_queue_len: 2
    messages: [{[alias|#Ref<15233.2209594722.1843724290.36905>],superseded},
                  {[alias|#Ref<15233.2209594722.1843724290.37182>],
                   superseded}]
    links: [<0.399.0>]
    dictionary: []
    trap_exit: false
    status: running
    heap_size: 4185
    stack_size: 29
    reductions: 7531
  neighbours:

[error_logger:error,2022-09-07T14:32:09.885Z,ns_1@127.0.0.1:ns_server_sup<0.399.0>:ale_error_logger_handler:do_log:101]
=========================SUPERVISOR REPORT=========================
    supervisor: {local,ns_server_sup}
    errorContext: child_terminated
    reason: {{nodedown,'babysitter_of_ns_1@cb.local'},
             {gen_server,call,
                         [{ns_babysitter_log,'babysitter_of_ns_1@cb.local'},
                          consume,infinity]}}
    offender: [{pid,<0.414.0>},
               {name,ns_babysitter_log_consumer},
               {mfargs,{ns_log,start_link_babysitter_log_consumer,[]}},
               {restart_type,{permanent,4}},
               {shutdown,1000},
               {child_type,worker}]
[error_logger:info,2022-09-07T14:32:09.885Z,ns_1@127.0.0.1:ns_server_sup<0.399.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.9156.0>},
              {name,ns_babysitter_log_consumer},
              {mfargs,{ns_log,start_link_babysitter_log_consumer,[]}},
              {restart_type,{permanent,4}},
              {shutdown,1000},
              {child_type,worker}]
[ns_server:debug,2022-09-07T14:32:09.974Z,ns_1@127.0.0.1:json_rpc_connection-index-service_api<0.9168.0>:json_rpc_connection:init:68]Observed revrpc connection: label "index-service_api", handling process <0.9168.0>
[ns_server:debug,2022-09-07T14:32:09.974Z,ns_1@127.0.0.1:service_agent-index<0.8948.0>:service_agent:do_handle_connection:319]Observed new json rpc connection for index: <0.9168.0>
[ns_server:debug,2022-09-07T14:32:09.974Z,ns_1@127.0.0.1:<0.8951.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {json_rpc_events,<0.8949.0>} exited with reason normal
[ns_server:debug,2022-09-07T14:32:09.983Z,ns_1@127.0.0.1:json_rpc_connection-projector-cbauth<0.9173.0>:json_rpc_connection:init:68]Observed revrpc connection: label "projector-cbauth", handling process <0.9173.0>
[ns_server:debug,2022-09-07T14:32:09.983Z,ns_1@127.0.0.1:menelaus_cbauth<0.558.0>:menelaus_cbauth:handle_cast:101]Observed json rpc process {"projector-cbauth",<0.9173.0>} started
[ns_server:debug,2022-09-07T14:32:09.998Z,ns_1@127.0.0.1:compiled_roles_cache<0.362.0>:menelaus_roles:build_compiled_roles:998]Compile roles for user {"@projector",admin}
[ns_server:debug,2022-09-07T14:32:10.014Z,ns_1@127.0.0.1:<0.8841.0>:auto_failover_logic:log_master_activity:141]Transitioned node {'ns_1@127.0.0.1',<<"1678dfae96c38e07dff43c49b9f6967b">>} state new -> up
[ns_server:debug,2022-09-07T14:32:10.189Z,ns_1@127.0.0.1:json_rpc_connection-cbq-engine-cbauth<0.9181.0>:json_rpc_connection:init:68]Observed revrpc connection: label "cbq-engine-cbauth", handling process <0.9181.0>
[ns_server:debug,2022-09-07T14:32:10.189Z,ns_1@127.0.0.1:menelaus_cbauth<0.558.0>:menelaus_cbauth:handle_cast:101]Observed json rpc process {"cbq-engine-cbauth",<0.9181.0>} started
[ns_server:debug,2022-09-07T14:32:10.242Z,ns_1@127.0.0.1:compiled_roles_cache<0.362.0>:menelaus_roles:build_compiled_roles:998]Compile roles for user {"@cbq-engine",admin}
[ns_server:debug,2022-09-07T14:32:10.269Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{metakv,<<"/query/functions_cache/counter">>} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780330}}]}|
 <<"[127.0.0.1:8091]0">>]
[ns_server:debug,2022-09-07T14:32:10.269Z,ns_1@127.0.0.1:ns_config_rep<0.444.0>:ns_config_rep:do_push_keys:383]Replicating some config keys ([{local_changes_count,
                                   <<"1678dfae96c38e07dff43c49b9f6967b">>},
                               {metakv,<<"/query/functions_cache/counter">>}]..)
[ns_server:debug,2022-09-07T14:32:10.269Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{local_changes_count,<<"1678dfae96c38e07dff43c49b9f6967b">>} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{25,63829780330}}]}]
[ns_server:debug,2022-09-07T14:32:11.336Z,ns_1@127.0.0.1:json_rpc_connection-cbq-engine-service_api<0.9233.0>:json_rpc_connection:init:68]Observed revrpc connection: label "cbq-engine-service_api", handling process <0.9233.0>
[ns_server:debug,2022-09-07T14:32:11.336Z,ns_1@127.0.0.1:service_agent-n1ql<0.8952.0>:service_agent:do_handle_connection:319]Observed new json rpc connection for n1ql: <0.9233.0>
[ns_server:debug,2022-09-07T14:32:11.336Z,ns_1@127.0.0.1:<0.8955.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {json_rpc_events,<0.8953.0>} exited with reason normal
[ns_server:debug,2022-09-07T14:32:11.700Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{local_changes_count,<<"1678dfae96c38e07dff43c49b9f6967b">>} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{26,63829780331}}]}]
[ns_server:debug,2022-09-07T14:32:11.700Z,ns_1@127.0.0.1:ns_config_rep<0.444.0>:ns_config_rep:do_push_keys:383]Replicating some config keys ([{local_changes_count,
                                   <<"1678dfae96c38e07dff43c49b9f6967b">>},
                               {metakv,
                                   <<"/fts/cbgt/cfg/nodeDefs-known/1678dfae96c38e07dff43c49b9f6967b">>}]..)
[ns_server:debug,2022-09-07T14:32:11.700Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{metakv,<<"/fts/cbgt/cfg/nodeDefs-known/1678dfae96c38e07dff43c49b9f6967b">>} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780331}}]}|
 <<31,139,8,0,0,0,0,0,0,255,132,145,79,111,212,48,16,197,191,10,122,103,19,98,
   118,243,207,215,130,218,3,72,209,238,138,11,65,149,99,143,179,70,169,29,
   108,103,219,106,181,223,29,37,180,162,20,36,174,239,253,52,51,111,222,25,
   243,108,53,4,182,220,108,250,178,40,229,182,202,11,83,107,48,56,175,233,3,
   153,8,113,6,47,171,90,27,73,77,169,54,...>>]
[ns_server:debug,2022-09-07T14:32:11.703Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{local_changes_count,<<"1678dfae96c38e07dff43c49b9f6967b">>} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{27,63829780331}}]}]
[ns_server:debug,2022-09-07T14:32:11.703Z,ns_1@127.0.0.1:ns_config_rep<0.444.0>:ns_config_rep:do_push_keys:383]Replicating some config keys ([{local_changes_count,
                                   <<"1678dfae96c38e07dff43c49b9f6967b">>},
                               {metakv,
                                   <<"/fts/cbgt/cfg/nodeDefs-wanted/1678dfae96c38e07dff43c49b9f6967b">>}]..)
[ns_server:debug,2022-09-07T14:32:11.703Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{metakv,<<"/fts/cbgt/cfg/nodeDefs-wanted/1678dfae96c38e07dff43c49b9f6967b">>} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780331}}]}|
 <<31,139,8,0,0,0,0,0,0,255,132,145,223,111,211,48,28,196,255,21,116,207,38,
   196,107,243,203,175,3,109,15,32,69,109,197,11,65,147,99,127,157,26,101,118,
   176,157,110,83,213,255,29,53,108,98,20,164,189,222,125,244,181,239,238,136,
   121,182,26,2,235,162,82,170,206,101,217,152,85,113,213,24,48,56,175,233,35,
   153,8,113,4,47,171,90,27,73,77,169,86,...>>]
[ns_server:debug,2022-09-07T14:32:12.194Z,ns_1@127.0.0.1:compiled_roles_cache<0.362.0>:menelaus_roles:build_compiled_roles:998]Compile roles for user {"@prometheus",stats_reader}
[ns_server:debug,2022-09-07T14:32:14.013Z,ns_1@127.0.0.1:cleanup_process<0.9372.0>:service_janitor:init_topology_aware_service:84]Doing initial topology change for service `backup'
[ns_server:debug,2022-09-07T14:32:14.016Z,ns_1@127.0.0.1:service_rebalancer-backup<0.9373.0>:service_agent:wait_for_agents:69]Waiting for the service agents for service backup to come up on nodes:
['ns_1@127.0.0.1']
[ns_server:debug,2022-09-07T14:32:14.016Z,ns_1@127.0.0.1:service_rebalancer-backup<0.9373.0>:service_agent:wait_for_agents_loop:87]All service agents are ready for backup
[rebalance:info,2022-09-07T14:32:14.016Z,ns_1@127.0.0.1:service_rebalancer-backup-worker<0.9383.0>:service_rebalancer:rebalance_worker:147]Rebalancing service backup with id <<"4307eb163446517dcbcbb482310263e6">>.
KeepNodes: ['ns_1@127.0.0.1']
EjectNodes: []
DeltaNodes: []
[ns_server:debug,2022-09-07T14:32:14.019Z,ns_1@127.0.0.1:service_rebalancer-backup-worker<0.9383.0>:service_rebalancer:rebalance_worker:153]Got node infos:
[{'ns_1@127.0.0.1',[{node_id,<<"1678dfae96c38e07dff43c49b9f6967b">>},
                    {priority,1},
                    {opaque,{[{<<"host">>,<<"127.0.0.1">>},
                              {<<"http_port">>,8097},
                              {<<"grpc_port">>,9124}]}}]}]
[ns_server:debug,2022-09-07T14:32:14.020Z,ns_1@127.0.0.1:service_rebalancer-backup-worker<0.9383.0>:service_rebalancer:rebalance_worker:162]Using node 'ns_1@127.0.0.1' as a leader
[ns_server:debug,2022-09-07T14:32:14.022Z,ns_1@127.0.0.1:service_rebalancer-backup-worker<0.9383.0>:service_janitor:do_orchestrate_initial_rebalance:102]Initial rebalance progress for `backup': [{'ns_1@127.0.0.1',0}]
[ns_server:debug,2022-09-07T14:32:14.024Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{local_changes_count,<<"1678dfae96c38e07dff43c49b9f6967b">>} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{28,63829780334}}]}]
[ns_server:debug,2022-09-07T14:32:14.024Z,ns_1@127.0.0.1:ns_config_rep<0.444.0>:ns_config_rep:do_push_keys:383]Replicating some config keys ([{local_changes_count,
                                   <<"1678dfae96c38e07dff43c49b9f6967b">>},
                               {metakv,<<"/cbbs/leader">>}]..)
[ns_server:debug,2022-09-07T14:32:14.024Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{metakv,<<"/cbbs/leader">>} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780334}}]}|
 <<"\"1678dfae96c38e07dff43c49b9f6967b\"">>]
[ns_server:debug,2022-09-07T14:32:14.026Z,ns_1@127.0.0.1:ns_config_rep<0.444.0>:ns_config_rep:do_push_keys:383]Replicating some config keys ([{local_changes_count,
                                   <<"1678dfae96c38e07dff43c49b9f6967b">>},
                               {metakv,
                                   <<"/cbbs/nodes/1678dfae96c38e07dff43c49b9f6967b">>}]..)
[ns_server:debug,2022-09-07T14:32:14.026Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{local_changes_count,<<"1678dfae96c38e07dff43c49b9f6967b">>} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{29,63829780334}}]}]
[ns_server:debug,2022-09-07T14:32:14.026Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{metakv,<<"/cbbs/nodes/1678dfae96c38e07dff43c49b9f6967b">>} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780334}}]}|
 <<"{\"node_id\":\"1678dfae96c38e07dff43c49b9f6967b\",\"host\":\"127.0.0.1\",\"http_port\":8097,\"grpc_port\":9124,\"status\":\"\",\"leader\":false}">>]
[ns_server:debug,2022-09-07T14:32:14.027Z,ns_1@127.0.0.1:service_rebalancer-backup-worker<0.9383.0>:service_janitor:do_orchestrate_initial_rebalance:102]Initial rebalance progress for `backup': [{'ns_1@127.0.0.1',0}]
[ns_server:debug,2022-09-07T14:32:14.030Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{metakv,<<"/cbbs/plan/_daily_backups">>} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780334}}]}|
 <<"{\"name\":\"_daily_backups\",\"description\":\"This plan does a backup a day and merges them at the end of the week. It then merges every four weeks together.\",\"services\":null,\"tasks\":[{\"name\":\"backup_monday_full\",\"task_type\":\"BACKUP\",\"schedule\":{\"job_type\":\"BACKUP\",\"frequency\":1,\"period\":\"MONDAY\",\"time\":\"22:00\"},\"full_backup\":true},{\"name\":\"backup_tuesday\",\"task_type\":\"BACKUP\",\"schedule\":{\"j"...>>]
[ns_server:debug,2022-09-07T14:32:14.030Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{local_changes_count,<<"1678dfae96c38e07dff43c49b9f6967b">>} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{30,63829780334}}]}]
[ns_server:debug,2022-09-07T14:32:14.030Z,ns_1@127.0.0.1:ns_config_rep<0.444.0>:ns_config_rep:do_push_keys:383]Replicating some config keys ([{local_changes_count,
                                   <<"1678dfae96c38e07dff43c49b9f6967b">>},
                               {metakv,<<"/cbbs/plan/_daily_backups">>}]..)
[ns_server:debug,2022-09-07T14:32:14.031Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{metakv,<<"/cbbs/plan/_hourly_backups">>} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780334}}]}|
 <<"{\"name\":\"_hourly_backups\",\"description\":\"This plan does a backup every hour and merges the previous 6 backups every 6 hours. At the end of the week it merges the last week of backups together.\",\"services\":null,\"tasks\":[{\"name\":\"backup_hourly\",\"task_type\":\"BACKUP\",\"schedule\":{\"job_type\":\"BACKUP\",\"frequency\":1,\"period\":\"HOURS\",\"time\":\"00:00\"},\"full_backup\":false},{\"name\":\"merge_every_6_h"...>>]
[ns_server:debug,2022-09-07T14:32:14.031Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{local_changes_count,<<"1678dfae96c38e07dff43c49b9f6967b">>} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{31,63829780334}}]}]
[ns_server:debug,2022-09-07T14:32:14.031Z,ns_1@127.0.0.1:ns_config_rep<0.444.0>:ns_config_rep:do_push_keys:383]Replicating some config keys ([{local_changes_count,
                                   <<"1678dfae96c38e07dff43c49b9f6967b">>},
                               {metakv,<<"/cbbs/plan/_hourly_backups">>}]..)
[ns_server:debug,2022-09-07T14:32:14.032Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{local_changes_count,<<"1678dfae96c38e07dff43c49b9f6967b">>} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{32,63829780334}}]}]
[ns_server:debug,2022-09-07T14:32:14.032Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{metakv,<<"/cbbs/internal/default_plans_loaded">>} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780334}}]}|
 <<"true">>]
[ns_server:debug,2022-09-07T14:32:14.032Z,ns_1@127.0.0.1:ns_config_rep<0.444.0>:ns_config_rep:do_push_keys:383]Replicating some config keys ([{local_changes_count,
                                   <<"1678dfae96c38e07dff43c49b9f6967b">>},
                               {metakv,
                                   <<"/cbbs/internal/default_plans_loaded">>}]..)
[error_logger:info,2022-09-07T14:32:15.891Z,ns_1@127.0.0.1:net_kernel<0.8750.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.8755.0>,setup_timer_timeout}}
[ns_server:debug,2022-09-07T14:32:15.891Z,ns_1@127.0.0.1:cb_dist<0.8747.0>:cb_dist:info_msg:872]cb_dist: Connection down: {con,#Ref<0.2209594722.1843658754.48723>,
                               inet_tcp_dist,<0.8755.0>,
                               #Ref<0.2209594722.1843658754.48725>}
[error_logger:info,2022-09-07T14:32:15.891Z,ns_1@127.0.0.1:net_kernel<0.8750.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1157,nodedown,'ns_1@cb.local'}}
[ns_server:debug,2022-09-07T14:32:15.991Z,ns_1@127.0.0.1:ns_config_rep<0.444.0>:ns_config_rep:do_push_keys:383]Replicating some config keys ([{local_changes_count,
                                   <<"1678dfae96c38e07dff43c49b9f6967b">>},
                               {metakv,<<"/indexing/settings/config">>}]..)
[ns_server:debug,2022-09-07T14:32:15.991Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{metakv,<<"/indexing/settings/config">>} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{5,63829780335}}]}|
 <<"{\"indexer.plasma.backIndex.enablePageBloomFilter\":false,\"indexer.settings.compaction.abort_exceed_interval\":false,\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.compaction.min_frag\":30,\"indexer.settings.en"...>>]
[ns_server:debug,2022-09-07T14:32:15.991Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{local_changes_count,<<"1678dfae96c38e07dff43c49b9f6967b">>} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{33,63829780335}}]}]
[ns_server:debug,2022-09-07T14:32:17.331Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_scheduler_message:1359]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2022-09-07T14:32:17.331Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-07T14:32:17.331Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_scheduler_message:1359]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2022-09-07T14:32:17.331Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:info,2022-09-07T14:32:19.014Z,ns_1@127.0.0.1:<0.8839.0>:ns_orchestrator:handle_event:494]Skipping janitor in state janitor_running
[ns_server:debug,2022-09-07T14:32:19.036Z,ns_1@127.0.0.1:service_rebalancer-backup-worker<0.9383.0>:service_janitor:do_orchestrate_initial_rebalance:102]Initial rebalance progress for `backup': [{'ns_1@127.0.0.1',0.5}]
[ns_server:debug,2022-09-07T14:32:19.036Z,ns_1@127.0.0.1:service_rebalancer-backup<0.9373.0>:service_rebalancer:run_rebalance_worker:116]Worker terminated normally
[ns_server:debug,2022-09-07T14:32:19.038Z,ns_1@127.0.0.1:cleanup_process<0.9372.0>:ns_cluster_membership:set_service_map:551]Set service map for service backup to ['ns_1@127.0.0.1']
[ns_server:debug,2022-09-07T14:32:19.066Z,ns_1@127.0.0.1:cleanup_process<0.9372.0>:service_janitor:init_topology_aware_service:87]Initial rebalance for `backup` finished successfully
[ns_server:debug,2022-09-07T14:32:19.066Z,ns_1@127.0.0.1:chronicle_kv_log<0.404.0>:chronicle_kv_log:log:59]update (key: {service_map,backup}, rev: {<<"1933459bb8a062c61b619e931c483c80">>,
                                         14})
['ns_1@127.0.0.1']
[ns_server:debug,2022-09-07T14:32:19.066Z,ns_1@127.0.0.1:cleanup_process<0.9372.0>:service_janitor:init_topology_aware_service:84]Doing initial topology change for service `cbas'
[ns_server:debug,2022-09-07T14:32:19.066Z,ns_1@127.0.0.1:service_rebalancer-cbas<0.9603.0>:service_agent:wait_for_agents:69]Waiting for the service agents for service cbas to come up on nodes:
['ns_1@127.0.0.1']
[ns_server:debug,2022-09-07T14:32:19.066Z,ns_1@127.0.0.1:service_rebalancer-cbas<0.9603.0>:service_agent:wait_for_agents_loop:87]All service agents are ready for cbas
[ns_server:debug,2022-09-07T14:32:19.067Z,ns_1@127.0.0.1:terse_cluster_info_uploader<0.613.0>:terse_cluster_info_uploader:handle_info:42]Refreshing terse cluster info with <<"{\"rev\":47,\"nodesExt\":[{\"services\":{\"backupAPI\":8097,\"backupAPIHTTPS\":18097,\"backupGRPC\":9124,\"capi\":8092,\"capiSSL\":18092,\"kv\":11210,\"kvSSL\":11207,\"mgmt\":8091,\"mgmtSSL\":18091,\"projector\":9999},\"thisNode\":true}],\"clusterCapabilitiesVer\":[1,0],\"clusterCapabilities\":{\"n1ql\":[\"enhancedPreparedStatements\"]},\"revEpoch\":1}">>
[rebalance:info,2022-09-07T14:32:19.067Z,ns_1@127.0.0.1:service_rebalancer-cbas-worker<0.9613.0>:service_rebalancer:rebalance_worker:147]Rebalancing service cbas with id <<"bd9f7bc826257f1781c2ba1e03188aa1">>.
KeepNodes: ['ns_1@127.0.0.1']
EjectNodes: []
DeltaNodes: []
[ns_server:debug,2022-09-07T14:32:19.069Z,ns_1@127.0.0.1:service_rebalancer-cbas-worker<0.9613.0>:service_rebalancer:rebalance_worker:153]Got node infos:
[{'ns_1@127.0.0.1',[{node_id,<<"1678dfae96c38e07dff43c49b9f6967b">>},
                    {priority,1970329132007424},
                    {opaque,{[{<<"cbas-version">>,<<"7.1.1-3175">>},
                              {<<"cc-http-port">>,<<"9111">>},
                              {<<"host">>,<<"127.0.0.1">>},
                              {<<"ns-server-port">>,<<"8091">>},
                              {<<"num-iodevices">>,<<"1">>},
                              {<<"svc-http-port">>,<<"8095">>}]}}]}]
[ns_server:debug,2022-09-07T14:32:19.070Z,ns_1@127.0.0.1:service_rebalancer-cbas-worker<0.9613.0>:service_rebalancer:rebalance_worker:162]Using node 'ns_1@127.0.0.1' as a leader
[ns_server:debug,2022-09-07T14:32:19.078Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{local_changes_count,<<"1678dfae96c38e07dff43c49b9f6967b">>} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{34,63829780339}}]}]
[ns_server:debug,2022-09-07T14:32:19.078Z,ns_1@127.0.0.1:ns_config_rep<0.444.0>:ns_config_rep:do_push_keys:383]Replicating some config keys ([{local_changes_count,
                                   <<"1678dfae96c38e07dff43c49b9f6967b">>},
                               {metakv,<<"/cbas/nextPartitionId">>}]..)
[ns_server:debug,2022-09-07T14:32:19.078Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{metakv,<<"/cbas/nextPartitionId">>} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780339}}]}|
 <<"1">>]
[ns_server:debug,2022-09-07T14:32:19.079Z,ns_1@127.0.0.1:ns_config_rep<0.444.0>:ns_config_rep:do_push_keys:383]Replicating some config keys ([{local_changes_count,
                                   <<"1678dfae96c38e07dff43c49b9f6967b">>},
                               {metakv,<<"/cbas/nextControllerId">>}]..)
[ns_server:debug,2022-09-07T14:32:19.080Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{local_changes_count,<<"1678dfae96c38e07dff43c49b9f6967b">>} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{35,63829780339}}]}]
[ns_server:debug,2022-09-07T14:32:19.080Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{metakv,<<"/cbas/nextControllerId">>} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780339}}]}|
 <<"1">>]
[ns_server:debug,2022-09-07T14:32:19.081Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{local_changes_count,<<"1678dfae96c38e07dff43c49b9f6967b">>} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{36,63829780339}}]}]
[ns_server:debug,2022-09-07T14:32:19.081Z,ns_1@127.0.0.1:ns_config_rep<0.444.0>:ns_config_rep:do_push_keys:383]Replicating some config keys ([{local_changes_count,
                                   <<"1678dfae96c38e07dff43c49b9f6967b">>},
                               {metakv,<<"/cbas/topology">>}]..)
[ns_server:debug,2022-09-07T14:32:19.081Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{metakv,<<"/cbas/topology">>} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780339}}]}|
 <<"{\"nodes\":[{\"nodeId\":\"1678dfae96c38e07dff43c49b9f6967b\",\"priority\":1970329132007424,\"opaque\":{\"cbas-version\":\"7.1.1-3175\",\"cc-http-port\":\"9111\",\"controller-id\":\"0\",\"host\":\"127.0.0.1\",\"ns-server-port\":\"8091\",\"num-iodevices\":\"1\",\"starting-partition-id\":\"0\",\"svc-http-port\":\"8095\"}}],\"id\":\"bd9f7bc826257f1781c2ba1e03188aa1\",\"type\":\"topology-change-rebalance\",\"ccNodeId\":\"1678dfae96c38e07dff43"...>>]
[ns_server:debug,2022-09-07T14:32:19.083Z,ns_1@127.0.0.1:service_rebalancer-cbas<0.9603.0>:service_rebalancer:run_rebalance_worker:116]Worker terminated normally
[ns_server:debug,2022-09-07T14:32:19.085Z,ns_1@127.0.0.1:service_agent-cbas<0.8936.0>:service_agent:cleanup_service:496]Cleaning up stale tasks:
[[{<<"rev">>,<<"NQ==">>},
  {<<"id">>,<<"prepare/bd9f7bc826257f1781c2ba1e03188aa1">>},
  {<<"type">>,<<"task-prepared">>},
  {<<"status">>,<<"task-running">>},
  {<<"isCancelable">>,true},
  {<<"progress">>,0},
  {<<"extra">>,
   {[{<<"rebalanceId">>,<<"bd9f7bc826257f1781c2ba1e03188aa1">>}]}}]]
[ns_server:debug,2022-09-07T14:32:19.087Z,ns_1@127.0.0.1:cleanup_process<0.9372.0>:ns_cluster_membership:set_service_map:551]Set service map for service cbas to ['ns_1@127.0.0.1']
[ns_server:debug,2022-09-07T14:32:19.116Z,ns_1@127.0.0.1:cleanup_process<0.9372.0>:service_janitor:init_topology_aware_service:87]Initial rebalance for `cbas` finished successfully
[ns_server:debug,2022-09-07T14:32:19.116Z,ns_1@127.0.0.1:chronicle_kv_log<0.404.0>:chronicle_kv_log:log:59]update (key: {service_map,cbas}, rev: {<<"1933459bb8a062c61b619e931c483c80">>,
                                       15})
['ns_1@127.0.0.1']
[ns_server:debug,2022-09-07T14:32:19.116Z,ns_1@127.0.0.1:cleanup_process<0.9372.0>:service_janitor:init_topology_aware_service:84]Doing initial topology change for service `eventing'
[ns_server:debug,2022-09-07T14:32:19.116Z,ns_1@127.0.0.1:service_rebalancer-eventing<0.9657.0>:service_agent:wait_for_agents:69]Waiting for the service agents for service eventing to come up on nodes:
['ns_1@127.0.0.1']
[ns_server:debug,2022-09-07T14:32:19.117Z,ns_1@127.0.0.1:service_rebalancer-eventing<0.9657.0>:service_agent:wait_for_agents_loop:87]All service agents are ready for eventing
[rebalance:info,2022-09-07T14:32:19.117Z,ns_1@127.0.0.1:service_rebalancer-eventing-worker<0.9667.0>:service_rebalancer:rebalance_worker:147]Rebalancing service eventing with id <<"a0af9b617919276996bda60f0d831b10">>.
KeepNodes: ['ns_1@127.0.0.1']
EjectNodes: []
DeltaNodes: []
[ns_server:debug,2022-09-07T14:32:19.117Z,ns_1@127.0.0.1:terse_cluster_info_uploader<0.613.0>:terse_cluster_info_uploader:handle_info:42]Refreshing terse cluster info with <<"{\"rev\":51,\"nodesExt\":[{\"services\":{\"backupAPI\":8097,\"backupAPIHTTPS\":18097,\"backupGRPC\":9124,\"capi\":8092,\"capiSSL\":18092,\"cbas\":8095,\"cbasSSL\":18095,\"kv\":11210,\"kvSSL\":11207,\"mgmt\":8091,\"mgmtSSL\":18091,\"projector\":9999},\"thisNode\":true}],\"clusterCapabilitiesVer\":[1,0],\"clusterCapabilities\":{\"n1ql\":[\"enhancedPreparedStatements\"]},\"revEpoch\":1}">>
[ns_server:debug,2022-09-07T14:32:19.118Z,ns_1@127.0.0.1:service_rebalancer-eventing-worker<0.9667.0>:service_rebalancer:rebalance_worker:153]Got node infos:
[{'ns_1@127.0.0.1',[{node_id,<<"1678dfae96c38e07dff43c49b9f6967b">>},
                    {priority,0},
                    {opaque,null}]}]
[ns_server:debug,2022-09-07T14:32:19.120Z,ns_1@127.0.0.1:service_rebalancer-eventing-worker<0.9667.0>:service_rebalancer:rebalance_worker:162]Using node 'ns_1@127.0.0.1' as a leader
[ns_server:debug,2022-09-07T14:32:19.121Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{local_changes_count,<<"1678dfae96c38e07dff43c49b9f6967b">>} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{37,63829780339}}]}]
[ns_server:debug,2022-09-07T14:32:19.121Z,ns_1@127.0.0.1:ns_config_rep<0.444.0>:ns_config_rep:do_push_keys:383]Replicating some config keys ([{local_changes_count,
                                   <<"1678dfae96c38e07dff43c49b9f6967b">>},
                               {metakv,<<"/eventing/config/keepNodes">>}]..)
[ns_server:debug,2022-09-07T14:32:19.121Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{metakv,<<"/eventing/config/keepNodes">>} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780339}}]}|
 <<"[\"1678dfae96c38e07dff43c49b9f6967b\"]">>]
[ns_server:debug,2022-09-07T14:32:19.123Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{local_changes_count,<<"1678dfae96c38e07dff43c49b9f6967b">>} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{38,63829780339}}]}]
[ns_server:debug,2022-09-07T14:32:19.123Z,ns_1@127.0.0.1:ns_config_rep<0.444.0>:ns_config_rep:do_push_keys:383]Replicating some config keys ([{local_changes_count,
                                   <<"1678dfae96c38e07dff43c49b9f6967b">>}]..)
[ns_server:debug,2022-09-07T14:32:19.123Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{local_changes_count,<<"1678dfae96c38e07dff43c49b9f6967b">>} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{39,63829780339}}]}]
[ns_server:debug,2022-09-07T14:32:19.123Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{metakv,<<"/eventing/rebalanceToken/a0af9b617919276996bda60f0d831b10">>} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780339}}]}|
 <<"a0af9b617919276996bda60f0d831b10">>]
[ns_server:debug,2022-09-07T14:32:19.123Z,ns_1@127.0.0.1:ns_config_rep<0.444.0>:ns_config_rep:do_push_keys:383]Replicating some config keys ([{local_changes_count,
                                   <<"1678dfae96c38e07dff43c49b9f6967b">>},
                               {metakv,
                                   <<"/eventing/rebalanceToken/a0af9b617919276996bda60f0d831b10">>}]..)
[ns_server:debug,2022-09-07T14:32:19.125Z,ns_1@127.0.0.1:service_rebalancer-eventing-worker<0.9667.0>:service_janitor:do_orchestrate_initial_rebalance:102]Initial rebalance progress for `eventing': [{'ns_1@127.0.0.1',0}]
[ns_server:debug,2022-09-07T14:32:19.128Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{metakv,<<"/eventing/rebalanceToken/a0af9b617919276996bda60f0d831b10">>} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780339}}]}|
 '_deleted']
[ns_server:debug,2022-09-07T14:32:19.128Z,ns_1@127.0.0.1:ns_config_rep<0.444.0>:ns_config_rep:do_push_keys:383]Replicating some config keys ([{local_changes_count,
                                   <<"1678dfae96c38e07dff43c49b9f6967b">>},
                               {metakv,
                                   <<"/eventing/rebalanceToken/a0af9b617919276996bda60f0d831b10">>}]..)
[ns_server:debug,2022-09-07T14:32:19.128Z,ns_1@127.0.0.1:service_rebalancer-eventing<0.9657.0>:service_rebalancer:run_rebalance_worker:116]Worker terminated normally
[ns_server:debug,2022-09-07T14:32:19.128Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{local_changes_count,<<"1678dfae96c38e07dff43c49b9f6967b">>} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{40,63829780339}}]}]
[ns_server:debug,2022-09-07T14:32:19.130Z,ns_1@127.0.0.1:cleanup_process<0.9372.0>:ns_cluster_membership:set_service_map:551]Set service map for service eventing to ['ns_1@127.0.0.1']
[ns_server:debug,2022-09-07T14:32:19.157Z,ns_1@127.0.0.1:cleanup_process<0.9372.0>:service_janitor:init_topology_aware_service:87]Initial rebalance for `eventing` finished successfully
[ns_server:debug,2022-09-07T14:32:19.157Z,ns_1@127.0.0.1:chronicle_kv_log<0.404.0>:chronicle_kv_log:log:59]update (key: {service_map,eventing}, rev: {<<"1933459bb8a062c61b619e931c483c80">>,
                                           16})
['ns_1@127.0.0.1']
[ns_server:debug,2022-09-07T14:32:19.157Z,ns_1@127.0.0.1:cleanup_process<0.9372.0>:service_janitor:init_topology_aware_service:84]Doing initial topology change for service `fts'
[ns_server:debug,2022-09-07T14:32:19.158Z,ns_1@127.0.0.1:service_rebalancer-fts<0.9717.0>:service_agent:wait_for_agents:69]Waiting for the service agents for service fts to come up on nodes:
['ns_1@127.0.0.1']
[ns_server:debug,2022-09-07T14:32:19.158Z,ns_1@127.0.0.1:service_rebalancer-fts<0.9717.0>:service_agent:wait_for_agents_loop:87]All service agents are ready for fts
[rebalance:info,2022-09-07T14:32:19.158Z,ns_1@127.0.0.1:service_rebalancer-fts-worker<0.9727.0>:service_rebalancer:rebalance_worker:147]Rebalancing service fts with id <<"e14cb017412f4ff159d26db5a5abf439">>.
KeepNodes: ['ns_1@127.0.0.1']
EjectNodes: []
DeltaNodes: []
[ns_server:debug,2022-09-07T14:32:19.158Z,ns_1@127.0.0.1:terse_cluster_info_uploader<0.613.0>:terse_cluster_info_uploader:handle_info:42]Refreshing terse cluster info with <<"{\"rev\":56,\"nodesExt\":[{\"services\":{\"backupAPI\":8097,\"backupAPIHTTPS\":18097,\"backupGRPC\":9124,\"capi\":8092,\"capiSSL\":18092,\"cbas\":8095,\"cbasSSL\":18095,\"eventingAdminPort\":8096,\"eventingDebug\":9140,\"eventingSSL\":18096,\"kv\":11210,\"kvSSL\":11207,\"mgmt\":8091,\"mgmtSSL\":18091,\"projector\":9999},\"thisNode\":true}],\"clusterCapabilitiesVer\":[1,0],\"clusterCapabilities\":{\"n1ql\":[\"enhancedPreparedStatements\"]},\"revEpoch\":1}">>
[ns_server:debug,2022-09-07T14:32:19.159Z,ns_1@127.0.0.1:service_rebalancer-fts-worker<0.9727.0>:service_rebalancer:rebalance_worker:153]Got node infos:
[{'ns_1@127.0.0.1',[{node_id,<<"1678dfae96c38e07dff43c49b9f6967b">>},
                    {priority,0},
                    {opaque,null}]}]
[ns_server:debug,2022-09-07T14:32:19.162Z,ns_1@127.0.0.1:service_rebalancer-fts-worker<0.9727.0>:service_rebalancer:rebalance_worker:162]Using node 'ns_1@127.0.0.1' as a leader
[ns_server:debug,2022-09-07T14:32:19.166Z,ns_1@127.0.0.1:service_rebalancer-fts-worker<0.9727.0>:service_janitor:do_orchestrate_initial_rebalance:102]Initial rebalance progress for `fts': [{'ns_1@127.0.0.1',0}]
[ns_server:debug,2022-09-07T14:32:19.183Z,ns_1@127.0.0.1:service_rebalancer-fts<0.9717.0>:service_rebalancer:run_rebalance_worker:116]Worker terminated normally
[ns_server:debug,2022-09-07T14:32:19.184Z,ns_1@127.0.0.1:cleanup_process<0.9372.0>:ns_cluster_membership:set_service_map:551]Set service map for service fts to ['ns_1@127.0.0.1']
[ns_server:debug,2022-09-07T14:32:19.211Z,ns_1@127.0.0.1:cleanup_process<0.9372.0>:service_janitor:init_topology_aware_service:87]Initial rebalance for `fts` finished successfully
[ns_server:debug,2022-09-07T14:32:19.211Z,ns_1@127.0.0.1:chronicle_kv_log<0.404.0>:chronicle_kv_log:log:59]update (key: {service_map,fts}, rev: {<<"1933459bb8a062c61b619e931c483c80">>,
                                      17})
['ns_1@127.0.0.1']
[ns_server:debug,2022-09-07T14:32:19.211Z,ns_1@127.0.0.1:cleanup_process<0.9372.0>:service_janitor:init_topology_aware_service:84]Doing initial topology change for service `index'
[ns_server:debug,2022-09-07T14:32:19.211Z,ns_1@127.0.0.1:service_rebalancer-index<0.9755.0>:service_agent:wait_for_agents:69]Waiting for the service agents for service index to come up on nodes:
['ns_1@127.0.0.1']
[ns_server:debug,2022-09-07T14:32:19.212Z,ns_1@127.0.0.1:service_rebalancer-index<0.9755.0>:service_agent:wait_for_agents_loop:87]All service agents are ready for index
[ns_server:debug,2022-09-07T14:32:19.212Z,ns_1@127.0.0.1:terse_cluster_info_uploader<0.613.0>:terse_cluster_info_uploader:handle_info:42]Refreshing terse cluster info with <<"{\"rev\":57,\"nodesExt\":[{\"services\":{\"backupAPI\":8097,\"backupAPIHTTPS\":18097,\"backupGRPC\":9124,\"capi\":8092,\"capiSSL\":18092,\"cbas\":8095,\"cbasSSL\":18095,\"eventingAdminPort\":8096,\"eventingDebug\":9140,\"eventingSSL\":18096,\"fts\":8094,\"ftsGRPC\":9130,\"ftsGRPCSSL\":19130,\"ftsSSL\":18094,\"kv\":11210,\"kvSSL\":11207,\"mgmt\":8091,\"mgmtSSL\":18091,\"projector\":9999},\"thisNode\":true}],\"clusterCapabilitiesVer\":[1,0],\"clusterCapabilities\":{\"n1ql\":[\"enhancedPreparedStatements\"]},\"revEpoch\":1}">>
[rebalance:info,2022-09-07T14:32:19.212Z,ns_1@127.0.0.1:service_rebalancer-index-worker<0.9767.0>:service_rebalancer:rebalance_worker:147]Rebalancing service index with id <<"237f95cc3538f9bec5a58b22006541c3">>.
KeepNodes: ['ns_1@127.0.0.1']
EjectNodes: []
DeltaNodes: []
[ns_server:debug,2022-09-07T14:32:19.213Z,ns_1@127.0.0.1:service_rebalancer-index-worker<0.9767.0>:service_rebalancer:rebalance_worker:153]Got node infos:
[{'ns_1@127.0.0.1',[{node_id,<<"1678dfae96c38e07dff43c49b9f6967b">>},
                    {priority,6},
                    {opaque,null}]}]
[ns_server:debug,2022-09-07T14:32:19.220Z,ns_1@127.0.0.1:service_rebalancer-index-worker<0.9767.0>:service_rebalancer:rebalance_worker:162]Using node 'ns_1@127.0.0.1' as a leader
[ns_server:debug,2022-09-07T14:32:19.232Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{local_changes_count,<<"1678dfae96c38e07dff43c49b9f6967b">>} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{41,63829780339}}]}]
[ns_server:debug,2022-09-07T14:32:19.232Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{metakv,<<"/indexing/rebalance/RebalanceToken">>} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780339}}]}|
 <<"{\"MasterId\":\"1678dfae96c38e07dff43c49b9f6967b\",\"RebalId\":\"237f95cc3538f9bec5a58b22006541c3\",\"Source\":0,\"Error\":\"\",\"MasterIP\":\"127.0.0.1\"}">>]
[ns_server:debug,2022-09-07T14:32:19.233Z,ns_1@127.0.0.1:ns_config_rep<0.444.0>:ns_config_rep:do_push_keys:383]Replicating some config keys ([{local_changes_count,
                                   <<"1678dfae96c38e07dff43c49b9f6967b">>},
                               {metakv,
                                   <<"/indexing/rebalance/RebalanceToken">>}]..)
[ns_server:debug,2022-09-07T14:32:19.235Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{local_changes_count,<<"1678dfae96c38e07dff43c49b9f6967b">>} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{42,63829780339}}]}]
[ns_server:debug,2022-09-07T14:32:19.235Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{metakv,<<"/indexing/rebalance/RebalanceToken">>} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780339}}]}|
 '_deleted']
[ns_server:debug,2022-09-07T14:32:19.235Z,ns_1@127.0.0.1:ns_config_rep<0.444.0>:ns_config_rep:do_push_keys:383]Replicating some config keys ([{local_changes_count,
                                   <<"1678dfae96c38e07dff43c49b9f6967b">>},
                               {metakv,
                                   <<"/indexing/rebalance/RebalanceToken">>}]..)
[ns_server:debug,2022-09-07T14:32:19.235Z,ns_1@127.0.0.1:service_rebalancer-index-worker<0.9767.0>:service_janitor:do_orchestrate_initial_rebalance:102]Initial rebalance progress for `index': [{'ns_1@127.0.0.1',1}]
[ns_server:debug,2022-09-07T14:32:19.244Z,ns_1@127.0.0.1:service_rebalancer-index<0.9755.0>:service_rebalancer:run_rebalance_worker:116]Worker terminated normally
[ns_server:debug,2022-09-07T14:32:19.246Z,ns_1@127.0.0.1:cleanup_process<0.9372.0>:ns_cluster_membership:set_service_map:551]Set service map for service index to ['ns_1@127.0.0.1']
[ns_server:debug,2022-09-07T14:32:19.273Z,ns_1@127.0.0.1:cleanup_process<0.9372.0>:service_janitor:init_topology_aware_service:87]Initial rebalance for `index` finished successfully
[ns_server:debug,2022-09-07T14:32:19.274Z,ns_1@127.0.0.1:cleanup_process<0.9372.0>:service_janitor:init_topology_aware_service:84]Doing initial topology change for service `n1ql'
[ns_server:debug,2022-09-07T14:32:19.273Z,ns_1@127.0.0.1:chronicle_kv_log<0.404.0>:chronicle_kv_log:log:59]update (key: {service_map,index}, rev: {<<"1933459bb8a062c61b619e931c483c80">>,
                                        18})
['ns_1@127.0.0.1']
[ns_server:debug,2022-09-07T14:32:19.274Z,ns_1@127.0.0.1:service_rebalancer-n1ql<0.9817.0>:service_agent:wait_for_agents:69]Waiting for the service agents for service n1ql to come up on nodes:
['ns_1@127.0.0.1']
[ns_server:debug,2022-09-07T14:32:19.274Z,ns_1@127.0.0.1:service_rebalancer-n1ql<0.9817.0>:service_agent:wait_for_agents_loop:87]All service agents are ready for n1ql
[rebalance:info,2022-09-07T14:32:19.274Z,ns_1@127.0.0.1:service_rebalancer-n1ql-worker<0.9827.0>:service_rebalancer:rebalance_worker:147]Rebalancing service n1ql with id <<"37c5bf6d3be5db71e0e151270395ec79">>.
KeepNodes: ['ns_1@127.0.0.1']
EjectNodes: []
DeltaNodes: []
[ns_server:debug,2022-09-07T14:32:19.274Z,ns_1@127.0.0.1:terse_cluster_info_uploader<0.613.0>:terse_cluster_info_uploader:handle_info:42]Refreshing terse cluster info with <<"{\"rev\":60,\"nodesExt\":[{\"services\":{\"backupAPI\":8097,\"backupAPIHTTPS\":18097,\"backupGRPC\":9124,\"capi\":8092,\"capiSSL\":18092,\"cbas\":8095,\"cbasSSL\":18095,\"eventingAdminPort\":8096,\"eventingDebug\":9140,\"eventingSSL\":18096,\"fts\":8094,\"ftsGRPC\":9130,\"ftsGRPCSSL\":19130,\"ftsSSL\":18094,\"indexAdmin\":9100,\"indexHttp\":9102,\"indexHttps\":19102,\"indexScan\":9101,\"indexStreamCatchup\":9104,\"indexStreamInit\":9103,\"indexStreamMaint\":9105,\"kv\":11210,\"kvSSL\":11207,\"mgmt\":8091,\"mgmtSSL\":18091,\"projector\":9999},\"thisNode\":true}],\"clusterCapabilitiesVer\":[1,0],\"clusterCapabilities\":{\"n1ql\":[\"enhancedPreparedStatements\"]},\"revEpoch\":1}">>
[ns_server:debug,2022-09-07T14:32:19.275Z,ns_1@127.0.0.1:service_rebalancer-n1ql-worker<0.9827.0>:service_rebalancer:rebalance_worker:153]Got node infos:
[{'ns_1@127.0.0.1',[{node_id,<<"1678dfae96c38e07dff43c49b9f6967b">>},
                    {priority,0},
                    {opaque,null}]}]
[ns_server:debug,2022-09-07T14:32:19.281Z,ns_1@127.0.0.1:service_rebalancer-n1ql-worker<0.9827.0>:service_rebalancer:rebalance_worker:162]Using node 'ns_1@127.0.0.1' as a leader
[ns_server:debug,2022-09-07T14:32:19.283Z,ns_1@127.0.0.1:service_rebalancer-n1ql<0.9817.0>:service_rebalancer:run_rebalance_worker:116]Worker terminated normally
[ns_server:debug,2022-09-07T14:32:19.285Z,ns_1@127.0.0.1:cleanup_process<0.9372.0>:ns_cluster_membership:set_service_map:551]Set service map for service n1ql to ['ns_1@127.0.0.1']
[ns_server:debug,2022-09-07T14:32:19.312Z,ns_1@127.0.0.1:cleanup_process<0.9372.0>:service_janitor:init_topology_aware_service:87]Initial rebalance for `n1ql` finished successfully
[ns_server:debug,2022-09-07T14:32:19.312Z,ns_1@127.0.0.1:chronicle_kv_log<0.404.0>:chronicle_kv_log:log:59]update (key: {service_map,n1ql}, rev: {<<"1933459bb8a062c61b619e931c483c80">>,
                                       19})
['ns_1@127.0.0.1']
[ns_server:debug,2022-09-07T14:32:19.313Z,ns_1@127.0.0.1:terse_cluster_info_uploader<0.613.0>:terse_cluster_info_uploader:handle_info:42]Refreshing terse cluster info with <<"{\"rev\":61,\"nodesExt\":[{\"services\":{\"backupAPI\":8097,\"backupAPIHTTPS\":18097,\"backupGRPC\":9124,\"capi\":8092,\"capiSSL\":18092,\"cbas\":8095,\"cbasSSL\":18095,\"eventingAdminPort\":8096,\"eventingDebug\":9140,\"eventingSSL\":18096,\"fts\":8094,\"ftsGRPC\":9130,\"ftsGRPCSSL\":19130,\"ftsSSL\":18094,\"indexAdmin\":9100,\"indexHttp\":9102,\"indexHttps\":19102,\"indexScan\":9101,\"indexStreamCatchup\":9104,\"indexStreamInit\":9103,\"indexStreamMaint\":9105,\"kv\":11210,\"kvSSL\":11207,\"mgmt\":8091,\"mgmtSSL\":18091,\"n1ql\":8093,\"n1qlSSL\":18093,\"projector\":9999},\"thisNode\":true}],\"clusterCapabilitiesVer\":[1,0],\"clusterCapabilities\":{\"n1ql\":[\"enhancedPreparedStatements\"]},\"revEpoch\":1}">>
[ns_server:debug,2022-09-07T14:32:21.108Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{local_changes_count,<<"1678dfae96c38e07dff43c49b9f6967b">>} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{43,63829780341}}]}]
[ns_server:debug,2022-09-07T14:32:21.108Z,ns_1@127.0.0.1:ns_config_rep<0.444.0>:ns_config_rep:do_push_keys:383]Replicating some config keys ([{local_changes_count,
                                   <<"1678dfae96c38e07dff43c49b9f6967b">>},
                               {metakv,
                                   <<"/cbas/config/node/1678dfae96c38e07dff43c49b9f6967b">>}]..)
[ns_server:debug,2022-09-07T14:32:21.108Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{metakv,<<"/cbas/config/node/1678dfae96c38e07dff43c49b9f6967b">>} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780341}}]}|
 <<"{\"configVersion\":1,\"config\":{\"additionalCcNodes\":[],\"address\":\"0.0.0.0\",\"analyticsCbHome\":\"/opt/couchbase\",\"analyticsCcHttpPort\":\"9111\",\"analyticsHttpAdminListenPort\":\"9110\",\"analyticsHttpListenAddress\":[\"127.0.0.1\",\"172.18.0.2\"],\"analyticsHttpListenPort\":\"8095\",\"analyticsHttpsListenAddress\":[\"127.0.0.1\",\"172.18.0.2\"],\"analyticsHttpsListenPort\":\"18095\",\"analyticsNodeName\":\"127.0.0.1:80"...>>]
[ns_server:debug,2022-09-07T14:32:22.506Z,ns_1@127.0.0.1:leader_lease_agent<0.8806.0>:leader_lease_agent:handle_lease_expired:277]Lease held by {lease_holder,<<"fa814c54dc8b348114b4d9a3567953c9">>,
                            'ns_1@cb.local'} expired. Starting expirer.
[ns_server:debug,2022-09-07T14:32:22.508Z,ns_1@127.0.0.1:leader_lease_agent<0.8806.0>:leader_lease_agent:do_handle_acquire_lease:140]Granting lease to {lease_holder,<<"c1569384114faab696b10950c44ef888">>,
                                'ns_1@127.0.0.1'} for 15000ms
[ns_server:info,2022-09-07T14:32:22.521Z,ns_1@127.0.0.1:<0.8817.0>:leader_lease_acquire_worker:handle_fresh_lease_acquired:296]Acquired lease from node 'ns_1@127.0.0.1' (lease uuid: <<"c1569384114faab696b10950c44ef888">>)
[ns_server:debug,2022-09-07T14:32:25.350Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{local_changes_count,<<"1678dfae96c38e07dff43c49b9f6967b">>} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{44,63829780345}}]}]
[ns_server:debug,2022-09-07T14:32:25.350Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{metakv,<<"/cbas/bootstrap/ensureCc/1678dfae96c38e07dff43c49b9f6967b">>} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780345}}]}|
 <<"{}">>]
[ns_server:debug,2022-09-07T14:32:25.352Z,ns_1@127.0.0.1:ns_config_rep<0.444.0>:ns_config_rep:do_push_keys:383]Replicating some config keys ([{local_changes_count,
                                   <<"1678dfae96c38e07dff43c49b9f6967b">>},
                               {metakv,
                                   <<"/cbas/bootstrap/ensureCc/1678dfae96c38e07dff43c49b9f6967b">>}]..)
[ns_server:debug,2022-09-07T14:32:25.728Z,ns_1@127.0.0.1:chronicle_kv_log<0.404.0>:chronicle_kv_log:log:59]update (key: bucket_names, rev: {<<"1933459bb8a062c61b619e931c483c80">>,20})
["todo"]
[ns_server:debug,2022-09-07T14:32:25.729Z,ns_1@127.0.0.1:roles_cache<0.365.0>:active_cache:clean:181]Clearing the roles_cache cache
[ns_server:debug,2022-09-07T14:32:25.729Z,ns_1@127.0.0.1:chronicle_kv_log<0.404.0>:chronicle_kv_log:log:59]update (key: {bucket,"todo",props}, rev: {<<"1933459bb8a062c61b619e931c483c80">>,
                                          20})
[{num_replicas,0},
 {replica_index,false},
 {ram_quota,3920625664},
 {durability_min_level,none},
 {autocompaction,false},
 {purge_interval,undefined},
 {flush_enabled,false},
 {num_threads,3},
 {eviction_policy,value_only},
 {conflict_resolution_type,seqno},
 {storage_mode,couchstore},
 {max_ttl,0},
 {compression_mode,passive},
 {type,membase},
 {num_vbuckets,1024},
 {replication_topology,star},
 {repl_type,dcp},
 {servers,[]}]
[ns_server:debug,2022-09-07T14:32:25.729Z,ns_1@127.0.0.1:ns_audit<0.597.0>:ns_audit:handle_call:148]Audit create_bucket: [{local,{[{ip,<<"172.18.0.2">>},{port,8091}]}},
                      {remote,{[{ip,<<"172.18.0.1">>},{port,35750}]}},
                      {sessionid,<<"b3740d8cf0080a8b89291f496feba96c33ca5f53">>},
                      {real_userid,{[{domain,builtin},
                                     {user,<<"<ud>admin</ud>">>}]}},
                      {timestamp,<<"2022-09-07T14:32:25.729Z">>},
                      {props,{[{compression_mode,passive},
                               {max_ttl,0},
                               {storage_mode,couchstore},
                               {conflict_resolution_type,seqno},
                               {eviction_policy,value_only},
                               {num_threads,3},
                               {flush_enabled,false},
                               {purge_interval,undefined},
                               {durability_min_level,none},
                               {ram_quota,3920625664},
                               {replica_index,false},
                               {num_replicas,0}]}},
                      {type,membase},
                      {bucket_name,<<"todo">>}]
[ns_server:debug,2022-09-07T14:32:25.729Z,ns_1@127.0.0.1:compiled_roles_cache<0.362.0>:versioned_cache:handle_info:89]Flushing cache compiled_roles_cache due to version change from {[7,1],
                                                                {0,1415866166},
                                                                {0,1415866166},
                                                                true,[]} to {[7,
                                                                              1],
                                                                             {0,
                                                                              1415866166},
                                                                             {0,
                                                                              1415866166},
                                                                             true,
                                                                             [{"todo",
                                                                               <<"1d5e549a39e5fbca838b18cff4b8f22b">>,
                                                                               0}]}
[ns_server:debug,2022-09-07T14:32:25.729Z,ns_1@127.0.0.1:roles_cache<0.365.0>:active_cache:clean:181]Clearing the roles_cache cache
[ns_server:debug,2022-09-07T14:32:25.729Z,ns_1@127.0.0.1:chronicle_kv_log<0.404.0>:chronicle_kv_log:log:59]update (key: {bucket,"todo",uuid}, rev: {<<"1933459bb8a062c61b619e931c483c80">>,
                                         20})
<<"1d5e549a39e5fbca838b18cff4b8f22b">>
[ns_server:debug,2022-09-07T14:32:25.729Z,ns_1@127.0.0.1:chronicle_kv_log<0.404.0>:chronicle_kv_log:log:59]update (key: {bucket,"todo",collections}, rev: {<<"1933459bb8a062c61b619e931c483c80">>,
                                                20})
[{uid,0},
 {next_uid,1},
 {next_scope_uid,8},
 {next_coll_uid,8},
 {num_scopes,0},
 {num_collections,0},
 {scopes,[{"_default",[{uid,0},{collections,[{"_default",[{uid,0}]}]}]}]}]
[menelaus:info,2022-09-07T14:32:25.729Z,ns_1@127.0.0.1:<0.9853.0>:menelaus_web_buckets:do_bucket_create:629]Created bucket "todo" of type: couchbase
[{num_replicas,0},
 {replica_index,false},
 {ram_quota,3920625664},
 {durability_min_level,none},
 {autocompaction,false},
 {purge_interval,undefined},
 {flush_enabled,false},
 {num_threads,3},
 {eviction_policy,value_only},
 {conflict_resolution_type,seqno},
 {storage_mode,couchstore},
 {max_ttl,0},
 {compression_mode,passive}]
[ns_server:debug,2022-09-07T14:32:25.729Z,ns_1@127.0.0.1:chronicle_kv_log<0.404.0>:chronicle_kv_log:log:59]update (key: {node,'ns_1@127.0.0.1',{"todo",last_seen_collection_ids}}, rev: {<<"1933459bb8a062c61b619e931c483c80">>,
                                                                              20})
[1,8,8]
[ns_server:debug,2022-09-07T14:32:25.730Z,ns_1@127.0.0.1:memcached_permissions<0.430.0>:memcached_cfg:write_cfg:148]Writing config file for: "/opt/couchbase/var/lib/couchbase/config/memcached.rbac"
[ns_server:debug,2022-09-07T14:32:25.735Z,ns_1@127.0.0.1:memcached_permissions<0.430.0>:replicated_dets:select_from_table:281][ets] Starting select with {users_storage,
                               [{{docv2,{user,{'_',local}},'_','_'},
                                 [],
                                 ['$_']}],
                               100}
[ns_server:debug,2022-09-07T14:32:25.737Z,ns_1@127.0.0.1:memcached_refresh<0.283.0>:memcached_refresh:handle_call:49]File rename from "/opt/couchbase/var/lib/couchbase/config/memcached.rbac.tmp" to "/opt/couchbase/var/lib/couchbase/config/memcached.rbac" is requested
[ns_server:debug,2022-09-07T14:32:25.739Z,ns_1@127.0.0.1:<0.10154.0>:ns_janitor:update_servers:83]janitor decided to update servers list for bucket "todo" to ['ns_1@127.0.0.1']
[ns_server:debug,2022-09-07T14:32:25.741Z,ns_1@127.0.0.1:memcached_permissions<0.430.0>:memcached_cfg:rename_and_refresh:170]Successfully renamed "/opt/couchbase/var/lib/couchbase/config/memcached.rbac.tmp" to "/opt/couchbase/var/lib/couchbase/config/memcached.rbac"
[ns_server:debug,2022-09-07T14:32:25.742Z,ns_1@127.0.0.1:memcached_refresh<0.283.0>:memcached_refresh:handle_cast:55]Refresh of rbac requested
[ns_server:debug,2022-09-07T14:32:25.742Z,ns_1@127.0.0.1:memcached_permissions<0.430.0>:memcached_cfg:write_cfg:148]Writing config file for: "/opt/couchbase/var/lib/couchbase/config/memcached.rbac"
[ns_server:debug,2022-09-07T14:32:25.746Z,ns_1@127.0.0.1:memcached_permissions<0.430.0>:replicated_dets:select_from_table:281][ets] Starting select with {users_storage,
                               [{{docv2,{user,{'_',local}},'_','_'},
                                 [],
                                 ['$_']}],
                               100}
[ns_server:debug,2022-09-07T14:32:25.751Z,ns_1@127.0.0.1:compiled_roles_cache<0.362.0>:menelaus_roles:build_compiled_roles:998]Compile roles for user {"<ud>admin</ud>",admin}
[ns_server:debug,2022-09-07T14:32:25.755Z,ns_1@127.0.0.1:memcached_refresh<0.283.0>:memcached_refresh:handle_info:89]Refresh of [rbac] succeeded
[ns_server:debug,2022-09-07T14:32:25.755Z,ns_1@127.0.0.1:memcached_refresh<0.283.0>:memcached_refresh:handle_call:49]File rename from "/opt/couchbase/var/lib/couchbase/config/memcached.rbac.tmp" to "/opt/couchbase/var/lib/couchbase/config/memcached.rbac" is requested
[ns_server:debug,2022-09-07T14:32:25.761Z,ns_1@127.0.0.1:memcached_permissions<0.430.0>:memcached_cfg:rename_and_refresh:170]Successfully renamed "/opt/couchbase/var/lib/couchbase/config/memcached.rbac.tmp" to "/opt/couchbase/var/lib/couchbase/config/memcached.rbac"
[ns_server:debug,2022-09-07T14:32:25.761Z,ns_1@127.0.0.1:memcached_refresh<0.283.0>:memcached_refresh:handle_cast:55]Refresh of rbac requested
[ns_server:debug,2022-09-07T14:32:25.770Z,ns_1@127.0.0.1:<0.10154.0>:ns_janitor:config_sync:257]Going to push config to/from nodes:
['ns_1@127.0.0.1']
[ns_server:debug,2022-09-07T14:32:25.771Z,ns_1@127.0.0.1:memcached_refresh<0.283.0>:memcached_refresh:handle_info:89]Refresh of [rbac] succeeded
[ns_server:debug,2022-09-07T14:32:25.771Z,ns_1@127.0.0.1:chronicle_kv_log<0.404.0>:chronicle_kv_log:log:59]update (key: {bucket,"todo",props}, rev: {<<"1933459bb8a062c61b619e931c483c80">>,
                                          21})
[{map,[]},
 {fastForwardMap,[]},
 {num_replicas,0},
 {replica_index,false},
 {ram_quota,3920625664},
 {durability_min_level,none},
 {autocompaction,false},
 {purge_interval,undefined},
 {flush_enabled,false},
 {num_threads,3},
 {eviction_policy,value_only},
 {conflict_resolution_type,seqno},
 {storage_mode,couchstore},
 {max_ttl,0},
 {compression_mode,passive},
 {type,membase},
 {num_vbuckets,1024},
 {replication_topology,star},
 {repl_type,dcp},
 {servers,['ns_1@127.0.0.1']}]
[ns_server:debug,2022-09-07T14:32:25.771Z,ns_1@127.0.0.1:ns_bucket_worker<0.636.0>:ns_bucket_worker:start_one_bucket:101]Starting new bucket: "todo"
[ns_server:debug,2022-09-07T14:32:25.772Z,ns_1@127.0.0.1:roles_cache<0.365.0>:active_cache:clean:181]Clearing the roles_cache cache
[ns_server:info,2022-09-07T14:32:25.772Z,ns_1@127.0.0.1:<0.10157.0>:ns_janitor:cleanup_with_membase_bucket_check_map:95]janitor decided to generate initial vbucket map
[ns_server:debug,2022-09-07T14:32:25.780Z,ns_1@127.0.0.1:compiled_roles_cache<0.362.0>:menelaus_roles:build_compiled_roles:998]Compile roles for user {"@eventing",admin}
[ns_server:debug,2022-09-07T14:32:25.781Z,ns_1@127.0.0.1:compiled_roles_cache<0.362.0>:menelaus_roles:build_compiled_roles:998]Compile roles for user {"@cbq-engine",admin}
[ns_server:debug,2022-09-07T14:32:25.781Z,ns_1@127.0.0.1:compiled_roles_cache<0.362.0>:menelaus_roles:build_compiled_roles:998]Compile roles for user {"@cbas",admin}
[ns_server:debug,2022-09-07T14:32:25.781Z,ns_1@127.0.0.1:compiled_roles_cache<0.362.0>:menelaus_roles:build_compiled_roles:998]Compile roles for user {"@index",admin}
[ns_server:debug,2022-09-07T14:32:25.783Z,ns_1@127.0.0.1:compiled_roles_cache<0.362.0>:menelaus_roles:build_compiled_roles:998]Compile roles for user {"@fts",admin}
[ns_server:debug,2022-09-07T14:32:25.783Z,ns_1@127.0.0.1:compiled_roles_cache<0.362.0>:menelaus_roles:build_compiled_roles:998]Compile roles for user {"@projector",admin}
[ns_server:debug,2022-09-07T14:32:25.791Z,ns_1@127.0.0.1:single_bucket_kv_sup-todo<0.10167.0>:single_bucket_kv_sup:sync_config_to_couchdb_node:64]Syncing config to couchdb node
[ns_server:debug,2022-09-07T14:32:25.792Z,ns_1@127.0.0.1:ns_config_rep<0.444.0>:ns_config_rep:handle_cast:173]Synchronized with merger in 10 us
[ns_server:debug,2022-09-07T14:32:25.795Z,ns_1@127.0.0.1:single_bucket_kv_sup-todo<0.10167.0>:single_bucket_kv_sup:sync_config_to_couchdb_node:70]Synced config to couchdb node successfully
[ns_server:debug,2022-09-07T14:32:25.843Z,ns_1@127.0.0.1:<0.10157.0>:mb_map:generate_map_new:325]Scores for past maps:
[]
[ns_server:debug,2022-09-07T14:32:25.843Z,ns_1@127.0.0.1:<0.10188.0>:mb_map:do_invoke_vbmap_body:748]Node Id Map: [{'ns_1@127.0.0.1',0}]
[ns_server:debug,2022-09-07T14:32:25.854Z,ns_1@127.0.0.1:<0.10188.0>:mb_map:do_invoke_vbmap_body:759]Wrote vbmap to "/opt/couchbase/var/lib/couchbase/tmp/prev-vbmap-576460752303423066_160.json"
[ns_server:debug,2022-09-07T14:32:25.860Z,ns_1@127.0.0.1:capi_doc_replicator-todo<0.10197.0>:replicated_storage:wait_for_startup:46]Start waiting for startup
[error_logger:info,2022-09-07T14:32:25.861Z,ns_1@127.0.0.1:<0.10185.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.10185.0>,docs_kv_sup}
    started: [{pid,<0.10197.0>},
              {id,doc_replicator},
              {mfargs,{capi_ddoc_manager,start_replicator,["todo"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2022-09-07T14:32:25.861Z,ns_1@127.0.0.1:capi_ddoc_replication_srv-todo<0.10198.0>:replicated_storage:wait_for_startup:46]Start waiting for startup
[error_logger:info,2022-09-07T14:32:25.861Z,ns_1@127.0.0.1:<0.10185.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.10185.0>,docs_kv_sup}
    started: [{pid,<0.10198.0>},
              {id,doc_replication_srv},
              {mfargs,{doc_replication_srv,start_link,["todo"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2022-09-07T14:32:25.875Z,ns_1@127.0.0.1:logger_proxy<0.70.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,'capi_ddoc_manager_sup-todo'}
    started: [{pid,<15903.2933.0>},
              {id,capi_ddoc_manager_events},
              {mfargs,{capi_ddoc_manager,start_link_event_manager,["todo"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[ns_server:debug,2022-09-07T14:32:25.880Z,ns_1@127.0.0.1:capi_doc_replicator-todo<0.10197.0>:replicated_storage:wait_for_startup:49]Received replicated storage registration from <15903.2934.0>
[ns_server:debug,2022-09-07T14:32:25.880Z,ns_1@127.0.0.1:capi_ddoc_replication_srv-todo<0.10198.0>:replicated_storage:wait_for_startup:49]Received replicated storage registration from <15903.2934.0>
[error_logger:info,2022-09-07T14:32:25.885Z,ns_1@127.0.0.1:<0.10185.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.10185.0>,docs_kv_sup}
    started: [{pid,<15903.2932.0>},
              {id,capi_ddoc_manager_sup},
              {mfargs,
                  {capi_ddoc_manager_sup,start_link_remote,
                      ['couchdb_ns_1@cb.local',"todo"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2022-09-07T14:32:25.885Z,ns_1@127.0.0.1:logger_proxy<0.70.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,'capi_ddoc_manager_sup-todo'}
    started: [{pid,<15903.2934.0>},
              {id,capi_ddoc_manager},
              {mfargs,
                  {capi_ddoc_manager,start_link,
                      ["todo",<0.10197.0>,<0.10198.0>]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2022-09-07T14:32:25.910Z,ns_1@127.0.0.1:<0.10185.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.10185.0>,docs_kv_sup}
    started: [{pid,<15903.2936.0>},
              {id,capi_set_view_manager},
              {mfargs,
                  {capi_set_view_manager,start_link_remote,
                      ['couchdb_ns_1@cb.local',"todo"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2022-09-07T14:32:25.928Z,ns_1@127.0.0.1:<0.10185.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.10185.0>,docs_kv_sup}
    started: [{pid,<15903.2938.0>},
              {id,couch_stats_reader},
              {mfargs,
                  {couch_stats_reader,start_link_remote,
                      ['couchdb_ns_1@cb.local',"todo"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2022-09-07T14:32:25.928Z,ns_1@127.0.0.1:single_bucket_kv_sup-todo<0.10167.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,'single_bucket_kv_sup-todo'}
    started: [{pid,<0.10185.0>},
              {id,{docs_kv_sup,"todo"}},
              {mfargs,{docs_kv_sup,start_link,["todo"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2022-09-07T14:32:25.936Z,ns_1@127.0.0.1:ns_memcached-todo<0.10209.0>:ns_memcached:init:148]Starting ns_memcached
[ns_server:debug,2022-09-07T14:32:25.936Z,ns_1@127.0.0.1:<0.10210.0>:ns_memcached:run_connect_phase:174]Started 'connecting' phase of ns_memcached-todo. Parent is <0.10209.0>
[error_logger:info,2022-09-07T14:32:25.936Z,ns_1@127.0.0.1:<0.10208.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.10208.0>,ns_memcached_sup}
    started: [{pid,<0.10209.0>},
              {id,{ns_memcached,"todo"}},
              {mfargs,{ns_memcached,start_link,["todo"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,86400000},
              {child_type,worker}]

[error_logger:info,2022-09-07T14:32:25.952Z,ns_1@127.0.0.1:<0.10208.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.10208.0>,ns_memcached_sup}
    started: [{pid,<0.10211.0>},
              {id,{terse_bucket_info_uploader,"todo"}},
              {mfargs,{terse_bucket_info_uploader,start_link,["todo"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2022-09-07T14:32:25.952Z,ns_1@127.0.0.1:single_bucket_kv_sup-todo<0.10167.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,'single_bucket_kv_sup-todo'}
    started: [{pid,<0.10208.0>},
              {id,{ns_memcached_sup,"todo"}},
              {mfargs,{ns_memcached_sup,start_link,["todo"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2022-09-07T14:32:25.970Z,ns_1@127.0.0.1:compiled_roles_cache<0.362.0>:menelaus_roles:build_compiled_roles:998]Compile roles for user {"@goxdcr",admin}
[ns_server:debug,2022-09-07T14:32:25.970Z,ns_1@127.0.0.1:compiled_roles_cache<0.362.0>:menelaus_roles:build_compiled_roles:998]Compile roles for user {"@",admin}
[error_logger:info,2022-09-07T14:32:25.989Z,ns_1@127.0.0.1:single_bucket_kv_sup-todo<0.10167.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,'single_bucket_kv_sup-todo'}
    started: [{pid,<0.10233.0>},
              {id,{dcp_sup,"todo"}},
              {mfargs,{dcp_sup,start_link,["todo"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2022-09-07T14:32:26.000Z,ns_1@127.0.0.1:single_bucket_kv_sup-todo<0.10167.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,'single_bucket_kv_sup-todo'}
    started: [{pid,<0.10234.0>},
              {id,{dcp_replication_manager,"todo"}},
              {mfargs,{dcp_replication_manager,start_link,["todo"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2022-09-07T14:32:26.004Z,ns_1@127.0.0.1:single_bucket_kv_sup-todo<0.10167.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,'single_bucket_kv_sup-todo'}
    started: [{pid,<0.10235.0>},
              {id,{replication_manager,"todo"}},
              {mfargs,{replication_manager,start_link,["todo"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2022-09-07T14:32:26.026Z,ns_1@127.0.0.1:janitor_agent_sup-todo<0.10236.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,'janitor_agent_sup-todo'}
    started: [{pid,<0.10237.0>},
              {id,rebalance_subprocesses_registry},
              {mfargs,
                  {ns_process_registry,start_link,
                      ['rebalance_subprocesses_registry-todo',
                       [{terminate_command,kill}]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,86400000},
              {child_type,worker}]

[ns_server:debug,2022-09-07T14:32:26.038Z,ns_1@127.0.0.1:janitor_agent-todo<0.10238.0>:dcp_sup:nuke:114]Nuking DCP replicators for bucket "todo":
[]
[error_logger:info,2022-09-07T14:32:26.038Z,ns_1@127.0.0.1:janitor_agent_sup-todo<0.10236.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,'janitor_agent_sup-todo'}
    started: [{pid,<0.10238.0>},
              {id,janitor_agent},
              {mfargs,{janitor_agent,start_link,["todo"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2022-09-07T14:32:26.039Z,ns_1@127.0.0.1:single_bucket_kv_sup-todo<0.10167.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,'single_bucket_kv_sup-todo'}
    started: [{pid,<0.10236.0>},
              {id,{janitor_agent_sup,"todo"}},
              {mfargs,{janitor_agent_sup,start_link,["todo"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2022-09-07T14:32:26.041Z,ns_1@127.0.0.1:single_bucket_kv_sup-todo<0.10167.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,'single_bucket_kv_sup-todo'}
    started: [{pid,<0.10239.0>},
              {id,{stats_reader,"todo"}},
              {mfargs,{stats_reader,start_link,["todo"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2022-09-07T14:32:26.042Z,ns_1@127.0.0.1:single_bucket_kv_sup-todo<0.10167.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,'single_bucket_kv_sup-todo'}
    started: [{pid,<0.10241.0>},
              {id,{goxdcr_stats_reader,"todo"}},
              {mfargs,{stats_reader,start_link,["@xdcr-todo"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2022-09-07T14:32:26.043Z,ns_1@127.0.0.1:ns_bucket_sup<0.635.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_bucket_sup}
    started: [{pid,<0.10167.0>},
              {id,{single_bucket_kv_sup,"todo"}},
              {mfargs,{single_bucket_kv_sup,start_link,["todo"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:info,2022-09-07T14:32:26.081Z,ns_1@127.0.0.1:ns_memcached-todo<0.10209.0>:ns_memcached:do_ensure_bucket:1365]Created bucket "todo" with config string "max_size=3920625664;dbname=/opt/couchbase/var/lib/couchbase/data/todo;backend=couchdb;couch_bucket=todo;max_vbuckets=1024;alog_path=/opt/couchbase/var/lib/couchbase/data/todo/access.log;data_traffic_enabled=false;max_num_workers=3;uuid=1d5e549a39e5fbca838b18cff4b8f22b;conflict_resolution_type=seqno;bucket_type=persistent;durability_min_level=none;pitr_enabled=false;pitr_granularity=600;pitr_max_history_age=86400;magma_fragmentation_percentage=50;magma_mem_quota_ratio=0.5;item_eviction_policy=value_only;persistent_metadata_purge_age=259200;max_ttl=0;ht_locks=47;compression_mode=passive;failpartialwarmup=false"
[ns_server:info,2022-09-07T14:32:26.086Z,ns_1@127.0.0.1:ns_memcached-todo<0.10209.0>:ns_memcached:handle_info:706]Main ns_memcached connection established: {ok,#Port<0.451>}
[ns_server:info,2022-09-07T14:32:26.094Z,ns_1@127.0.0.1:janitor_agent-todo<0.10238.0>:janitor_agent:read_flush_counter:915]Loading flushseq failed: {error,enoent}. Assuming it's equal to global config.
[ns_server:info,2022-09-07T14:32:26.095Z,ns_1@127.0.0.1:janitor_agent-todo<0.10238.0>:janitor_agent:read_flush_counter_from_config:923]Initialized flushseq 0 from bucket config
[ns_server:debug,2022-09-07T14:32:26.348Z,ns_1@127.0.0.1:<0.10188.0>:mb_map:do_invoke_vbmap_body:783]vbmap diag output:
Started as:
  /opt/couchbase/bin/vbmap --diag /opt/couchbase/var/lib/couchbase/tmp/vbmap_diag-576460752303423098_160 --output-format json --num-vbuckets 1024 --num-nodes 1 --num-slaves 10 --num-replicas 0 --relax-all --greedy --current-map /opt/couchbase/var/lib/couchbase/tmp/prev-vbmap-576460752303423066_160.json
Using 1662561145910331503 as a seed
Tags are not specified. Assuming every node on a separate tag.
Finalized parameters
  Number of nodes: 1
  Number of slaves: 0
  Number of vbuckets: 1024
  Number of replicas: 0
  Tags assignments:
    0 -> 0
Succesfully read current vbucket map:
[
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1]
]
Prev R:
RI (greedy):
  N |    |  0 |
----|---------|
    |  T |  0 |
----|---------|
  0 |  0 |  0 | 0
____|_________|
    |    |  0 |
R (greedy):
  N |    |  0 |
----|---------|
    |  T |  0 |
----|---------|
  0 |  0 |  0 | 0
____|_________|
    |    |  0 |
Time spent for vbmap generation (greedy) - 1.481778ms
Per-node Vb Stats (greedy):
   Active Vbs:   map[0:1024]
   Replica Vbs:  map[0:0]
Move stats:
    Active takeovers - 1024
    Total new replicas - 1024

[ns_server:debug,2022-09-07T14:32:26.351Z,ns_1@127.0.0.1:<0.10188.0>:mb_map:do_invoke_vbmap_body:801]Score before simple minimization: {1024,1024}
[ns_server:debug,2022-09-07T14:32:26.352Z,ns_1@127.0.0.1:<0.10188.0>:mb_map:do_invoke_vbmap_body:807]Score after simple minimization: {1024,1024}
[ns_server:debug,2022-09-07T14:32:26.353Z,ns_1@127.0.0.1:<0.10188.0>:mb_map:do_invoke_vbmap_body:816]Map better after simple minimization;
                                       using it
[ns_server:debug,2022-09-07T14:32:26.356Z,ns_1@127.0.0.1:<0.10277.0>:mb_map:do_invoke_vbmap_body:748]Node Id Map: [{'ns_1@127.0.0.1',0}]
[ns_server:debug,2022-09-07T14:32:26.363Z,ns_1@127.0.0.1:<0.10277.0>:mb_map:do_invoke_vbmap_body:759]Wrote vbmap to "/opt/couchbase/var/lib/couchbase/tmp/prev-vbmap-576460752303423321_160.json"
[user:info,2022-09-07T14:32:26.588Z,ns_1@127.0.0.1:ns_memcached-todo<0.10209.0>:ns_memcached:handle_cast:676]Bucket "todo" loaded on node 'ns_1@127.0.0.1' in 0 seconds.
[ns_server:debug,2022-09-07T14:32:26.682Z,ns_1@127.0.0.1:<0.10277.0>:mb_map:do_invoke_vbmap_body:783]vbmap diag output:
Started as:
  /opt/couchbase/bin/vbmap --diag /opt/couchbase/var/lib/couchbase/tmp/vbmap_diag-576460752303423353_160 --output-format json --num-vbuckets 1024 --num-nodes 1 --num-slaves 10 --num-replicas 0 --relax-all --greedy --current-map /opt/couchbase/var/lib/couchbase/tmp/prev-vbmap-576460752303423321_160.json
Using 1662561146368533697 as a seed
Tags are not specified. Assuming every node on a separate tag.
Finalized parameters
  Number of nodes: 1
  Number of slaves: 0
  Number of vbuckets: 1024
  Number of replicas: 0
  Tags assignments:
    0 -> 0
Succesfully read current vbucket map:
[
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1]
]
Prev R:
RI (greedy):
  N |    |  0 |
----|---------|
    |  T |  0 |
----|---------|
  0 |  0 |  0 | 0
____|_________|
    |    |  0 |
R (greedy):
  N |    |  0 |
----|---------|
    |  T |  0 |
----|---------|
  0 |  0 |  0 | 0
____|_________|
    |    |  0 |
Time spent for vbmap generation (greedy) - 819.14Âµs
Per-node Vb Stats (greedy):
   Active Vbs:   map[0:1024]
   Replica Vbs:  map[0:0]
Move stats:
    Active takeovers - 1024
    Total new replicas - 1024

[ns_server:debug,2022-09-07T14:32:26.684Z,ns_1@127.0.0.1:<0.10277.0>:mb_map:do_invoke_vbmap_body:801]Score before simple minimization: {1024,1024}
[ns_server:debug,2022-09-07T14:32:26.685Z,ns_1@127.0.0.1:<0.10277.0>:mb_map:do_invoke_vbmap_body:807]Score after simple minimization: {1024,1024}
[ns_server:debug,2022-09-07T14:32:26.686Z,ns_1@127.0.0.1:<0.10277.0>:mb_map:do_invoke_vbmap_body:816]Map better after simple minimization;
                                       using it
[ns_server:debug,2022-09-07T14:32:26.689Z,ns_1@127.0.0.1:<0.10285.0>:mb_map:do_invoke_vbmap_body:748]Node Id Map: [{'ns_1@127.0.0.1',0}]
[ns_server:debug,2022-09-07T14:32:26.700Z,ns_1@127.0.0.1:<0.10285.0>:mb_map:do_invoke_vbmap_body:759]Wrote vbmap to "/opt/couchbase/var/lib/couchbase/tmp/prev-vbmap-576460752303423002_160.json"
[ns_server:debug,2022-09-07T14:32:27.049Z,ns_1@127.0.0.1:<0.10285.0>:mb_map:do_invoke_vbmap_body:783]vbmap diag output:
Started as:
  /opt/couchbase/bin/vbmap --diag /opt/couchbase/var/lib/couchbase/tmp/vbmap_diag-576460752303423289_160 --output-format json --num-vbuckets 1024 --num-nodes 1 --num-slaves 10 --num-replicas 0 --relax-all --greedy --current-map /opt/couchbase/var/lib/couchbase/tmp/prev-vbmap-576460752303423002_160.json
Using 1662561146703954240 as a seed
Tags are not specified. Assuming every node on a separate tag.
Finalized parameters
  Number of nodes: 1
  Number of slaves: 0
  Number of vbuckets: 1024
  Number of replicas: 0
  Tags assignments:
    0 -> 0
Succesfully read current vbucket map:
[
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1]
]
Prev R:
RI (greedy):
  N |    |  0 |
----|---------|
    |  T |  0 |
----|---------|
  0 |  0 |  0 | 0
____|_________|
    |    |  0 |
R (greedy):
  N |    |  0 |
----|---------|
    |  T |  0 |
----|---------|
  0 |  0 |  0 | 0
____|_________|
    |    |  0 |
Time spent for vbmap generation (greedy) - 548.707Âµs
Per-node Vb Stats (greedy):
   Active Vbs:   map[0:1024]
   Replica Vbs:  map[0:0]
Move stats:
    Active takeovers - 1024
    Total new replicas - 1024

[ns_server:debug,2022-09-07T14:32:27.050Z,ns_1@127.0.0.1:<0.10285.0>:mb_map:do_invoke_vbmap_body:801]Score before simple minimization: {1024,1024}
[ns_server:debug,2022-09-07T14:32:27.051Z,ns_1@127.0.0.1:<0.10285.0>:mb_map:do_invoke_vbmap_body:807]Score after simple minimization: {1024,1024}
[ns_server:debug,2022-09-07T14:32:27.051Z,ns_1@127.0.0.1:<0.10285.0>:mb_map:do_invoke_vbmap_body:816]Map better after simple minimization;
                                       using it
[ns_server:debug,2022-09-07T14:32:27.054Z,ns_1@127.0.0.1:<0.10323.0>:mb_map:do_invoke_vbmap_body:748]Node Id Map: [{'ns_1@127.0.0.1',0}]
[ns_server:debug,2022-09-07T14:32:27.059Z,ns_1@127.0.0.1:<0.10323.0>:mb_map:do_invoke_vbmap_body:759]Wrote vbmap to "/opt/couchbase/var/lib/couchbase/tmp/prev-vbmap-576460752303422750_160.json"
[ns_server:debug,2022-09-07T14:32:27.321Z,ns_1@127.0.0.1:<0.10323.0>:mb_map:do_invoke_vbmap_body:783]vbmap diag output:
Started as:
  /opt/couchbase/bin/vbmap --diag /opt/couchbase/var/lib/couchbase/tmp/vbmap_diag-576460752303422782_160 --output-format json --num-vbuckets 1024 --num-nodes 1 --num-slaves 10 --num-replicas 0 --relax-all --greedy --current-map /opt/couchbase/var/lib/couchbase/tmp/prev-vbmap-576460752303422750_160.json
Using 1662561147062040018 as a seed
Tags are not specified. Assuming every node on a separate tag.
Finalized parameters
  Number of nodes: 1
  Number of slaves: 0
  Number of vbuckets: 1024
  Number of replicas: 0
  Tags assignments:
    0 -> 0
Succesfully read current vbucket map:
[
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1]
]
Prev R:
RI (greedy):
  N |    |  0 |
----|---------|
    |  T |  0 |
----|---------|
  0 |  0 |  0 | 0
____|_________|
    |    |  0 |
R (greedy):
  N |    |  0 |
----|---------|
    |  T |  0 |
----|---------|
  0 |  0 |  0 | 0
____|_________|
    |    |  0 |
Time spent for vbmap generation (greedy) - 1.244408ms
Per-node Vb Stats (greedy):
   Active Vbs:   map[0:1024]
   Replica Vbs:  map[0:0]
Move stats:
    Active takeovers - 1024
    Total new replicas - 1024

[ns_server:debug,2022-09-07T14:32:27.323Z,ns_1@127.0.0.1:<0.10323.0>:mb_map:do_invoke_vbmap_body:801]Score before simple minimization: {1024,1024}
[ns_server:debug,2022-09-07T14:32:27.323Z,ns_1@127.0.0.1:<0.10323.0>:mb_map:do_invoke_vbmap_body:807]Score after simple minimization: {1024,1024}
[ns_server:debug,2022-09-07T14:32:27.323Z,ns_1@127.0.0.1:<0.10323.0>:mb_map:do_invoke_vbmap_body:816]Map better after simple minimization;
                                       using it
[ns_server:debug,2022-09-07T14:32:27.326Z,ns_1@127.0.0.1:<0.10345.0>:mb_map:do_invoke_vbmap_body:748]Node Id Map: [{'ns_1@127.0.0.1',0}]
[ns_server:debug,2022-09-07T14:32:27.330Z,ns_1@127.0.0.1:<0.10345.0>:mb_map:do_invoke_vbmap_body:759]Wrote vbmap to "/opt/couchbase/var/lib/couchbase/tmp/prev-vbmap-576460752303422686_160.json"
[ns_server:debug,2022-09-07T14:32:27.575Z,ns_1@127.0.0.1:<0.10345.0>:mb_map:do_invoke_vbmap_body:783]vbmap diag output:
Started as:
  /opt/couchbase/bin/vbmap --diag /opt/couchbase/var/lib/couchbase/tmp/vbmap_diag-576460752303422718_160 --output-format json --num-vbuckets 1024 --num-nodes 1 --num-slaves 10 --num-replicas 0 --relax-all --greedy --current-map /opt/couchbase/var/lib/couchbase/tmp/prev-vbmap-576460752303422686_160.json
Using 1662561147333645997 as a seed
Tags are not specified. Assuming every node on a separate tag.
Finalized parameters
  Number of nodes: 1
  Number of slaves: 0
  Number of vbuckets: 1024
  Number of replicas: 0
  Tags assignments:
    0 -> 0
Succesfully read current vbucket map:
[
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1]
]
Prev R:
RI (greedy):
  N |    |  0 |
----|---------|
    |  T |  0 |
----|---------|
  0 |  0 |  0 | 0
____|_________|
    |    |  0 |
R (greedy):
  N |    |  0 |
----|---------|
    |  T |  0 |
----|---------|
  0 |  0 |  0 | 0
____|_________|
    |    |  0 |
Time spent for vbmap generation (greedy) - 487.065Âµs
Per-node Vb Stats (greedy):
   Active Vbs:   map[0:1024]
   Replica Vbs:  map[0:0]
Move stats:
    Active takeovers - 1024
    Total new replicas - 1024

[ns_server:debug,2022-09-07T14:32:27.576Z,ns_1@127.0.0.1:<0.10345.0>:mb_map:do_invoke_vbmap_body:801]Score before simple minimization: {1024,1024}
[ns_server:debug,2022-09-07T14:32:27.577Z,ns_1@127.0.0.1:<0.10345.0>:mb_map:do_invoke_vbmap_body:807]Score after simple minimization: {1024,1024}
[ns_server:debug,2022-09-07T14:32:27.577Z,ns_1@127.0.0.1:<0.10345.0>:mb_map:do_invoke_vbmap_body:816]Map better after simple minimization;
                                       using it
[ns_server:debug,2022-09-07T14:32:27.578Z,ns_1@127.0.0.1:<0.10357.0>:mb_map:do_invoke_vbmap_body:748]Node Id Map: [{'ns_1@127.0.0.1',0}]
[ns_server:debug,2022-09-07T14:32:27.582Z,ns_1@127.0.0.1:<0.10357.0>:mb_map:do_invoke_vbmap_body:759]Wrote vbmap to "/opt/couchbase/var/lib/couchbase/tmp/prev-vbmap-576460752303422622_160.json"
[ns_server:debug,2022-09-07T14:32:27.648Z,ns_1@127.0.0.1:compiled_roles_cache<0.362.0>:menelaus_roles:build_compiled_roles:998]Compile roles for user {"@prometheus",stats_reader}
[ns_server:debug,2022-09-07T14:32:27.823Z,ns_1@127.0.0.1:<0.10357.0>:mb_map:do_invoke_vbmap_body:783]vbmap diag output:
Started as:
  /opt/couchbase/bin/vbmap --diag /opt/couchbase/var/lib/couchbase/tmp/vbmap_diag-576460752303422654_160 --output-format json --num-vbuckets 1024 --num-nodes 1 --num-slaves 10 --num-replicas 0 --relax-all --greedy --current-map /opt/couchbase/var/lib/couchbase/tmp/prev-vbmap-576460752303422622_160.json
Using 1662561147584944014 as a seed
Tags are not specified. Assuming every node on a separate tag.
Finalized parameters
  Number of nodes: 1
  Number of slaves: 0
  Number of vbuckets: 1024
  Number of replicas: 0
  Tags assignments:
    0 -> 0
Succesfully read current vbucket map:
[
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1]
]
Prev R:
RI (greedy):
  N |    |  0 |
----|---------|
    |  T |  0 |
----|---------|
  0 |  0 |  0 | 0
____|_________|
    |    |  0 |
R (greedy):
  N |    |  0 |
----|---------|
    |  T |  0 |
----|---------|
  0 |  0 |  0 | 0
____|_________|
    |    |  0 |
Time spent for vbmap generation (greedy) - 407.235Âµs
Per-node Vb Stats (greedy):
   Active Vbs:   map[0:1024]
   Replica Vbs:  map[0:0]
Move stats:
    Active takeovers - 1024
    Total new replicas - 1024

[ns_server:debug,2022-09-07T14:32:27.824Z,ns_1@127.0.0.1:<0.10357.0>:mb_map:do_invoke_vbmap_body:801]Score before simple minimization: {1024,1024}
[ns_server:debug,2022-09-07T14:32:27.825Z,ns_1@127.0.0.1:<0.10357.0>:mb_map:do_invoke_vbmap_body:807]Score after simple minimization: {1024,1024}
[ns_server:debug,2022-09-07T14:32:27.825Z,ns_1@127.0.0.1:<0.10357.0>:mb_map:do_invoke_vbmap_body:816]Map better after simple minimization;
                                       using it
[ns_server:debug,2022-09-07T14:32:27.827Z,ns_1@127.0.0.1:<0.10362.0>:mb_map:do_invoke_vbmap_body:748]Node Id Map: [{'ns_1@127.0.0.1',0}]
[ns_server:debug,2022-09-07T14:32:27.830Z,ns_1@127.0.0.1:<0.10362.0>:mb_map:do_invoke_vbmap_body:759]Wrote vbmap to "/opt/couchbase/var/lib/couchbase/tmp/prev-vbmap-576460752303422558_160.json"
[ns_server:debug,2022-09-07T14:32:28.072Z,ns_1@127.0.0.1:<0.10362.0>:mb_map:do_invoke_vbmap_body:783]vbmap diag output:
Started as:
  /opt/couchbase/bin/vbmap --diag /opt/couchbase/var/lib/couchbase/tmp/vbmap_diag-576460752303422590_160 --output-format json --num-vbuckets 1024 --num-nodes 1 --num-slaves 10 --num-replicas 0 --relax-all --greedy --current-map /opt/couchbase/var/lib/couchbase/tmp/prev-vbmap-576460752303422558_160.json
Using 1662561147832860805 as a seed
Tags are not specified. Assuming every node on a separate tag.
Finalized parameters
  Number of nodes: 1
  Number of slaves: 0
  Number of vbuckets: 1024
  Number of replicas: 0
  Tags assignments:
    0 -> 0
Succesfully read current vbucket map:
[
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1]
]
Prev R:
RI (greedy):
  N |    |  0 |
----|---------|
    |  T |  0 |
----|---------|
  0 |  0 |  0 | 0
____|_________|
    |    |  0 |
R (greedy):
  N |    |  0 |
----|---------|
    |  T |  0 |
----|---------|
  0 |  0 |  0 | 0
____|_________|
    |    |  0 |
Time spent for vbmap generation (greedy) - 570.07Âµs
Per-node Vb Stats (greedy):
   Active Vbs:   map[0:1024]
   Replica Vbs:  map[0:0]
Move stats:
    Active takeovers - 1024
    Total new replicas - 1024

[ns_server:debug,2022-09-07T14:32:28.074Z,ns_1@127.0.0.1:<0.10362.0>:mb_map:do_invoke_vbmap_body:801]Score before simple minimization: {1024,1024}
[ns_server:debug,2022-09-07T14:32:28.074Z,ns_1@127.0.0.1:<0.10362.0>:mb_map:do_invoke_vbmap_body:807]Score after simple minimization: {1024,1024}
[ns_server:debug,2022-09-07T14:32:28.074Z,ns_1@127.0.0.1:<0.10362.0>:mb_map:do_invoke_vbmap_body:816]Map better after simple minimization;
                                       using it
[ns_server:debug,2022-09-07T14:32:28.077Z,ns_1@127.0.0.1:<0.10377.0>:mb_map:do_invoke_vbmap_body:748]Node Id Map: [{'ns_1@127.0.0.1',0}]
[ns_server:debug,2022-09-07T14:32:28.081Z,ns_1@127.0.0.1:<0.10377.0>:mb_map:do_invoke_vbmap_body:759]Wrote vbmap to "/opt/couchbase/var/lib/couchbase/tmp/prev-vbmap-576460752303422494_160.json"
[ns_server:debug,2022-09-07T14:32:28.319Z,ns_1@127.0.0.1:<0.10377.0>:mb_map:do_invoke_vbmap_body:783]vbmap diag output:
Started as:
  /opt/couchbase/bin/vbmap --diag /opt/couchbase/var/lib/couchbase/tmp/vbmap_diag-576460752303422526_160 --output-format json --num-vbuckets 1024 --num-nodes 1 --num-slaves 10 --num-replicas 0 --relax-all --greedy --current-map /opt/couchbase/var/lib/couchbase/tmp/prev-vbmap-576460752303422494_160.json
Using 1662561148084086165 as a seed
Tags are not specified. Assuming every node on a separate tag.
Finalized parameters
  Number of nodes: 1
  Number of slaves: 0
  Number of vbuckets: 1024
  Number of replicas: 0
  Tags assignments:
    0 -> 0
Succesfully read current vbucket map:
[
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1]
]
Prev R:
RI (greedy):
  N |    |  0 |
----|---------|
    |  T |  0 |
----|---------|
  0 |  0 |  0 | 0
____|_________|
    |    |  0 |
R (greedy):
  N |    |  0 |
----|---------|
    |  T |  0 |
----|---------|
  0 |  0 |  0 | 0
____|_________|
    |    |  0 |
Time spent for vbmap generation (greedy) - 494.092Âµs
Per-node Vb Stats (greedy):
   Active Vbs:   map[0:1024]
   Replica Vbs:  map[0:0]
Move stats:
    Active takeovers - 1024
    Total new replicas - 1024

[ns_server:debug,2022-09-07T14:32:28.321Z,ns_1@127.0.0.1:<0.10377.0>:mb_map:do_invoke_vbmap_body:801]Score before simple minimization: {1024,1024}
[ns_server:debug,2022-09-07T14:32:28.322Z,ns_1@127.0.0.1:<0.10377.0>:mb_map:do_invoke_vbmap_body:807]Score after simple minimization: {1024,1024}
[ns_server:debug,2022-09-07T14:32:28.322Z,ns_1@127.0.0.1:<0.10377.0>:mb_map:do_invoke_vbmap_body:816]Map better after simple minimization;
                                       using it
[ns_server:debug,2022-09-07T14:32:28.324Z,ns_1@127.0.0.1:<0.10379.0>:mb_map:do_invoke_vbmap_body:748]Node Id Map: [{'ns_1@127.0.0.1',0}]
[ns_server:debug,2022-09-07T14:32:28.329Z,ns_1@127.0.0.1:<0.10379.0>:mb_map:do_invoke_vbmap_body:759]Wrote vbmap to "/opt/couchbase/var/lib/couchbase/tmp/prev-vbmap-576460752303422815_160.json"
[ns_server:debug,2022-09-07T14:32:28.591Z,ns_1@127.0.0.1:<0.10379.0>:mb_map:do_invoke_vbmap_body:783]vbmap diag output:
Started as:
  /opt/couchbase/bin/vbmap --diag /opt/couchbase/var/lib/couchbase/tmp/vbmap_diag-576460752303422462_160 --output-format json --num-vbuckets 1024 --num-nodes 1 --num-slaves 10 --num-replicas 0 --relax-all --greedy --current-map /opt/couchbase/var/lib/couchbase/tmp/prev-vbmap-576460752303422815_160.json
Using 1662561148331414837 as a seed
Tags are not specified. Assuming every node on a separate tag.
Finalized parameters
  Number of nodes: 1
  Number of slaves: 0
  Number of vbuckets: 1024
  Number of replicas: 0
  Tags assignments:
    0 -> 0
Succesfully read current vbucket map:
[
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1],
  [-1]
]
Prev R:
RI (greedy):
  N |    |  0 |
----|---------|
    |  T |  0 |
----|---------|
  0 |  0 |  0 | 0
____|_________|
    |    |  0 |
R (greedy):
  N |    |  0 |
----|---------|
    |  T |  0 |
----|---------|
  0 |  0 |  0 | 0
____|_________|
    |    |  0 |
Time spent for vbmap generation (greedy) - 694.419Âµs
Per-node Vb Stats (greedy):
   Active Vbs:   map[0:1024]
   Replica Vbs:  map[0:0]
Move stats:
    Active takeovers - 1024
    Total new replicas - 1024

[ns_server:debug,2022-09-07T14:32:28.592Z,ns_1@127.0.0.1:<0.10379.0>:mb_map:do_invoke_vbmap_body:801]Score before simple minimization: {1024,1024}
[ns_server:debug,2022-09-07T14:32:28.593Z,ns_1@127.0.0.1:<0.10379.0>:mb_map:do_invoke_vbmap_body:807]Score after simple minimization: {1024,1024}
[ns_server:debug,2022-09-07T14:32:28.593Z,ns_1@127.0.0.1:<0.10379.0>:mb_map:do_invoke_vbmap_body:816]Map better after simple minimization;
                                       using it
[ns_server:debug,2022-09-07T14:32:28.596Z,ns_1@127.0.0.1:<0.10157.0>:mb_map:generate_map_new:335]Scores for generated maps:
[{1024,1024},
 {1024,1024},
 {1024,1024},
 {1024,1024},
 {1024,1024},
 {1024,1024},
 {1024,1024},
 {1024,1024},
 {1024,1024}]
[ns_server:debug,2022-09-07T14:32:28.597Z,ns_1@127.0.0.1:<0.10157.0>:mb_map:generate_map_new:339]Considering 1 maps:
[{1024,1024}]
[ns_server:debug,2022-09-07T14:32:28.597Z,ns_1@127.0.0.1:<0.10157.0>:mb_map:generate_map_new:343]Best map score: {1024,1024} (true)
[ns_server:debug,2022-09-07T14:32:28.598Z,ns_1@127.0.0.1:ns_config_rep<0.444.0>:ns_config_rep:do_push_keys:383]Replicating some config keys ([vbucket_map_history,
                               {local_changes_count,
                                   <<"1678dfae96c38e07dff43c49b9f6967b">>}]..)
[ns_server:debug,2022-09-07T14:32:28.600Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{local_changes_count,<<"1678dfae96c38e07dff43c49b9f6967b">>} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{45,63829780348}}]}]
[ns_server:debug,2022-09-07T14:32:28.605Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
vbucket_map_history ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780348}}]},
 {[['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   ['ns_1@127.0.0.1'],
   [...]|...],
  [{replication_topology,star},
   {tags,undefined},
   {use_vbmap_greedy_optimization,true},
   {max_slaves,10}]}]
[ns_server:debug,2022-09-07T14:32:28.628Z,ns_1@127.0.0.1:chronicle_kv_log<0.404.0>:chronicle_kv_log:log:59]update (key: {bucket,"todo",props}, rev: {<<"1933459bb8a062c61b619e931c483c80">>,
                                          22})
[{map,[]},
 {fastForwardMap,[]},
 {num_replicas,0},
 {replica_index,false},
 {ram_quota,3920625664},
 {durability_min_level,none},
 {autocompaction,false},
 {purge_interval,undefined},
 {flush_enabled,false},
 {num_threads,3},
 {eviction_policy,value_only},
 {conflict_resolution_type,seqno},
 {storage_mode,couchstore},
 {max_ttl,0},
 {compression_mode,passive},
 {type,membase},
 {num_vbuckets,1024},
 {replication_topology,star},
 {repl_type,dcp},
 {servers,['ns_1@127.0.0.1']},
 {map_opts_hash,42591107}]
[ns_server:debug,2022-09-07T14:32:28.628Z,ns_1@127.0.0.1:roles_cache<0.365.0>:active_cache:clean:181]Clearing the roles_cache cache
[ns_server:debug,2022-09-07T14:32:28.660Z,ns_1@127.0.0.1:<0.10157.0>:ns_janitor:config_sync:257]Going to push config to/from nodes:
['ns_1@127.0.0.1']
[ns_server:debug,2022-09-07T14:32:28.661Z,ns_1@127.0.0.1:roles_cache<0.365.0>:active_cache:clean:181]Clearing the roles_cache cache
[ns_server:debug,2022-09-07T14:32:28.661Z,ns_1@127.0.0.1:chronicle_kv_log<0.404.0>:chronicle_kv_log:log:59]update (key: {bucket,"todo",props}, rev: {<<"1933459bb8a062c61b619e931c483c80">>,
                                          23})
[{map,[{0,[],['ns_1@127.0.0.1']},
       {1,[],['ns_1@127.0.0.1']},
       {2,[],['ns_1@127.0.0.1']},
       {3,[],['ns_1@127.0.0.1']},
       {4,[],['ns_1@127.0.0.1']},
       {5,[],['ns_1@127.0.0.1']},
       {6,[],['ns_1@127.0.0.1']},
       {7,[],['ns_1@127.0.0.1']},
       {8,[],['ns_1@127.0.0.1']},
       {9,[],['ns_1@127.0.0.1']},
       {10,[],['ns_1@127.0.0.1']},
       {11,[],['ns_1@127.0.0.1']},
       {12,[],['ns_1@127.0.0.1']},
       {13,[],['ns_1@127.0.0.1']},
       {14,[],['ns_1@127.0.0.1']},
       {15,[],['ns_1@127.0.0.1']},
       {16,[],['ns_1@127.0.0.1']},
       {17,[],['ns_1@127.0.0.1']},
       {18,[],['ns_1@127.0.0.1']},
       {19,[],['ns_1@127.0.0.1']},
       {20,[],['ns_1@127.0.0.1']},
       {21,[],['ns_1@127.0.0.1']},
       {22,[],['ns_1@127.0.0.1']},
       {23,[],['ns_1@127.0.0.1']},
       {24,[],['ns_1@127.0.0.1']},
       {25,[],['ns_1@127.0.0.1']},
       {26,[],['ns_1@127.0.0.1']},
       {27,[],['ns_1@127.0.0.1']},
       {28,[],['ns_1@127.0.0.1']},
       {29,[],['ns_1@127.0.0.1']},
       {30,[],['ns_1@127.0.0.1']},
       {31,[],['ns_1@127.0.0.1']},
       {32,[],['ns_1@127.0.0.1']},
       {33,[],['ns_1@127.0.0.1']},
       {34,[],['ns_1@127.0.0.1']},
       {35,[],['ns_1@127.0.0.1']},
       {36,[],['ns_1@127.0.0.1']},
       {37,[],['ns_1@127.0.0.1']},
       {38,[],['ns_1@127.0.0.1']},
       {39,[],['ns_1@127.0.0.1']},
       {40,[],['ns_1@127.0.0.1']},
       {41,[],['ns_1@127.0.0.1']},
       {42,[],['ns_1@127.0.0.1']},
       {43,[],['ns_1@127.0.0.1']},
       {44,[],['ns_1@127.0.0.1']},
       {45,[],['ns_1@127.0.0.1']},
       {46,[],['ns_1@127.0.0.1']},
       {47,[],['ns_1@127.0.0.1']},
       {48,[],['ns_1@127.0.0.1']},
       {49,[],['ns_1@127.0.0.1']},
       {50,[],['ns_1@127.0.0.1']},
       {51,[],['ns_1@127.0.0.1']},
       {52,[],['ns_1@127.0.0.1']},
       {53,[],['ns_1@127.0.0.1']},
       {54,[],['ns_1@127.0.0.1']},
       {55,[],['ns_1@127.0.0.1']},
       {56,[],['ns_1@127.0.0.1']},
       {57,[],['ns_1@127.0.0.1']},
       {58,[],['ns_1@127.0.0.1']},
       {59,[],['ns_1@127.0.0.1']},
       {60,[],['ns_1@127.0.0.1']},
       {61,[],['ns_1@127.0.0.1']},
       {62,[],['ns_1@127.0.0.1']},
       {63,[],['ns_1@127.0.0.1']},
       {64,[],['ns_1@127.0.0.1']},
       {65,[],['ns_1@127.0.0.1']},
       {66,[],['ns_1@127.0.0.1']},
       {67,[],['ns_1@127.0.0.1']},
       {68,[],['ns_1@127.0.0.1']},
       {69,[],['ns_1@127.0.0.1']},
       {70,[],['ns_1@127.0.0.1']},
       {71,[],['ns_1@127.0.0.1']},
       {72,[],['ns_1@127.0.0.1']},
       {73,[],['ns_1@127.0.0.1']},
       {74,[],['ns_1@127.0.0.1']},
       {75,[],['ns_1@127.0.0.1']},
       {76,[],['ns_1@127.0.0.1']},
       {77,[],['ns_1@127.0.0.1']},
       {78,[],['ns_1@127.0.0.1']},
       {79,[],['ns_1@127.0.0.1']},
       {80,[],['ns_1@127.0.0.1']},
       {81,[],['ns_1@127.0.0.1']},
       {82,[],['ns_1@127.0.0.1']},
       {83,[],['ns_1@127.0.0.1']},
       {84,[],['ns_1@127.0.0.1']},
       {85,[],['ns_1@127.0.0.1']},
       {86,[],['ns_1@127.0.0.1']},
       {87,[],['ns_1@127.0.0.1']},
       {88,[],['ns_1@127.0.0.1']},
       {89,[],['ns_1@127.0.0.1']},
       {90,[],['ns_1@127.0.0.1']},
       {91,[],['ns_1@127.0.0.1']},
       {92,[],[...]},
       {93,[],...},
       {94,...},
       {...}|...]},
 {fastForwardMap,[]},
 {num_replicas,0},
 {replica_index,false},
 {ram_quota,3920625664},
 {durability_min_level,none},
 {autocompaction,false},
 {purge_interval,undefined},
 {flush_enabled,false},
 {num_threads,3},
 {eviction_policy,value_only},
 {conflict_resolution_type,seqno},
 {storage_mode,couchstore},
 {max_ttl,0},
 {compression_mode,passive},
 {type,membase},
 {num_vbuckets,1024},
 {replication_topology,star},
 {repl_type,dcp},
 {servers,['ns_1@127.0.0.1']},
 {map_opts_hash,42591107}]
[ns_server:debug,2022-09-07T14:32:28.663Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:maybe_config_sync:240]Found states mismatch in bucket "todo":
[{0,['ns_1@127.0.0.1'],[]},
 {1,['ns_1@127.0.0.1'],[]},
 {2,['ns_1@127.0.0.1'],[]},
 {3,['ns_1@127.0.0.1'],[]},
 {4,['ns_1@127.0.0.1'],[]},
 {5,['ns_1@127.0.0.1'],[]},
 {6,['ns_1@127.0.0.1'],[]},
 {7,['ns_1@127.0.0.1'],[]},
 {8,['ns_1@127.0.0.1'],[]},
 {9,['ns_1@127.0.0.1'],[]},
 {10,['ns_1@127.0.0.1'],[]},
 {11,['ns_1@127.0.0.1'],[]},
 {12,['ns_1@127.0.0.1'],[]},
 {13,['ns_1@127.0.0.1'],[]},
 {14,['ns_1@127.0.0.1'],[]},
 {15,['ns_1@127.0.0.1'],[]},
 {16,['ns_1@127.0.0.1'],[]},
 {17,['ns_1@127.0.0.1'],[]},
 {18,['ns_1@127.0.0.1'],[]},
 {19,['ns_1@127.0.0.1'],[]},
 {20,['ns_1@127.0.0.1'],[]},
 {21,['ns_1@127.0.0.1'],[]},
 {22,['ns_1@127.0.0.1'],[]},
 {23,['ns_1@127.0.0.1'],[]},
 {24,['ns_1@127.0.0.1'],[]},
 {25,['ns_1@127.0.0.1'],[]},
 {26,['ns_1@127.0.0.1'],[]},
 {27,['ns_1@127.0.0.1'],[]},
 {28,['ns_1@127.0.0.1'],[]},
 {29,['ns_1@127.0.0.1'],[]},
 {30,['ns_1@127.0.0.1'],[]},
 {31,['ns_1@127.0.0.1'],[]},
 {32,['ns_1@127.0.0.1'],[]},
 {33,['ns_1@127.0.0.1'],[]},
 {34,['ns_1@127.0.0.1'],[]},
 {35,['ns_1@127.0.0.1'],[]},
 {36,['ns_1@127.0.0.1'],[]},
 {37,['ns_1@127.0.0.1'],[]},
 {38,['ns_1@127.0.0.1'],[]},
 {39,['ns_1@127.0.0.1'],[]},
 {40,['ns_1@127.0.0.1'],[]},
 {41,['ns_1@127.0.0.1'],[]},
 {42,['ns_1@127.0.0.1'],[]},
 {43,['ns_1@127.0.0.1'],[]},
 {44,['ns_1@127.0.0.1'],[]},
 {45,['ns_1@127.0.0.1'],[]},
 {46,['ns_1@127.0.0.1'],[]},
 {47,['ns_1@127.0.0.1'],[]},
 {48,['ns_1@127.0.0.1'],[]},
 {49,['ns_1@127.0.0.1'],[]},
 {50,['ns_1@127.0.0.1'],[]},
 {51,['ns_1@127.0.0.1'],[]},
 {52,['ns_1@127.0.0.1'],[]},
 {53,['ns_1@127.0.0.1'],[]},
 {54,['ns_1@127.0.0.1'],[]},
 {55,['ns_1@127.0.0.1'],[]},
 {56,['ns_1@127.0.0.1'],[]},
 {57,['ns_1@127.0.0.1'],[]},
 {58,['ns_1@127.0.0.1'],[]},
 {59,['ns_1@127.0.0.1'],[]},
 {60,['ns_1@127.0.0.1'],[]},
 {61,['ns_1@127.0.0.1'],[]},
 {62,['ns_1@127.0.0.1'],[]},
 {63,['ns_1@127.0.0.1'],[]},
 {64,['ns_1@127.0.0.1'],[]},
 {65,['ns_1@127.0.0.1'],[]},
 {66,['ns_1@127.0.0.1'],[]},
 {67,['ns_1@127.0.0.1'],[]},
 {68,['ns_1@127.0.0.1'],[]},
 {69,['ns_1@127.0.0.1'],[]},
 {70,['ns_1@127.0.0.1'],[]},
 {71,['ns_1@127.0.0.1'],[]},
 {72,['ns_1@127.0.0.1'],[]},
 {73,['ns_1@127.0.0.1'],[]},
 {74,['ns_1@127.0.0.1'],[]},
 {75,['ns_1@127.0.0.1'],[]},
 {76,['ns_1@127.0.0.1'],[]},
 {77,['ns_1@127.0.0.1'],[]},
 {78,['ns_1@127.0.0.1'],[]},
 {79,['ns_1@127.0.0.1'],[]},
 {80,['ns_1@127.0.0.1'],[]},
 {81,['ns_1@127.0.0.1'],[]},
 {82,['ns_1@127.0.0.1'],[]},
 {83,['ns_1@127.0.0.1'],[]},
 {84,['ns_1@127.0.0.1'],[]},
 {85,['ns_1@127.0.0.1'],[]},
 {86,['ns_1@127.0.0.1'],[]},
 {87,['ns_1@127.0.0.1'],[]},
 {88,['ns_1@127.0.0.1'],[]},
 {89,['ns_1@127.0.0.1'],[]},
 {90,['ns_1@127.0.0.1'],[]},
 {91,['ns_1@127.0.0.1'],[]},
 {92,['ns_1@127.0.0.1'],[]},
 {93,['ns_1@127.0.0.1'],[]},
 {94,['ns_1@127.0.0.1'],[]},
 {95,['ns_1@127.0.0.1'],[]},
 {96,['ns_1@127.0.0.1'],[]},
 {97,['ns_1@127.0.0.1'],[]},
 {98,['ns_1@127.0.0.1'],[]},
 {99,['ns_1@127.0.0.1'],[]},
 {100,['ns_1@127.0.0.1'],[]},
 {101,['ns_1@127.0.0.1'],[]},
 {102,['ns_1@127.0.0.1'],[]},
 {103,['ns_1@127.0.0.1'],[]},
 {104,['ns_1@127.0.0.1'],[]},
 {105,['ns_1@127.0.0.1'],[]},
 {106,['ns_1@127.0.0.1'],[]},
 {107,['ns_1@127.0.0.1'],[]},
 {108,['ns_1@127.0.0.1'],[]},
 {109,['ns_1@127.0.0.1'],[]},
 {110,['ns_1@127.0.0.1'],[]},
 {111,['ns_1@127.0.0.1'],[]},
 {112,['ns_1@127.0.0.1'],[]},
 {113,['ns_1@127.0.0.1'],[]},
 {114,['ns_1@127.0.0.1'],[]},
 {115,['ns_1@127.0.0.1'],[]},
 {116,['ns_1@127.0.0.1'],[]},
 {117,['ns_1@127.0.0.1'],[]},
 {118,['ns_1@127.0.0.1'],[]},
 {119,['ns_1@127.0.0.1'],[]},
 {120,['ns_1@127.0.0.1'],[]},
 {121,['ns_1@127.0.0.1'],[]},
 {122,['ns_1@127.0.0.1'],[]},
 {123,['ns_1@127.0.0.1'],[]},
 {124,['ns_1@127.0.0.1'],[]},
 {125,['ns_1@127.0.0.1'],[]},
 {126,['ns_1@127.0.0.1'],[]},
 {127,['ns_1@127.0.0.1'],[]},
 {128,['ns_1@127.0.0.1'],[]},
 {129,['ns_1@127.0.0.1'],[]},
 {130,['ns_1@127.0.0.1'],[]},
 {131,['ns_1@127.0.0.1'],[]},
 {132,['ns_1@127.0.0.1'],[]},
 {133,['ns_1@127.0.0.1'],[]},
 {134,['ns_1@127.0.0.1'],[]},
 {135,['ns_1@127.0.0.1'],[]},
 {136,['ns_1@127.0.0.1'],[]},
 {137,['ns_1@127.0.0.1'],[]},
 {138,['ns_1@127.0.0.1'],[]},
 {139,['ns_1@127.0.0.1'],[]},
 {140,['ns_1@127.0.0.1'],[]},
 {141,['ns_1@127.0.0.1'],[]},
 {142,['ns_1@127.0.0.1'],[]},
 {143,['ns_1@127.0.0.1'],[]},
 {144,['ns_1@127.0.0.1'],[]},
 {145,['ns_1@127.0.0.1'],[]},
 {146,['ns_1@127.0.0.1'],[]},
 {147,['ns_1@127.0.0.1'],[]},
 {148,['ns_1@127.0.0.1'],[]},
 {149,['ns_1@127.0.0.1'],[]},
 {150,['ns_1@127.0.0.1'],[]},
 {151,['ns_1@127.0.0.1'],[]},
 {152,['ns_1@127.0.0.1'],[]},
 {153,['ns_1@127.0.0.1'],[]},
 {154,['ns_1@127.0.0.1'],[]},
 {155,['ns_1@127.0.0.1'],[]},
 {156,['ns_1@127.0.0.1'],[]},
 {157,['ns_1@127.0.0.1'],[]},
 {158,['ns_1@127.0.0.1'],[]},
 {159,['ns_1@127.0.0.1'],[]},
 {160,['ns_1@127.0.0.1'],[]},
 {161,['ns_1@127.0.0.1'],[]},
 {162,['ns_1@127.0.0.1'],[]},
 {163,['ns_1@127.0.0.1'],[]},
 {164,['ns_1@127.0.0.1'],[]},
 {165,['ns_1@127.0.0.1'],[]},
 {166,['ns_1@127.0.0.1'],[]},
 {167,['ns_1@127.0.0.1'],[]},
 {168,['ns_1@127.0.0.1'],[]},
 {169,['ns_1@127.0.0.1'],[]},
 {170,['ns_1@127.0.0.1'],[]},
 {171,['ns_1@127.0.0.1'],[]},
 {172,['ns_1@127.0.0.1'],[]},
 {173,['ns_1@127.0.0.1'],[]},
 {174,['ns_1@127.0.0.1'],[]},
 {175,['ns_1@127.0.0.1'],[]},
 {176,['ns_1@127.0.0.1'],[]},
 {177,['ns_1@127.0.0.1'],[]},
 {178,['ns_1@127.0.0.1'],[]},
 {179,['ns_1@127.0.0.1'],[]},
 {180,['ns_1@127.0.0.1'],[]},
 {181,['ns_1@127.0.0.1'],[]},
 {182,['ns_1@127.0.0.1'],[]},
 {183,['ns_1@127.0.0.1'],[]},
 {184,['ns_1@127.0.0.1'],[]},
 {185,['ns_1@127.0.0.1'],[]},
 {186,['ns_1@127.0.0.1'],[]},
 {187,['ns_1@127.0.0.1'],[]},
 {188,['ns_1@127.0.0.1'],[]},
 {189,['ns_1@127.0.0.1'],[]},
 {190,['ns_1@127.0.0.1'],[]},
 {191,['ns_1@127.0.0.1'],[]},
 {192,['ns_1@127.0.0.1'],[]},
 {193,['ns_1@127.0.0.1'],[]},
 {194,['ns_1@127.0.0.1'],[]},
 {195,['ns_1@127.0.0.1'],[]},
 {196,['ns_1@127.0.0.1'],[]},
 {197,['ns_1@127.0.0.1'],[]},
 {198,['ns_1@127.0.0.1'],[]},
 {199,['ns_1@127.0.0.1'],[]},
 {200,['ns_1@127.0.0.1'],[]},
 {201,['ns_1@127.0.0.1'],[]},
 {202,['ns_1@127.0.0.1'],[]},
 {203,['ns_1@127.0.0.1'],[]},
 {204,['ns_1@127.0.0.1'],[]},
 {205,['ns_1@127.0.0.1'],[]},
 {206,['ns_1@127.0.0.1'],[]},
 {207,['ns_1@127.0.0.1'],[]},
 {208,['ns_1@127.0.0.1'],[]},
 {209,['ns_1@127.0.0.1'],[]},
 {210,['ns_1@127.0.0.1'],[]},
 {211,['ns_1@127.0.0.1'],[]},
 {212,['ns_1@127.0.0.1'],[]},
 {213,['ns_1@127.0.0.1'],[]},
 {214,['ns_1@127.0.0.1'],[]},
 {215,['ns_1@127.0.0.1'],[]},
 {216,['ns_1@127.0.0.1'],[]},
 {217,['ns_1@127.0.0.1'],[]},
 {218,['ns_1@127.0.0.1'],[]},
 {219,['ns_1@127.0.0.1'],[]},
 {220,['ns_1@127.0.0.1'],[]},
 {221,['ns_1@127.0.0.1'],[]},
 {222,['ns_1@127.0.0.1'],[]},
 {223,['ns_1@127.0.0.1'],[]},
 {224,['ns_1@127.0.0.1'],[]},
 {225,['ns_1@127.0.0.1'],[]},
 {226,['ns_1@127.0.0.1'],[]},
 {227,['ns_1@127.0.0.1'],[]},
 {228,['ns_1@127.0.0.1'],[]},
 {229,['ns_1@127.0.0.1'],[]},
 {230,['ns_1@127.0.0.1'],[]},
 {231,['ns_1@127.0.0.1'],[]},
 {232,['ns_1@127.0.0.1'],[]},
 {233,['ns_1@127.0.0.1'],[]},
 {234,['ns_1@127.0.0.1'],[]},
 {235,['ns_1@127.0.0.1'],[]},
 {236,['ns_1@127.0.0.1'],[]},
 {237,['ns_1@127.0.0.1'],[]},
 {238,['ns_1@127.0.0.1'],[]},
 {239,['ns_1@127.0.0.1'],[]},
 {240,['ns_1@127.0.0.1'],[]},
 {241,['ns_1@127.0.0.1'],[]},
 {242,['ns_1@127.0.0.1'],[]},
 {243,['ns_1@127.0.0.1'],[]},
 {244,['ns_1@127.0.0.1'],[]},
 {245,['ns_1@127.0.0.1'],[]},
 {246,['ns_1@127.0.0.1'],[]},
 {247,['ns_1@127.0.0.1'],[]},
 {248,['ns_1@127.0.0.1'],[]},
 {249,['ns_1@127.0.0.1'],[]},
 {250,['ns_1@127.0.0.1'],[]},
 {251,['ns_1@127.0.0.1'],[]},
 {252,['ns_1@127.0.0.1'],[]},
 {253,['ns_1@127.0.0.1'],[]},
 {254,['ns_1@127.0.0.1'],[]},
 {255,['ns_1@127.0.0.1'],[]},
 {256,['ns_1@127.0.0.1'],[]},
 {257,['ns_1@127.0.0.1'],[]},
 {258,['ns_1@127.0.0.1'],[]},
 {259,['ns_1@127.0.0.1'],[]},
 {260,['ns_1@127.0.0.1'],[]},
 {261,['ns_1@127.0.0.1'],[]},
 {262,['ns_1@127.0.0.1'],[]},
 {263,['ns_1@127.0.0.1'],[]},
 {264,['ns_1@127.0.0.1'],[]},
 {265,['ns_1@127.0.0.1'],[]},
 {266,['ns_1@127.0.0.1'],[]},
 {267,['ns_1@127.0.0.1'],[]},
 {268,['ns_1@127.0.0.1'],[]},
 {269,['ns_1@127.0.0.1'],[]},
 {270,['ns_1@127.0.0.1'],[]},
 {271,['ns_1@127.0.0.1'],[]},
 {272,['ns_1@127.0.0.1'],[]},
 {273,['ns_1@127.0.0.1'],[]},
 {274,['ns_1@127.0.0.1'],[]},
 {275,['ns_1@127.0.0.1'],[]},
 {276,['ns_1@127.0.0.1'],[]},
 {277,['ns_1@127.0.0.1'],[]},
 {278,['ns_1@127.0.0.1'],[]},
 {279,['ns_1@127.0.0.1'],[]},
 {280,['ns_1@127.0.0.1'],[]},
 {281,['ns_1@127.0.0.1'],[]},
 {282,['ns_1@127.0.0.1'],[]},
 {283,['ns_1@127.0.0.1'],[]},
 {284,['ns_1@127.0.0.1'],[]},
 {285,['ns_1@127.0.0.1'],[]},
 {286,['ns_1@127.0.0.1'],[]},
 {287,['ns_1@127.0.0.1'],[]},
 {288,['ns_1@127.0.0.1'],[]},
 {289,['ns_1@127.0.0.1'],[]},
 {290,['ns_1@127.0.0.1'],[]},
 {291,['ns_1@127.0.0.1'],[]},
 {292,['ns_1@127.0.0.1'],[]},
 {293,['ns_1@127.0.0.1'],[]},
 {294,['ns_1@127.0.0.1'],[]},
 {295,['ns_1@127.0.0.1'],[]},
 {296,['ns_1@127.0.0.1'],[]},
 {297,['ns_1@127.0.0.1'],[]},
 {298,['ns_1@127.0.0.1'],[]},
 {299,['ns_1@127.0.0.1'],[]},
 {300,['ns_1@127.0.0.1'],[]},
 {301,['ns_1@127.0.0.1'],[]},
 {302,['ns_1@127.0.0.1'],[]},
 {303,['ns_1@127.0.0.1'],[]},
 {304,['ns_1@127.0.0.1'],[]},
 {305,['ns_1@127.0.0.1'],[]},
 {306,['ns_1@127.0.0.1'],[]},
 {307,['ns_1@127.0.0.1'],[]},
 {308,['ns_1@127.0.0.1'],[]},
 {309,['ns_1@127.0.0.1'],[]},
 {310,['ns_1@127.0.0.1'],[]},
 {311,['ns_1@127.0.0.1'],[]},
 {312,['ns_1@127.0.0.1'],[]},
 {313,['ns_1@127.0.0.1'],[]},
 {314,['ns_1@127.0.0.1'],[]},
 {315,['ns_1@127.0.0.1'],[]},
 {316,['ns_1@127.0.0.1'],[]},
 {317,['ns_1@127.0.0.1'],[]},
 {318,['ns_1@127.0.0.1'],[]},
 {319,['ns_1@127.0.0.1'],[]},
 {320,['ns_1@127.0.0.1'],[]},
 {321,['ns_1@127.0.0.1'],[]},
 {322,['ns_1@127.0.0.1'],[]},
 {323,['ns_1@127.0.0.1'],[]},
 {324,['ns_1@127.0.0.1'],[]},
 {325,['ns_1@127.0.0.1'],[]},
 {326,['ns_1@127.0.0.1'],[]},
 {327,['ns_1@127.0.0.1'],[]},
 {328,['ns_1@127.0.0.1'],[]},
 {329,['ns_1@127.0.0.1'],[]},
 {330,['ns_1@127.0.0.1'],[]},
 {331,['ns_1@127.0.0.1'],[]},
 {332,['ns_1@127.0.0.1'],[]},
 {333,['ns_1@127.0.0.1'],[]},
 {334,['ns_1@127.0.0.1'],[]},
 {335,['ns_1@127.0.0.1'],[]},
 {336,['ns_1@127.0.0.1'],[]},
 {337,['ns_1@127.0.0.1'],[]},
 {338,['ns_1@127.0.0.1'],[]},
 {339,['ns_1@127.0.0.1'],[]},
 {340,['ns_1@127.0.0.1'],[]},
 {341,['ns_1@127.0.0.1'],[]},
 {342,['ns_1@127.0.0.1'],[]},
 {343,['ns_1@127.0.0.1'],[]},
 {344,['ns_1@127.0.0.1'],[]},
 {345,['ns_1@127.0.0.1'],[]},
 {346,['ns_1@127.0.0.1'],[]},
 {347,['ns_1@127.0.0.1'],[]},
 {348,['ns_1@127.0.0.1'],[]},
 {349,['ns_1@127.0.0.1'],[]},
 {350,['ns_1@127.0.0.1'],[]},
 {351,['ns_1@127.0.0.1'],[]},
 {352,['ns_1@127.0.0.1'],[]},
 {353,['ns_1@127.0.0.1'],[]},
 {354,['ns_1@127.0.0.1'],[]},
 {355,['ns_1@127.0.0.1'],[]},
 {356,['ns_1@127.0.0.1'],[]},
 {357,['ns_1@127.0.0.1'],[]},
 {358,['ns_1@127.0.0.1'],[]},
 {359,['ns_1@127.0.0.1'],[]},
 {360,['ns_1@127.0.0.1'],[]},
 {361,['ns_1@127.0.0.1'],[]},
 {362,['ns_1@127.0.0.1'],[]},
 {363,['ns_1@127.0.0.1'],[]},
 {364,['ns_1@127.0.0.1'],[]},
 {365,['ns_1@127.0.0.1'],[]},
 {366,['ns_1@127.0.0.1'],[]},
 {367,['ns_1@127.0.0.1'],[]},
 {368,['ns_1@127.0.0.1'],[]},
 {369,['ns_1@127.0.0.1'],[]},
 {370,['ns_1@127.0.0.1'],[]},
 {371,['ns_1@127.0.0.1'],[]},
 {372,['ns_1@127.0.0.1'],[]},
 {373,['ns_1@127.0.0.1'],[]},
 {374,['ns_1@127.0.0.1'],[]},
 {375,['ns_1@127.0.0.1'],[]},
 {376,['ns_1@127.0.0.1'],[]},
 {377,['ns_1@127.0.0.1'],[]},
 {378,['ns_1@127.0.0.1'],[]},
 {379,['ns_1@127.0.0.1'],[]},
 {380,['ns_1@127.0.0.1'],[]},
 {381,['ns_1@127.0.0.1'],[]},
 {382,['ns_1@127.0.0.1'],[]},
 {383,['ns_1@127.0.0.1'],[]},
 {384,['ns_1@127.0.0.1'],[]},
 {385,['ns_1@127.0.0.1'],[]},
 {386,['ns_1@127.0.0.1'],[]},
 {387,['ns_1@127.0.0.1'],[]},
 {388,['ns_1@127.0.0.1'],[]},
 {389,['ns_1@127.0.0.1'],[]},
 {390,['ns_1@127.0.0.1'],[]},
 {391,['ns_1@127.0.0.1'],[]},
 {392,['ns_1@127.0.0.1'],[]},
 {393,['ns_1@127.0.0.1'],[]},
 {394,['ns_1@127.0.0.1'],[]},
 {395,['ns_1@127.0.0.1'],[]},
 {396,['ns_1@127.0.0.1'],[]},
 {397,['ns_1@127.0.0.1'],[]},
 {398,['ns_1@127.0.0.1'],[]},
 {399,['ns_1@127.0.0.1'],[]},
 {400,['ns_1@127.0.0.1'],[]},
 {401,['ns_1@127.0.0.1'],[]},
 {402,['ns_1@127.0.0.1'],[]},
 {403,['ns_1@127.0.0.1'],[]},
 {404,['ns_1@127.0.0.1'],[]},
 {405,['ns_1@127.0.0.1'],[]},
 {406,['ns_1@127.0.0.1'],[]},
 {407,['ns_1@127.0.0.1'],[]},
 {408,['ns_1@127.0.0.1'],[]},
 {409,['ns_1@127.0.0.1'],[]},
 {410,['ns_1@127.0.0.1'],[]},
 {411,['ns_1@127.0.0.1'],[]},
 {412,['ns_1@127.0.0.1'],[]},
 {413,['ns_1@127.0.0.1'],[]},
 {414,['ns_1@127.0.0.1'],[]},
 {415,['ns_1@127.0.0.1'],[]},
 {416,['ns_1@127.0.0.1'],[]},
 {417,['ns_1@127.0.0.1'],[]},
 {418,['ns_1@127.0.0.1'],[]},
 {419,['ns_1@127.0.0.1'],[]},
 {420,['ns_1@127.0.0.1'],[]},
 {421,['ns_1@127.0.0.1'],[]},
 {422,['ns_1@127.0.0.1'],[]},
 {423,['ns_1@127.0.0.1'],[]},
 {424,['ns_1@127.0.0.1'],[]},
 {425,['ns_1@127.0.0.1'],[]},
 {426,['ns_1@127.0.0.1'],[]},
 {427,['ns_1@127.0.0.1'],[]},
 {428,['ns_1@127.0.0.1'],[]},
 {429,['ns_1@127.0.0.1'],[]},
 {430,['ns_1@127.0.0.1'],[]},
 {431,['ns_1@127.0.0.1'],[]},
 {432,['ns_1@127.0.0.1'],[]},
 {433,['ns_1@127.0.0.1'],[]},
 {434,['ns_1@127.0.0.1'],[]},
 {435,['ns_1@127.0.0.1'],[]},
 {436,['ns_1@127.0.0.1'],[]},
 {437,['ns_1@127.0.0.1'],[]},
 {438,['ns_1@127.0.0.1'],[]},
 {439,['ns_1@127.0.0.1'],[]},
 {440,['ns_1@127.0.0.1'],[]},
 {441,['ns_1@127.0.0.1'],[]},
 {442,['ns_1@127.0.0.1'],[]},
 {443,['ns_1@127.0.0.1'],[]},
 {444,['ns_1@127.0.0.1'],[]},
 {445,['ns_1@127.0.0.1'],[]},
 {446,['ns_1@127.0.0.1'],[]},
 {447,['ns_1@127.0.0.1'],[]},
 {448,['ns_1@127.0.0.1'],[]},
 {449,['ns_1@127.0.0.1'],[]},
 {450,['ns_1@127.0.0.1'],[]},
 {451,['ns_1@127.0.0.1'],[]},
 {452,['ns_1@127.0.0.1'],[]},
 {453,['ns_1@127.0.0.1'],[]},
 {454,['ns_1@127.0.0.1'],[]},
 {455,['ns_1@127.0.0.1'],[]},
 {456,['ns_1@127.0.0.1'],[]},
 {457,['ns_1@127.0.0.1'],[]},
 {458,['ns_1@127.0.0.1'],[]},
 {459,['ns_1@127.0.0.1'],[]},
 {460,['ns_1@127.0.0.1'],[]},
 {461,['ns_1@127.0.0.1'],[]},
 {462,['ns_1@127.0.0.1'],[]},
 {463,['ns_1@127.0.0.1'],[]},
 {464,['ns_1@127.0.0.1'],[]},
 {465,['ns_1@127.0.0.1'],[]},
 {466,['ns_1@127.0.0.1'],[]},
 {467,['ns_1@127.0.0.1'],[]},
 {468,['ns_1@127.0.0.1'],[]},
 {469,['ns_1@127.0.0.1'],[]},
 {470,['ns_1@127.0.0.1'],[]},
 {471,['ns_1@127.0.0.1'],[]},
 {472,['ns_1@127.0.0.1'],[]},
 {473,['ns_1@127.0.0.1'],[]},
 {474,['ns_1@127.0.0.1'],[]},
 {475,['ns_1@127.0.0.1'],[]},
 {476,['ns_1@127.0.0.1'],[]},
 {477,['ns_1@127.0.0.1'],[]},
 {478,['ns_1@127.0.0.1'],[]},
 {479,['ns_1@127.0.0.1'],[]},
 {480,['ns_1@127.0.0.1'],[]},
 {481,['ns_1@127.0.0.1'],[]},
 {482,['ns_1@127.0.0.1'],[]},
 {483,['ns_1@127.0.0.1'],[]},
 {484,['ns_1@127.0.0.1'],[]},
 {485,['ns_1@127.0.0.1'],[]},
 {486,['ns_1@127.0.0.1'],[]},
 {487,['ns_1@127.0.0.1'],[]},
 {488,['ns_1@127.0.0.1'],[]},
 {489,['ns_1@127.0.0.1'],[]},
 {490,['ns_1@127.0.0.1'],[]},
 {491,['ns_1@127.0.0.1'],[]},
 {492,['ns_1@127.0.0.1'],[]},
 {493,['ns_1@127.0.0.1'],[]},
 {494,['ns_1@127.0.0.1'],[]},
 {495,['ns_1@127.0.0.1'],[]},
 {496,['ns_1@127.0.0.1'],[]},
 {497,['ns_1@127.0.0.1'],[]},
 {498,['ns_1@127.0.0.1'],[]},
 {499,['ns_1@127.0.0.1'],[]},
 {500,['ns_1@127.0.0.1'],[]},
 {501,['ns_1@127.0.0.1'],[]},
 {502,['ns_1@127.0.0.1'],[]},
 {503,['ns_1@127.0.0.1'],[]},
 {504,['ns_1@127.0.0.1'],[]},
 {505,['ns_1@127.0.0.1'],[]},
 {506,['ns_1@127.0.0.1'],[]},
 {507,['ns_1@127.0.0.1'],[]},
 {508,['ns_1@127.0.0.1'],[]},
 {509,['ns_1@127.0.0.1'],[]},
 {510,['ns_1@127.0.0.1'],[]},
 {511,['ns_1@127.0.0.1'],[]},
 {512,['ns_1@127.0.0.1'],[]},
 {513,['ns_1@127.0.0.1'],[]},
 {514,['ns_1@127.0.0.1'],[]},
 {515,['ns_1@127.0.0.1'],[]},
 {516,['ns_1@127.0.0.1'],[]},
 {517,['ns_1@127.0.0.1'],[]},
 {518,['ns_1@127.0.0.1'],[]},
 {519,['ns_1@127.0.0.1'],[]},
 {520,['ns_1@127.0.0.1'],[]},
 {521,['ns_1@127.0.0.1'],[]},
 {522,['ns_1@127.0.0.1'],[]},
 {523,['ns_1@127.0.0.1'],[]},
 {524,['ns_1@127.0.0.1'],[]},
 {525,['ns_1@127.0.0.1'],[]},
 {526,['ns_1@127.0.0.1'],[]},
 {527,['ns_1@127.0.0.1'],[]},
 {528,['ns_1@127.0.0.1'],[]},
 {529,['ns_1@127.0.0.1'],[]},
 {530,['ns_1@127.0.0.1'],[]},
 {531,['ns_1@127.0.0.1'],[]},
 {532,['ns_1@127.0.0.1'],[]},
 {533,['ns_1@127.0.0.1'],[]},
 {534,['ns_1@127.0.0.1'],[]},
 {535,['ns_1@127.0.0.1'],[]},
 {536,['ns_1@127.0.0.1'],[]},
 {537,['ns_1@127.0.0.1'],[]},
 {538,['ns_1@127.0.0.1'],[]},
 {539,['ns_1@127.0.0.1'],[]},
 {540,['ns_1@127.0.0.1'],[]},
 {541,['ns_1@127.0.0.1'],[]},
 {542,['ns_1@127.0.0.1'],[]},
 {543,['ns_1@127.0.0.1'],[]},
 {544,['ns_1@127.0.0.1'],[]},
 {545,['ns_1@127.0.0.1'],[]},
 {546,['ns_1@127.0.0.1'],[]},
 {547,['ns_1@127.0.0.1'],[]},
 {548,['ns_1@127.0.0.1'],[]},
 {549,['ns_1@127.0.0.1'],[]},
 {550,['ns_1@127.0.0.1'],[]},
 {551,['ns_1@127.0.0.1'],[]},
 {552,['ns_1@127.0.0.1'],[]},
 {553,['ns_1@127.0.0.1'],[]},
 {554,['ns_1@127.0.0.1'],[]},
 {555,['ns_1@127.0.0.1'],[]},
 {556,['ns_1@127.0.0.1'],[]},
 {557,['ns_1@127.0.0.1'],[]},
 {558,['ns_1@127.0.0.1'],[]},
 {559,['ns_1@127.0.0.1'],[]},
 {560,['ns_1@127.0.0.1'],[]},
 {561,['ns_1@127.0.0.1'],[]},
 {562,['ns_1@127.0.0.1'],[]},
 {563,['ns_1@127.0.0.1'],[]},
 {564,['ns_1@127.0.0.1'],[]},
 {565,['ns_1@127.0.0.1'],[]},
 {566,['ns_1@127.0.0.1'],[]},
 {567,['ns_1@127.0.0.1'],[]},
 {568,['ns_1@127.0.0.1'],[]},
 {569,['ns_1@127.0.0.1'],[]},
 {570,['ns_1@127.0.0.1'],[]},
 {571,['ns_1@127.0.0.1'],[]},
 {572,['ns_1@127.0.0.1'],[]},
 {573,['ns_1@127.0.0.1'],[]},
 {574,['ns_1@127.0.0.1'],[]},
 {575,['ns_1@127.0.0.1'],[]},
 {576,['ns_1@127.0.0.1'],[]},
 {577,['ns_1@127.0.0.1'],[]},
 {578,['ns_1@127.0.0.1'],[]},
 {579,['ns_1@127.0.0.1'],[]},
 {580,['ns_1@127.0.0.1'],[]},
 {581,['ns_1@127.0.0.1'],[]},
 {582,['ns_1@127.0.0.1'],[]},
 {583,['ns_1@127.0.0.1'],[]},
 {584,['ns_1@127.0.0.1'],[]},
 {585,['ns_1@127.0.0.1'],[]},
 {586,['ns_1@127.0.0.1'],[]},
 {587,['ns_1@127.0.0.1'],[]},
 {588,['ns_1@127.0.0.1'],[]},
 {589,['ns_1@127.0.0.1'],[]},
 {590,['ns_1@127.0.0.1'],[]},
 {591,['ns_1@127.0.0.1'],[]},
 {592,['ns_1@127.0.0.1'],[]},
 {593,['ns_1@127.0.0.1'],[]},
 {594,['ns_1@127.0.0.1'],[]},
 {595,['ns_1@127.0.0.1'],[]},
 {596,['ns_1@127.0.0.1'],[]},
 {597,['ns_1@127.0.0.1'],[]},
 {598,['ns_1@127.0.0.1'],[]},
 {599,['ns_1@127.0.0.1'],[]},
 {600,['ns_1@127.0.0.1'],[]},
 {601,['ns_1@127.0.0.1'],[]},
 {602,['ns_1@127.0.0.1'],[]},
 {603,['ns_1@127.0.0.1'],[]},
 {604,['ns_1@127.0.0.1'],[]},
 {605,['ns_1@127.0.0.1'],[]},
 {606,['ns_1@127.0.0.1'],[]},
 {607,['ns_1@127.0.0.1'],[]},
 {608,['ns_1@127.0.0.1'],[]},
 {609,['ns_1@127.0.0.1'],[]},
 {610,['ns_1@127.0.0.1'],[]},
 {611,['ns_1@127.0.0.1'],[]},
 {612,['ns_1@127.0.0.1'],[]},
 {613,['ns_1@127.0.0.1'],[]},
 {614,['ns_1@127.0.0.1'],[]},
 {615,['ns_1@127.0.0.1'],[]},
 {616,['ns_1@127.0.0.1'],[]},
 {617,['ns_1@127.0.0.1'],[]},
 {618,['ns_1@127.0.0.1'],[]},
 {619,['ns_1@127.0.0.1'],[]},
 {620,['ns_1@127.0.0.1'],[]},
 {621,['ns_1@127.0.0.1'],[]},
 {622,['ns_1@127.0.0.1'],[]},
 {623,['ns_1@127.0.0.1'],[]},
 {624,['ns_1@127.0.0.1'],[]},
 {625,['ns_1@127.0.0.1'],[]},
 {626,['ns_1@127.0.0.1'],[]},
 {627,['ns_1@127.0.0.1'],[]},
 {628,['ns_1@127.0.0.1'],[]},
 {629,['ns_1@127.0.0.1'],[]},
 {630,['ns_1@127.0.0.1'],[]},
 {631,['ns_1@127.0.0.1'],[]},
 {632,['ns_1@127.0.0.1'],[]},
 {633,['ns_1@127.0.0.1'],[]},
 {634,['ns_1@127.0.0.1'],[]},
 {635,['ns_1@127.0.0.1'],[]},
 {636,['ns_1@127.0.0.1'],[]},
 {637,['ns_1@127.0.0.1'],[]},
 {638,['ns_1@127.0.0.1'],[]},
 {639,['ns_1@127.0.0.1'],[]},
 {640,['ns_1@127.0.0.1'],[]},
 {641,['ns_1@127.0.0.1'],[]},
 {642,['ns_1@127.0.0.1'],[]},
 {643,['ns_1@127.0.0.1'],[]},
 {644,['ns_1@127.0.0.1'],[]},
 {645,['ns_1@127.0.0.1'],[]},
 {646,['ns_1@127.0.0.1'],[]},
 {647,['ns_1@127.0.0.1'],[]},
 {648,['ns_1@127.0.0.1'],[]},
 {649,['ns_1@127.0.0.1'],[]},
 {650,['ns_1@127.0.0.1'],[]},
 {651,['ns_1@127.0.0.1'],[]},
 {652,['ns_1@127.0.0.1'],[]},
 {653,['ns_1@127.0.0.1'],[]},
 {654,['ns_1@127.0.0.1'],[]},
 {655,['ns_1@127.0.0.1'],[]},
 {656,['ns_1@127.0.0.1'],[]},
 {657,['ns_1@127.0.0.1'],[]},
 {658,['ns_1@127.0.0.1'],[]},
 {659,['ns_1@127.0.0.1'],[]},
 {660,['ns_1@127.0.0.1'],[]},
 {661,['ns_1@127.0.0.1'],[]},
 {662,['ns_1@127.0.0.1'],[]},
 {663,['ns_1@127.0.0.1'],[]},
 {664,['ns_1@127.0.0.1'],[]},
 {665,['ns_1@127.0.0.1'],[]},
 {666,['ns_1@127.0.0.1'],[]},
 {667,['ns_1@127.0.0.1'],[]},
 {668,['ns_1@127.0.0.1'],[]},
 {669,['ns_1@127.0.0.1'],[]},
 {670,['ns_1@127.0.0.1'],[]},
 {671,['ns_1@127.0.0.1'],[]},
 {672,['ns_1@127.0.0.1'],[]},
 {673,['ns_1@127.0.0.1'],[]},
 {674,['ns_1@127.0.0.1'],[]},
 {675,['ns_1@127.0.0.1'],[]},
 {676,['ns_1@127.0.0.1'],[]},
 {677,['ns_1@127.0.0.1'],[]},
 {678,['ns_1@127.0.0.1'],[]},
 {679,['ns_1@127.0.0.1'],[]},
 {680,['ns_1@127.0.0.1'],[]},
 {681,['ns_1@127.0.0.1'],[]},
 {682,['ns_1@127.0.0.1'],[]},
 {683,['ns_1@127.0.0.1'],[]},
 {684,['ns_1@127.0.0.1'],[]},
 {685,['ns_1@127.0.0.1'],[]},
 {686,['ns_1@127.0.0.1'],[]},
 {687,['ns_1@127.0.0.1'],[]},
 {688,['ns_1@127.0.0.1'],[]},
 {689,['ns_1@127.0.0.1'],[]},
 {690,['ns_1@127.0.0.1'],[]},
 {691,['ns_1@127.0.0.1'],[]},
 {692,['ns_1@127.0.0.1'],[]},
 {693,['ns_1@127.0.0.1'],[]},
 {694,['ns_1@127.0.0.1'],[]},
 {695,['ns_1@127.0.0.1'],[]},
 {696,['ns_1@127.0.0.1'],[]},
 {697,['ns_1@127.0.0.1'],[]},
 {698,['ns_1@127.0.0.1'],[]},
 {699,['ns_1@127.0.0.1'],[]},
 {700,['ns_1@127.0.0.1'],[]},
 {701,['ns_1@127.0.0.1'],[]},
 {702,['ns_1@127.0.0.1'],[]},
 {703,['ns_1@127.0.0.1'],[]},
 {704,['ns_1@127.0.0.1'],[]},
 {705,['ns_1@127.0.0.1'],[]},
 {706,['ns_1@127.0.0.1'],[]},
 {707,['ns_1@127.0.0.1'],[]},
 {708,['ns_1@127.0.0.1'],[]},
 {709,['ns_1@127.0.0.1'],[]},
 {710,['ns_1@127.0.0.1'],[]},
 {711,['ns_1@127.0.0.1'],[]},
 {712,['ns_1@127.0.0.1'],[]},
 {713,['ns_1@127.0.0.1'],[]},
 {714,['ns_1@127.0.0.1'],[]},
 {715,['ns_1@127.0.0.1'],[]},
 {716,['ns_1@127.0.0.1'],[]},
 {717,['ns_1@127.0.0.1'],[]},
 {718,['ns_1@127.0.0.1'],[]},
 {719,['ns_1@127.0.0.1'],[]},
 {720,['ns_1@127.0.0.1'],[]},
 {721,['ns_1@127.0.0.1'],[]},
 {722,['ns_1@127.0.0.1'],[]},
 {723,['ns_1@127.0.0.1'],[]},
 {724,['ns_1@127.0.0.1'],[]},
 {725,['ns_1@127.0.0.1'],[]},
 {726,['ns_1@127.0.0.1'],[]},
 {727,['ns_1@127.0.0.1'],[]},
 {728,['ns_1@127.0.0.1'],[]},
 {729,['ns_1@127.0.0.1'],[]},
 {730,['ns_1@127.0.0.1'],[]},
 {731,['ns_1@127.0.0.1'],[]},
 {732,['ns_1@127.0.0.1'],[]},
 {733,['ns_1@127.0.0.1'],[]},
 {734,['ns_1@127.0.0.1'],[]},
 {735,['ns_1@127.0.0.1'],[]},
 {736,['ns_1@127.0.0.1'],[]},
 {737,['ns_1@127.0.0.1'],[]},
 {738,['ns_1@127.0.0.1'],[]},
 {739,['ns_1@127.0.0.1'],[]},
 {740,['ns_1@127.0.0.1'],[]},
 {741,['ns_1@127.0.0.1'],[]},
 {742,['ns_1@127.0.0.1'],[]},
 {743,['ns_1@127.0.0.1'],[]},
 {744,['ns_1@127.0.0.1'],[]},
 {745,['ns_1@127.0.0.1'],[]},
 {746,['ns_1@127.0.0.1'],[]},
 {747,['ns_1@127.0.0.1'],[]},
 {748,['ns_1@127.0.0.1'],[]},
 {749,['ns_1@127.0.0.1'],[]},
 {750,['ns_1@127.0.0.1'],[]},
 {751,['ns_1@127.0.0.1'],[]},
 {752,['ns_1@127.0.0.1'],[]},
 {753,['ns_1@127.0.0.1'],[]},
 {754,['ns_1@127.0.0.1'],[]},
 {755,['ns_1@127.0.0.1'],[]},
 {756,['ns_1@127.0.0.1'],[]},
 {757,['ns_1@127.0.0.1'],[]},
 {758,['ns_1@127.0.0.1'],[]},
 {759,['ns_1@127.0.0.1'],[]},
 {760,['ns_1@127.0.0.1'],[]},
 {761,['ns_1@127.0.0.1'],[]},
 {762,['ns_1@127.0.0.1'],[]},
 {763,['ns_1@127.0.0.1'],[]},
 {764,['ns_1@127.0.0.1'],[]},
 {765,['ns_1@127.0.0.1'],[]},
 {766,['ns_1@127.0.0.1'],[]},
 {767,['ns_1@127.0.0.1'],[]},
 {768,['ns_1@127.0.0.1'],[]},
 {769,['ns_1@127.0.0.1'],[]},
 {770,['ns_1@127.0.0.1'],[]},
 {771,['ns_1@127.0.0.1'],[]},
 {772,['ns_1@127.0.0.1'],[]},
 {773,['ns_1@127.0.0.1'],[]},
 {774,['ns_1@127.0.0.1'],[]},
 {775,['ns_1@127.0.0.1'],[]},
 {776,['ns_1@127.0.0.1'],[]},
 {777,['ns_1@127.0.0.1'],[]},
 {778,['ns_1@127.0.0.1'],[]},
 {779,['ns_1@127.0.0.1'],[]},
 {780,['ns_1@127.0.0.1'],[]},
 {781,['ns_1@127.0.0.1'],[]},
 {782,['ns_1@127.0.0.1'],[]},
 {783,['ns_1@127.0.0.1'],[]},
 {784,['ns_1@127.0.0.1'],[]},
 {785,['ns_1@127.0.0.1'],[]},
 {786,['ns_1@127.0.0.1'],[]},
 {787,['ns_1@127.0.0.1'],[]},
 {788,['ns_1@127.0.0.1'],[]},
 {789,['ns_1@127.0.0.1'],[]},
 {790,['ns_1@127.0.0.1'],[]},
 {791,['ns_1@127.0.0.1'],[]},
 {792,['ns_1@127.0.0.1'],[]},
 {793,['ns_1@127.0.0.1'],[]},
 {794,['ns_1@127.0.0.1'],[]},
 {795,['ns_1@127.0.0.1'],[]},
 {796,['ns_1@127.0.0.1'],[]},
 {797,['ns_1@127.0.0.1'],[]},
 {798,['ns_1@127.0.0.1'],[]},
 {799,['ns_1@127.0.0.1'],[]},
 {800,['ns_1@127.0.0.1'],[]},
 {801,['ns_1@127.0.0.1'],[]},
 {802,['ns_1@127.0.0.1'],[]},
 {803,['ns_1@127.0.0.1'],[]},
 {804,['ns_1@127.0.0.1'],[]},
 {805,['ns_1@127.0.0.1'],[]},
 {806,['ns_1@127.0.0.1'],[]},
 {807,['ns_1@127.0.0.1'],[]},
 {808,['ns_1@127.0.0.1'],[]},
 {809,['ns_1@127.0.0.1'],[]},
 {810,['ns_1@127.0.0.1'],[]},
 {811,['ns_1@127.0.0.1'],[]},
 {812,['ns_1@127.0.0.1'],[]},
 {813,['ns_1@127.0.0.1'],[]},
 {814,['ns_1@127.0.0.1'],[]},
 {815,['ns_1@127.0.0.1'],[]},
 {816,['ns_1@127.0.0.1'],[]},
 {817,['ns_1@127.0.0.1'],[]},
 {818,['ns_1@127.0.0.1'],[]},
 {819,['ns_1@127.0.0.1'],[]},
 {820,['ns_1@127.0.0.1'],[]},
 {821,['ns_1@127.0.0.1'],[]},
 {822,['ns_1@127.0.0.1'],[]},
 {823,['ns_1@127.0.0.1'],[]},
 {824,['ns_1@127.0.0.1'],[]},
 {825,['ns_1@127.0.0.1'],[]},
 {826,['ns_1@127.0.0.1'],[]},
 {827,['ns_1@127.0.0.1'],[]},
 {828,['ns_1@127.0.0.1'],[]},
 {829,['ns_1@127.0.0.1'],[]},
 {830,['ns_1@127.0.0.1'],[]},
 {831,['ns_1@127.0.0.1'],[]},
 {832,['ns_1@127.0.0.1'],[]},
 {833,['ns_1@127.0.0.1'],[]},
 {834,['ns_1@127.0.0.1'],[]},
 {835,['ns_1@127.0.0.1'],[]},
 {836,['ns_1@127.0.0.1'],[]},
 {837,['ns_1@127.0.0.1'],[]},
 {838,['ns_1@127.0.0.1'],[]},
 {839,['ns_1@127.0.0.1'],[]},
 {840,['ns_1@127.0.0.1'],[]},
 {841,['ns_1@127.0.0.1'],[]},
 {842,['ns_1@127.0.0.1'],[]},
 {843,['ns_1@127.0.0.1'],[]},
 {844,['ns_1@127.0.0.1'],[]},
 {845,['ns_1@127.0.0.1'],[]},
 {846,['ns_1@127.0.0.1'],[]},
 {847,['ns_1@127.0.0.1'],[]},
 {848,['ns_1@127.0.0.1'],[]},
 {849,['ns_1@127.0.0.1'],[]},
 {850,['ns_1@127.0.0.1'],[]},
 {851,['ns_1@127.0.0.1'],[]},
 {852,['ns_1@127.0.0.1'],[]},
 {853,['ns_1@127.0.0.1'],[]},
 {854,['ns_1@127.0.0.1'],[]},
 {855,['ns_1@127.0.0.1'],[]},
 {856,['ns_1@127.0.0.1'],[]},
 {857,['ns_1@127.0.0.1'],[]},
 {858,['ns_1@127.0.0.1'],[]},
 {859,['ns_1@127.0.0.1'],[]},
 {860,['ns_1@127.0.0.1'],[]},
 {861,['ns_1@127.0.0.1'],[]},
 {862,['ns_1@127.0.0.1'],[]},
 {863,['ns_1@127.0.0.1'],[]},
 {864,['ns_1@127.0.0.1'],[]},
 {865,['ns_1@127.0.0.1'],[]},
 {866,['ns_1@127.0.0.1'],[]},
 {867,['ns_1@127.0.0.1'],[]},
 {868,['ns_1@127.0.0.1'],[]},
 {869,['ns_1@127.0.0.1'],[]},
 {870,['ns_1@127.0.0.1'],[]},
 {871,['ns_1@127.0.0.1'],[]},
 {872,['ns_1@127.0.0.1'],[]},
 {873,['ns_1@127.0.0.1'],[]},
 {874,['ns_1@127.0.0.1'],[]},
 {875,['ns_1@127.0.0.1'],[]},
 {876,['ns_1@127.0.0.1'],[]},
 {877,['ns_1@127.0.0.1'],[]},
 {878,['ns_1@127.0.0.1'],[]},
 {879,['ns_1@127.0.0.1'],[]},
 {880,['ns_1@127.0.0.1'],[]},
 {881,['ns_1@127.0.0.1'],[]},
 {882,['ns_1@127.0.0.1'],[]},
 {883,['ns_1@127.0.0.1'],[]},
 {884,['ns_1@127.0.0.1'],[]},
 {885,['ns_1@127.0.0.1'],[]},
 {886,['ns_1@127.0.0.1'],[]},
 {887,['ns_1@127.0.0.1'],[]},
 {888,['ns_1@127.0.0.1'],[]},
 {889,['ns_1@127.0.0.1'],[]},
 {890,['ns_1@127.0.0.1'],[]},
 {891,['ns_1@127.0.0.1'],[]},
 {892,['ns_1@127.0.0.1'],[]},
 {893,['ns_1@127.0.0.1'],[]},
 {894,['ns_1@127.0.0.1'],[]},
 {895,['ns_1@127.0.0.1'],[]},
 {896,['ns_1@127.0.0.1'],[]},
 {897,['ns_1@127.0.0.1'],[]},
 {898,['ns_1@127.0.0.1'],[]},
 {899,['ns_1@127.0.0.1'],[]},
 {900,['ns_1@127.0.0.1'],[]},
 {901,['ns_1@127.0.0.1'],[]},
 {902,['ns_1@127.0.0.1'],[]},
 {903,['ns_1@127.0.0.1'],[]},
 {904,['ns_1@127.0.0.1'],[]},
 {905,['ns_1@127.0.0.1'],[]},
 {906,['ns_1@127.0.0.1'],[]},
 {907,['ns_1@127.0.0.1'],[]},
 {908,['ns_1@127.0.0.1'],[]},
 {909,['ns_1@127.0.0.1'],[]},
 {910,['ns_1@127.0.0.1'],[]},
 {911,['ns_1@127.0.0.1'],[]},
 {912,['ns_1@127.0.0.1'],[]},
 {913,['ns_1@127.0.0.1'],[]},
 {914,['ns_1@127.0.0.1'],[]},
 {915,['ns_1@127.0.0.1'],[]},
 {916,['ns_1@127.0.0.1'],[]},
 {917,['ns_1@127.0.0.1'],[]},
 {918,['ns_1@127.0.0.1'],[]},
 {919,['ns_1@127.0.0.1'],[]},
 {920,['ns_1@127.0.0.1'],[]},
 {921,['ns_1@127.0.0.1'],[]},
 {922,['ns_1@127.0.0.1'],[]},
 {923,['ns_1@127.0.0.1'],[]},
 {924,['ns_1@127.0.0.1'],[]},
 {925,['ns_1@127.0.0.1'],[]},
 {926,['ns_1@127.0.0.1'],[]},
 {927,['ns_1@127.0.0.1'],[]},
 {928,['ns_1@127.0.0.1'],[]},
 {929,['ns_1@127.0.0.1'],[]},
 {930,['ns_1@127.0.0.1'],[]},
 {931,['ns_1@127.0.0.1'],[]},
 {932,['ns_1@127.0.0.1'],[]},
 {933,['ns_1@127.0.0.1'],[]},
 {934,['ns_1@127.0.0.1'],[]},
 {935,['ns_1@127.0.0.1'],[]},
 {936,['ns_1@127.0.0.1'],[]},
 {937,['ns_1@127.0.0.1'],[]},
 {938,['ns_1@127.0.0.1'],[]},
 {939,['ns_1@127.0.0.1'],[]},
 {940,['ns_1@127.0.0.1'],[]},
 {941,['ns_1@127.0.0.1'],[]},
 {942,['ns_1@127.0.0.1'],[]},
 {943,['ns_1@127.0.0.1'],[]},
 {944,['ns_1@127.0.0.1'],[]},
 {945,['ns_1@127.0.0.1'],[]},
 {946,['ns_1@127.0.0.1'],[]},
 {947,['ns_1@127.0.0.1'],[]},
 {948,['ns_1@127.0.0.1'],[]},
 {949,['ns_1@127.0.0.1'],[]},
 {950,['ns_1@127.0.0.1'],[]},
 {951,['ns_1@127.0.0.1'],[]},
 {952,['ns_1@127.0.0.1'],[]},
 {953,['ns_1@127.0.0.1'],[]},
 {954,['ns_1@127.0.0.1'],[]},
 {955,['ns_1@127.0.0.1'],[]},
 {956,['ns_1@127.0.0.1'],[]},
 {957,['ns_1@127.0.0.1'],[]},
 {958,['ns_1@127.0.0.1'],[]},
 {959,['ns_1@127.0.0.1'],[]},
 {960,['ns_1@127.0.0.1'],[]},
 {961,['ns_1@127.0.0.1'],[]},
 {962,['ns_1@127.0.0.1'],[]},
 {963,['ns_1@127.0.0.1'],[]},
 {964,['ns_1@127.0.0.1'],[]},
 {965,['ns_1@127.0.0.1'],[]},
 {966,['ns_1@127.0.0.1'],[]},
 {967,['ns_1@127.0.0.1'],[]},
 {968,['ns_1@127.0.0.1'],[]},
 {969,['ns_1@127.0.0.1'],[]},
 {970,['ns_1@127.0.0.1'],[]},
 {971,['ns_1@127.0.0.1'],[]},
 {972,['ns_1@127.0.0.1'],[]},
 {973,['ns_1@127.0.0.1'],[]},
 {974,['ns_1@127.0.0.1'],[]},
 {975,['ns_1@127.0.0.1'],[]},
 {976,['ns_1@127.0.0.1'],[]},
 {977,['ns_1@127.0.0.1'],[]},
 {978,['ns_1@127.0.0.1'],[]},
 {979,['ns_1@127.0.0.1'],[]},
 {980,['ns_1@127.0.0.1'],[]},
 {981,['ns_1@127.0.0.1'],[]},
 {982,['ns_1@127.0.0.1'],[]},
 {983,['ns_1@127.0.0.1'],[]},
 {984,['ns_1@127.0.0.1'],[]},
 {985,['ns_1@127.0.0.1'],[]},
 {986,['ns_1@127.0.0.1'],[]},
 {987,['ns_1@127.0.0.1'],[]},
 {988,['ns_1@127.0.0.1'],[]},
 {989,['ns_1@127.0.0.1'],[]},
 {990,['ns_1@127.0.0.1'],[]},
 {991,['ns_1@127.0.0.1'],[]},
 {992,['ns_1@127.0.0.1'],[]},
 {993,['ns_1@127.0.0.1'],[]},
 {994,['ns_1@127.0.0.1'],[]},
 {995,['ns_1@127.0.0.1'],[]},
 {996,['ns_1@127.0.0.1'],[]},
 {997,['ns_1@127.0.0.1'],[]},
 {998,['ns_1@127.0.0.1'],[]},
 {999,['ns_1@127.0.0.1'],[]},
 {1000,['ns_1@127.0.0.1'],[]},
 {1001,['ns_1@127.0.0.1'],[]},
 {1002,['ns_1@127.0.0.1'],[]},
 {1003,['ns_1@127.0.0.1'],[]},
 {1004,['ns_1@127.0.0.1'],[]},
 {1005,['ns_1@127.0.0.1'],[]},
 {1006,['ns_1@127.0.0.1'],[]},
 {1007,['ns_1@127.0.0.1'],[]},
 {1008,['ns_1@127.0.0.1'],[]},
 {1009,['ns_1@127.0.0.1'],[]},
 {1010,['ns_1@127.0.0.1'],[]},
 {1011,['ns_1@127.0.0.1'],[]},
 {1012,['ns_1@127.0.0.1'],[]},
 {1013,['ns_1@127.0.0.1'],[]},
 {1014,['ns_1@127.0.0.1'],[]},
 {1015,['ns_1@127.0.0.1'],[]},
 {1016,['ns_1@127.0.0.1'],[]},
 {1017,['ns_1@127.0.0.1'],[]},
 {1018,['ns_1@127.0.0.1'],[]},
 {1019,['ns_1@127.0.0.1'],[]},
 {1020,['ns_1@127.0.0.1'],[]},
 {1021,['ns_1@127.0.0.1'],[]},
 {1022,['ns_1@127.0.0.1'],[]},
 {1023,['ns_1@127.0.0.1'],[]}]
[ns_server:debug,2022-09-07T14:32:28.702Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:config_sync:257]Going to pull config to/from nodes:
['ns_1@127.0.0.1']
[ns_server:info,2022-09-07T14:32:28.710Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 0 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.710Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 1 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.710Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 2 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.710Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 3 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.710Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 4 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.711Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 5 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.711Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 6 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.711Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 7 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.711Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 8 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.711Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 9 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.711Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 10 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.711Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 11 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.711Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 12 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.712Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 13 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.712Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 14 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.712Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 15 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.712Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 16 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.712Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 17 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.712Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 18 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.712Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 19 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.712Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 20 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.712Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 21 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.713Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 22 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.713Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 23 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.713Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 24 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.713Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 25 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.713Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 26 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.713Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 27 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.713Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 28 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.713Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 29 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.713Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 30 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.713Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 31 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.713Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 32 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.713Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 33 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.713Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 34 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.714Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 35 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.714Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 36 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.714Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 37 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.714Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 38 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.714Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 39 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.714Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 40 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.714Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 41 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.714Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 42 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.714Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 43 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.715Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 44 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.715Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 45 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.715Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 46 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.715Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 47 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.715Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 48 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.715Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 49 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.715Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 50 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.715Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 51 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.715Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 52 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.715Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 53 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.716Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 54 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.716Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 55 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.716Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 56 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.716Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 57 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.716Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 58 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.716Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 59 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.716Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 60 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.717Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 61 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.717Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 62 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.717Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 63 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.717Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 64 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.717Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 65 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.717Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 66 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.717Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 67 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.717Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 68 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.717Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 69 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.717Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 70 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.717Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 71 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.717Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 72 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.717Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 73 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.718Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 74 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.718Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 75 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.718Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 76 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.718Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 77 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.718Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 78 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.718Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 79 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.718Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 80 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.718Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 81 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.718Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 82 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.718Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 83 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.718Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 84 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.718Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 85 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.718Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 86 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.718Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 87 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.718Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 88 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.718Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 89 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.718Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 90 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.718Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 91 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.718Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 92 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.719Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 93 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.719Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 94 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.719Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 95 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.719Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 96 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.719Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 97 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.719Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 98 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.719Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 99 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.719Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 100 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.719Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 101 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.719Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 102 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.719Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 103 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.719Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 104 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.719Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 105 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.719Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 106 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.719Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 107 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.720Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 108 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.720Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 109 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.720Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 110 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.720Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 111 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.720Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 112 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.720Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 113 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.720Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 114 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.720Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 115 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.720Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 116 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.721Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 117 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.721Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 118 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.721Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 119 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.721Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 120 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.721Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 121 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.721Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 122 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.721Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 123 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.721Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 124 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.722Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 125 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.722Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 126 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.722Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 127 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.722Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 128 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.722Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 129 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.722Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 130 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.722Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 131 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.722Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 132 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.723Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 133 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.723Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 134 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.723Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 135 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.723Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 136 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.723Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 137 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.723Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 138 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.723Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 139 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.724Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 140 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.724Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 141 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.724Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 142 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.724Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 143 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.724Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 144 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.724Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 145 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.724Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 146 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.724Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 147 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.724Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 148 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.724Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 149 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.724Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 150 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.725Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 151 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.725Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 152 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.725Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 153 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.725Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 154 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.725Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 155 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.725Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 156 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.725Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 157 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.725Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 158 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.725Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 159 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.725Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 160 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.726Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 161 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.726Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 162 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.726Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 163 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.726Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 164 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.726Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 165 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.726Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 166 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.726Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 167 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.726Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 168 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.726Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 169 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.727Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 170 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.727Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 171 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.727Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 172 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.727Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 173 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.727Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 174 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.727Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 175 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.727Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 176 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.727Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 177 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.728Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 178 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.728Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 179 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.728Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 180 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.728Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 181 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.728Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 182 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.728Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 183 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.728Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 184 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.728Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 185 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.728Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 186 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.728Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 187 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.728Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 188 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.729Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 189 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.729Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 190 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.729Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 191 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.729Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 192 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.729Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 193 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.729Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 194 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.729Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 195 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.729Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 196 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.729Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 197 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.729Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 198 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.729Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 199 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.729Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 200 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.730Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 201 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.730Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 202 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.730Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 203 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.730Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 204 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.730Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 205 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.730Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 206 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.730Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 207 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.730Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 208 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.730Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 209 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.731Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 210 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.731Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 211 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.731Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 212 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.731Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 213 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.731Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 214 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.731Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 215 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.731Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 216 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.731Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 217 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.732Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 218 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.732Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 219 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.732Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 220 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.732Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 221 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.732Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 222 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.732Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 223 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.732Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 224 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.732Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 225 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.732Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 226 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.733Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 227 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.733Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 228 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.733Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 229 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.733Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 230 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.733Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 231 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.733Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 232 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.733Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 233 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.733Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 234 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.733Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 235 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.733Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 236 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.734Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 237 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.734Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 238 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.734Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 239 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.734Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 240 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.734Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 241 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.734Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 242 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.734Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 243 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.734Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 244 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.734Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 245 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.734Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 246 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.734Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 247 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.734Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 248 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.734Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 249 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.735Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 250 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.735Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 251 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.735Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 252 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.735Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 253 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.735Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 254 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.735Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 255 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.735Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 256 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.735Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 257 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.735Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 258 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.735Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 259 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.735Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 260 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.736Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 261 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.736Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 262 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.736Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 263 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.736Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 264 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.736Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 265 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.736Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 266 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.736Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 267 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.736Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 268 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.736Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 269 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.736Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 270 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.737Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 271 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.737Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 272 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.737Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 273 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.737Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 274 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.737Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 275 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.737Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 276 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.737Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 277 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.737Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 278 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.737Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 279 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.737Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 280 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.737Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 281 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.737Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 282 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.738Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 283 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.738Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 284 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.738Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 285 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.738Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 286 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.738Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 287 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.738Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 288 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.738Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 289 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.738Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 290 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.738Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 291 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.738Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 292 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.738Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 293 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.738Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 294 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.738Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 295 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.739Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 296 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.739Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 297 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.739Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 298 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.739Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 299 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.739Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 300 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.739Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 301 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.739Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 302 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.739Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 303 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.739Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 304 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.740Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 305 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.740Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 306 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.740Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 307 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.740Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 308 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.741Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 309 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.741Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 310 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.741Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 311 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.741Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 312 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.741Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 313 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.741Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 314 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.741Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 315 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.741Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 316 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.741Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 317 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.741Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 318 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.742Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 319 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.742Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 320 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.742Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 321 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.742Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 322 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.742Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 323 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.742Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 324 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.742Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 325 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.743Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 326 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.743Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 327 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.743Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 328 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.743Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 329 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.743Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 330 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.743Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 331 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.743Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 332 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.743Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 333 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.743Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 334 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.743Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 335 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.744Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 336 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.744Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 337 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.744Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 338 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.744Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 339 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.744Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 340 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.744Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 341 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.744Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 342 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.744Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 343 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.744Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 344 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.744Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 345 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.744Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 346 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.744Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 347 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.744Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 348 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.745Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 349 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.745Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 350 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.745Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 351 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.745Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 352 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.745Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 353 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.745Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 354 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.745Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 355 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.745Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 356 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.745Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 357 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.745Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 358 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.745Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 359 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.745Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 360 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.745Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 361 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.746Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 362 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.746Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 363 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.746Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 364 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.746Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 365 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.746Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 366 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.746Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 367 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.746Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 368 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.746Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 369 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.746Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 370 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.746Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 371 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.746Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 372 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.747Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 373 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.747Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 374 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.747Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 375 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.747Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 376 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.747Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 377 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.747Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 378 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.747Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 379 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.747Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 380 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.747Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 381 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.748Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 382 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.748Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 383 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.748Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 384 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.748Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 385 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.748Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 386 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.748Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 387 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.748Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 388 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.748Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 389 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.748Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 390 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.748Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 391 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.748Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 392 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.748Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 393 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.748Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 394 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.749Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 395 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.749Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 396 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.749Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 397 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.749Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 398 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.749Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 399 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.749Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 400 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.749Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 401 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.749Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 402 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.749Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 403 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.749Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 404 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.750Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 405 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.750Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 406 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.750Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 407 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.750Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 408 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.750Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 409 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.750Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 410 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.750Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 411 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.750Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 412 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.750Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 413 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.750Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 414 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.750Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 415 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.750Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 416 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.750Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 417 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.751Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 418 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.751Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 419 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.751Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 420 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.751Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 421 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.751Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 422 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.751Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 423 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.751Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 424 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.751Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 425 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.751Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 426 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.751Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 427 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.751Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 428 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.751Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 429 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.751Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 430 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.751Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 431 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.751Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 432 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.752Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 433 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.752Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 434 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.752Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 435 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.752Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 436 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.752Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 437 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.752Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 438 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.752Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 439 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.752Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 440 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.752Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 441 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.752Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 442 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.753Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 443 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.753Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 444 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.753Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 445 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.753Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 446 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.753Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 447 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.753Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 448 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.753Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 449 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.753Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 450 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.753Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 451 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.753Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 452 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.753Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 453 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.753Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 454 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.753Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 455 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.753Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 456 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.753Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 457 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.753Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 458 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.754Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 459 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.754Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 460 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.754Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 461 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.754Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 462 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.754Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 463 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.754Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 464 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.754Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 465 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.754Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 466 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.754Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 467 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.754Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 468 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.755Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 469 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.755Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 470 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.755Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 471 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.755Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 472 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.755Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 473 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.755Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 474 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.755Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 475 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.755Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 476 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.755Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 477 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.755Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 478 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.756Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 479 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.756Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 480 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.756Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 481 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.756Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 482 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.756Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 483 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.756Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 484 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.756Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 485 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.756Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 486 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.756Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 487 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.756Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 488 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.756Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 489 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.757Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 490 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.757Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 491 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.757Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 492 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.757Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 493 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.757Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 494 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.757Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 495 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.757Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 496 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.757Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 497 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.757Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 498 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.757Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 499 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.757Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 500 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.757Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 501 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.757Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 502 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.758Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 503 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.758Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 504 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.758Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 505 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.758Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 506 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.758Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 507 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.758Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 508 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.758Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 509 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.758Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 510 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.758Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 511 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.758Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 512 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.758Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 513 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.758Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 514 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.758Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 515 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.759Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 516 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.759Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 517 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.759Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 518 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.759Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 519 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.759Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 520 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.759Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 521 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.759Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 522 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.759Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 523 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.759Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 524 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.759Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 525 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.759Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 526 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.759Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 527 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.759Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 528 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.759Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 529 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.760Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 530 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.760Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 531 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.760Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 532 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.760Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 533 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.760Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 534 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.760Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 535 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.760Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 536 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.760Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 537 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.760Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 538 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.760Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 539 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.760Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 540 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.761Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 541 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.761Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 542 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.761Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 543 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.761Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 544 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.761Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 545 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.761Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 546 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.761Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 547 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.761Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 548 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.761Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 549 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.761Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 550 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.761Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 551 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.761Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 552 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.762Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 553 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.762Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 554 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.762Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 555 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.762Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 556 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.762Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 557 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.762Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 558 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.762Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 559 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.762Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 560 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.762Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 561 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.762Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 562 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.762Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 563 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.762Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 564 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.763Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 565 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.763Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 566 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.763Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 567 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.763Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 568 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.763Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 569 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.763Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 570 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.763Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 571 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.763Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 572 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.763Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 573 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.763Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 574 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.763Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 575 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.764Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 576 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.764Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 577 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.764Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 578 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.764Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 579 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.764Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 580 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.764Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 581 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.764Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 582 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.764Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 583 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.764Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 584 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.764Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 585 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.764Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 586 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.764Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 587 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.764Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 588 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.765Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 589 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.765Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 590 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.765Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 591 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.765Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 592 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.765Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 593 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.765Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 594 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.765Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 595 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.765Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 596 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.765Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 597 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.765Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 598 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.765Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 599 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.766Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 600 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.766Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 601 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.766Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 602 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.766Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 603 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.766Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 604 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.766Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 605 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.766Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 606 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.766Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 607 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.766Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 608 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.766Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 609 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.766Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 610 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.766Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 611 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.767Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 612 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.767Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 613 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.767Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 614 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.767Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 615 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.767Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 616 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.767Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 617 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.767Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 618 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.767Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 619 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.767Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 620 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.767Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 621 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.767Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 622 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.767Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 623 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.768Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 624 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.768Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 625 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.768Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 626 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.768Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 627 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.768Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 628 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.768Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 629 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.768Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 630 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.768Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 631 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.768Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 632 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.768Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 633 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.769Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 634 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.769Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 635 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.769Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 636 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.769Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 637 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.769Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 638 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.769Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 639 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.769Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 640 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.769Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 641 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.769Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 642 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.769Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 643 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.770Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 644 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.770Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 645 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.770Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 646 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.770Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 647 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.770Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 648 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.770Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 649 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.770Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 650 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.770Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 651 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.770Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 652 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.770Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 653 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.770Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 654 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.771Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 655 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.771Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 656 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.771Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 657 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.771Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 658 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.771Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 659 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.771Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 660 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.771Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 661 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.771Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 662 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.771Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 663 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.771Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 664 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.771Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 665 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.772Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 666 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.772Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 667 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.772Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 668 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.772Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 669 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.772Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 670 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.772Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 671 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.772Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 672 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.772Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 673 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.772Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 674 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.772Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 675 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.773Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 676 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.773Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 677 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.773Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 678 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.773Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 679 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.773Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 680 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.773Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 681 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.773Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 682 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.773Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 683 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.773Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 684 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.774Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 685 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.774Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 686 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.774Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 687 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.774Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 688 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.774Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 689 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.774Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 690 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.774Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 691 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.774Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 692 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.774Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 693 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.774Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 694 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.774Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 695 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.775Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 696 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.775Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 697 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.775Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 698 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.775Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 699 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.775Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 700 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.775Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 701 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.775Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 702 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.775Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 703 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.775Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 704 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.775Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 705 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.776Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 706 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.776Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 707 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.776Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 708 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.776Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 709 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.776Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 710 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.776Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 711 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.776Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 712 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.776Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 713 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.776Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 714 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.776Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 715 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.776Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 716 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.776Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 717 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.776Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 718 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.776Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 719 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.777Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 720 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.777Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 721 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.777Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 722 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.777Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 723 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.777Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 724 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.777Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 725 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.777Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 726 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.777Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 727 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.777Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 728 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.777Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 729 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.778Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 730 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.778Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 731 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.778Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 732 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.778Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 733 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.778Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 734 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.778Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 735 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.778Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 736 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.778Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 737 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.778Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 738 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.778Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 739 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.778Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 740 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.779Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 741 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.779Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 742 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.779Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 743 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.779Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 744 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.779Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 745 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.779Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 746 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.779Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 747 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.779Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 748 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.779Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 749 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.779Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 750 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.779Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 751 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.780Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 752 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.780Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 753 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.780Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 754 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.780Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 755 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.780Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 756 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.780Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 757 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.780Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 758 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.780Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 759 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.780Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 760 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.780Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 761 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.781Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 762 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.781Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 763 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.781Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 764 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.781Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 765 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.781Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 766 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.781Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 767 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.781Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 768 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.781Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 769 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.781Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 770 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.781Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 771 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.781Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 772 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.782Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 773 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.782Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 774 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.782Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 775 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.782Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 776 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.782Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 777 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.782Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 778 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.782Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 779 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.782Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 780 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.782Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 781 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.782Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 782 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.782Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 783 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.782Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 784 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.783Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 785 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.783Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 786 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.783Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 787 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.783Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 788 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.783Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 789 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.783Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 790 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.783Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 791 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.783Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 792 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.783Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 793 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.783Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 794 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.783Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 795 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.784Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 796 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.784Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 797 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.784Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 798 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.784Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 799 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.784Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 800 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.784Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 801 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.784Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 802 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.784Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 803 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.784Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 804 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.784Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 805 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.784Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 806 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.785Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 807 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.785Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 808 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.785Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 809 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.785Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 810 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.785Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 811 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.785Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 812 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.785Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 813 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.785Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 814 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.785Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 815 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.786Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 816 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.786Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 817 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.786Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 818 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.786Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 819 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.786Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 820 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.786Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 821 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.786Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 822 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.786Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 823 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.786Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 824 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.786Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 825 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.786Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 826 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.786Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 827 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.787Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 828 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.787Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 829 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.787Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 830 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.787Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 831 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.787Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 832 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.787Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 833 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.787Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 834 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.787Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 835 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.787Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 836 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.788Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 837 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.788Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 838 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.788Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 839 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.788Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 840 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.788Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 841 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.788Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 842 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.788Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 843 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.788Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 844 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.788Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 845 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.788Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 846 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.788Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 847 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.789Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 848 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.789Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 849 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.789Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 850 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.789Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 851 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.789Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 852 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.789Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 853 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.789Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 854 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.789Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 855 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.789Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 856 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.790Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 857 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.790Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 858 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.790Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 859 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.790Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 860 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.790Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 861 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.790Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 862 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.790Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 863 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.790Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 864 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.790Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 865 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.790Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 866 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.790Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 867 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.790Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 868 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.790Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 869 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.791Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 870 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.791Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 871 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.791Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 872 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.791Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 873 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.791Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 874 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.791Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 875 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.791Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 876 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.791Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 877 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.791Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 878 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.791Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 879 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.791Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 880 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.791Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 881 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.791Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 882 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.792Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 883 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.792Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 884 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.792Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 885 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.792Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 886 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.792Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 887 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.792Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 888 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.792Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 889 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.792Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 890 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.792Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 891 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.793Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 892 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.793Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 893 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.793Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 894 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.793Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 895 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.793Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 896 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.793Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 897 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.793Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 898 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.793Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 899 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.793Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 900 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.793Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 901 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.793Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 902 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.793Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 903 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.793Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 904 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.793Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 905 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.793Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 906 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.793Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 907 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.793Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 908 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.793Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 909 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.793Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 910 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.793Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 911 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.793Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 912 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.793Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 913 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.793Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 914 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.794Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 915 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.794Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 916 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.794Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 917 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.794Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 918 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.794Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 919 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.794Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 920 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.794Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 921 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.794Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 922 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.794Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 923 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.794Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 924 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.794Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 925 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.794Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 926 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.794Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 927 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.794Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 928 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.794Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 929 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.794Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 930 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.794Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 931 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.794Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 932 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.794Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 933 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.795Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 934 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.795Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 935 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.795Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 936 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.795Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 937 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.795Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 938 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.795Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 939 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.795Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 940 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.795Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 941 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.795Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 942 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.795Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 943 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.796Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 944 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.796Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 945 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.796Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 946 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.796Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 947 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.796Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 948 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.796Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 949 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.796Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 950 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.796Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 951 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.796Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 952 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.796Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 953 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.796Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 954 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.796Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 955 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.797Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 956 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.797Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 957 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.797Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 958 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.797Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 959 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.797Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 960 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.797Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 961 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.797Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 962 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.797Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 963 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.798Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 964 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.798Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 965 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.798Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 966 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.798Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 967 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.798Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 968 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.798Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 969 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.798Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 970 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.798Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 971 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.798Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 972 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.798Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 973 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.798Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 974 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.798Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 975 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.799Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 976 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.799Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 977 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.799Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 978 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.799Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 979 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.799Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 980 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.799Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 981 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.799Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 982 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.799Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 983 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.799Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 984 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.799Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 985 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.799Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 986 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.799Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 987 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.799Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 988 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.799Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 989 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.800Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 990 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.800Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 991 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.800Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 992 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.800Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 993 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.800Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 994 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.800Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 995 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.800Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 996 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.800Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 997 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.800Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 998 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.800Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 999 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.800Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 1000 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.800Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 1001 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.800Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 1002 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.801Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 1003 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.801Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 1004 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.801Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 1005 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.801Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 1006 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.801Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 1007 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.801Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 1008 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.801Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 1009 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.801Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 1010 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.801Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 1011 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.801Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 1012 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.801Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 1013 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.801Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 1014 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.801Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 1015 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.801Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 1016 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.801Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 1017 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.802Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 1018 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.802Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 1019 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.802Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 1020 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.802Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 1021 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.802Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 1022 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2022-09-07T14:32:28.802Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:sanify_chain:482]Setting vbucket 1023 in "todo" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:debug,2022-09-07T14:32:28.803Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:maybe_config_sync:240]Found states mismatch in bucket "todo":
[{0,['ns_1@127.0.0.1'],[]},
 {1,['ns_1@127.0.0.1'],[]},
 {2,['ns_1@127.0.0.1'],[]},
 {3,['ns_1@127.0.0.1'],[]},
 {4,['ns_1@127.0.0.1'],[]},
 {5,['ns_1@127.0.0.1'],[]},
 {6,['ns_1@127.0.0.1'],[]},
 {7,['ns_1@127.0.0.1'],[]},
 {8,['ns_1@127.0.0.1'],[]},
 {9,['ns_1@127.0.0.1'],[]},
 {10,['ns_1@127.0.0.1'],[]},
 {11,['ns_1@127.0.0.1'],[]},
 {12,['ns_1@127.0.0.1'],[]},
 {13,['ns_1@127.0.0.1'],[]},
 {14,['ns_1@127.0.0.1'],[]},
 {15,['ns_1@127.0.0.1'],[]},
 {16,['ns_1@127.0.0.1'],[]},
 {17,['ns_1@127.0.0.1'],[]},
 {18,['ns_1@127.0.0.1'],[]},
 {19,['ns_1@127.0.0.1'],[]},
 {20,['ns_1@127.0.0.1'],[]},
 {21,['ns_1@127.0.0.1'],[]},
 {22,['ns_1@127.0.0.1'],[]},
 {23,['ns_1@127.0.0.1'],[]},
 {24,['ns_1@127.0.0.1'],[]},
 {25,['ns_1@127.0.0.1'],[]},
 {26,['ns_1@127.0.0.1'],[]},
 {27,['ns_1@127.0.0.1'],[]},
 {28,['ns_1@127.0.0.1'],[]},
 {29,['ns_1@127.0.0.1'],[]},
 {30,['ns_1@127.0.0.1'],[]},
 {31,['ns_1@127.0.0.1'],[]},
 {32,['ns_1@127.0.0.1'],[]},
 {33,['ns_1@127.0.0.1'],[]},
 {34,['ns_1@127.0.0.1'],[]},
 {35,['ns_1@127.0.0.1'],[]},
 {36,['ns_1@127.0.0.1'],[]},
 {37,['ns_1@127.0.0.1'],[]},
 {38,['ns_1@127.0.0.1'],[]},
 {39,['ns_1@127.0.0.1'],[]},
 {40,['ns_1@127.0.0.1'],[]},
 {41,['ns_1@127.0.0.1'],[]},
 {42,['ns_1@127.0.0.1'],[]},
 {43,['ns_1@127.0.0.1'],[]},
 {44,['ns_1@127.0.0.1'],[]},
 {45,['ns_1@127.0.0.1'],[]},
 {46,['ns_1@127.0.0.1'],[]},
 {47,['ns_1@127.0.0.1'],[]},
 {48,['ns_1@127.0.0.1'],[]},
 {49,['ns_1@127.0.0.1'],[]},
 {50,['ns_1@127.0.0.1'],[]},
 {51,['ns_1@127.0.0.1'],[]},
 {52,['ns_1@127.0.0.1'],[]},
 {53,['ns_1@127.0.0.1'],[]},
 {54,['ns_1@127.0.0.1'],[]},
 {55,['ns_1@127.0.0.1'],[]},
 {56,['ns_1@127.0.0.1'],[]},
 {57,['ns_1@127.0.0.1'],[]},
 {58,['ns_1@127.0.0.1'],[]},
 {59,['ns_1@127.0.0.1'],[]},
 {60,['ns_1@127.0.0.1'],[]},
 {61,['ns_1@127.0.0.1'],[]},
 {62,['ns_1@127.0.0.1'],[]},
 {63,['ns_1@127.0.0.1'],[]},
 {64,['ns_1@127.0.0.1'],[]},
 {65,['ns_1@127.0.0.1'],[]},
 {66,['ns_1@127.0.0.1'],[]},
 {67,['ns_1@127.0.0.1'],[]},
 {68,['ns_1@127.0.0.1'],[]},
 {69,['ns_1@127.0.0.1'],[]},
 {70,['ns_1@127.0.0.1'],[]},
 {71,['ns_1@127.0.0.1'],[]},
 {72,['ns_1@127.0.0.1'],[]},
 {73,['ns_1@127.0.0.1'],[]},
 {74,['ns_1@127.0.0.1'],[]},
 {75,['ns_1@127.0.0.1'],[]},
 {76,['ns_1@127.0.0.1'],[]},
 {77,['ns_1@127.0.0.1'],[]},
 {78,['ns_1@127.0.0.1'],[]},
 {79,['ns_1@127.0.0.1'],[]},
 {80,['ns_1@127.0.0.1'],[]},
 {81,['ns_1@127.0.0.1'],[]},
 {82,['ns_1@127.0.0.1'],[]},
 {83,['ns_1@127.0.0.1'],[]},
 {84,['ns_1@127.0.0.1'],[]},
 {85,['ns_1@127.0.0.1'],[]},
 {86,['ns_1@127.0.0.1'],[]},
 {87,['ns_1@127.0.0.1'],[]},
 {88,['ns_1@127.0.0.1'],[]},
 {89,['ns_1@127.0.0.1'],[]},
 {90,['ns_1@127.0.0.1'],[]},
 {91,['ns_1@127.0.0.1'],[]},
 {92,['ns_1@127.0.0.1'],[]},
 {93,['ns_1@127.0.0.1'],[]},
 {94,['ns_1@127.0.0.1'],[]},
 {95,['ns_1@127.0.0.1'],[]},
 {96,['ns_1@127.0.0.1'],[]},
 {97,['ns_1@127.0.0.1'],[]},
 {98,['ns_1@127.0.0.1'],[]},
 {99,['ns_1@127.0.0.1'],[]},
 {100,['ns_1@127.0.0.1'],[]},
 {101,['ns_1@127.0.0.1'],[]},
 {102,['ns_1@127.0.0.1'],[]},
 {103,['ns_1@127.0.0.1'],[]},
 {104,['ns_1@127.0.0.1'],[]},
 {105,['ns_1@127.0.0.1'],[]},
 {106,['ns_1@127.0.0.1'],[]},
 {107,['ns_1@127.0.0.1'],[]},
 {108,['ns_1@127.0.0.1'],[]},
 {109,['ns_1@127.0.0.1'],[]},
 {110,['ns_1@127.0.0.1'],[]},
 {111,['ns_1@127.0.0.1'],[]},
 {112,['ns_1@127.0.0.1'],[]},
 {113,['ns_1@127.0.0.1'],[]},
 {114,['ns_1@127.0.0.1'],[]},
 {115,['ns_1@127.0.0.1'],[]},
 {116,['ns_1@127.0.0.1'],[]},
 {117,['ns_1@127.0.0.1'],[]},
 {118,['ns_1@127.0.0.1'],[]},
 {119,['ns_1@127.0.0.1'],[]},
 {120,['ns_1@127.0.0.1'],[]},
 {121,['ns_1@127.0.0.1'],[]},
 {122,['ns_1@127.0.0.1'],[]},
 {123,['ns_1@127.0.0.1'],[]},
 {124,['ns_1@127.0.0.1'],[]},
 {125,['ns_1@127.0.0.1'],[]},
 {126,['ns_1@127.0.0.1'],[]},
 {127,['ns_1@127.0.0.1'],[]},
 {128,['ns_1@127.0.0.1'],[]},
 {129,['ns_1@127.0.0.1'],[]},
 {130,['ns_1@127.0.0.1'],[]},
 {131,['ns_1@127.0.0.1'],[]},
 {132,['ns_1@127.0.0.1'],[]},
 {133,['ns_1@127.0.0.1'],[]},
 {134,['ns_1@127.0.0.1'],[]},
 {135,['ns_1@127.0.0.1'],[]},
 {136,['ns_1@127.0.0.1'],[]},
 {137,['ns_1@127.0.0.1'],[]},
 {138,['ns_1@127.0.0.1'],[]},
 {139,['ns_1@127.0.0.1'],[]},
 {140,['ns_1@127.0.0.1'],[]},
 {141,['ns_1@127.0.0.1'],[]},
 {142,['ns_1@127.0.0.1'],[]},
 {143,['ns_1@127.0.0.1'],[]},
 {144,['ns_1@127.0.0.1'],[]},
 {145,['ns_1@127.0.0.1'],[]},
 {146,['ns_1@127.0.0.1'],[]},
 {147,['ns_1@127.0.0.1'],[]},
 {148,['ns_1@127.0.0.1'],[]},
 {149,['ns_1@127.0.0.1'],[]},
 {150,['ns_1@127.0.0.1'],[]},
 {151,['ns_1@127.0.0.1'],[]},
 {152,['ns_1@127.0.0.1'],[]},
 {153,['ns_1@127.0.0.1'],[]},
 {154,['ns_1@127.0.0.1'],[]},
 {155,['ns_1@127.0.0.1'],[]},
 {156,['ns_1@127.0.0.1'],[]},
 {157,['ns_1@127.0.0.1'],[]},
 {158,['ns_1@127.0.0.1'],[]},
 {159,['ns_1@127.0.0.1'],[]},
 {160,['ns_1@127.0.0.1'],[]},
 {161,['ns_1@127.0.0.1'],[]},
 {162,['ns_1@127.0.0.1'],[]},
 {163,['ns_1@127.0.0.1'],[]},
 {164,['ns_1@127.0.0.1'],[]},
 {165,['ns_1@127.0.0.1'],[]},
 {166,['ns_1@127.0.0.1'],[]},
 {167,['ns_1@127.0.0.1'],[]},
 {168,['ns_1@127.0.0.1'],[]},
 {169,['ns_1@127.0.0.1'],[]},
 {170,['ns_1@127.0.0.1'],[]},
 {171,['ns_1@127.0.0.1'],[]},
 {172,['ns_1@127.0.0.1'],[]},
 {173,['ns_1@127.0.0.1'],[]},
 {174,['ns_1@127.0.0.1'],[]},
 {175,['ns_1@127.0.0.1'],[]},
 {176,['ns_1@127.0.0.1'],[]},
 {177,['ns_1@127.0.0.1'],[]},
 {178,['ns_1@127.0.0.1'],[]},
 {179,['ns_1@127.0.0.1'],[]},
 {180,['ns_1@127.0.0.1'],[]},
 {181,['ns_1@127.0.0.1'],[]},
 {182,['ns_1@127.0.0.1'],[]},
 {183,['ns_1@127.0.0.1'],[]},
 {184,['ns_1@127.0.0.1'],[]},
 {185,['ns_1@127.0.0.1'],[]},
 {186,['ns_1@127.0.0.1'],[]},
 {187,['ns_1@127.0.0.1'],[]},
 {188,['ns_1@127.0.0.1'],[]},
 {189,['ns_1@127.0.0.1'],[]},
 {190,['ns_1@127.0.0.1'],[]},
 {191,['ns_1@127.0.0.1'],[]},
 {192,['ns_1@127.0.0.1'],[]},
 {193,['ns_1@127.0.0.1'],[]},
 {194,['ns_1@127.0.0.1'],[]},
 {195,['ns_1@127.0.0.1'],[]},
 {196,['ns_1@127.0.0.1'],[]},
 {197,['ns_1@127.0.0.1'],[]},
 {198,['ns_1@127.0.0.1'],[]},
 {199,['ns_1@127.0.0.1'],[]},
 {200,['ns_1@127.0.0.1'],[]},
 {201,['ns_1@127.0.0.1'],[]},
 {202,['ns_1@127.0.0.1'],[]},
 {203,['ns_1@127.0.0.1'],[]},
 {204,['ns_1@127.0.0.1'],[]},
 {205,['ns_1@127.0.0.1'],[]},
 {206,['ns_1@127.0.0.1'],[]},
 {207,['ns_1@127.0.0.1'],[]},
 {208,['ns_1@127.0.0.1'],[]},
 {209,['ns_1@127.0.0.1'],[]},
 {210,['ns_1@127.0.0.1'],[]},
 {211,['ns_1@127.0.0.1'],[]},
 {212,['ns_1@127.0.0.1'],[]},
 {213,['ns_1@127.0.0.1'],[]},
 {214,['ns_1@127.0.0.1'],[]},
 {215,['ns_1@127.0.0.1'],[]},
 {216,['ns_1@127.0.0.1'],[]},
 {217,['ns_1@127.0.0.1'],[]},
 {218,['ns_1@127.0.0.1'],[]},
 {219,['ns_1@127.0.0.1'],[]},
 {220,['ns_1@127.0.0.1'],[]},
 {221,['ns_1@127.0.0.1'],[]},
 {222,['ns_1@127.0.0.1'],[]},
 {223,['ns_1@127.0.0.1'],[]},
 {224,['ns_1@127.0.0.1'],[]},
 {225,['ns_1@127.0.0.1'],[]},
 {226,['ns_1@127.0.0.1'],[]},
 {227,['ns_1@127.0.0.1'],[]},
 {228,['ns_1@127.0.0.1'],[]},
 {229,['ns_1@127.0.0.1'],[]},
 {230,['ns_1@127.0.0.1'],[]},
 {231,['ns_1@127.0.0.1'],[]},
 {232,['ns_1@127.0.0.1'],[]},
 {233,['ns_1@127.0.0.1'],[]},
 {234,['ns_1@127.0.0.1'],[]},
 {235,['ns_1@127.0.0.1'],[]},
 {236,['ns_1@127.0.0.1'],[]},
 {237,['ns_1@127.0.0.1'],[]},
 {238,['ns_1@127.0.0.1'],[]},
 {239,['ns_1@127.0.0.1'],[]},
 {240,['ns_1@127.0.0.1'],[]},
 {241,['ns_1@127.0.0.1'],[]},
 {242,['ns_1@127.0.0.1'],[]},
 {243,['ns_1@127.0.0.1'],[]},
 {244,['ns_1@127.0.0.1'],[]},
 {245,['ns_1@127.0.0.1'],[]},
 {246,['ns_1@127.0.0.1'],[]},
 {247,['ns_1@127.0.0.1'],[]},
 {248,['ns_1@127.0.0.1'],[]},
 {249,['ns_1@127.0.0.1'],[]},
 {250,['ns_1@127.0.0.1'],[]},
 {251,['ns_1@127.0.0.1'],[]},
 {252,['ns_1@127.0.0.1'],[]},
 {253,['ns_1@127.0.0.1'],[]},
 {254,['ns_1@127.0.0.1'],[]},
 {255,['ns_1@127.0.0.1'],[]},
 {256,['ns_1@127.0.0.1'],[]},
 {257,['ns_1@127.0.0.1'],[]},
 {258,['ns_1@127.0.0.1'],[]},
 {259,['ns_1@127.0.0.1'],[]},
 {260,['ns_1@127.0.0.1'],[]},
 {261,['ns_1@127.0.0.1'],[]},
 {262,['ns_1@127.0.0.1'],[]},
 {263,['ns_1@127.0.0.1'],[]},
 {264,['ns_1@127.0.0.1'],[]},
 {265,['ns_1@127.0.0.1'],[]},
 {266,['ns_1@127.0.0.1'],[]},
 {267,['ns_1@127.0.0.1'],[]},
 {268,['ns_1@127.0.0.1'],[]},
 {269,['ns_1@127.0.0.1'],[]},
 {270,['ns_1@127.0.0.1'],[]},
 {271,['ns_1@127.0.0.1'],[]},
 {272,['ns_1@127.0.0.1'],[]},
 {273,['ns_1@127.0.0.1'],[]},
 {274,['ns_1@127.0.0.1'],[]},
 {275,['ns_1@127.0.0.1'],[]},
 {276,['ns_1@127.0.0.1'],[]},
 {277,['ns_1@127.0.0.1'],[]},
 {278,['ns_1@127.0.0.1'],[]},
 {279,['ns_1@127.0.0.1'],[]},
 {280,['ns_1@127.0.0.1'],[]},
 {281,['ns_1@127.0.0.1'],[]},
 {282,['ns_1@127.0.0.1'],[]},
 {283,['ns_1@127.0.0.1'],[]},
 {284,['ns_1@127.0.0.1'],[]},
 {285,['ns_1@127.0.0.1'],[]},
 {286,['ns_1@127.0.0.1'],[]},
 {287,['ns_1@127.0.0.1'],[]},
 {288,['ns_1@127.0.0.1'],[]},
 {289,['ns_1@127.0.0.1'],[]},
 {290,['ns_1@127.0.0.1'],[]},
 {291,['ns_1@127.0.0.1'],[]},
 {292,['ns_1@127.0.0.1'],[]},
 {293,['ns_1@127.0.0.1'],[]},
 {294,['ns_1@127.0.0.1'],[]},
 {295,['ns_1@127.0.0.1'],[]},
 {296,['ns_1@127.0.0.1'],[]},
 {297,['ns_1@127.0.0.1'],[]},
 {298,['ns_1@127.0.0.1'],[]},
 {299,['ns_1@127.0.0.1'],[]},
 {300,['ns_1@127.0.0.1'],[]},
 {301,['ns_1@127.0.0.1'],[]},
 {302,['ns_1@127.0.0.1'],[]},
 {303,['ns_1@127.0.0.1'],[]},
 {304,['ns_1@127.0.0.1'],[]},
 {305,['ns_1@127.0.0.1'],[]},
 {306,['ns_1@127.0.0.1'],[]},
 {307,['ns_1@127.0.0.1'],[]},
 {308,['ns_1@127.0.0.1'],[]},
 {309,['ns_1@127.0.0.1'],[]},
 {310,['ns_1@127.0.0.1'],[]},
 {311,['ns_1@127.0.0.1'],[]},
 {312,['ns_1@127.0.0.1'],[]},
 {313,['ns_1@127.0.0.1'],[]},
 {314,['ns_1@127.0.0.1'],[]},
 {315,['ns_1@127.0.0.1'],[]},
 {316,['ns_1@127.0.0.1'],[]},
 {317,['ns_1@127.0.0.1'],[]},
 {318,['ns_1@127.0.0.1'],[]},
 {319,['ns_1@127.0.0.1'],[]},
 {320,['ns_1@127.0.0.1'],[]},
 {321,['ns_1@127.0.0.1'],[]},
 {322,['ns_1@127.0.0.1'],[]},
 {323,['ns_1@127.0.0.1'],[]},
 {324,['ns_1@127.0.0.1'],[]},
 {325,['ns_1@127.0.0.1'],[]},
 {326,['ns_1@127.0.0.1'],[]},
 {327,['ns_1@127.0.0.1'],[]},
 {328,['ns_1@127.0.0.1'],[]},
 {329,['ns_1@127.0.0.1'],[]},
 {330,['ns_1@127.0.0.1'],[]},
 {331,['ns_1@127.0.0.1'],[]},
 {332,['ns_1@127.0.0.1'],[]},
 {333,['ns_1@127.0.0.1'],[]},
 {334,['ns_1@127.0.0.1'],[]},
 {335,['ns_1@127.0.0.1'],[]},
 {336,['ns_1@127.0.0.1'],[]},
 {337,['ns_1@127.0.0.1'],[]},
 {338,['ns_1@127.0.0.1'],[]},
 {339,['ns_1@127.0.0.1'],[]},
 {340,['ns_1@127.0.0.1'],[]},
 {341,['ns_1@127.0.0.1'],[]},
 {342,['ns_1@127.0.0.1'],[]},
 {343,['ns_1@127.0.0.1'],[]},
 {344,['ns_1@127.0.0.1'],[]},
 {345,['ns_1@127.0.0.1'],[]},
 {346,['ns_1@127.0.0.1'],[]},
 {347,['ns_1@127.0.0.1'],[]},
 {348,['ns_1@127.0.0.1'],[]},
 {349,['ns_1@127.0.0.1'],[]},
 {350,['ns_1@127.0.0.1'],[]},
 {351,['ns_1@127.0.0.1'],[]},
 {352,['ns_1@127.0.0.1'],[]},
 {353,['ns_1@127.0.0.1'],[]},
 {354,['ns_1@127.0.0.1'],[]},
 {355,['ns_1@127.0.0.1'],[]},
 {356,['ns_1@127.0.0.1'],[]},
 {357,['ns_1@127.0.0.1'],[]},
 {358,['ns_1@127.0.0.1'],[]},
 {359,['ns_1@127.0.0.1'],[]},
 {360,['ns_1@127.0.0.1'],[]},
 {361,['ns_1@127.0.0.1'],[]},
 {362,['ns_1@127.0.0.1'],[]},
 {363,['ns_1@127.0.0.1'],[]},
 {364,['ns_1@127.0.0.1'],[]},
 {365,['ns_1@127.0.0.1'],[]},
 {366,['ns_1@127.0.0.1'],[]},
 {367,['ns_1@127.0.0.1'],[]},
 {368,['ns_1@127.0.0.1'],[]},
 {369,['ns_1@127.0.0.1'],[]},
 {370,['ns_1@127.0.0.1'],[]},
 {371,['ns_1@127.0.0.1'],[]},
 {372,['ns_1@127.0.0.1'],[]},
 {373,['ns_1@127.0.0.1'],[]},
 {374,['ns_1@127.0.0.1'],[]},
 {375,['ns_1@127.0.0.1'],[]},
 {376,['ns_1@127.0.0.1'],[]},
 {377,['ns_1@127.0.0.1'],[]},
 {378,['ns_1@127.0.0.1'],[]},
 {379,['ns_1@127.0.0.1'],[]},
 {380,['ns_1@127.0.0.1'],[]},
 {381,['ns_1@127.0.0.1'],[]},
 {382,['ns_1@127.0.0.1'],[]},
 {383,['ns_1@127.0.0.1'],[]},
 {384,['ns_1@127.0.0.1'],[]},
 {385,['ns_1@127.0.0.1'],[]},
 {386,['ns_1@127.0.0.1'],[]},
 {387,['ns_1@127.0.0.1'],[]},
 {388,['ns_1@127.0.0.1'],[]},
 {389,['ns_1@127.0.0.1'],[]},
 {390,['ns_1@127.0.0.1'],[]},
 {391,['ns_1@127.0.0.1'],[]},
 {392,['ns_1@127.0.0.1'],[]},
 {393,['ns_1@127.0.0.1'],[]},
 {394,['ns_1@127.0.0.1'],[]},
 {395,['ns_1@127.0.0.1'],[]},
 {396,['ns_1@127.0.0.1'],[]},
 {397,['ns_1@127.0.0.1'],[]},
 {398,['ns_1@127.0.0.1'],[]},
 {399,['ns_1@127.0.0.1'],[]},
 {400,['ns_1@127.0.0.1'],[]},
 {401,['ns_1@127.0.0.1'],[]},
 {402,['ns_1@127.0.0.1'],[]},
 {403,['ns_1@127.0.0.1'],[]},
 {404,['ns_1@127.0.0.1'],[]},
 {405,['ns_1@127.0.0.1'],[]},
 {406,['ns_1@127.0.0.1'],[]},
 {407,['ns_1@127.0.0.1'],[]},
 {408,['ns_1@127.0.0.1'],[]},
 {409,['ns_1@127.0.0.1'],[]},
 {410,['ns_1@127.0.0.1'],[]},
 {411,['ns_1@127.0.0.1'],[]},
 {412,['ns_1@127.0.0.1'],[]},
 {413,['ns_1@127.0.0.1'],[]},
 {414,['ns_1@127.0.0.1'],[]},
 {415,['ns_1@127.0.0.1'],[]},
 {416,['ns_1@127.0.0.1'],[]},
 {417,['ns_1@127.0.0.1'],[]},
 {418,['ns_1@127.0.0.1'],[]},
 {419,['ns_1@127.0.0.1'],[]},
 {420,['ns_1@127.0.0.1'],[]},
 {421,['ns_1@127.0.0.1'],[]},
 {422,['ns_1@127.0.0.1'],[]},
 {423,['ns_1@127.0.0.1'],[]},
 {424,['ns_1@127.0.0.1'],[]},
 {425,['ns_1@127.0.0.1'],[]},
 {426,['ns_1@127.0.0.1'],[]},
 {427,['ns_1@127.0.0.1'],[]},
 {428,['ns_1@127.0.0.1'],[]},
 {429,['ns_1@127.0.0.1'],[]},
 {430,['ns_1@127.0.0.1'],[]},
 {431,['ns_1@127.0.0.1'],[]},
 {432,['ns_1@127.0.0.1'],[]},
 {433,['ns_1@127.0.0.1'],[]},
 {434,['ns_1@127.0.0.1'],[]},
 {435,['ns_1@127.0.0.1'],[]},
 {436,['ns_1@127.0.0.1'],[]},
 {437,['ns_1@127.0.0.1'],[]},
 {438,['ns_1@127.0.0.1'],[]},
 {439,['ns_1@127.0.0.1'],[]},
 {440,['ns_1@127.0.0.1'],[]},
 {441,['ns_1@127.0.0.1'],[]},
 {442,['ns_1@127.0.0.1'],[]},
 {443,['ns_1@127.0.0.1'],[]},
 {444,['ns_1@127.0.0.1'],[]},
 {445,['ns_1@127.0.0.1'],[]},
 {446,['ns_1@127.0.0.1'],[]},
 {447,['ns_1@127.0.0.1'],[]},
 {448,['ns_1@127.0.0.1'],[]},
 {449,['ns_1@127.0.0.1'],[]},
 {450,['ns_1@127.0.0.1'],[]},
 {451,['ns_1@127.0.0.1'],[]},
 {452,['ns_1@127.0.0.1'],[]},
 {453,['ns_1@127.0.0.1'],[]},
 {454,['ns_1@127.0.0.1'],[]},
 {455,['ns_1@127.0.0.1'],[]},
 {456,['ns_1@127.0.0.1'],[]},
 {457,['ns_1@127.0.0.1'],[]},
 {458,['ns_1@127.0.0.1'],[]},
 {459,['ns_1@127.0.0.1'],[]},
 {460,['ns_1@127.0.0.1'],[]},
 {461,['ns_1@127.0.0.1'],[]},
 {462,['ns_1@127.0.0.1'],[]},
 {463,['ns_1@127.0.0.1'],[]},
 {464,['ns_1@127.0.0.1'],[]},
 {465,['ns_1@127.0.0.1'],[]},
 {466,['ns_1@127.0.0.1'],[]},
 {467,['ns_1@127.0.0.1'],[]},
 {468,['ns_1@127.0.0.1'],[]},
 {469,['ns_1@127.0.0.1'],[]},
 {470,['ns_1@127.0.0.1'],[]},
 {471,['ns_1@127.0.0.1'],[]},
 {472,['ns_1@127.0.0.1'],[]},
 {473,['ns_1@127.0.0.1'],[]},
 {474,['ns_1@127.0.0.1'],[]},
 {475,['ns_1@127.0.0.1'],[]},
 {476,['ns_1@127.0.0.1'],[]},
 {477,['ns_1@127.0.0.1'],[]},
 {478,['ns_1@127.0.0.1'],[]},
 {479,['ns_1@127.0.0.1'],[]},
 {480,['ns_1@127.0.0.1'],[]},
 {481,['ns_1@127.0.0.1'],[]},
 {482,['ns_1@127.0.0.1'],[]},
 {483,['ns_1@127.0.0.1'],[]},
 {484,['ns_1@127.0.0.1'],[]},
 {485,['ns_1@127.0.0.1'],[]},
 {486,['ns_1@127.0.0.1'],[]},
 {487,['ns_1@127.0.0.1'],[]},
 {488,['ns_1@127.0.0.1'],[]},
 {489,['ns_1@127.0.0.1'],[]},
 {490,['ns_1@127.0.0.1'],[]},
 {491,['ns_1@127.0.0.1'],[]},
 {492,['ns_1@127.0.0.1'],[]},
 {493,['ns_1@127.0.0.1'],[]},
 {494,['ns_1@127.0.0.1'],[]},
 {495,['ns_1@127.0.0.1'],[]},
 {496,['ns_1@127.0.0.1'],[]},
 {497,['ns_1@127.0.0.1'],[]},
 {498,['ns_1@127.0.0.1'],[]},
 {499,['ns_1@127.0.0.1'],[]},
 {500,['ns_1@127.0.0.1'],[]},
 {501,['ns_1@127.0.0.1'],[]},
 {502,['ns_1@127.0.0.1'],[]},
 {503,['ns_1@127.0.0.1'],[]},
 {504,['ns_1@127.0.0.1'],[]},
 {505,['ns_1@127.0.0.1'],[]},
 {506,['ns_1@127.0.0.1'],[]},
 {507,['ns_1@127.0.0.1'],[]},
 {508,['ns_1@127.0.0.1'],[]},
 {509,['ns_1@127.0.0.1'],[]},
 {510,['ns_1@127.0.0.1'],[]},
 {511,['ns_1@127.0.0.1'],[]},
 {512,['ns_1@127.0.0.1'],[]},
 {513,['ns_1@127.0.0.1'],[]},
 {514,['ns_1@127.0.0.1'],[]},
 {515,['ns_1@127.0.0.1'],[]},
 {516,['ns_1@127.0.0.1'],[]},
 {517,['ns_1@127.0.0.1'],[]},
 {518,['ns_1@127.0.0.1'],[]},
 {519,['ns_1@127.0.0.1'],[]},
 {520,['ns_1@127.0.0.1'],[]},
 {521,['ns_1@127.0.0.1'],[]},
 {522,['ns_1@127.0.0.1'],[]},
 {523,['ns_1@127.0.0.1'],[]},
 {524,['ns_1@127.0.0.1'],[]},
 {525,['ns_1@127.0.0.1'],[]},
 {526,['ns_1@127.0.0.1'],[]},
 {527,['ns_1@127.0.0.1'],[]},
 {528,['ns_1@127.0.0.1'],[]},
 {529,['ns_1@127.0.0.1'],[]},
 {530,['ns_1@127.0.0.1'],[]},
 {531,['ns_1@127.0.0.1'],[]},
 {532,['ns_1@127.0.0.1'],[]},
 {533,['ns_1@127.0.0.1'],[]},
 {534,['ns_1@127.0.0.1'],[]},
 {535,['ns_1@127.0.0.1'],[]},
 {536,['ns_1@127.0.0.1'],[]},
 {537,['ns_1@127.0.0.1'],[]},
 {538,['ns_1@127.0.0.1'],[]},
 {539,['ns_1@127.0.0.1'],[]},
 {540,['ns_1@127.0.0.1'],[]},
 {541,['ns_1@127.0.0.1'],[]},
 {542,['ns_1@127.0.0.1'],[]},
 {543,['ns_1@127.0.0.1'],[]},
 {544,['ns_1@127.0.0.1'],[]},
 {545,['ns_1@127.0.0.1'],[]},
 {546,['ns_1@127.0.0.1'],[]},
 {547,['ns_1@127.0.0.1'],[]},
 {548,['ns_1@127.0.0.1'],[]},
 {549,['ns_1@127.0.0.1'],[]},
 {550,['ns_1@127.0.0.1'],[]},
 {551,['ns_1@127.0.0.1'],[]},
 {552,['ns_1@127.0.0.1'],[]},
 {553,['ns_1@127.0.0.1'],[]},
 {554,['ns_1@127.0.0.1'],[]},
 {555,['ns_1@127.0.0.1'],[]},
 {556,['ns_1@127.0.0.1'],[]},
 {557,['ns_1@127.0.0.1'],[]},
 {558,['ns_1@127.0.0.1'],[]},
 {559,['ns_1@127.0.0.1'],[]},
 {560,['ns_1@127.0.0.1'],[]},
 {561,['ns_1@127.0.0.1'],[]},
 {562,['ns_1@127.0.0.1'],[]},
 {563,['ns_1@127.0.0.1'],[]},
 {564,['ns_1@127.0.0.1'],[]},
 {565,['ns_1@127.0.0.1'],[]},
 {566,['ns_1@127.0.0.1'],[]},
 {567,['ns_1@127.0.0.1'],[]},
 {568,['ns_1@127.0.0.1'],[]},
 {569,['ns_1@127.0.0.1'],[]},
 {570,['ns_1@127.0.0.1'],[]},
 {571,['ns_1@127.0.0.1'],[]},
 {572,['ns_1@127.0.0.1'],[]},
 {573,['ns_1@127.0.0.1'],[]},
 {574,['ns_1@127.0.0.1'],[]},
 {575,['ns_1@127.0.0.1'],[]},
 {576,['ns_1@127.0.0.1'],[]},
 {577,['ns_1@127.0.0.1'],[]},
 {578,['ns_1@127.0.0.1'],[]},
 {579,['ns_1@127.0.0.1'],[]},
 {580,['ns_1@127.0.0.1'],[]},
 {581,['ns_1@127.0.0.1'],[]},
 {582,['ns_1@127.0.0.1'],[]},
 {583,['ns_1@127.0.0.1'],[]},
 {584,['ns_1@127.0.0.1'],[]},
 {585,['ns_1@127.0.0.1'],[]},
 {586,['ns_1@127.0.0.1'],[]},
 {587,['ns_1@127.0.0.1'],[]},
 {588,['ns_1@127.0.0.1'],[]},
 {589,['ns_1@127.0.0.1'],[]},
 {590,['ns_1@127.0.0.1'],[]},
 {591,['ns_1@127.0.0.1'],[]},
 {592,['ns_1@127.0.0.1'],[]},
 {593,['ns_1@127.0.0.1'],[]},
 {594,['ns_1@127.0.0.1'],[]},
 {595,['ns_1@127.0.0.1'],[]},
 {596,['ns_1@127.0.0.1'],[]},
 {597,['ns_1@127.0.0.1'],[]},
 {598,['ns_1@127.0.0.1'],[]},
 {599,['ns_1@127.0.0.1'],[]},
 {600,['ns_1@127.0.0.1'],[]},
 {601,['ns_1@127.0.0.1'],[]},
 {602,['ns_1@127.0.0.1'],[]},
 {603,['ns_1@127.0.0.1'],[]},
 {604,['ns_1@127.0.0.1'],[]},
 {605,['ns_1@127.0.0.1'],[]},
 {606,['ns_1@127.0.0.1'],[]},
 {607,['ns_1@127.0.0.1'],[]},
 {608,['ns_1@127.0.0.1'],[]},
 {609,['ns_1@127.0.0.1'],[]},
 {610,['ns_1@127.0.0.1'],[]},
 {611,['ns_1@127.0.0.1'],[]},
 {612,['ns_1@127.0.0.1'],[]},
 {613,['ns_1@127.0.0.1'],[]},
 {614,['ns_1@127.0.0.1'],[]},
 {615,['ns_1@127.0.0.1'],[]},
 {616,['ns_1@127.0.0.1'],[]},
 {617,['ns_1@127.0.0.1'],[]},
 {618,['ns_1@127.0.0.1'],[]},
 {619,['ns_1@127.0.0.1'],[]},
 {620,['ns_1@127.0.0.1'],[]},
 {621,['ns_1@127.0.0.1'],[]},
 {622,['ns_1@127.0.0.1'],[]},
 {623,['ns_1@127.0.0.1'],[]},
 {624,['ns_1@127.0.0.1'],[]},
 {625,['ns_1@127.0.0.1'],[]},
 {626,['ns_1@127.0.0.1'],[]},
 {627,['ns_1@127.0.0.1'],[]},
 {628,['ns_1@127.0.0.1'],[]},
 {629,['ns_1@127.0.0.1'],[]},
 {630,['ns_1@127.0.0.1'],[]},
 {631,['ns_1@127.0.0.1'],[]},
 {632,['ns_1@127.0.0.1'],[]},
 {633,['ns_1@127.0.0.1'],[]},
 {634,['ns_1@127.0.0.1'],[]},
 {635,['ns_1@127.0.0.1'],[]},
 {636,['ns_1@127.0.0.1'],[]},
 {637,['ns_1@127.0.0.1'],[]},
 {638,['ns_1@127.0.0.1'],[]},
 {639,['ns_1@127.0.0.1'],[]},
 {640,['ns_1@127.0.0.1'],[]},
 {641,['ns_1@127.0.0.1'],[]},
 {642,['ns_1@127.0.0.1'],[]},
 {643,['ns_1@127.0.0.1'],[]},
 {644,['ns_1@127.0.0.1'],[]},
 {645,['ns_1@127.0.0.1'],[]},
 {646,['ns_1@127.0.0.1'],[]},
 {647,['ns_1@127.0.0.1'],[]},
 {648,['ns_1@127.0.0.1'],[]},
 {649,['ns_1@127.0.0.1'],[]},
 {650,['ns_1@127.0.0.1'],[]},
 {651,['ns_1@127.0.0.1'],[]},
 {652,['ns_1@127.0.0.1'],[]},
 {653,['ns_1@127.0.0.1'],[]},
 {654,['ns_1@127.0.0.1'],[]},
 {655,['ns_1@127.0.0.1'],[]},
 {656,['ns_1@127.0.0.1'],[]},
 {657,['ns_1@127.0.0.1'],[]},
 {658,['ns_1@127.0.0.1'],[]},
 {659,['ns_1@127.0.0.1'],[]},
 {660,['ns_1@127.0.0.1'],[]},
 {661,['ns_1@127.0.0.1'],[]},
 {662,['ns_1@127.0.0.1'],[]},
 {663,['ns_1@127.0.0.1'],[]},
 {664,['ns_1@127.0.0.1'],[]},
 {665,['ns_1@127.0.0.1'],[]},
 {666,['ns_1@127.0.0.1'],[]},
 {667,['ns_1@127.0.0.1'],[]},
 {668,['ns_1@127.0.0.1'],[]},
 {669,['ns_1@127.0.0.1'],[]},
 {670,['ns_1@127.0.0.1'],[]},
 {671,['ns_1@127.0.0.1'],[]},
 {672,['ns_1@127.0.0.1'],[]},
 {673,['ns_1@127.0.0.1'],[]},
 {674,['ns_1@127.0.0.1'],[]},
 {675,['ns_1@127.0.0.1'],[]},
 {676,['ns_1@127.0.0.1'],[]},
 {677,['ns_1@127.0.0.1'],[]},
 {678,['ns_1@127.0.0.1'],[]},
 {679,['ns_1@127.0.0.1'],[]},
 {680,['ns_1@127.0.0.1'],[]},
 {681,['ns_1@127.0.0.1'],[]},
 {682,['ns_1@127.0.0.1'],[]},
 {683,['ns_1@127.0.0.1'],[]},
 {684,['ns_1@127.0.0.1'],[]},
 {685,['ns_1@127.0.0.1'],[]},
 {686,['ns_1@127.0.0.1'],[]},
 {687,['ns_1@127.0.0.1'],[]},
 {688,['ns_1@127.0.0.1'],[]},
 {689,['ns_1@127.0.0.1'],[]},
 {690,['ns_1@127.0.0.1'],[]},
 {691,['ns_1@127.0.0.1'],[]},
 {692,['ns_1@127.0.0.1'],[]},
 {693,['ns_1@127.0.0.1'],[]},
 {694,['ns_1@127.0.0.1'],[]},
 {695,['ns_1@127.0.0.1'],[]},
 {696,['ns_1@127.0.0.1'],[]},
 {697,['ns_1@127.0.0.1'],[]},
 {698,['ns_1@127.0.0.1'],[]},
 {699,['ns_1@127.0.0.1'],[]},
 {700,['ns_1@127.0.0.1'],[]},
 {701,['ns_1@127.0.0.1'],[]},
 {702,['ns_1@127.0.0.1'],[]},
 {703,['ns_1@127.0.0.1'],[]},
 {704,['ns_1@127.0.0.1'],[]},
 {705,['ns_1@127.0.0.1'],[]},
 {706,['ns_1@127.0.0.1'],[]},
 {707,['ns_1@127.0.0.1'],[]},
 {708,['ns_1@127.0.0.1'],[]},
 {709,['ns_1@127.0.0.1'],[]},
 {710,['ns_1@127.0.0.1'],[]},
 {711,['ns_1@127.0.0.1'],[]},
 {712,['ns_1@127.0.0.1'],[]},
 {713,['ns_1@127.0.0.1'],[]},
 {714,['ns_1@127.0.0.1'],[]},
 {715,['ns_1@127.0.0.1'],[]},
 {716,['ns_1@127.0.0.1'],[]},
 {717,['ns_1@127.0.0.1'],[]},
 {718,['ns_1@127.0.0.1'],[]},
 {719,['ns_1@127.0.0.1'],[]},
 {720,['ns_1@127.0.0.1'],[]},
 {721,['ns_1@127.0.0.1'],[]},
 {722,['ns_1@127.0.0.1'],[]},
 {723,['ns_1@127.0.0.1'],[]},
 {724,['ns_1@127.0.0.1'],[]},
 {725,['ns_1@127.0.0.1'],[]},
 {726,['ns_1@127.0.0.1'],[]},
 {727,['ns_1@127.0.0.1'],[]},
 {728,['ns_1@127.0.0.1'],[]},
 {729,['ns_1@127.0.0.1'],[]},
 {730,['ns_1@127.0.0.1'],[]},
 {731,['ns_1@127.0.0.1'],[]},
 {732,['ns_1@127.0.0.1'],[]},
 {733,['ns_1@127.0.0.1'],[]},
 {734,['ns_1@127.0.0.1'],[]},
 {735,['ns_1@127.0.0.1'],[]},
 {736,['ns_1@127.0.0.1'],[]},
 {737,['ns_1@127.0.0.1'],[]},
 {738,['ns_1@127.0.0.1'],[]},
 {739,['ns_1@127.0.0.1'],[]},
 {740,['ns_1@127.0.0.1'],[]},
 {741,['ns_1@127.0.0.1'],[]},
 {742,['ns_1@127.0.0.1'],[]},
 {743,['ns_1@127.0.0.1'],[]},
 {744,['ns_1@127.0.0.1'],[]},
 {745,['ns_1@127.0.0.1'],[]},
 {746,['ns_1@127.0.0.1'],[]},
 {747,['ns_1@127.0.0.1'],[]},
 {748,['ns_1@127.0.0.1'],[]},
 {749,['ns_1@127.0.0.1'],[]},
 {750,['ns_1@127.0.0.1'],[]},
 {751,['ns_1@127.0.0.1'],[]},
 {752,['ns_1@127.0.0.1'],[]},
 {753,['ns_1@127.0.0.1'],[]},
 {754,['ns_1@127.0.0.1'],[]},
 {755,['ns_1@127.0.0.1'],[]},
 {756,['ns_1@127.0.0.1'],[]},
 {757,['ns_1@127.0.0.1'],[]},
 {758,['ns_1@127.0.0.1'],[]},
 {759,['ns_1@127.0.0.1'],[]},
 {760,['ns_1@127.0.0.1'],[]},
 {761,['ns_1@127.0.0.1'],[]},
 {762,['ns_1@127.0.0.1'],[]},
 {763,['ns_1@127.0.0.1'],[]},
 {764,['ns_1@127.0.0.1'],[]},
 {765,['ns_1@127.0.0.1'],[]},
 {766,['ns_1@127.0.0.1'],[]},
 {767,['ns_1@127.0.0.1'],[]},
 {768,['ns_1@127.0.0.1'],[]},
 {769,['ns_1@127.0.0.1'],[]},
 {770,['ns_1@127.0.0.1'],[]},
 {771,['ns_1@127.0.0.1'],[]},
 {772,['ns_1@127.0.0.1'],[]},
 {773,['ns_1@127.0.0.1'],[]},
 {774,['ns_1@127.0.0.1'],[]},
 {775,['ns_1@127.0.0.1'],[]},
 {776,['ns_1@127.0.0.1'],[]},
 {777,['ns_1@127.0.0.1'],[]},
 {778,['ns_1@127.0.0.1'],[]},
 {779,['ns_1@127.0.0.1'],[]},
 {780,['ns_1@127.0.0.1'],[]},
 {781,['ns_1@127.0.0.1'],[]},
 {782,['ns_1@127.0.0.1'],[]},
 {783,['ns_1@127.0.0.1'],[]},
 {784,['ns_1@127.0.0.1'],[]},
 {785,['ns_1@127.0.0.1'],[]},
 {786,['ns_1@127.0.0.1'],[]},
 {787,['ns_1@127.0.0.1'],[]},
 {788,['ns_1@127.0.0.1'],[]},
 {789,['ns_1@127.0.0.1'],[]},
 {790,['ns_1@127.0.0.1'],[]},
 {791,['ns_1@127.0.0.1'],[]},
 {792,['ns_1@127.0.0.1'],[]},
 {793,['ns_1@127.0.0.1'],[]},
 {794,['ns_1@127.0.0.1'],[]},
 {795,['ns_1@127.0.0.1'],[]},
 {796,['ns_1@127.0.0.1'],[]},
 {797,['ns_1@127.0.0.1'],[]},
 {798,['ns_1@127.0.0.1'],[]},
 {799,['ns_1@127.0.0.1'],[]},
 {800,['ns_1@127.0.0.1'],[]},
 {801,['ns_1@127.0.0.1'],[]},
 {802,['ns_1@127.0.0.1'],[]},
 {803,['ns_1@127.0.0.1'],[]},
 {804,['ns_1@127.0.0.1'],[]},
 {805,['ns_1@127.0.0.1'],[]},
 {806,['ns_1@127.0.0.1'],[]},
 {807,['ns_1@127.0.0.1'],[]},
 {808,['ns_1@127.0.0.1'],[]},
 {809,['ns_1@127.0.0.1'],[]},
 {810,['ns_1@127.0.0.1'],[]},
 {811,['ns_1@127.0.0.1'],[]},
 {812,['ns_1@127.0.0.1'],[]},
 {813,['ns_1@127.0.0.1'],[]},
 {814,['ns_1@127.0.0.1'],[]},
 {815,['ns_1@127.0.0.1'],[]},
 {816,['ns_1@127.0.0.1'],[]},
 {817,['ns_1@127.0.0.1'],[]},
 {818,['ns_1@127.0.0.1'],[]},
 {819,['ns_1@127.0.0.1'],[]},
 {820,['ns_1@127.0.0.1'],[]},
 {821,['ns_1@127.0.0.1'],[]},
 {822,['ns_1@127.0.0.1'],[]},
 {823,['ns_1@127.0.0.1'],[]},
 {824,['ns_1@127.0.0.1'],[]},
 {825,['ns_1@127.0.0.1'],[]},
 {826,['ns_1@127.0.0.1'],[]},
 {827,['ns_1@127.0.0.1'],[]},
 {828,['ns_1@127.0.0.1'],[]},
 {829,['ns_1@127.0.0.1'],[]},
 {830,['ns_1@127.0.0.1'],[]},
 {831,['ns_1@127.0.0.1'],[]},
 {832,['ns_1@127.0.0.1'],[]},
 {833,['ns_1@127.0.0.1'],[]},
 {834,['ns_1@127.0.0.1'],[]},
 {835,['ns_1@127.0.0.1'],[]},
 {836,['ns_1@127.0.0.1'],[]},
 {837,['ns_1@127.0.0.1'],[]},
 {838,['ns_1@127.0.0.1'],[]},
 {839,['ns_1@127.0.0.1'],[]},
 {840,['ns_1@127.0.0.1'],[]},
 {841,['ns_1@127.0.0.1'],[]},
 {842,['ns_1@127.0.0.1'],[]},
 {843,['ns_1@127.0.0.1'],[]},
 {844,['ns_1@127.0.0.1'],[]},
 {845,['ns_1@127.0.0.1'],[]},
 {846,['ns_1@127.0.0.1'],[]},
 {847,['ns_1@127.0.0.1'],[]},
 {848,['ns_1@127.0.0.1'],[]},
 {849,['ns_1@127.0.0.1'],[]},
 {850,['ns_1@127.0.0.1'],[]},
 {851,['ns_1@127.0.0.1'],[]},
 {852,['ns_1@127.0.0.1'],[]},
 {853,['ns_1@127.0.0.1'],[]},
 {854,['ns_1@127.0.0.1'],[]},
 {855,['ns_1@127.0.0.1'],[]},
 {856,['ns_1@127.0.0.1'],[]},
 {857,['ns_1@127.0.0.1'],[]},
 {858,['ns_1@127.0.0.1'],[]},
 {859,['ns_1@127.0.0.1'],[]},
 {860,['ns_1@127.0.0.1'],[]},
 {861,['ns_1@127.0.0.1'],[]},
 {862,['ns_1@127.0.0.1'],[]},
 {863,['ns_1@127.0.0.1'],[]},
 {864,['ns_1@127.0.0.1'],[]},
 {865,['ns_1@127.0.0.1'],[]},
 {866,['ns_1@127.0.0.1'],[]},
 {867,['ns_1@127.0.0.1'],[]},
 {868,['ns_1@127.0.0.1'],[]},
 {869,['ns_1@127.0.0.1'],[]},
 {870,['ns_1@127.0.0.1'],[]},
 {871,['ns_1@127.0.0.1'],[]},
 {872,['ns_1@127.0.0.1'],[]},
 {873,['ns_1@127.0.0.1'],[]},
 {874,['ns_1@127.0.0.1'],[]},
 {875,['ns_1@127.0.0.1'],[]},
 {876,['ns_1@127.0.0.1'],[]},
 {877,['ns_1@127.0.0.1'],[]},
 {878,['ns_1@127.0.0.1'],[]},
 {879,['ns_1@127.0.0.1'],[]},
 {880,['ns_1@127.0.0.1'],[]},
 {881,['ns_1@127.0.0.1'],[]},
 {882,['ns_1@127.0.0.1'],[]},
 {883,['ns_1@127.0.0.1'],[]},
 {884,['ns_1@127.0.0.1'],[]},
 {885,['ns_1@127.0.0.1'],[]},
 {886,['ns_1@127.0.0.1'],[]},
 {887,['ns_1@127.0.0.1'],[]},
 {888,['ns_1@127.0.0.1'],[]},
 {889,['ns_1@127.0.0.1'],[]},
 {890,['ns_1@127.0.0.1'],[]},
 {891,['ns_1@127.0.0.1'],[]},
 {892,['ns_1@127.0.0.1'],[]},
 {893,['ns_1@127.0.0.1'],[]},
 {894,['ns_1@127.0.0.1'],[]},
 {895,['ns_1@127.0.0.1'],[]},
 {896,['ns_1@127.0.0.1'],[]},
 {897,['ns_1@127.0.0.1'],[]},
 {898,['ns_1@127.0.0.1'],[]},
 {899,['ns_1@127.0.0.1'],[]},
 {900,['ns_1@127.0.0.1'],[]},
 {901,['ns_1@127.0.0.1'],[]},
 {902,['ns_1@127.0.0.1'],[]},
 {903,['ns_1@127.0.0.1'],[]},
 {904,['ns_1@127.0.0.1'],[]},
 {905,['ns_1@127.0.0.1'],[]},
 {906,['ns_1@127.0.0.1'],[]},
 {907,['ns_1@127.0.0.1'],[]},
 {908,['ns_1@127.0.0.1'],[]},
 {909,['ns_1@127.0.0.1'],[]},
 {910,['ns_1@127.0.0.1'],[]},
 {911,['ns_1@127.0.0.1'],[]},
 {912,['ns_1@127.0.0.1'],[]},
 {913,['ns_1@127.0.0.1'],[]},
 {914,['ns_1@127.0.0.1'],[]},
 {915,['ns_1@127.0.0.1'],[]},
 {916,['ns_1@127.0.0.1'],[]},
 {917,['ns_1@127.0.0.1'],[]},
 {918,['ns_1@127.0.0.1'],[]},
 {919,['ns_1@127.0.0.1'],[]},
 {920,['ns_1@127.0.0.1'],[]},
 {921,['ns_1@127.0.0.1'],[]},
 {922,['ns_1@127.0.0.1'],[]},
 {923,['ns_1@127.0.0.1'],[]},
 {924,['ns_1@127.0.0.1'],[]},
 {925,['ns_1@127.0.0.1'],[]},
 {926,['ns_1@127.0.0.1'],[]},
 {927,['ns_1@127.0.0.1'],[]},
 {928,['ns_1@127.0.0.1'],[]},
 {929,['ns_1@127.0.0.1'],[]},
 {930,['ns_1@127.0.0.1'],[]},
 {931,['ns_1@127.0.0.1'],[]},
 {932,['ns_1@127.0.0.1'],[]},
 {933,['ns_1@127.0.0.1'],[]},
 {934,['ns_1@127.0.0.1'],[]},
 {935,['ns_1@127.0.0.1'],[]},
 {936,['ns_1@127.0.0.1'],[]},
 {937,['ns_1@127.0.0.1'],[]},
 {938,['ns_1@127.0.0.1'],[]},
 {939,['ns_1@127.0.0.1'],[]},
 {940,['ns_1@127.0.0.1'],[]},
 {941,['ns_1@127.0.0.1'],[]},
 {942,['ns_1@127.0.0.1'],[]},
 {943,['ns_1@127.0.0.1'],[]},
 {944,['ns_1@127.0.0.1'],[]},
 {945,['ns_1@127.0.0.1'],[]},
 {946,['ns_1@127.0.0.1'],[]},
 {947,['ns_1@127.0.0.1'],[]},
 {948,['ns_1@127.0.0.1'],[]},
 {949,['ns_1@127.0.0.1'],[]},
 {950,['ns_1@127.0.0.1'],[]},
 {951,['ns_1@127.0.0.1'],[]},
 {952,['ns_1@127.0.0.1'],[]},
 {953,['ns_1@127.0.0.1'],[]},
 {954,['ns_1@127.0.0.1'],[]},
 {955,['ns_1@127.0.0.1'],[]},
 {956,['ns_1@127.0.0.1'],[]},
 {957,['ns_1@127.0.0.1'],[]},
 {958,['ns_1@127.0.0.1'],[]},
 {959,['ns_1@127.0.0.1'],[]},
 {960,['ns_1@127.0.0.1'],[]},
 {961,['ns_1@127.0.0.1'],[]},
 {962,['ns_1@127.0.0.1'],[]},
 {963,['ns_1@127.0.0.1'],[]},
 {964,['ns_1@127.0.0.1'],[]},
 {965,['ns_1@127.0.0.1'],[]},
 {966,['ns_1@127.0.0.1'],[]},
 {967,['ns_1@127.0.0.1'],[]},
 {968,['ns_1@127.0.0.1'],[]},
 {969,['ns_1@127.0.0.1'],[]},
 {970,['ns_1@127.0.0.1'],[]},
 {971,['ns_1@127.0.0.1'],[]},
 {972,['ns_1@127.0.0.1'],[]},
 {973,['ns_1@127.0.0.1'],[]},
 {974,['ns_1@127.0.0.1'],[]},
 {975,['ns_1@127.0.0.1'],[]},
 {976,['ns_1@127.0.0.1'],[]},
 {977,['ns_1@127.0.0.1'],[]},
 {978,['ns_1@127.0.0.1'],[]},
 {979,['ns_1@127.0.0.1'],[]},
 {980,['ns_1@127.0.0.1'],[]},
 {981,['ns_1@127.0.0.1'],[]},
 {982,['ns_1@127.0.0.1'],[]},
 {983,['ns_1@127.0.0.1'],[]},
 {984,['ns_1@127.0.0.1'],[]},
 {985,['ns_1@127.0.0.1'],[]},
 {986,['ns_1@127.0.0.1'],[]},
 {987,['ns_1@127.0.0.1'],[]},
 {988,['ns_1@127.0.0.1'],[]},
 {989,['ns_1@127.0.0.1'],[]},
 {990,['ns_1@127.0.0.1'],[]},
 {991,['ns_1@127.0.0.1'],[]},
 {992,['ns_1@127.0.0.1'],[]},
 {993,['ns_1@127.0.0.1'],[]},
 {994,['ns_1@127.0.0.1'],[]},
 {995,['ns_1@127.0.0.1'],[]},
 {996,['ns_1@127.0.0.1'],[]},
 {997,['ns_1@127.0.0.1'],[]},
 {998,['ns_1@127.0.0.1'],[]},
 {999,['ns_1@127.0.0.1'],[]},
 {1000,['ns_1@127.0.0.1'],[]},
 {1001,['ns_1@127.0.0.1'],[]},
 {1002,['ns_1@127.0.0.1'],[]},
 {1003,['ns_1@127.0.0.1'],[]},
 {1004,['ns_1@127.0.0.1'],[]},
 {1005,['ns_1@127.0.0.1'],[]},
 {1006,['ns_1@127.0.0.1'],[]},
 {1007,['ns_1@127.0.0.1'],[]},
 {1008,['ns_1@127.0.0.1'],[]},
 {1009,['ns_1@127.0.0.1'],[]},
 {1010,['ns_1@127.0.0.1'],[]},
 {1011,['ns_1@127.0.0.1'],[]},
 {1012,['ns_1@127.0.0.1'],[]},
 {1013,['ns_1@127.0.0.1'],[]},
 {1014,['ns_1@127.0.0.1'],[]},
 {1015,['ns_1@127.0.0.1'],[]},
 {1016,['ns_1@127.0.0.1'],[]},
 {1017,['ns_1@127.0.0.1'],[]},
 {1018,['ns_1@127.0.0.1'],[]},
 {1019,['ns_1@127.0.0.1'],[]},
 {1020,['ns_1@127.0.0.1'],[]},
 {1021,['ns_1@127.0.0.1'],[]},
 {1022,['ns_1@127.0.0.1'],[]},
 {1023,['ns_1@127.0.0.1'],[]}]
[ns_server:debug,2022-09-07T14:32:28.855Z,ns_1@127.0.0.1:<0.10392.0>:ns_janitor:config_sync:257]Going to push config to/from nodes:
['ns_1@127.0.0.1']
[ns_server:info,2022-09-07T14:32:29.695Z,ns_1@127.0.0.1:<0.10244.0>:ns_memcached:do_handle_call:591]Changed vbucket state 
[{1023,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {1022,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {1021,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {1020,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {1019,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {1018,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {1017,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {1016,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {1015,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {1014,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {1013,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {1012,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {1011,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {1010,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {1009,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {1008,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {1007,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {1006,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {1005,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {1004,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {1003,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {1002,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {1001,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {1000,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {999,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {998,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {997,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {996,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {995,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {994,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {993,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {992,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {991,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {990,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {989,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {988,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {987,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {986,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {985,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {984,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {983,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {982,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {981,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {980,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {979,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {978,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {977,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {976,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {975,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {974,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {973,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {972,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {971,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {970,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {969,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {968,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {967,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {966,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {965,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {964,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {963,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {962,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {961,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {960,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {959,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {958,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {957,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {956,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {955,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {954,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {953,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {952,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {951,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {950,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {949,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {948,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {947,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {946,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {945,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {944,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {943,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {942,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {941,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {940,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {939,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {938,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {937,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {936,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {935,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {934,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {933,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {932,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {931,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {930,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {929,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {928,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {927,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {926,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {925,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {924,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {923,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {922,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {921,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {920,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {919,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {918,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {917,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {916,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {915,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {914,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {913,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {912,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {911,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {910,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {909,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {908,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {907,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {906,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {905,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {904,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {903,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {902,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {901,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {900,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {899,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {898,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {897,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {896,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {895,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {894,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {893,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {892,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {891,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {890,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {889,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {888,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {887,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {886,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {885,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {884,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {883,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {882,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {881,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {880,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {879,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {878,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {877,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {876,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {875,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {874,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {873,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {872,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {871,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {870,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {869,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {868,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {867,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {866,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {865,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {864,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {863,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {862,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {861,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {860,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {859,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {858,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {857,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {856,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {855,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {854,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {853,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {852,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {851,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {850,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {849,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {848,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {847,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {846,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {845,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {844,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {843,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {842,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {841,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {840,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {839,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {838,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {837,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {836,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {835,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {834,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {833,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {832,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {831,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {830,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {829,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {828,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {827,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {826,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {825,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {824,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {823,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {822,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {821,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {820,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {819,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {818,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {817,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {816,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {815,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {814,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {813,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {812,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {811,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {810,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {809,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {808,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {807,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {806,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {805,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {804,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {803,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {802,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {801,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {800,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {799,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {798,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {797,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {796,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {795,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {794,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {793,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {792,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {791,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {790,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {789,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {788,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {787,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {786,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {785,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {784,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {783,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {782,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {781,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {780,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {779,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {778,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {777,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {776,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {775,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {774,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {773,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {772,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {771,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {770,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {769,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {768,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {767,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {766,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {765,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {764,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {763,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {762,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {761,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {760,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {759,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {758,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {757,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {756,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {755,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {754,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {753,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {752,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {751,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {750,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {749,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {748,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {747,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {746,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {745,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {744,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {743,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {742,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {741,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {740,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {739,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {738,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {737,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {736,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {735,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {734,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {733,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {732,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {731,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {730,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {729,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {728,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {727,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {726,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {725,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {724,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {723,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {722,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {721,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {720,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {719,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {718,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {717,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {716,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {715,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {714,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {713,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {712,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {711,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {710,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {709,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {708,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {707,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {706,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {705,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {704,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {703,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {702,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {701,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {700,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {699,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {698,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {697,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {696,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {695,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {694,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {693,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {692,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {691,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {690,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {689,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {688,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {687,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {686,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {685,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {684,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {683,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {682,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {681,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {680,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {679,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {678,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {677,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {676,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {675,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {674,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {673,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {672,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {671,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {670,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {669,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {668,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {667,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {666,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {665,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {664,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {663,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {662,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {661,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {660,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {659,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {658,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {657,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {656,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {655,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {654,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {653,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {652,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {651,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {650,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {649,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {648,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {647,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {646,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {645,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {644,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {643,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {642,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {641,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {640,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {639,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {638,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {637,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {636,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {635,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {634,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {633,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {632,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {631,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {630,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {629,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {628,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {627,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {626,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {625,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {624,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {623,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {622,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {621,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {620,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {619,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {618,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {617,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {616,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {615,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {614,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {613,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {612,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {611,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {610,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {609,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {608,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {607,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {606,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {605,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {604,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {603,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {602,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {601,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {600,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {599,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {598,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {597,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {596,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {595,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {594,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {593,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {592,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {591,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {590,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {589,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {588,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {587,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {586,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {585,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {584,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {583,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {582,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {581,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {580,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {579,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {578,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {577,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {576,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {575,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {574,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {573,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {572,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {571,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {570,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {569,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {568,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {567,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {566,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {565,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {564,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {563,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {562,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {561,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {560,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {559,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {558,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {557,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {556,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {555,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {554,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {553,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {552,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {551,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {550,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {549,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {548,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {547,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {546,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {545,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {544,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {543,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {542,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {541,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {540,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {539,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {538,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {537,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {536,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {535,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {534,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {533,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {532,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {531,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {530,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {529,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {528,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {527,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {526,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {525,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {524,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {523,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {522,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {521,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {520,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {519,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {518,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {517,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {516,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {515,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {514,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {513,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {512,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {511,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {510,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {509,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {508,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {507,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {506,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {505,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {504,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {503,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {502,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {501,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {500,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {499,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {498,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {497,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {496,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {495,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {494,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {493,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {492,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {491,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {490,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {489,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {488,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {487,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {486,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {485,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {484,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {483,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {482,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {481,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {480,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {479,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {478,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {477,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {476,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {475,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {474,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {473,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {472,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {471,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {470,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {469,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {468,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {467,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {466,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {465,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {464,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {463,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {462,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {461,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {460,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {459,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {458,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {457,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {456,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {455,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {454,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {453,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {452,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {451,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {450,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {449,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {448,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {447,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {446,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {445,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {444,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {443,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {442,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {441,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {440,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {439,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {438,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {437,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {436,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {435,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {434,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {433,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {432,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {431,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {430,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {429,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {428,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {427,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {426,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {425,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {424,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {423,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {422,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {421,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {420,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {419,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {418,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {417,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {416,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {415,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {414,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {413,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {412,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {411,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {410,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {409,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {408,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {407,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {406,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {405,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {404,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {403,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {402,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {401,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {400,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {399,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {398,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {397,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {396,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {395,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {394,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {393,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {392,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {391,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {390,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {389,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {388,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {387,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {386,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {385,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {384,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {383,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {382,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {381,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {380,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {379,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {378,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {377,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {376,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {375,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {374,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {373,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {372,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {371,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {370,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {369,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {368,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {367,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {366,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {365,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {364,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {363,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {362,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {361,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {360,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {359,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {358,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {357,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {356,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {355,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {354,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {353,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {352,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {351,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {350,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {349,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {348,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {347,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {346,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {345,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {344,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {343,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {342,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {341,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {340,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {339,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {338,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {337,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {336,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {335,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {334,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {333,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {332,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {331,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {330,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {329,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {328,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {327,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {326,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {325,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {324,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {323,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {322,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {321,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {320,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {319,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {318,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {317,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {316,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {315,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {314,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {313,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {312,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {311,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {310,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {309,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {308,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {307,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {306,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {305,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {304,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {303,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {302,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {301,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {300,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {299,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {298,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {297,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {296,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {295,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {294,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {293,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {292,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {291,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {290,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {289,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {288,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {287,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {286,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {285,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {284,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {283,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {282,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {281,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {280,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {279,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {278,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {277,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {276,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {275,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {274,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {273,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {272,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {271,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {270,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {269,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {268,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {267,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {266,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {265,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {264,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {263,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {262,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {261,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {260,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {259,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {258,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {257,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {256,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {255,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {254,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {253,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {252,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {251,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {250,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {249,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {248,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {247,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {246,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {245,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {244,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {243,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {242,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {241,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {240,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {239,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {238,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {237,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {236,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {235,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {234,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {233,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {232,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {231,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {230,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {229,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {228,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {227,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {226,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {225,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {224,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {223,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {222,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {221,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {220,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {219,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {218,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {217,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {216,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {215,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {214,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {213,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {212,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {211,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {210,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {209,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {208,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {207,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {206,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {205,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {204,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {203,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {202,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {201,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {200,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {199,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {198,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {197,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {196,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {195,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {194,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {193,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {192,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {191,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {190,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {189,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {188,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {187,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {186,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {185,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {184,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {183,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {182,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {181,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {180,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {179,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {178,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {177,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {176,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {175,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {174,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {173,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {172,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {171,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {170,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {169,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {168,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {167,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {166,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {165,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {164,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {163,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {162,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {161,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {160,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {159,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {158,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {157,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {156,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {155,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {154,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {153,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {152,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {151,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {150,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {149,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {148,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {147,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {146,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {145,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {144,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {143,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {142,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {141,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {140,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {139,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {138,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {137,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {136,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {135,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {134,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {133,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {132,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {131,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {130,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {129,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {128,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {127,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {126,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {125,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {124,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {123,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {122,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {121,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {120,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {119,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {118,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {117,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {116,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {115,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {114,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {113,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {112,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {111,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {110,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {109,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {108,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {107,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {106,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {105,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {104,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {103,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {102,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {101,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {100,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {99,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {98,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {97,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {96,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {95,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {94,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {93,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {92,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {91,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {90,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {89,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {88,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {87,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {86,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {85,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {84,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {83,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {82,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {81,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {80,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {79,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {78,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {77,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {76,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {75,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {74,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {73,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {72,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {71,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {70,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {69,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {68,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {67,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {66,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {65,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {64,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {63,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {62,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {61,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {60,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {59,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {58,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {57,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {56,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {55,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {54,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {53,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {52,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {51,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {50,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {49,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {48,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {47,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {46,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {45,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {44,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {43,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {42,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {41,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {40,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {39,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {38,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {37,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {36,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {35,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {34,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {33,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {32,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {31,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {30,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {29,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {28,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {27,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {26,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {25,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {24,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {23,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {22,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {21,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {20,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {19,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {18,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {17,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {16,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {15,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {14,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {13,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {12,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {11,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {10,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {9,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {8,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {7,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {6,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {5,active,{[{topology,[['ns_1@127.0.0.1']]}]}},
 {4,active,{[{topology,[[...]]}]}},
 {3,active,{[{topology,[...]}]}},
 {2,active,{[{topology,...}]}},
 {1,active,{[{...}]}},
 {0,...}]
[ns_server:info,2022-09-07T14:32:29.920Z,ns_1@127.0.0.1:ns_memcached-todo<0.10209.0>:ns_memcached:handle_call:294]Enabling traffic to bucket "todo"
[ns_server:info,2022-09-07T14:32:29.921Z,ns_1@127.0.0.1:ns_memcached-todo<0.10209.0>:ns_memcached:handle_call:298]Bucket "todo" marked as warmed in 3 seconds
[ns_server:debug,2022-09-07T14:32:29.960Z,ns_1@127.0.0.1:chronicle_kv_log<0.404.0>:chronicle_kv_log:log:59]update (key: {node,'ns_1@127.0.0.1',buckets_with_data}, rev: {<<"1933459bb8a062c61b619e931c483c80">>,
                                                              25})
[{"todo",<<"1d5e549a39e5fbca838b18cff4b8f22b">>}]
[ns_server:debug,2022-09-07T14:32:29.918Z,ns_1@127.0.0.1:<0.10244.0>:ns_memcached:verify_report_long_call:357]call {set_vbuckets,[{1023,active,[['ns_1@127.0.0.1']]},
                    {1022,active,[['ns_1@127.0.0.1']]},
                    {1021,active,[['ns_1@127.0.0.1']]},
                    {1020,active,[['ns_1@127.0.0.1']]},
                    {1019,active,[['ns_1@127.0.0.1']]},
                    {1018,active,[['ns_1@127.0.0.1']]},
                    {1017,active,[['ns_1@127.0.0.1']]},
                    {1016,active,[['ns_1@127.0.0.1']]},
                    {1015,active,[['ns_1@127.0.0.1']]},
                    {1014,active,[['ns_1@127.0.0.1']]},
                    {1013,active,[['ns_1@127.0.0.1']]},
                    {1012,active,[['ns_1@127.0.0.1']]},
                    {1011,active,[['ns_1@127.0.0.1']]},
                    {1010,active,[['ns_1@127.0.0.1']]},
                    {1009,active,[['ns_1@127.0.0.1']]},
                    {1008,active,[['ns_1@127.0.0.1']]},
                    {1007,active,[['ns_1@127.0.0.1']]},
                    {1006,active,[['ns_1@127.0.0.1']]},
                    {1005,active,[['ns_1@127.0.0.1']]},
                    {1004,active,[['ns_1@127.0.0.1']]},
                    {1003,active,[['ns_1@127.0.0.1']]},
                    {1002,active,[['ns_1@127.0.0.1']]},
                    {1001,active,[['ns_1@127.0.0.1']]},
                    {1000,active,[['ns_1@127.0.0.1']]},
                    {999,active,[['ns_1@127.0.0.1']]},
                    {998,active,[['ns_1@127.0.0.1']]},
                    {997,active,[['ns_1@127.0.0.1']]},
                    {996,active,[['ns_1@127.0.0.1']]},
                    {995,active,[['ns_1@127.0.0.1']]},
                    {994,active,[['ns_1@127.0.0.1']]},
                    {993,active,[['ns_1@127.0.0.1']]},
                    {992,active,[['ns_1@127.0.0.1']]},
                    {991,active,[['ns_1@127.0.0.1']]},
                    {990,active,[['ns_1@127.0.0.1']]},
                    {989,active,[['ns_1@127.0.0.1']]},
                    {988,active,[['ns_1@127.0.0.1']]},
                    {987,active,[['ns_1@127.0.0.1']]},
                    {986,active,[['ns_1@127.0.0.1']]},
                    {985,active,[['ns_1@127.0.0.1']]},
                    {984,active,[['ns_1@127.0.0.1']]},
                    {983,active,[['ns_1@127.0.0.1']]},
                    {982,active,[['ns_1@127.0.0.1']]},
                    {981,active,[['ns_1@127.0.0.1']]},
                    {980,active,[['ns_1@127.0.0.1']]},
                    {979,active,[['ns_1@127.0.0.1']]},
                    {978,active,[['ns_1@127.0.0.1']]},
                    {977,active,[['ns_1@127.0.0.1']]},
                    {976,active,[['ns_1@127.0.0.1']]},
                    {975,active,[['ns_1@127.0.0.1']]},
                    {974,active,[['ns_1@127.0.0.1']]},
                    {973,active,[['ns_1@127.0.0.1']]},
                    {972,active,[['ns_1@127.0.0.1']]},
                    {971,active,[['ns_1@127.0.0.1']]},
                    {970,active,[['ns_1@127.0.0.1']]},
                    {969,active,[['ns_1@127.0.0.1']]},
                    {968,active,[['ns_1@127.0.0.1']]},
                    {967,active,[['ns_1@127.0.0.1']]},
                    {966,active,[['ns_1@127.0.0.1']]},
                    {965,active,[['ns_1@127.0.0.1']]},
                    {964,active,[['ns_1@127.0.0.1']]},
                    {963,active,[['ns_1@127.0.0.1']]},
                    {962,active,[['ns_1@127.0.0.1']]},
                    {961,active,[['ns_1@127.0.0.1']]},
                    {960,active,[['ns_1@127.0.0.1']]},
                    {959,active,[['ns_1@127.0.0.1']]},
                    {958,active,[['ns_1@127.0.0.1']]},
                    {957,active,[['ns_1@127.0.0.1']]},
                    {956,active,[['ns_1@127.0.0.1']]},
                    {955,active,[['ns_1@127.0.0.1']]},
                    {954,active,[['ns_1@127.0.0.1']]},
                    {953,active,[['ns_1@127.0.0.1']]},
                    {952,active,[['ns_1@127.0.0.1']]},
                    {951,active,[['ns_1@127.0.0.1']]},
                    {950,active,[['ns_1@127.0.0.1']]},
                    {949,active,[['ns_1@127.0.0.1']]},
                    {948,active,[['ns_1@127.0.0.1']]},
                    {947,active,[['ns_1@127.0.0.1']]},
                    {946,active,[['ns_1@127.0.0.1']]},
                    {945,active,[['ns_1@127.0.0.1']]},
                    {944,active,[['ns_1@127.0.0.1']]},
                    {943,active,[['ns_1@127.0.0.1']]},
                    {942,active,[['ns_1@127.0.0.1']]},
                    {941,active,[['ns_1@127.0.0.1']]},
                    {940,active,[['ns_1@127.0.0.1']]},
                    {939,active,[['ns_1@127.0.0.1']]},
                    {938,active,[['ns_1@127.0.0.1']]},
                    {937,active,[['ns_1@127.0.0.1']]},
                    {936,active,[['ns_1@127.0.0.1']]},
                    {935,active,[['ns_1@127.0.0.1']]},
                    {934,active,[['ns_1@127.0.0.1']]},
                    {933,active,[['ns_1@127.0.0.1']]},
                    {932,active,[['ns_1@127.0.0.1']]},
                    {931,active,[['ns_1@127.0.0.1']]},
                    {930,active,[['ns_1@127.0.0.1']]},
                    {929,active,[['ns_1@127.0.0.1']]},
                    {928,active,[['ns_1@127.0.0.1']]},
                    {927,active,[['ns_1@127.0.0.1']]},
                    {926,active,[['ns_1@127.0.0.1']]},
                    {925,active,[['ns_1@127.0.0.1']]},
                    {924,active,[['ns_1@127.0.0.1']]},
                    {923,active,[['ns_1@127.0.0.1']]},
                    {922,active,[['ns_1@127.0.0.1']]},
                    {921,active,[['ns_1@127.0.0.1']]},
                    {920,active,[['ns_1@127.0.0.1']]},
                    {919,active,[['ns_1@127.0.0.1']]},
                    {918,active,[['ns_1@127.0.0.1']]},
                    {917,active,[['ns_1@127.0.0.1']]},
                    {916,active,[['ns_1@127.0.0.1']]},
                    {915,active,[['ns_1@127.0.0.1']]},
                    {914,active,[['ns_1@127.0.0.1']]},
                    {913,active,[['ns_1@127.0.0.1']]},
                    {912,active,[['ns_1@127.0.0.1']]},
                    {911,active,[['ns_1@127.0.0.1']]},
                    {910,active,[['ns_1@127.0.0.1']]},
                    {909,active,[['ns_1@127.0.0.1']]},
                    {908,active,[['ns_1@127.0.0.1']]},
                    {907,active,[['ns_1@127.0.0.1']]},
                    {906,active,[['ns_1@127.0.0.1']]},
                    {905,active,[['ns_1@127.0.0.1']]},
                    {904,active,[['ns_1@127.0.0.1']]},
                    {903,active,[['ns_1@127.0.0.1']]},
                    {902,active,[['ns_1@127.0.0.1']]},
                    {901,active,[['ns_1@127.0.0.1']]},
                    {900,active,[['ns_1@127.0.0.1']]},
                    {899,active,[['ns_1@127.0.0.1']]},
                    {898,active,[['ns_1@127.0.0.1']]},
                    {897,active,[['ns_1@127.0.0.1']]},
                    {896,active,[['ns_1@127.0.0.1']]},
                    {895,active,[['ns_1@127.0.0.1']]},
                    {894,active,[['ns_1@127.0.0.1']]},
                    {893,active,[['ns_1@127.0.0.1']]},
                    {892,active,[['ns_1@127.0.0.1']]},
                    {891,active,[['ns_1@127.0.0.1']]},
                    {890,active,[['ns_1@127.0.0.1']]},
                    {889,active,[['ns_1@127.0.0.1']]},
                    {888,active,[['ns_1@127.0.0.1']]},
                    {887,active,[['ns_1@127.0.0.1']]},
                    {886,active,[['ns_1@127.0.0.1']]},
                    {885,active,[['ns_1@127.0.0.1']]},
                    {884,active,[['ns_1@127.0.0.1']]},
                    {883,active,[['ns_1@127.0.0.1']]},
                    {882,active,[['ns_1@127.0.0.1']]},
                    {881,active,[['ns_1@127.0.0.1']]},
                    {880,active,[['ns_1@127.0.0.1']]},
                    {879,active,[['ns_1@127.0.0.1']]},
                    {878,active,[['ns_1@127.0.0.1']]},
                    {877,active,[['ns_1@127.0.0.1']]},
                    {876,active,[['ns_1@127.0.0.1']]},
                    {875,active,[['ns_1@127.0.0.1']]},
                    {874,active,[['ns_1@127.0.0.1']]},
                    {873,active,[['ns_1@127.0.0.1']]},
                    {872,active,[['ns_1@127.0.0.1']]},
                    {871,active,[['ns_1@127.0.0.1']]},
                    {870,active,[['ns_1@127.0.0.1']]},
                    {869,active,[['ns_1@127.0.0.1']]},
                    {868,active,[['ns_1@127.0.0.1']]},
                    {867,active,[['ns_1@127.0.0.1']]},
                    {866,active,[['ns_1@127.0.0.1']]},
                    {865,active,[['ns_1@127.0.0.1']]},
                    {864,active,[['ns_1@127.0.0.1']]},
                    {863,active,[['ns_1@127.0.0.1']]},
                    {862,active,[['ns_1@127.0.0.1']]},
                    {861,active,[['ns_1@127.0.0.1']]},
                    {860,active,[['ns_1@127.0.0.1']]},
                    {859,active,[['ns_1@127.0.0.1']]},
                    {858,active,[['ns_1@127.0.0.1']]},
                    {857,active,[['ns_1@127.0.0.1']]},
                    {856,active,[['ns_1@127.0.0.1']]},
                    {855,active,[['ns_1@127.0.0.1']]},
                    {854,active,[['ns_1@127.0.0.1']]},
                    {853,active,[['ns_1@127.0.0.1']]},
                    {852,active,[['ns_1@127.0.0.1']]},
                    {851,active,[['ns_1@127.0.0.1']]},
                    {850,active,[['ns_1@127.0.0.1']]},
                    {849,active,[['ns_1@127.0.0.1']]},
                    {848,active,[['ns_1@127.0.0.1']]},
                    {847,active,[['ns_1@127.0.0.1']]},
                    {846,active,[['ns_1@127.0.0.1']]},
                    {845,active,[['ns_1@127.0.0.1']]},
                    {844,active,[['ns_1@127.0.0.1']]},
                    {843,active,[['ns_1@127.0.0.1']]},
                    {842,active,[['ns_1@127.0.0.1']]},
                    {841,active,[['ns_1@127.0.0.1']]},
                    {840,active,[['ns_1@127.0.0.1']]},
                    {839,active,[['ns_1@127.0.0.1']]},
                    {838,active,[['ns_1@127.0.0.1']]},
                    {837,active,[['ns_1@127.0.0.1']]},
                    {836,active,[['ns_1@127.0.0.1']]},
                    {835,active,[['ns_1@127.0.0.1']]},
                    {834,active,[['ns_1@127.0.0.1']]},
                    {833,active,[['ns_1@127.0.0.1']]},
                    {832,active,[['ns_1@127.0.0.1']]},
                    {831,active,[['ns_1@127.0.0.1']]},
                    {830,active,[['ns_1@127.0.0.1']]},
                    {829,active,[['ns_1@127.0.0.1']]},
                    {828,active,[['ns_1@127.0.0.1']]},
                    {827,active,[['ns_1@127.0.0.1']]},
                    {826,active,[['ns_1@127.0.0.1']]},
                    {825,active,[['ns_1@127.0.0.1']]},
                    {824,active,[['ns_1@127.0.0.1']]},
                    {823,active,[['ns_1@127.0.0.1']]},
                    {822,active,[['ns_1@127.0.0.1']]},
                    {821,active,[['ns_1@127.0.0.1']]},
                    {820,active,[['ns_1@127.0.0.1']]},
                    {819,active,[['ns_1@127.0.0.1']]},
                    {818,active,[['ns_1@127.0.0.1']]},
                    {817,active,[['ns_1@127.0.0.1']]},
                    {816,active,[['ns_1@127.0.0.1']]},
                    {815,active,[['ns_1@127.0.0.1']]},
                    {814,active,[['ns_1@127.0.0.1']]},
                    {813,active,[['ns_1@127.0.0.1']]},
                    {812,active,[['ns_1@127.0.0.1']]},
                    {811,active,[['ns_1@127.0.0.1']]},
                    {810,active,[['ns_1@127.0.0.1']]},
                    {809,active,[['ns_1@127.0.0.1']]},
                    {808,active,[['ns_1@127.0.0.1']]},
                    {807,active,[['ns_1@127.0.0.1']]},
                    {806,active,[['ns_1@127.0.0.1']]},
                    {805,active,[['ns_1@127.0.0.1']]},
                    {804,active,[['ns_1@127.0.0.1']]},
                    {803,active,[['ns_1@127.0.0.1']]},
                    {802,active,[['ns_1@127.0.0.1']]},
                    {801,active,[['ns_1@127.0.0.1']]},
                    {800,active,[['ns_1@127.0.0.1']]},
                    {799,active,[['ns_1@127.0.0.1']]},
                    {798,active,[['ns_1@127.0.0.1']]},
                    {797,active,[['ns_1@127.0.0.1']]},
                    {796,active,[['ns_1@127.0.0.1']]},
                    {795,active,[['ns_1@127.0.0.1']]},
                    {794,active,[['ns_1@127.0.0.1']]},
                    {793,active,[['ns_1@127.0.0.1']]},
                    {792,active,[['ns_1@127.0.0.1']]},
                    {791,active,[['ns_1@127.0.0.1']]},
                    {790,active,[['ns_1@127.0.0.1']]},
                    {789,active,[['ns_1@127.0.0.1']]},
                    {788,active,[['ns_1@127.0.0.1']]},
                    {787,active,[['ns_1@127.0.0.1']]},
                    {786,active,[['ns_1@127.0.0.1']]},
                    {785,active,[['ns_1@127.0.0.1']]},
                    {784,active,[['ns_1@127.0.0.1']]},
                    {783,active,[['ns_1@127.0.0.1']]},
                    {782,active,[['ns_1@127.0.0.1']]},
                    {781,active,[['ns_1@127.0.0.1']]},
                    {780,active,[['ns_1@127.0.0.1']]},
                    {779,active,[['ns_1@127.0.0.1']]},
                    {778,active,[['ns_1@127.0.0.1']]},
                    {777,active,[['ns_1@127.0.0.1']]},
                    {776,active,[['ns_1@127.0.0.1']]},
                    {775,active,[['ns_1@127.0.0.1']]},
                    {774,active,[['ns_1@127.0.0.1']]},
                    {773,active,[['ns_1@127.0.0.1']]},
                    {772,active,[['ns_1@127.0.0.1']]},
                    {771,active,[['ns_1@127.0.0.1']]},
                    {770,active,[['ns_1@127.0.0.1']]},
                    {769,active,[['ns_1@127.0.0.1']]},
                    {768,active,[['ns_1@127.0.0.1']]},
                    {767,active,[['ns_1@127.0.0.1']]},
                    {766,active,[['ns_1@127.0.0.1']]},
                    {765,active,[['ns_1@127.0.0.1']]},
                    {764,active,[['ns_1@127.0.0.1']]},
                    {763,active,[['ns_1@127.0.0.1']]},
                    {762,active,[['ns_1@127.0.0.1']]},
                    {761,active,[['ns_1@127.0.0.1']]},
                    {760,active,[['ns_1@127.0.0.1']]},
                    {759,active,[['ns_1@127.0.0.1']]},
                    {758,active,[['ns_1@127.0.0.1']]},
                    {757,active,[['ns_1@127.0.0.1']]},
                    {756,active,[['ns_1@127.0.0.1']]},
                    {755,active,[['ns_1@127.0.0.1']]},
                    {754,active,[['ns_1@127.0.0.1']]},
                    {753,active,[['ns_1@127.0.0.1']]},
                    {752,active,[['ns_1@127.0.0.1']]},
                    {751,active,[['ns_1@127.0.0.1']]},
                    {750,active,[['ns_1@127.0.0.1']]},
                    {749,active,[['ns_1@127.0.0.1']]},
                    {748,active,[['ns_1@127.0.0.1']]},
                    {747,active,[['ns_1@127.0.0.1']]},
                    {746,active,[['ns_1@127.0.0.1']]},
                    {745,active,[['ns_1@127.0.0.1']]},
                    {744,active,[['ns_1@127.0.0.1']]},
                    {743,active,[['ns_1@127.0.0.1']]},
                    {742,active,[['ns_1@127.0.0.1']]},
                    {741,active,[['ns_1@127.0.0.1']]},
                    {740,active,[['ns_1@127.0.0.1']]},
                    {739,active,[['ns_1@127.0.0.1']]},
                    {738,active,[['ns_1@127.0.0.1']]},
                    {737,active,[['ns_1@127.0.0.1']]},
                    {736,active,[['ns_1@127.0.0.1']]},
                    {735,active,[['ns_1@127.0.0.1']]},
                    {734,active,[['ns_1@127.0.0.1']]},
                    {733,active,[['ns_1@127.0.0.1']]},
                    {732,active,[['ns_1@127.0.0.1']]},
                    {731,active,[['ns_1@127.0.0.1']]},
                    {730,active,[['ns_1@127.0.0.1']]},
                    {729,active,[['ns_1@127.0.0.1']]},
                    {728,active,[['ns_1@127.0.0.1']]},
                    {727,active,[['ns_1@127.0.0.1']]},
                    {726,active,[['ns_1@127.0.0.1']]},
                    {725,active,[['ns_1@127.0.0.1']]},
                    {724,active,[['ns_1@127.0.0.1']]},
                    {723,active,[['ns_1@127.0.0.1']]},
                    {722,active,[['ns_1@127.0.0.1']]},
                    {721,active,[['ns_1@127.0.0.1']]},
                    {720,active,[['ns_1@127.0.0.1']]},
                    {719,active,[['ns_1@127.0.0.1']]},
                    {718,active,[['ns_1@127.0.0.1']]},
                    {717,active,[['ns_1@127.0.0.1']]},
                    {716,active,[['ns_1@127.0.0.1']]},
                    {715,active,[['ns_1@127.0.0.1']]},
                    {714,active,[['ns_1@127.0.0.1']]},
                    {713,active,[['ns_1@127.0.0.1']]},
                    {712,active,[['ns_1@127.0.0.1']]},
                    {711,active,[['ns_1@127.0.0.1']]},
                    {710,active,[['ns_1@127.0.0.1']]},
                    {709,active,[['ns_1@127.0.0.1']]},
                    {708,active,[['ns_1@127.0.0.1']]},
                    {707,active,[['ns_1@127.0.0.1']]},
                    {706,active,[['ns_1@127.0.0.1']]},
                    {705,active,[['ns_1@127.0.0.1']]},
                    {704,active,[['ns_1@127.0.0.1']]},
                    {703,active,[['ns_1@127.0.0.1']]},
                    {702,active,[['ns_1@127.0.0.1']]},
                    {701,active,[['ns_1@127.0.0.1']]},
                    {700,active,[['ns_1@127.0.0.1']]},
                    {699,active,[['ns_1@127.0.0.1']]},
                    {698,active,[['ns_1@127.0.0.1']]},
                    {697,active,[['ns_1@127.0.0.1']]},
                    {696,active,[['ns_1@127.0.0.1']]},
                    {695,active,[['ns_1@127.0.0.1']]},
                    {694,active,[['ns_1@127.0.0.1']]},
                    {693,active,[['ns_1@127.0.0.1']]},
                    {692,active,[['ns_1@127.0.0.1']]},
                    {691,active,[['ns_1@127.0.0.1']]},
                    {690,active,[['ns_1@127.0.0.1']]},
                    {689,active,[['ns_1@127.0.0.1']]},
                    {688,active,[['ns_1@127.0.0.1']]},
                    {687,active,[['ns_1@127.0.0.1']]},
                    {686,active,[['ns_1@127.0.0.1']]},
                    {685,active,[['ns_1@127.0.0.1']]},
                    {684,active,[['ns_1@127.0.0.1']]},
                    {683,active,[['ns_1@127.0.0.1']]},
                    {682,active,[['ns_1@127.0.0.1']]},
                    {681,active,[['ns_1@127.0.0.1']]},
                    {680,active,[['ns_1@127.0.0.1']]},
                    {679,active,[['ns_1@127.0.0.1']]},
                    {678,active,[['ns_1@127.0.0.1']]},
                    {677,active,[['ns_1@127.0.0.1']]},
                    {676,active,[['ns_1@127.0.0.1']]},
                    {675,active,[['ns_1@127.0.0.1']]},
                    {674,active,[['ns_1@127.0.0.1']]},
                    {673,active,[['ns_1@127.0.0.1']]},
                    {672,active,[['ns_1@127.0.0.1']]},
                    {671,active,[['ns_1@127.0.0.1']]},
                    {670,active,[['ns_1@127.0.0.1']]},
                    {669,active,[['ns_1@127.0.0.1']]},
                    {668,active,[['ns_1@127.0.0.1']]},
                    {667,active,[['ns_1@127.0.0.1']]},
                    {666,active,[['ns_1@127.0.0.1']]},
                    {665,active,[['ns_1@127.0.0.1']]},
                    {664,active,[['ns_1@127.0.0.1']]},
                    {663,active,[['ns_1@127.0.0.1']]},
                    {662,active,[['ns_1@127.0.0.1']]},
                    {661,active,[['ns_1@127.0.0.1']]},
                    {660,active,[['ns_1@127.0.0.1']]},
                    {659,active,[['ns_1@127.0.0.1']]},
                    {658,active,[['ns_1@127.0.0.1']]},
                    {657,active,[['ns_1@127.0.0.1']]},
                    {656,active,[['ns_1@127.0.0.1']]},
                    {655,active,[['ns_1@127.0.0.1']]},
                    {654,active,[['ns_1@127.0.0.1']]},
                    {653,active,[['ns_1@127.0.0.1']]},
                    {652,active,[['ns_1@127.0.0.1']]},
                    {651,active,[['ns_1@127.0.0.1']]},
                    {650,active,[['ns_1@127.0.0.1']]},
                    {649,active,[['ns_1@127.0.0.1']]},
                    {648,active,[['ns_1@127.0.0.1']]},
                    {647,active,[['ns_1@127.0.0.1']]},
                    {646,active,[['ns_1@127.0.0.1']]},
                    {645,active,[['ns_1@127.0.0.1']]},
                    {644,active,[['ns_1@127.0.0.1']]},
                    {643,active,[['ns_1@127.0.0.1']]},
                    {642,active,[['ns_1@127.0.0.1']]},
                    {641,active,[['ns_1@127.0.0.1']]},
                    {640,active,[['ns_1@127.0.0.1']]},
                    {639,active,[['ns_1@127.0.0.1']]},
                    {638,active,[['ns_1@127.0.0.1']]},
                    {637,active,[['ns_1@127.0.0.1']]},
                    {636,active,[['ns_1@127.0.0.1']]},
                    {635,active,[['ns_1@127.0.0.1']]},
                    {634,active,[['ns_1@127.0.0.1']]},
                    {633,active,[['ns_1@127.0.0.1']]},
                    {632,active,[['ns_1@127.0.0.1']]},
                    {631,active,[['ns_1@127.0.0.1']]},
                    {630,active,[['ns_1@127.0.0.1']]},
                    {629,active,[['ns_1@127.0.0.1']]},
                    {628,active,[['ns_1@127.0.0.1']]},
                    {627,active,[['ns_1@127.0.0.1']]},
                    {626,active,[['ns_1@127.0.0.1']]},
                    {625,active,[['ns_1@127.0.0.1']]},
                    {624,active,[['ns_1@127.0.0.1']]},
                    {623,active,[['ns_1@127.0.0.1']]},
                    {622,active,[['ns_1@127.0.0.1']]},
                    {621,active,[['ns_1@127.0.0.1']]},
                    {620,active,[['ns_1@127.0.0.1']]},
                    {619,active,[['ns_1@127.0.0.1']]},
                    {618,active,[['ns_1@127.0.0.1']]},
                    {617,active,[['ns_1@127.0.0.1']]},
                    {616,active,[['ns_1@127.0.0.1']]},
                    {615,active,[['ns_1@127.0.0.1']]},
                    {614,active,[['ns_1@127.0.0.1']]},
                    {613,active,[['ns_1@127.0.0.1']]},
                    {612,active,[['ns_1@127.0.0.1']]},
                    {611,active,[['ns_1@127.0.0.1']]},
                    {610,active,[['ns_1@127.0.0.1']]},
                    {609,active,[['ns_1@127.0.0.1']]},
                    {608,active,[['ns_1@127.0.0.1']]},
                    {607,active,[['ns_1@127.0.0.1']]},
                    {606,active,[['ns_1@127.0.0.1']]},
                    {605,active,[['ns_1@127.0.0.1']]},
                    {604,active,[['ns_1@127.0.0.1']]},
                    {603,active,[['ns_1@127.0.0.1']]},
                    {602,active,[['ns_1@127.0.0.1']]},
                    {601,active,[['ns_1@127.0.0.1']]},
                    {600,active,[['ns_1@127.0.0.1']]},
                    {599,active,[['ns_1@127.0.0.1']]},
                    {598,active,[['ns_1@127.0.0.1']]},
                    {597,active,[['ns_1@127.0.0.1']]},
                    {596,active,[['ns_1@127.0.0.1']]},
                    {595,active,[['ns_1@127.0.0.1']]},
                    {594,active,[['ns_1@127.0.0.1']]},
                    {593,active,[['ns_1@127.0.0.1']]},
                    {592,active,[['ns_1@127.0.0.1']]},
                    {591,active,[['ns_1@127.0.0.1']]},
                    {590,active,[['ns_1@127.0.0.1']]},
                    {589,active,[['ns_1@127.0.0.1']]},
                    {588,active,[['ns_1@127.0.0.1']]},
                    {587,active,[['ns_1@127.0.0.1']]},
                    {586,active,[['ns_1@127.0.0.1']]},
                    {585,active,[['ns_1@127.0.0.1']]},
                    {584,active,[['ns_1@127.0.0.1']]},
                    {583,active,[['ns_1@127.0.0.1']]},
                    {582,active,[['ns_1@127.0.0.1']]},
                    {581,active,[['ns_1@127.0.0.1']]},
                    {580,active,[['ns_1@127.0.0.1']]},
                    {579,active,[['ns_1@127.0.0.1']]},
                    {578,active,[['ns_1@127.0.0.1']]},
                    {577,active,[['ns_1@127.0.0.1']]},
                    {576,active,[['ns_1@127.0.0.1']]},
                    {575,active,[['ns_1@127.0.0.1']]},
                    {574,active,[['ns_1@127.0.0.1']]},
                    {573,active,[['ns_1@127.0.0.1']]},
                    {572,active,[['ns_1@127.0.0.1']]},
                    {571,active,[['ns_1@127.0.0.1']]},
                    {570,active,[['ns_1@127.0.0.1']]},
                    {569,active,[['ns_1@127.0.0.1']]},
                    {568,active,[['ns_1@127.0.0.1']]},
                    {567,active,[['ns_1@127.0.0.1']]},
                    {566,active,[['ns_1@127.0.0.1']]},
                    {565,active,[['ns_1@127.0.0.1']]},
                    {564,active,[['ns_1@127.0.0.1']]},
                    {563,active,[['ns_1@127.0.0.1']]},
                    {562,active,[['ns_1@127.0.0.1']]},
                    {561,active,[['ns_1@127.0.0.1']]},
                    {560,active,[['ns_1@127.0.0.1']]},
                    {559,active,[['ns_1@127.0.0.1']]},
                    {558,active,[['ns_1@127.0.0.1']]},
                    {557,active,[['ns_1@127.0.0.1']]},
                    {556,active,[['ns_1@127.0.0.1']]},
                    {555,active,[['ns_1@127.0.0.1']]},
                    {554,active,[['ns_1@127.0.0.1']]},
                    {553,active,[['ns_1@127.0.0.1']]},
                    {552,active,[['ns_1@127.0.0.1']]},
                    {551,active,[['ns_1@127.0.0.1']]},
                    {550,active,[['ns_1@127.0.0.1']]},
                    {549,active,[['ns_1@127.0.0.1']]},
                    {548,active,[['ns_1@127.0.0.1']]},
                    {547,active,[['ns_1@127.0.0.1']]},
                    {546,active,[['ns_1@127.0.0.1']]},
                    {545,active,[['ns_1@127.0.0.1']]},
                    {544,active,[['ns_1@127.0.0.1']]},
                    {543,active,[['ns_1@127.0.0.1']]},
                    {542,active,[['ns_1@127.0.0.1']]},
                    {541,active,[['ns_1@127.0.0.1']]},
                    {540,active,[['ns_1@127.0.0.1']]},
                    {539,active,[['ns_1@127.0.0.1']]},
                    {538,active,[['ns_1@127.0.0.1']]},
                    {537,active,[['ns_1@127.0.0.1']]},
                    {536,active,[['ns_1@127.0.0.1']]},
                    {535,active,[['ns_1@127.0.0.1']]},
                    {534,active,[['ns_1@127.0.0.1']]},
                    {533,active,[['ns_1@127.0.0.1']]},
                    {532,active,[['ns_1@127.0.0.1']]},
                    {531,active,[['ns_1@127.0.0.1']]},
                    {530,active,[['ns_1@127.0.0.1']]},
                    {529,active,[['ns_1@127.0.0.1']]},
                    {528,active,[['ns_1@127.0.0.1']]},
                    {527,active,[['ns_1@127.0.0.1']]},
                    {526,active,[['ns_1@127.0.0.1']]},
                    {525,active,[['ns_1@127.0.0.1']]},
                    {524,active,[['ns_1@127.0.0.1']]},
                    {523,active,[['ns_1@127.0.0.1']]},
                    {522,active,[['ns_1@127.0.0.1']]},
                    {521,active,[['ns_1@127.0.0.1']]},
                    {520,active,[['ns_1@127.0.0.1']]},
                    {519,active,[['ns_1@127.0.0.1']]},
                    {518,active,[['ns_1@127.0.0.1']]},
                    {517,active,[['ns_1@127.0.0.1']]},
                    {516,active,[['ns_1@127.0.0.1']]},
                    {515,active,[['ns_1@127.0.0.1']]},
                    {514,active,[['ns_1@127.0.0.1']]},
                    {513,active,[['ns_1@127.0.0.1']]},
                    {512,active,[['ns_1@127.0.0.1']]},
                    {511,active,[['ns_1@127.0.0.1']]},
                    {510,active,[['ns_1@127.0.0.1']]},
                    {509,active,[['ns_1@127.0.0.1']]},
                    {508,active,[['ns_1@127.0.0.1']]},
                    {507,active,[['ns_1@127.0.0.1']]},
                    {506,active,[['ns_1@127.0.0.1']]},
                    {505,active,[['ns_1@127.0.0.1']]},
                    {504,active,[['ns_1@127.0.0.1']]},
                    {503,active,[['ns_1@127.0.0.1']]},
                    {502,active,[['ns_1@127.0.0.1']]},
                    {501,active,[['ns_1@127.0.0.1']]},
                    {500,active,[['ns_1@127.0.0.1']]},
                    {499,active,[['ns_1@127.0.0.1']]},
                    {498,active,[['ns_1@127.0.0.1']]},
                    {497,active,[['ns_1@127.0.0.1']]},
                    {496,active,[['ns_1@127.0.0.1']]},
                    {495,active,[['ns_1@127.0.0.1']]},
                    {494,active,[['ns_1@127.0.0.1']]},
                    {493,active,[['ns_1@127.0.0.1']]},
                    {492,active,[['ns_1@127.0.0.1']]},
                    {491,active,[['ns_1@127.0.0.1']]},
                    {490,active,[['ns_1@127.0.0.1']]},
                    {489,active,[['ns_1@127.0.0.1']]},
                    {488,active,[['ns_1@127.0.0.1']]},
                    {487,active,[['ns_1@127.0.0.1']]},
                    {486,active,[['ns_1@127.0.0.1']]},
                    {485,active,[['ns_1@127.0.0.1']]},
                    {484,active,[['ns_1@127.0.0.1']]},
                    {483,active,[['ns_1@127.0.0.1']]},
                    {482,active,[['ns_1@127.0.0.1']]},
                    {481,active,[['ns_1@127.0.0.1']]},
                    {480,active,[['ns_1@127.0.0.1']]},
                    {479,active,[['ns_1@127.0.0.1']]},
                    {478,active,[['ns_1@127.0.0.1']]},
                    {477,active,[['ns_1@127.0.0.1']]},
                    {476,active,[['ns_1@127.0.0.1']]},
                    {475,active,[['ns_1@127.0.0.1']]},
                    {474,active,[['ns_1@127.0.0.1']]},
                    {473,active,[['ns_1@127.0.0.1']]},
                    {472,active,[['ns_1@127.0.0.1']]},
                    {471,active,[['ns_1@127.0.0.1']]},
                    {470,active,[['ns_1@127.0.0.1']]},
                    {469,active,[['ns_1@127.0.0.1']]},
                    {468,active,[['ns_1@127.0.0.1']]},
                    {467,active,[['ns_1@127.0.0.1']]},
                    {466,active,[['ns_1@127.0.0.1']]},
                    {465,active,[['ns_1@127.0.0.1']]},
                    {464,active,[['ns_1@127.0.0.1']]},
                    {463,active,[['ns_1@127.0.0.1']]},
                    {462,active,[['ns_1@127.0.0.1']]},
                    {461,active,[['ns_1@127.0.0.1']]},
                    {460,active,[['ns_1@127.0.0.1']]},
                    {459,active,[['ns_1@127.0.0.1']]},
                    {458,active,[['ns_1@127.0.0.1']]},
                    {457,active,[['ns_1@127.0.0.1']]},
                    {456,active,[['ns_1@127.0.0.1']]},
                    {455,active,[['ns_1@127.0.0.1']]},
                    {454,active,[['ns_1@127.0.0.1']]},
                    {453,active,[['ns_1@127.0.0.1']]},
                    {452,active,[['ns_1@127.0.0.1']]},
                    {451,active,[['ns_1@127.0.0.1']]},
                    {450,active,[['ns_1@127.0.0.1']]},
                    {449,active,[['ns_1@127.0.0.1']]},
                    {448,active,[['ns_1@127.0.0.1']]},
                    {447,active,[['ns_1@127.0.0.1']]},
                    {446,active,[['ns_1@127.0.0.1']]},
                    {445,active,[['ns_1@127.0.0.1']]},
                    {444,active,[['ns_1@127.0.0.1']]},
                    {443,active,[['ns_1@127.0.0.1']]},
                    {442,active,[['ns_1@127.0.0.1']]},
                    {441,active,[['ns_1@127.0.0.1']]},
                    {440,active,[['ns_1@127.0.0.1']]},
                    {439,active,[['ns_1@127.0.0.1']]},
                    {438,active,[['ns_1@127.0.0.1']]},
                    {437,active,[['ns_1@127.0.0.1']]},
                    {436,active,[['ns_1@127.0.0.1']]},
                    {435,active,[['ns_1@127.0.0.1']]},
                    {434,active,[['ns_1@127.0.0.1']]},
                    {433,active,[['ns_1@127.0.0.1']]},
                    {432,active,[['ns_1@127.0.0.1']]},
                    {431,active,[['ns_1@127.0.0.1']]},
                    {430,active,[['ns_1@127.0.0.1']]},
                    {429,active,[['ns_1@127.0.0.1']]},
                    {428,active,[['ns_1@127.0.0.1']]},
                    {427,active,[['ns_1@127.0.0.1']]},
                    {426,active,[['ns_1@127.0.0.1']]},
                    {425,active,[['ns_1@127.0.0.1']]},
                    {424,active,[['ns_1@127.0.0.1']]},
                    {423,active,[['ns_1@127.0.0.1']]},
                    {422,active,[['ns_1@127.0.0.1']]},
                    {421,active,[['ns_1@127.0.0.1']]},
                    {420,active,[['ns_1@127.0.0.1']]},
                    {419,active,[['ns_1@127.0.0.1']]},
                    {418,active,[['ns_1@127.0.0.1']]},
                    {417,active,[['ns_1@127.0.0.1']]},
                    {416,active,[['ns_1@127.0.0.1']]},
                    {415,active,[['ns_1@127.0.0.1']]},
                    {414,active,[['ns_1@127.0.0.1']]},
                    {413,active,[['ns_1@127.0.0.1']]},
                    {412,active,[['ns_1@127.0.0.1']]},
                    {411,active,[['ns_1@127.0.0.1']]},
                    {410,active,[['ns_1@127.0.0.1']]},
                    {409,active,[['ns_1@127.0.0.1']]},
                    {408,active,[['ns_1@127.0.0.1']]},
                    {407,active,[['ns_1@127.0.0.1']]},
                    {406,active,[['ns_1@127.0.0.1']]},
                    {405,active,[['ns_1@127.0.0.1']]},
                    {404,active,[['ns_1@127.0.0.1']]},
                    {403,active,[['ns_1@127.0.0.1']]},
                    {402,active,[['ns_1@127.0.0.1']]},
                    {401,active,[['ns_1@127.0.0.1']]},
                    {400,active,[['ns_1@127.0.0.1']]},
                    {399,active,[['ns_1@127.0.0.1']]},
                    {398,active,[['ns_1@127.0.0.1']]},
                    {397,active,[['ns_1@127.0.0.1']]},
                    {396,active,[['ns_1@127.0.0.1']]},
                    {395,active,[['ns_1@127.0.0.1']]},
                    {394,active,[['ns_1@127.0.0.1']]},
                    {393,active,[['ns_1@127.0.0.1']]},
                    {392,active,[['ns_1@127.0.0.1']]},
                    {391,active,[['ns_1@127.0.0.1']]},
                    {390,active,[['ns_1@127.0.0.1']]},
                    {389,active,[['ns_1@127.0.0.1']]},
                    {388,active,[['ns_1@127.0.0.1']]},
                    {387,active,[['ns_1@127.0.0.1']]},
                    {386,active,[['ns_1@127.0.0.1']]},
                    {385,active,[['ns_1@127.0.0.1']]},
                    {384,active,[['ns_1@127.0.0.1']]},
                    {383,active,[['ns_1@127.0.0.1']]},
                    {382,active,[['ns_1@127.0.0.1']]},
                    {381,active,[['ns_1@127.0.0.1']]},
                    {380,active,[['ns_1@127.0.0.1']]},
                    {379,active,[['ns_1@127.0.0.1']]},
                    {378,active,[['ns_1@127.0.0.1']]},
                    {377,active,[['ns_1@127.0.0.1']]},
                    {376,active,[['ns_1@127.0.0.1']]},
                    {375,active,[['ns_1@127.0.0.1']]},
                    {374,active,[['ns_1@127.0.0.1']]},
                    {373,active,[['ns_1@127.0.0.1']]},
                    {372,active,[['ns_1@127.0.0.1']]},
                    {371,active,[['ns_1@127.0.0.1']]},
                    {370,active,[['ns_1@127.0.0.1']]},
                    {369,active,[['ns_1@127.0.0.1']]},
                    {368,active,[['ns_1@127.0.0.1']]},
                    {367,active,[['ns_1@127.0.0.1']]},
                    {366,active,[['ns_1@127.0.0.1']]},
                    {365,active,[['ns_1@127.0.0.1']]},
                    {364,active,[['ns_1@127.0.0.1']]},
                    {363,active,[['ns_1@127.0.0.1']]},
                    {362,active,[['ns_1@127.0.0.1']]},
                    {361,active,[['ns_1@127.0.0.1']]},
                    {360,active,[['ns_1@127.0.0.1']]},
                    {359,active,[['ns_1@127.0.0.1']]},
                    {358,active,[['ns_1@127.0.0.1']]},
                    {357,active,[['ns_1@127.0.0.1']]},
                    {356,active,[['ns_1@127.0.0.1']]},
                    {355,active,[['ns_1@127.0.0.1']]},
                    {354,active,[['ns_1@127.0.0.1']]},
                    {353,active,[['ns_1@127.0.0.1']]},
                    {352,active,[['ns_1@127.0.0.1']]},
                    {351,active,[['ns_1@127.0.0.1']]},
                    {350,active,[['ns_1@127.0.0.1']]},
                    {349,active,[['ns_1@127.0.0.1']]},
                    {348,active,[['ns_1@127.0.0.1']]},
                    {347,active,[['ns_1@127.0.0.1']]},
                    {346,active,[['ns_1@127.0.0.1']]},
                    {345,active,[['ns_1@127.0.0.1']]},
                    {344,active,[['ns_1@127.0.0.1']]},
                    {343,active,[['ns_1@127.0.0.1']]},
                    {342,active,[['ns_1@127.0.0.1']]},
                    {341,active,[['ns_1@127.0.0.1']]},
                    {340,active,[['ns_1@127.0.0.1']]},
                    {339,active,[['ns_1@127.0.0.1']]},
                    {338,active,[['ns_1@127.0.0.1']]},
                    {337,active,[['ns_1@127.0.0.1']]},
                    {336,active,[['ns_1@127.0.0.1']]},
                    {335,active,[['ns_1@127.0.0.1']]},
                    {334,active,[['ns_1@127.0.0.1']]},
                    {333,active,[['ns_1@127.0.0.1']]},
                    {332,active,[['ns_1@127.0.0.1']]},
                    {331,active,[['ns_1@127.0.0.1']]},
                    {330,active,[['ns_1@127.0.0.1']]},
                    {329,active,[['ns_1@127.0.0.1']]},
                    {328,active,[['ns_1@127.0.0.1']]},
                    {327,active,[['ns_1@127.0.0.1']]},
                    {326,active,[['ns_1@127.0.0.1']]},
                    {325,active,[['ns_1@127.0.0.1']]},
                    {324,active,[['ns_1@127.0.0.1']]},
                    {323,active,[['ns_1@127.0.0.1']]},
                    {322,active,[['ns_1@127.0.0.1']]},
                    {321,active,[['ns_1@127.0.0.1']]},
                    {320,active,[['ns_1@127.0.0.1']]},
                    {319,active,[['ns_1@127.0.0.1']]},
                    {318,active,[['ns_1@127.0.0.1']]},
                    {317,active,[['ns_1@127.0.0.1']]},
                    {316,active,[['ns_1@127.0.0.1']]},
                    {315,active,[['ns_1@127.0.0.1']]},
                    {314,active,[['ns_1@127.0.0.1']]},
                    {313,active,[['ns_1@127.0.0.1']]},
                    {312,active,[['ns_1@127.0.0.1']]},
                    {311,active,[['ns_1@127.0.0.1']]},
                    {310,active,[['ns_1@127.0.0.1']]},
                    {309,active,[['ns_1@127.0.0.1']]},
                    {308,active,[['ns_1@127.0.0.1']]},
                    {307,active,[['ns_1@127.0.0.1']]},
                    {306,active,[['ns_1@127.0.0.1']]},
                    {305,active,[['ns_1@127.0.0.1']]},
                    {304,active,[['ns_1@127.0.0.1']]},
                    {303,active,[['ns_1@127.0.0.1']]},
                    {302,active,[['ns_1@127.0.0.1']]},
                    {301,active,[['ns_1@127.0.0.1']]},
                    {300,active,[['ns_1@127.0.0.1']]},
                    {299,active,[['ns_1@127.0.0.1']]},
                    {298,active,[['ns_1@127.0.0.1']]},
                    {297,active,[['ns_1@127.0.0.1']]},
                    {296,active,[['ns_1@127.0.0.1']]},
                    {295,active,[['ns_1@127.0.0.1']]},
                    {294,active,[['ns_1@127.0.0.1']]},
                    {293,active,[['ns_1@127.0.0.1']]},
                    {292,active,[['ns_1@127.0.0.1']]},
                    {291,active,[[...]]},
                    {290,active,[...]},
                    {289,active,...},
                    {...}|...]} took too long: 1058309 us
[ns_server:info,2022-09-07T14:32:30.178Z,ns_1@127.0.0.1:ns_doctor<0.8787.0>:ns_doctor:update_status:309]The following buckets became ready on node 'ns_1@127.0.0.1': ["todo"]
[ns_server:debug,2022-09-07T14:32:32.670Z,ns_1@127.0.0.1:compiled_roles_cache<0.362.0>:menelaus_roles:build_compiled_roles:998]Compile roles for user {"@backup",admin}
[ns_server:debug,2022-09-07T14:32:34.955Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{local_changes_count,<<"1678dfae96c38e07dff43c49b9f6967b">>} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{46,63829780354}}]}]
[ns_server:debug,2022-09-07T14:32:34.955Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{metakv,<<"/cbas/cluster/state/partitions/topology">>} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780354}}]}|
 <<"{\"version\":1,\"revision\":0,\"balanced\":true,\"ccNodeId\":\"1678dfae96c38e07dff43c49b9f6967b\",\"metadataPartition\":-1,\"numReplicas\":0,\"partitions\":[{\"id\":\"0\",\"origin\":\"1678dfae96c38e07dff43c49b9f6967b\",\"master\":\"1678dfae96c38e07dff43c49b9f6967b\",\"replicas\":[]},{\"id\":\"-1\",\"origin\":\"1678dfae96c38e07dff43c49b9f6967b\",\"master\":\"1678dfae96c38e07dff43c49b9f6967b\",\"replicas\":[]}]}">>]
[ns_server:debug,2022-09-07T14:32:34.957Z,ns_1@127.0.0.1:ns_config_rep<0.444.0>:ns_config_rep:do_push_keys:383]Replicating some config keys ([{local_changes_count,
                                   <<"1678dfae96c38e07dff43c49b9f6967b">>},
                               {metakv,
                                   <<"/cbas/cluster/state/partitions/topology">>}]..)
[ns_server:debug,2022-09-07T14:32:36.029Z,ns_1@127.0.0.1:ns_config_rep<0.444.0>:ns_config_rep:do_push_keys:383]Replicating some config keys ([{local_changes_count,
                                   <<"1678dfae96c38e07dff43c49b9f6967b">>},
                               {metakv,<<"/indexing/info/versionToken">>}]..)
[ns_server:debug,2022-09-07T14:32:36.029Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{local_changes_count,<<"1678dfae96c38e07dff43c49b9f6967b">>} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{47,63829780356}}]}]
[ns_server:debug,2022-09-07T14:32:36.029Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{metakv,<<"/indexing/info/versionToken">>} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780356}}]}|
 <<"{\"Version\":6}">>]
[ns_server:debug,2022-09-07T14:32:47.332Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:debug,2022-09-07T14:32:47.332Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-07T14:32:47.332Z,ns_1@127.0.0.1:<0.11392.0>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-07T14:32:47.333Z,ns_1@127.0.0.1:<0.11394.0>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 401684, disk size is 8443904
[ns_server:debug,2022-09-07T14:32:47.334Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-07T14:32:47.334Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:info,2022-09-07T14:32:47.334Z,ns_1@127.0.0.1:<0.11395.0>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-07T14:32:47.334Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-07T14:32:47.334Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-07T14:33:09.979Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{metakv,<<"/indexing/settings/config">>} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{6,63829780389}}]}|
 <<"{\"indexer.plasma.backIndex.enableInMemoryCompression\":true,\"indexer.plasma.backIndex.enablePageBloomFilter\":false,\"indexer.plasma.mainIndex.enableInMemoryCompression\":true,\"indexer.settings.allow_large_keys\":true,\"indexer.settings.bufferPoolBlockSize\":16384,\"indexer.settings.build.batch_size\":5,\"indexer.settings.compaction.abort_exceed_interval\":false,\"indexer.settings.compaction.check"...>>]
[ns_server:debug,2022-09-07T14:33:09.979Z,ns_1@127.0.0.1:ns_config_rep<0.444.0>:ns_config_rep:do_push_keys:383]Replicating some config keys ([{local_changes_count,
                                   <<"1678dfae96c38e07dff43c49b9f6967b">>},
                               {metakv,<<"/indexing/settings/config">>}]..)
[ns_server:debug,2022-09-07T14:33:09.979Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{local_changes_count,<<"1678dfae96c38e07dff43c49b9f6967b">>} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{48,63829780389}}]}]
[ns_server:debug,2022-09-07T14:33:09.982Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{local_changes_count,<<"1678dfae96c38e07dff43c49b9f6967b">>} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{49,63829780389}}]}]
[ns_server:debug,2022-09-07T14:33:09.983Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{metakv,<<"/indexing/settings/config/features/PlasmaInMemoryCompression">>} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780389}}]}|
 <<"{}">>]
[ns_server:debug,2022-09-07T14:33:09.983Z,ns_1@127.0.0.1:ns_config_rep<0.444.0>:ns_config_rep:do_push_keys:383]Replicating some config keys ([{local_changes_count,
                                   <<"1678dfae96c38e07dff43c49b9f6967b">>},
                               {metakv,
                                   <<"/indexing/settings/config/features/PlasmaInMemoryCompression">>}]..)
[ns_server:debug,2022-09-07T14:33:13.071Z,ns_1@127.0.0.1:ldap_auth_cache<0.354.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2022-09-07T14:33:17.335Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-07T14:33:17.336Z,ns_1@127.0.0.1:<0.12899.0>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-07T14:33:17.336Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:debug,2022-09-07T14:33:17.337Z,ns_1@127.0.0.1:<0.12901.0>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 401684, disk size is 8443904
[ns_server:debug,2022-09-07T14:33:17.337Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-07T14:33:17.337Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:info,2022-09-07T14:33:17.337Z,ns_1@127.0.0.1:<0.12902.0>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-07T14:33:17.338Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-07T14:33:17.338Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-07T14:33:31.956Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{local_changes_count,<<"1678dfae96c38e07dff43c49b9f6967b">>} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{50,63829780411}}]}]
[ns_server:debug,2022-09-07T14:33:31.957Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{metakv,<<"/indexing/ddl/commandToken/create/7024789625474220722/0">>} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780411}}]}|
 <<"{\"DefnId\":7024789625474220722,\"BucketUUID\":\"1d5e549a39e5fbca838b18cff4b8f22b\",\"ScopeId\":\"0\",\"CollectionId\":\"0\",\"Definitions\":{\"1678dfae96c38e07dff43c49b9f6967b\":[{\"defnId\":7024789625474220722,\"name\":\"#primary\",\"using\":\"GSI\",\"bucket\":\"todo\",\"isPrimary\":true,\"exprType\":\"N1QL\",\"partitionScheme\":\"SINGLE\",\"NumReplica2\":{\"HasValue\":true,\"Base\":0,\"Incr\":0,\"Decr\":0},\"Scope\":\"_default\",\"Collect"...>>]
[ns_server:debug,2022-09-07T14:33:31.958Z,ns_1@127.0.0.1:ns_config_rep<0.444.0>:ns_config_rep:do_push_keys:383]Replicating some config keys ([{local_changes_count,
                                   <<"1678dfae96c38e07dff43c49b9f6967b">>},
                               {metakv,
                                   <<"/indexing/ddl/commandToken/create/7024789625474220722/0">>}]..)
[ns_server:debug,2022-09-07T14:33:32.005Z,ns_1@127.0.0.1:ns_config_rep<0.444.0>:ns_config_rep:do_push_keys:383]Replicating some config keys ([{local_changes_count,
                                   <<"1678dfae96c38e07dff43c49b9f6967b">>},
                               {metakv,
                                   <<"/indexing/ddl/commandToken/create/7024789625474220722/0">>}]..)
[ns_server:debug,2022-09-07T14:33:32.005Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{metakv,<<"/indexing/ddl/commandToken/create/7024789625474220722/0">>} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780411}}]}|
 '_deleted']
[ns_server:debug,2022-09-07T14:33:32.005Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{local_changes_count,<<"1678dfae96c38e07dff43c49b9f6967b">>} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{51,63829780411}}]}]
[ns_server:debug,2022-09-07T14:33:47.338Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-07T14:33:47.338Z,ns_1@127.0.0.1:<0.14413.0>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-07T14:33:47.339Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:debug,2022-09-07T14:33:47.339Z,ns_1@127.0.0.1:<0.14415.0>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 401684, disk size is 8443904
[ns_server:debug,2022-09-07T14:33:47.340Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-07T14:33:47.340Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:info,2022-09-07T14:33:47.341Z,ns_1@127.0.0.1:<0.14416.0>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-07T14:33:47.341Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-07T14:33:47.341Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-07T14:34:17.341Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-07T14:34:17.342Z,ns_1@127.0.0.1:<0.15897.0>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-07T14:34:17.342Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:debug,2022-09-07T14:34:17.344Z,ns_1@127.0.0.1:<0.15899.0>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 401684, disk size is 8443904
[ns_server:debug,2022-09-07T14:34:17.344Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-07T14:34:17.344Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:info,2022-09-07T14:34:17.346Z,ns_1@127.0.0.1:<0.15900.0>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-07T14:34:17.347Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-07T14:34:17.347Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-07T14:34:28.072Z,ns_1@127.0.0.1:ldap_auth_cache<0.354.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2022-09-07T14:34:47.345Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-07T14:34:47.346Z,ns_1@127.0.0.1:<0.17382.0>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-07T14:34:47.347Z,ns_1@127.0.0.1:<0.17384.0>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 401684, disk size is 8443904
[ns_server:debug,2022-09-07T14:34:47.347Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-07T14:34:47.347Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-07T14:34:47.348Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-07T14:34:47.351Z,ns_1@127.0.0.1:<0.17385.0>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-07T14:34:47.351Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-07T14:34:47.351Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-07T14:35:09.113Z,ns_1@127.0.0.1:chronicle_kv_log<0.404.0>:chronicle_kv_log:log:59]update (key: ns_config_vclock_ts, rev: {<<"1933459bb8a062c61b619e931c483c80">>,
                                        30})
63829780209
[ns_server:debug,2022-09-07T14:35:09.113Z,ns_1@127.0.0.1:ns_config_rep<0.444.0>:ns_config_rep:handle_cast:173]Synchronized with merger in 23 us
[ns_server:debug,2022-09-07T14:35:09.113Z,ns_1@127.0.0.1:chronicle_kv_log<0.404.0>:chronicle_kv_log:log:59]update (key: ns_config_purger, rev: {<<"1933459bb8a062c61b619e931c483c80">>,
                                     30})
'ns_1@127.0.0.1'
[ns_server:debug,2022-09-07T14:35:09.114Z,ns_1@127.0.0.1:ns_config_rep<0.444.0>:ns_config_rep:handle_cast:173]Synchronized with merger in 13 us
[ns_server:debug,2022-09-07T14:35:09.115Z,ns_1@127.0.0.1:ns_config_rep<0.444.0>:ns_config_rep:do_push_keys:383]Replicating some config keys ([after_upgrade_cleanup,auto_reprovision_cfg,
                               counters,server_groups]..)
[ns_server:debug,2022-09-07T14:35:09.116Z,ns_1@127.0.0.1:ns_config_rep<0.444.0>:ns_config_rep:handle_cast:173]Synchronized with merger in 30 us
[ns_server:debug,2022-09-07T14:35:09.147Z,ns_1@127.0.0.1:chronicle_kv_log<0.404.0>:chronicle_kv_log:log:59]update (key: ns_config_purge_ts, rev: {<<"1933459bb8a062c61b619e931c483c80">>,
                                       31})
63829780209
[ns_server:debug,2022-09-07T14:35:09.148Z,ns_1@127.0.0.1:tombstone_agent<0.454.0>:tombstone_agent:purge:195]Purged 4 ns_config tombstone(s) up to timestamp 63829780209. Tombstones:
[after_upgrade_cleanup,counters,auto_reprovision_cfg,server_groups]
[ns_server:debug,2022-09-07T14:35:17.348Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-07T14:35:17.349Z,ns_1@127.0.0.1:<0.18879.0>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-07T14:35:17.351Z,ns_1@127.0.0.1:<0.18881.0>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 401684, disk size is 8443904
[ns_server:debug,2022-09-07T14:35:17.351Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-07T14:35:17.352Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-07T14:35:17.353Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-07T14:35:17.356Z,ns_1@127.0.0.1:<0.18882.0>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-07T14:35:17.356Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-07T14:35:17.356Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-07T14:35:43.073Z,ns_1@127.0.0.1:ldap_auth_cache<0.354.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2022-09-07T14:35:43.138Z,ns_1@127.0.0.1:roles_cache<0.365.0>:active_cache:renew:238]Starting roles_cache cache renewal
[ns_server:debug,2022-09-07T14:35:43.138Z,ns_1@127.0.0.1:roles_cache<0.365.0>:active_cache:renew:244]roles_cache cache renewal process originated 0 queries
[ns_server:debug,2022-09-07T14:35:47.353Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-07T14:35:47.353Z,ns_1@127.0.0.1:<0.20345.0>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-07T14:35:47.355Z,ns_1@127.0.0.1:<0.20347.0>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 401684, disk size is 8443904
[ns_server:debug,2022-09-07T14:35:47.355Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-07T14:35:47.355Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-07T14:35:47.357Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-07T14:35:47.359Z,ns_1@127.0.0.1:<0.20364.0>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-07T14:35:47.359Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-07T14:35:47.359Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-07T14:36:17.356Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-07T14:36:17.356Z,ns_1@127.0.0.1:<0.21833.0>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-07T14:36:17.358Z,ns_1@127.0.0.1:<0.21835.0>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 401684, disk size is 8443904
[ns_server:debug,2022-09-07T14:36:17.358Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-07T14:36:17.358Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-07T14:36:17.360Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-07T14:36:17.362Z,ns_1@127.0.0.1:<0.21836.0>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-07T14:36:17.363Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-07T14:36:17.363Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-07T14:36:47.359Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-07T14:36:47.359Z,ns_1@127.0.0.1:<0.23315.0>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-07T14:36:47.360Z,ns_1@127.0.0.1:<0.23317.0>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 401684, disk size is 8443904
[ns_server:debug,2022-09-07T14:36:47.360Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-07T14:36:47.360Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-07T14:36:47.364Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-07T14:36:47.365Z,ns_1@127.0.0.1:<0.23318.0>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-07T14:36:47.366Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-07T14:36:47.366Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-07T14:36:58.074Z,ns_1@127.0.0.1:ldap_auth_cache<0.354.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2022-09-07T14:37:09.177Z,ns_1@127.0.0.1:chronicle_kv_log<0.404.0>:chronicle_kv_log:log:59]update (key: ns_config_vclock_ts, rev: {<<"1933459bb8a062c61b619e931c483c80">>,
                                        33})
63829780329
[ns_server:debug,2022-09-07T14:37:09.177Z,ns_1@127.0.0.1:ns_config_rep<0.444.0>:ns_config_rep:handle_cast:173]Synchronized with merger in 40 us
[ns_server:debug,2022-09-07T14:37:09.178Z,ns_1@127.0.0.1:chronicle_kv_log<0.404.0>:chronicle_kv_log:log:59]update (key: ns_config_purger, rev: {<<"1933459bb8a062c61b619e931c483c80">>,
                                     33})
'ns_1@127.0.0.1'
[ns_server:debug,2022-09-07T14:37:09.178Z,ns_1@127.0.0.1:ns_config_rep<0.444.0>:ns_config_rep:handle_cast:173]Synchronized with merger in 139 us
[ns_server:debug,2022-09-07T14:37:09.179Z,ns_1@127.0.0.1:ns_config_rep<0.444.0>:ns_config_rep:do_push_keys:383]Replicating some config keys ([{node,'ns_1@127.0.0.1',membership}]..)
[ns_server:debug,2022-09-07T14:37:09.179Z,ns_1@127.0.0.1:ns_config_rep<0.444.0>:ns_config_rep:handle_cast:173]Synchronized with merger in 25 us
[ns_server:debug,2022-09-07T14:37:09.207Z,ns_1@127.0.0.1:chronicle_kv_log<0.404.0>:chronicle_kv_log:log:59]update (key: ns_config_purge_ts, rev: {<<"1933459bb8a062c61b619e931c483c80">>,
                                       34})
63829780329
[ns_server:debug,2022-09-07T14:37:09.207Z,ns_1@127.0.0.1:tombstone_agent<0.454.0>:tombstone_agent:purge:195]Purged 1 ns_config tombstone(s) up to timestamp 63829780329. Tombstones:
[{node,'ns_1@127.0.0.1',membership}]
[ns_server:debug,2022-09-07T14:37:17.362Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-07T14:37:17.362Z,ns_1@127.0.0.1:<0.24815.0>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-07T14:37:17.363Z,ns_1@127.0.0.1:<0.24817.0>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 401684, disk size is 8443904
[ns_server:debug,2022-09-07T14:37:17.363Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-07T14:37:17.363Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-07T14:37:17.367Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-07T14:37:17.368Z,ns_1@127.0.0.1:<0.24818.0>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-07T14:37:17.368Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-07T14:37:17.368Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-07T14:37:47.365Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-07T14:37:47.365Z,ns_1@127.0.0.1:<0.26299.0>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-07T14:37:47.366Z,ns_1@127.0.0.1:<0.26301.0>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 401684, disk size is 8443904
[ns_server:debug,2022-09-07T14:37:47.366Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-07T14:37:47.366Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-07T14:37:47.370Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-07T14:37:47.372Z,ns_1@127.0.0.1:<0.26302.0>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-07T14:37:47.372Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-07T14:37:47.372Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-07T14:38:09.236Z,ns_1@127.0.0.1:chronicle_kv_log<0.404.0>:chronicle_kv_log:log:59]update (key: ns_config_vclock_ts, rev: {<<"1933459bb8a062c61b619e931c483c80">>,
                                        36})
63829780389
[ns_server:debug,2022-09-07T14:38:09.236Z,ns_1@127.0.0.1:ns_config_rep<0.444.0>:ns_config_rep:handle_cast:173]Synchronized with merger in 4 us
[ns_server:debug,2022-09-07T14:38:09.236Z,ns_1@127.0.0.1:chronicle_kv_log<0.404.0>:chronicle_kv_log:log:59]update (key: ns_config_purger, rev: {<<"1933459bb8a062c61b619e931c483c80">>,
                                     36})
'ns_1@127.0.0.1'
[ns_server:debug,2022-09-07T14:38:09.236Z,ns_1@127.0.0.1:ns_config_rep<0.444.0>:ns_config_rep:handle_cast:173]Synchronized with merger in 10 us
[ns_server:debug,2022-09-07T14:38:09.237Z,ns_1@127.0.0.1:ns_config_rep<0.444.0>:ns_config_rep:do_push_keys:383]Replicating some config keys ([{metakv,
                                   <<"/eventing/rebalanceToken/a0af9b617919276996bda60f0d831b10">>},
                               {metakv,
                                   <<"/indexing/rebalance/RebalanceToken">>}]..)
[ns_server:debug,2022-09-07T14:38:09.237Z,ns_1@127.0.0.1:ns_config_rep<0.444.0>:ns_config_rep:handle_cast:173]Synchronized with merger in 14 us
[ns_server:debug,2022-09-07T14:38:09.264Z,ns_1@127.0.0.1:chronicle_kv_log<0.404.0>:chronicle_kv_log:log:59]update (key: ns_config_purge_ts, rev: {<<"1933459bb8a062c61b619e931c483c80">>,
                                       37})
63829780389
[ns_server:debug,2022-09-07T14:38:09.265Z,ns_1@127.0.0.1:tombstone_agent<0.454.0>:tombstone_agent:purge:195]Purged 2 ns_config tombstone(s) up to timestamp 63829780389. Tombstones:
[{metakv,<<"/eventing/rebalanceToken/a0af9b617919276996bda60f0d831b10">>},{metakv,<<"/indexing/rebalance/RebalanceToken">>}]
[ns_server:debug,2022-09-07T14:38:13.075Z,ns_1@127.0.0.1:ldap_auth_cache<0.354.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2022-09-07T14:38:13.097Z,ns_1@127.0.0.1:roles_cache<0.365.0>:active_cache:cleanup:258]Cache roles_cache cleanup: 0/0 records deleted
[ns_server:debug,2022-09-07T14:38:17.368Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-07T14:38:17.368Z,ns_1@127.0.0.1:<0.27776.0>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-07T14:38:17.369Z,ns_1@127.0.0.1:<0.27778.0>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 401684, disk size is 8443904
[ns_server:debug,2022-09-07T14:38:17.369Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-07T14:38:17.369Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-07T14:38:17.373Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-07T14:38:17.375Z,ns_1@127.0.0.1:<0.27779.0>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-07T14:38:17.375Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-07T14:38:17.375Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-07T14:38:47.370Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-07T14:38:47.371Z,ns_1@127.0.0.1:<0.29261.0>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-07T14:38:47.373Z,ns_1@127.0.0.1:<0.29263.0>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 401684, disk size is 8443904
[ns_server:debug,2022-09-07T14:38:47.373Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-07T14:38:47.373Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-07T14:38:47.377Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-07T14:38:47.380Z,ns_1@127.0.0.1:<0.29264.0>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-07T14:38:47.380Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-07T14:38:47.381Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-07T14:39:09.294Z,ns_1@127.0.0.1:chronicle_kv_log<0.404.0>:chronicle_kv_log:log:59]update (key: ns_config_vclock_ts, rev: {<<"1933459bb8a062c61b619e931c483c80">>,
                                        39})
63829780449
[ns_server:debug,2022-09-07T14:39:09.294Z,ns_1@127.0.0.1:chronicle_kv_log<0.404.0>:chronicle_kv_log:log:59]update (key: ns_config_purger, rev: {<<"1933459bb8a062c61b619e931c483c80">>,
                                     39})
'ns_1@127.0.0.1'
[ns_server:debug,2022-09-07T14:39:09.294Z,ns_1@127.0.0.1:ns_config_rep<0.444.0>:ns_config_rep:handle_cast:173]Synchronized with merger in 6 us
[ns_server:debug,2022-09-07T14:39:09.295Z,ns_1@127.0.0.1:ns_config_rep<0.444.0>:ns_config_rep:handle_cast:173]Synchronized with merger in 7 us
[ns_server:debug,2022-09-07T14:39:09.295Z,ns_1@127.0.0.1:ns_config_rep<0.444.0>:ns_config_rep:do_push_keys:383]Replicating some config keys ([{metakv,
                                   <<"/indexing/ddl/commandToken/create/7024789625474220722/0">>}]..)
[ns_server:debug,2022-09-07T14:39:09.296Z,ns_1@127.0.0.1:ns_config_rep<0.444.0>:ns_config_rep:handle_cast:173]Synchronized with merger in 23 us
[ns_server:debug,2022-09-07T14:39:09.324Z,ns_1@127.0.0.1:chronicle_kv_log<0.404.0>:chronicle_kv_log:log:59]update (key: ns_config_purge_ts, rev: {<<"1933459bb8a062c61b619e931c483c80">>,
                                       40})
63829780449
[ns_server:debug,2022-09-07T14:39:09.324Z,ns_1@127.0.0.1:tombstone_agent<0.454.0>:tombstone_agent:purge:195]Purged 1 ns_config tombstone(s) up to timestamp 63829780449. Tombstones:
[{metakv,<<"/indexing/ddl/commandToken/create/7024789625474220722/0">>}]
[ns_server:debug,2022-09-07T14:39:17.374Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-07T14:39:17.374Z,ns_1@127.0.0.1:<0.30761.0>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-07T14:39:17.375Z,ns_1@127.0.0.1:<0.30763.0>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 401684, disk size is 8443904
[ns_server:debug,2022-09-07T14:39:17.375Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-07T14:39:17.376Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-07T14:39:17.382Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-07T14:39:17.384Z,ns_1@127.0.0.1:<0.30764.0>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-07T14:39:17.384Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-07T14:39:17.384Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-07T14:39:28.076Z,ns_1@127.0.0.1:ldap_auth_cache<0.354.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2022-09-07T14:39:47.377Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-07T14:39:47.378Z,ns_1@127.0.0.1:<0.32246.0>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-07T14:39:47.379Z,ns_1@127.0.0.1:<0.32248.0>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 401684, disk size is 8443904
[ns_server:debug,2022-09-07T14:39:47.379Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-07T14:39:47.379Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-07T14:39:47.385Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-07T14:39:47.387Z,ns_1@127.0.0.1:<0.32249.0>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-07T14:39:47.387Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-07T14:39:47.387Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-07T14:40:17.381Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-07T14:40:17.382Z,ns_1@127.0.0.1:<0.969.1>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-07T14:40:17.383Z,ns_1@127.0.0.1:<0.971.1>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 401684, disk size is 8443904
[ns_server:debug,2022-09-07T14:40:17.383Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-07T14:40:17.384Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-07T14:40:17.388Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-07T14:40:17.391Z,ns_1@127.0.0.1:<0.972.1>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-07T14:40:17.391Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-07T14:40:17.391Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-07T14:40:43.077Z,ns_1@127.0.0.1:ldap_auth_cache<0.354.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2022-09-07T14:40:47.385Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-07T14:40:47.385Z,ns_1@127.0.0.1:<0.2454.1>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-07T14:40:47.387Z,ns_1@127.0.0.1:<0.2456.1>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 401684, disk size is 8443904
[ns_server:debug,2022-09-07T14:40:47.387Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-07T14:40:47.387Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-07T14:40:47.392Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-07T14:40:47.394Z,ns_1@127.0.0.1:<0.2457.1>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-07T14:40:47.394Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-07T14:40:47.394Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-07T14:41:17.388Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-07T14:41:17.389Z,ns_1@127.0.0.1:<0.3935.1>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-07T14:41:17.391Z,ns_1@127.0.0.1:<0.3937.1>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 401684, disk size is 8443904
[ns_server:debug,2022-09-07T14:41:17.391Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-07T14:41:17.391Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-07T14:41:17.395Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-07T14:41:17.398Z,ns_1@127.0.0.1:<0.3938.1>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-07T14:41:17.398Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-07T14:41:17.398Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-07T14:41:47.392Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-07T14:41:47.392Z,ns_1@127.0.0.1:<0.5420.1>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-07T14:41:47.394Z,ns_1@127.0.0.1:<0.5422.1>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 402073, disk size is 8448045
[ns_server:debug,2022-09-07T14:41:47.394Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-07T14:41:47.394Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-07T14:41:47.399Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-07T14:41:47.401Z,ns_1@127.0.0.1:<0.5423.1>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-07T14:41:47.401Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-07T14:41:47.401Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-07T14:41:58.078Z,ns_1@127.0.0.1:ldap_auth_cache<0.354.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2022-09-07T14:42:09.090Z,ns_1@127.0.0.1:prometheus_cfg<0.415.0>:prometheus_cfg:maybe_update_scrape_dynamic_intervals:941]Recalculating prometheus scrape intervals for high cardinality metrics
[ns_server:debug,2022-09-07T14:42:09.097Z,ns_1@127.0.0.1:prometheus_cfg<0.415.0>:prometheus_cfg:maybe_update_scrape_dynamic_intervals:969]New scrape intervals:
[]
Previous scrape intervals: 
undefined
CBCollect stats dir size estimation: 598691520
Calculated based on scrapes info:
[{backup,low_cardinality,0},
 {cbas,high_cardinality,0},
 {cbas,low_cardinality,14},
 {eventing,high_cardinality,0},
 {eventing,low_cardinality,1},
 {fts,high_cardinality,0},
 {fts,low_cardinality,25},
 {index,low_cardinality,2},
 {index,high_cardinality,24},
 {kv,low_cardinality,291},
 {kv,high_cardinality,1033},
 {n1ql,low_cardinality,33},
 {ns_server,low_cardinality,167},
 {ns_server,high_cardinality,359},
 {xdcr,low_cardinality,0}]
Raw intervals:
[]
[ns_server:debug,2022-09-07T14:42:09.097Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{local_changes_count,<<"1678dfae96c38e07dff43c49b9f6967b">>} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{52,63829780929}}]}]
[ns_server:debug,2022-09-07T14:42:09.097Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',stats_scrape_dynamic_intervals} ->
[{'_vclock',63829780449,
            [{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780929}}]}]
[ns_server:debug,2022-09-07T14:42:09.097Z,ns_1@127.0.0.1:ns_config_rep<0.444.0>:ns_config_rep:do_push_keys:383]Replicating some config keys ([{local_changes_count,
                                   <<"1678dfae96c38e07dff43c49b9f6967b">>},
                               {node,'ns_1@127.0.0.1',
                                   stats_scrape_dynamic_intervals}]..)
[ns_server:debug,2022-09-07T14:42:09.098Z,ns_1@127.0.0.1:prometheus_cfg<0.415.0>:prometheus_cfg:maybe_apply_new_settings:608]Settings didn't change, ignoring update
[ns_server:debug,2022-09-07T14:42:17.139Z,ns_1@127.0.0.1:ns_audit<0.597.0>:ns_audit:handle_call:148]Audit session_expired: [{timestamp,<<"2022-09-07T14:42:17.139Z">>},
                        {sessionid,<<"b3740d8cf0080a8b89291f496feba96c33ca5f53">>},
                        {real_userid,{[{domain,builtin},
                                       {user,<<"<ud>admin</ud>">>}]}}]
[ns_server:debug,2022-09-07T14:42:17.395Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-07T14:42:17.396Z,ns_1@127.0.0.1:<0.6923.1>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-07T14:42:17.398Z,ns_1@127.0.0.1:<0.6925.1>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 402073, disk size is 8448045
[ns_server:debug,2022-09-07T14:42:17.398Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-07T14:42:17.398Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-07T14:42:17.402Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-07T14:42:17.407Z,ns_1@127.0.0.1:<0.6926.1>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-07T14:42:17.407Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-07T14:42:17.407Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-07T14:42:47.400Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-07T14:42:47.400Z,ns_1@127.0.0.1:<0.8408.1>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-07T14:42:47.401Z,ns_1@127.0.0.1:<0.8410.1>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 402073, disk size is 8448045
[ns_server:debug,2022-09-07T14:42:47.401Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-07T14:42:47.401Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-07T14:42:47.408Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-07T14:42:47.409Z,ns_1@127.0.0.1:<0.8411.1>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-07T14:42:47.410Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-07T14:42:47.410Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-07T14:42:53.181Z,ns_1@127.0.0.1:ns_audit<0.597.0>:ns_audit:handle_call:148]Audit read_doc: [{local,{[{ip,<<"172.18.0.2">>},{port,8091}]}},
                 {remote,{[{ip,<<"172.18.0.1">>},{port,37808}]}},
                 {sessionid,<<"fd08c2c0c6c2d0a53aa547858c675dc16173404e">>},
                 {real_userid,{[{domain,builtin},
                                {user,<<"<ud>admin</ud>">>}]}},
                 {timestamp,<<"2022-09-07T14:42:53.181Z">>},
                 {doc_id,<<"<ud>9cd38dc3-2678-4318-8542-51749f26939e</ud>">>},
                 {bucket_name,<<"todo">>}]
[ns_server:debug,2022-09-07T14:42:58.658Z,ns_1@127.0.0.1:ns_audit<0.597.0>:ns_audit:handle_call:148]Audit mutate_doc: [{local,{[{ip,<<"172.18.0.2">>},{port,8091}]}},
                   {remote,{[{ip,<<"172.18.0.1">>},{port,37808}]}},
                   {sessionid,<<"fd08c2c0c6c2d0a53aa547858c675dc16173404e">>},
                   {real_userid,{[{domain,builtin},
                                  {user,<<"<ud>admin</ud>">>}]}},
                   {timestamp,<<"2022-09-07T14:42:58.658Z">>},
                   {operation,delete},
                   {doc_id,<<"<ud>9cd38dc3-2678-4318-8542-51749f26939e</ud>">>},
                   {bucket_name,<<"todo">>}]
[ns_server:debug,2022-09-07T14:43:13.079Z,ns_1@127.0.0.1:ldap_auth_cache<0.354.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2022-09-07T14:43:13.139Z,ns_1@127.0.0.1:roles_cache<0.365.0>:active_cache:renew:238]Starting roles_cache cache renewal
[ns_server:debug,2022-09-07T14:43:13.139Z,ns_1@127.0.0.1:roles_cache<0.365.0>:active_cache:renew:244]roles_cache cache renewal process originated 0 queries
[ns_server:debug,2022-09-07T14:43:17.402Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-07T14:43:17.402Z,ns_1@127.0.0.1:<0.9914.1>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-07T14:43:17.404Z,ns_1@127.0.0.1:<0.9916.1>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 402285, disk size is 8456282
[ns_server:debug,2022-09-07T14:43:17.404Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-07T14:43:17.404Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-07T14:43:17.411Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-07T14:43:17.413Z,ns_1@127.0.0.1:<0.9917.1>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-07T14:43:17.413Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-07T14:43:17.414Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-07T14:43:47.405Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-07T14:43:47.405Z,ns_1@127.0.0.1:<0.11397.1>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-07T14:43:47.406Z,ns_1@127.0.0.1:<0.11399.1>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 402285, disk size is 8456282
[ns_server:debug,2022-09-07T14:43:47.406Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-07T14:43:47.406Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-07T14:43:47.415Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-07T14:43:47.416Z,ns_1@127.0.0.1:<0.11400.1>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-07T14:43:47.417Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-07T14:43:47.417Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-07T14:44:17.407Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-07T14:44:17.407Z,ns_1@127.0.0.1:<0.12881.1>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-07T14:44:17.409Z,ns_1@127.0.0.1:<0.12883.1>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 402285, disk size is 8456282
[ns_server:debug,2022-09-07T14:44:17.409Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-07T14:44:17.409Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-07T14:44:17.418Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-07T14:44:17.420Z,ns_1@127.0.0.1:<0.12888.1>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-07T14:44:17.420Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-07T14:44:17.420Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-07T14:44:28.080Z,ns_1@127.0.0.1:ldap_auth_cache<0.354.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2022-09-07T14:44:47.410Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-07T14:44:47.410Z,ns_1@127.0.0.1:<0.14364.1>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-07T14:44:47.411Z,ns_1@127.0.0.1:<0.14366.1>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 402285, disk size is 8456282
[ns_server:debug,2022-09-07T14:44:47.411Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-07T14:44:47.411Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-07T14:44:47.421Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-07T14:44:47.422Z,ns_1@127.0.0.1:<0.14367.1>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-07T14:44:47.423Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-07T14:44:47.423Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-07T14:45:17.413Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-07T14:45:17.413Z,ns_1@127.0.0.1:<0.15853.1>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-07T14:45:17.414Z,ns_1@127.0.0.1:<0.15855.1>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 402285, disk size is 8456282
[ns_server:debug,2022-09-07T14:45:17.414Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-07T14:45:17.414Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-07T14:45:17.424Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-07T14:45:17.425Z,ns_1@127.0.0.1:<0.15856.1>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-07T14:45:17.426Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-07T14:45:17.426Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-07T14:45:43.081Z,ns_1@127.0.0.1:ldap_auth_cache<0.354.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2022-09-07T14:45:47.415Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-07T14:45:47.415Z,ns_1@127.0.0.1:<0.17336.1>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-07T14:45:47.416Z,ns_1@127.0.0.1:<0.17338.1>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 402285, disk size is 8456282
[ns_server:debug,2022-09-07T14:45:47.416Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-07T14:45:47.416Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-07T14:45:47.427Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-07T14:45:47.428Z,ns_1@127.0.0.1:<0.17341.1>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-07T14:45:47.429Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-07T14:45:47.429Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-07T14:46:17.417Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-07T14:46:17.418Z,ns_1@127.0.0.1:<0.18827.1>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-07T14:46:17.420Z,ns_1@127.0.0.1:<0.18829.1>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 402285, disk size is 8456282
[ns_server:debug,2022-09-07T14:46:17.420Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-07T14:46:17.420Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-07T14:46:17.430Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-07T14:46:17.433Z,ns_1@127.0.0.1:<0.18830.1>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-07T14:46:17.433Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-07T14:46:17.433Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-07T14:46:47.422Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-07T14:46:47.423Z,ns_1@127.0.0.1:<0.20312.1>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-07T14:46:47.425Z,ns_1@127.0.0.1:<0.20314.1>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 402285, disk size is 8456282
[ns_server:debug,2022-09-07T14:46:47.425Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-07T14:46:47.425Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-07T14:46:47.435Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-07T14:46:47.439Z,ns_1@127.0.0.1:<0.20315.1>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-07T14:46:47.441Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-07T14:46:47.441Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-07T14:46:58.082Z,ns_1@127.0.0.1:ldap_auth_cache<0.354.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2022-09-07T14:47:17.426Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-07T14:47:17.427Z,ns_1@127.0.0.1:<0.21801.1>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-07T14:47:17.428Z,ns_1@127.0.0.1:<0.21803.1>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 402285, disk size is 8456282
[ns_server:debug,2022-09-07T14:47:17.428Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-07T14:47:17.428Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-07T14:47:17.442Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-07T14:47:17.443Z,ns_1@127.0.0.1:<0.21804.1>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-07T14:47:17.444Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-07T14:47:17.444Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-07T14:47:32.160Z,ns_1@127.0.0.1:ns_audit<0.597.0>:ns_audit:handle_call:148]Audit session_expired: [{timestamp,<<"2022-09-07T14:47:32.160Z">>},
                        {sessionid,<<"9117501b0a6d5fcbdb56f811619e4332b92e0c48">>},
                        {real_userid,{[{domain,builtin},
                                       {user,<<"<ud>admin</ud>">>}]}}]
[ns_server:debug,2022-09-07T14:47:47.429Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-07T14:47:47.429Z,ns_1@127.0.0.1:<0.23288.1>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-07T14:47:47.431Z,ns_1@127.0.0.1:<0.23290.1>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 402285, disk size is 8456282
[ns_server:debug,2022-09-07T14:47:47.431Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-07T14:47:47.431Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-07T14:47:47.445Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-07T14:47:47.447Z,ns_1@127.0.0.1:<0.23291.1>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-07T14:47:47.448Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-07T14:47:47.448Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-07T14:48:13.083Z,ns_1@127.0.0.1:ldap_auth_cache<0.354.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2022-09-07T14:48:13.098Z,ns_1@127.0.0.1:roles_cache<0.365.0>:active_cache:cleanup:258]Cache roles_cache cleanup: 0/0 records deleted
[ns_server:debug,2022-09-07T14:48:17.432Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-07T14:48:17.433Z,ns_1@127.0.0.1:<0.24785.1>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-07T14:48:17.434Z,ns_1@127.0.0.1:<0.24787.1>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 402285, disk size is 8456282
[ns_server:debug,2022-09-07T14:48:17.434Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-07T14:48:17.434Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-07T14:48:17.449Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-07T14:48:17.451Z,ns_1@127.0.0.1:<0.24788.1>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-07T14:48:17.451Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-07T14:48:17.451Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-07T14:48:47.436Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-07T14:48:47.436Z,ns_1@127.0.0.1:<0.26267.1>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-07T14:48:47.438Z,ns_1@127.0.0.1:<0.26269.1>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 402285, disk size is 8456282
[ns_server:debug,2022-09-07T14:48:47.438Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-07T14:48:47.438Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-07T14:48:47.452Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-07T14:48:47.454Z,ns_1@127.0.0.1:<0.26270.1>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-07T14:48:47.454Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-07T14:48:47.455Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-07T14:49:17.439Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-07T14:49:17.439Z,ns_1@127.0.0.1:<0.27754.1>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-07T14:49:17.440Z,ns_1@127.0.0.1:<0.27756.1>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 402285, disk size is 8456282
[ns_server:debug,2022-09-07T14:49:17.441Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-07T14:49:17.441Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-07T14:49:17.456Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-07T14:49:17.457Z,ns_1@127.0.0.1:<0.27757.1>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-07T14:49:17.458Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-07T14:49:17.458Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-07T14:49:28.084Z,ns_1@127.0.0.1:ldap_auth_cache<0.354.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2022-09-07T14:49:47.442Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-07T14:49:47.442Z,ns_1@127.0.0.1:<0.29239.1>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-07T14:49:47.443Z,ns_1@127.0.0.1:<0.29241.1>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 402285, disk size is 8456282
[ns_server:debug,2022-09-07T14:49:47.444Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-07T14:49:47.444Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-07T14:49:47.459Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-07T14:49:47.461Z,ns_1@127.0.0.1:<0.29242.1>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-07T14:49:47.461Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-07T14:49:47.461Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-07T14:50:17.445Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-07T14:50:17.445Z,ns_1@127.0.0.1:<0.30730.1>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-07T14:50:17.447Z,ns_1@127.0.0.1:<0.30732.1>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 402285, disk size is 8456282
[ns_server:debug,2022-09-07T14:50:17.447Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-07T14:50:17.447Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-07T14:50:17.462Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-07T14:50:17.464Z,ns_1@127.0.0.1:<0.30733.1>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-07T14:50:17.464Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-07T14:50:17.464Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-07T14:50:43.085Z,ns_1@127.0.0.1:ldap_auth_cache<0.354.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2022-09-07T14:50:43.140Z,ns_1@127.0.0.1:roles_cache<0.365.0>:active_cache:renew:238]Starting roles_cache cache renewal
[ns_server:debug,2022-09-07T14:50:43.140Z,ns_1@127.0.0.1:roles_cache<0.365.0>:active_cache:renew:244]roles_cache cache renewal process originated 0 queries
[ns_server:debug,2022-09-07T14:50:47.448Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-07T14:50:47.448Z,ns_1@127.0.0.1:<0.32189.1>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-07T14:50:47.450Z,ns_1@127.0.0.1:<0.32191.1>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 402285, disk size is 8456282
[ns_server:debug,2022-09-07T14:50:47.450Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-07T14:50:47.450Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-07T14:50:47.465Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-07T14:50:47.467Z,ns_1@127.0.0.1:<0.32192.1>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-07T14:50:47.468Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-07T14:50:47.468Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-07T14:51:17.452Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-07T14:51:17.453Z,ns_1@127.0.0.1:<0.910.2>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-07T14:51:17.454Z,ns_1@127.0.0.1:<0.912.2>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 402285, disk size is 8456282
[ns_server:debug,2022-09-07T14:51:17.455Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-07T14:51:17.455Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-07T14:51:17.469Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-07T14:51:17.471Z,ns_1@127.0.0.1:<0.913.2>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-07T14:51:17.472Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-07T14:51:17.472Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-07T14:51:47.456Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-07T14:51:47.456Z,ns_1@127.0.0.1:<0.2395.2>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-07T14:51:47.457Z,ns_1@127.0.0.1:<0.2397.2>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 402285, disk size is 8456282
[ns_server:debug,2022-09-07T14:51:47.458Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-07T14:51:47.458Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-07T14:51:47.473Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-07T14:51:47.474Z,ns_1@127.0.0.1:<0.2398.2>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-07T14:51:47.475Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-07T14:51:47.475Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-07T14:51:58.086Z,ns_1@127.0.0.1:ldap_auth_cache<0.354.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2022-09-07T14:52:09.098Z,ns_1@127.0.0.1:prometheus_cfg<0.415.0>:prometheus_cfg:maybe_update_scrape_dynamic_intervals:941]Recalculating prometheus scrape intervals for high cardinality metrics
[ns_server:debug,2022-09-07T14:52:09.103Z,ns_1@127.0.0.1:prometheus_cfg<0.415.0>:prometheus_cfg:maybe_update_scrape_dynamic_intervals:958]Scrape intervals haven't changed:
[]
CBCollect stats dir size estimation: 642297600
Calculated based on scrapes info:
[{backup,low_cardinality,0},
 {cbas,high_cardinality,0},
 {cbas,low_cardinality,14},
 {eventing,high_cardinality,0},
 {eventing,low_cardinality,1},
 {fts,high_cardinality,0},
 {fts,low_cardinality,25},
 {index,low_cardinality,2},
 {index,high_cardinality,24},
 {kv,low_cardinality,291},
 {kv,high_cardinality,1153},
 {n1ql,low_cardinality,33},
 {ns_server,low_cardinality,167},
 {ns_server,high_cardinality,360},
 {xdcr,low_cardinality,0}]
Raw intervals:
[]
[ns_server:debug,2022-09-07T14:52:17.459Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-07T14:52:17.459Z,ns_1@127.0.0.1:<0.3889.2>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-07T14:52:17.461Z,ns_1@127.0.0.1:<0.3891.2>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 402285, disk size is 8456282
[ns_server:debug,2022-09-07T14:52:17.462Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-07T14:52:17.462Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-07T14:52:17.476Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-07T14:52:17.479Z,ns_1@127.0.0.1:<0.3892.2>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-07T14:52:17.480Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-07T14:52:17.480Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-07T14:52:32.181Z,ns_1@127.0.0.1:ns_audit<0.597.0>:ns_audit:handle_call:148]Audit session_expired: [{timestamp,<<"2022-09-07T14:52:32.181Z">>},
                        {sessionid,<<"fd08c2c0c6c2d0a53aa547858c675dc16173404e">>},
                        {real_userid,{[{domain,builtin},
                                       {user,<<"<ud>admin</ud>">>}]}}]
[ns_server:debug,2022-09-07T14:52:47.463Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-07T14:52:47.463Z,ns_1@127.0.0.1:<0.5376.2>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-07T14:52:47.464Z,ns_1@127.0.0.1:<0.5378.2>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 402285, disk size is 8456282
[ns_server:debug,2022-09-07T14:52:47.465Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-07T14:52:47.465Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-07T14:52:47.481Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-07T14:52:47.483Z,ns_1@127.0.0.1:<0.5379.2>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-07T14:52:47.483Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-07T14:52:47.483Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-07T14:53:13.087Z,ns_1@127.0.0.1:ldap_auth_cache<0.354.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2022-09-07T14:53:17.466Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-07T14:53:17.467Z,ns_1@127.0.0.1:<0.6866.2>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-07T14:53:17.469Z,ns_1@127.0.0.1:<0.6868.2>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 402285, disk size is 8456282
[ns_server:debug,2022-09-07T14:53:17.469Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-07T14:53:17.470Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-07T14:53:17.484Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-07T14:53:17.487Z,ns_1@127.0.0.1:<0.6869.2>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-07T14:53:17.487Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-07T14:53:17.487Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-07T14:53:47.471Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-07T14:53:47.471Z,ns_1@127.0.0.1:<0.8349.2>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-07T14:53:47.472Z,ns_1@127.0.0.1:<0.8351.2>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 402285, disk size is 8456282
[ns_server:debug,2022-09-07T14:53:47.472Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-07T14:53:47.472Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-07T14:53:47.488Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-07T14:53:47.490Z,ns_1@127.0.0.1:<0.8352.2>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-07T14:53:47.490Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-07T14:53:47.490Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-07T14:54:17.473Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-07T14:54:17.474Z,ns_1@127.0.0.1:<0.9834.2>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-07T14:54:17.476Z,ns_1@127.0.0.1:<0.9837.2>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 402285, disk size is 8456282
[ns_server:debug,2022-09-07T14:54:17.476Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-07T14:54:17.476Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-07T14:54:17.491Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-07T14:54:17.493Z,ns_1@127.0.0.1:<0.9841.2>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-07T14:54:17.494Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-07T14:54:17.494Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-07T14:54:28.088Z,ns_1@127.0.0.1:ldap_auth_cache<0.354.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2022-09-07T14:54:47.477Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-07T14:54:47.477Z,ns_1@127.0.0.1:<0.11309.2>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-07T14:54:47.478Z,ns_1@127.0.0.1:<0.11311.2>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 402285, disk size is 8456282
[ns_server:debug,2022-09-07T14:54:47.478Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-07T14:54:47.478Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-07T14:54:47.495Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-07T14:54:47.496Z,ns_1@127.0.0.1:<0.11317.2>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-07T14:54:47.497Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-07T14:54:47.497Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-07T14:55:17.479Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-07T14:55:17.480Z,ns_1@127.0.0.1:<0.12791.2>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-07T14:55:17.482Z,ns_1@127.0.0.1:<0.12793.2>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 402285, disk size is 8456282
[ns_server:debug,2022-09-07T14:55:17.482Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-07T14:55:17.482Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-07T14:55:17.498Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-07T14:55:17.501Z,ns_1@127.0.0.1:<0.12800.2>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-07T14:55:17.501Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-07T14:55:17.501Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-07T14:55:43.089Z,ns_1@127.0.0.1:ldap_auth_cache<0.354.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2022-09-07T14:55:47.483Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-07T14:55:47.483Z,ns_1@127.0.0.1:<0.14275.2>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-07T14:55:47.484Z,ns_1@127.0.0.1:<0.14277.2>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 402285, disk size is 8456282
[ns_server:debug,2022-09-07T14:55:47.484Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-07T14:55:47.484Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-07T14:55:47.503Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-07T14:55:47.504Z,ns_1@127.0.0.1:<0.14278.2>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-07T14:55:47.505Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-07T14:55:47.505Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-07T14:56:17.486Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-07T14:56:17.486Z,ns_1@127.0.0.1:<0.15769.2>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-07T14:56:17.488Z,ns_1@127.0.0.1:<0.15771.2>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 402285, disk size is 8456282
[ns_server:debug,2022-09-07T14:56:17.488Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-07T14:56:17.488Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-07T14:56:17.506Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-07T14:56:17.508Z,ns_1@127.0.0.1:<0.15772.2>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-07T14:56:17.508Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-07T14:56:17.508Z,ns_1@127.0.0.1:compaction_daemon<0.667.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:info,2022-09-08T12:07:38.571Z,nonode@nohost:<0.145.0>:ns_server:init_logging:120]Started & configured logging
[ns_server:info,2022-09-08T12:07:38.576Z,nonode@nohost:<0.145.0>:ns_server:log_pending:29]Static config terms:
[{error_logger_mf_dir,"/opt/couchbase/var/lib/couchbase/logs"},
 {path_config_bindir,"/opt/couchbase/bin"},
 {path_config_etcdir,"/opt/couchbase/etc/couchbase"},
 {path_config_libdir,"/opt/couchbase/lib"},
 {path_config_datadir,"/opt/couchbase/var/lib/couchbase"},
 {path_config_tmpdir,"/opt/couchbase/var/lib/couchbase/tmp"},
 {path_config_secdir,"/opt/couchbase/etc/security"},
 {nodefile,"/opt/couchbase/var/lib/couchbase/couchbase-server.node"},
 {loglevel_default,debug},
 {loglevel_couchdb,info},
 {loglevel_ns_server,debug},
 {loglevel_error_logger,debug},
 {loglevel_user,debug},
 {loglevel_menelaus,debug},
 {loglevel_ns_doctor,debug},
 {loglevel_stats,debug},
 {loglevel_rebalance,debug},
 {loglevel_cluster,debug},
 {loglevel_views,debug},
 {loglevel_mapreduce_errors,debug},
 {loglevel_xdcr,debug},
 {loglevel_access,info},
 {loglevel_cbas,debug},
 {disk_sink_opts,[{rotation,[{compress,true},
                             {size,41943040},
                             {num_files,10},
                             {buffer_size_max,52428800}]}]},
 {disk_sink_opts_json_rpc,[{rotation,[{compress,true},
                                      {size,41943040},
                                      {num_files,2},
                                      {buffer_size_max,52428800}]}]},
 {net_kernel_verbosity,10}]
[ns_server:warn,2022-09-08T12:07:38.577Z,nonode@nohost:<0.145.0>:ns_server:log_pending:29]not overriding parameter error_logger_mf_dir, which is given from command line
[ns_server:warn,2022-09-08T12:07:38.577Z,nonode@nohost:<0.145.0>:ns_server:log_pending:29]not overriding parameter path_config_bindir, which is given from command line
[ns_server:warn,2022-09-08T12:07:38.577Z,nonode@nohost:<0.145.0>:ns_server:log_pending:29]not overriding parameter path_config_etcdir, which is given from command line
[ns_server:warn,2022-09-08T12:07:38.577Z,nonode@nohost:<0.145.0>:ns_server:log_pending:29]not overriding parameter path_config_libdir, which is given from command line
[ns_server:warn,2022-09-08T12:07:38.577Z,nonode@nohost:<0.145.0>:ns_server:log_pending:29]not overriding parameter path_config_datadir, which is given from command line
[ns_server:warn,2022-09-08T12:07:38.577Z,nonode@nohost:<0.145.0>:ns_server:log_pending:29]not overriding parameter path_config_tmpdir, which is given from command line
[ns_server:warn,2022-09-08T12:07:38.577Z,nonode@nohost:<0.145.0>:ns_server:log_pending:29]not overriding parameter path_config_secdir, which is given from command line
[ns_server:warn,2022-09-08T12:07:38.577Z,nonode@nohost:<0.145.0>:ns_server:log_pending:29]not overriding parameter nodefile, which is given from command line
[ns_server:warn,2022-09-08T12:07:38.577Z,nonode@nohost:<0.145.0>:ns_server:log_pending:29]not overriding parameter loglevel_default, which is given from command line
[ns_server:warn,2022-09-08T12:07:38.577Z,nonode@nohost:<0.145.0>:ns_server:log_pending:29]not overriding parameter loglevel_couchdb, which is given from command line
[ns_server:warn,2022-09-08T12:07:38.577Z,nonode@nohost:<0.145.0>:ns_server:log_pending:29]not overriding parameter loglevel_ns_server, which is given from command line
[ns_server:warn,2022-09-08T12:07:38.577Z,nonode@nohost:<0.145.0>:ns_server:log_pending:29]not overriding parameter loglevel_error_logger, which is given from command line
[ns_server:warn,2022-09-08T12:07:38.577Z,nonode@nohost:<0.145.0>:ns_server:log_pending:29]not overriding parameter loglevel_user, which is given from command line
[ns_server:warn,2022-09-08T12:07:38.577Z,nonode@nohost:<0.145.0>:ns_server:log_pending:29]not overriding parameter loglevel_menelaus, which is given from command line
[ns_server:warn,2022-09-08T12:07:38.577Z,nonode@nohost:<0.145.0>:ns_server:log_pending:29]not overriding parameter loglevel_ns_doctor, which is given from command line
[ns_server:warn,2022-09-08T12:07:38.578Z,nonode@nohost:<0.145.0>:ns_server:log_pending:29]not overriding parameter loglevel_stats, which is given from command line
[ns_server:warn,2022-09-08T12:07:38.578Z,nonode@nohost:<0.145.0>:ns_server:log_pending:29]not overriding parameter loglevel_rebalance, which is given from command line
[ns_server:warn,2022-09-08T12:07:38.578Z,nonode@nohost:<0.145.0>:ns_server:log_pending:29]not overriding parameter loglevel_cluster, which is given from command line
[ns_server:warn,2022-09-08T12:07:38.578Z,nonode@nohost:<0.145.0>:ns_server:log_pending:29]not overriding parameter loglevel_views, which is given from command line
[ns_server:warn,2022-09-08T12:07:38.578Z,nonode@nohost:<0.145.0>:ns_server:log_pending:29]not overriding parameter loglevel_mapreduce_errors, which is given from command line
[ns_server:warn,2022-09-08T12:07:38.578Z,nonode@nohost:<0.145.0>:ns_server:log_pending:29]not overriding parameter loglevel_xdcr, which is given from command line
[ns_server:warn,2022-09-08T12:07:38.578Z,nonode@nohost:<0.145.0>:ns_server:log_pending:29]not overriding parameter loglevel_access, which is given from command line
[ns_server:warn,2022-09-08T12:07:38.578Z,nonode@nohost:<0.145.0>:ns_server:log_pending:29]not overriding parameter loglevel_cbas, which is given from command line
[ns_server:warn,2022-09-08T12:07:38.578Z,nonode@nohost:<0.145.0>:ns_server:log_pending:29]not overriding parameter disk_sink_opts, which is given from command line
[ns_server:warn,2022-09-08T12:07:38.578Z,nonode@nohost:<0.145.0>:ns_server:log_pending:29]not overriding parameter disk_sink_opts_json_rpc, which is given from command line
[ns_server:warn,2022-09-08T12:07:38.578Z,nonode@nohost:<0.145.0>:ns_server:log_pending:29]not overriding parameter net_kernel_verbosity, which is given from command line
[ns_server:info,2022-09-08T12:07:38.596Z,nonode@nohost:dist_manager<0.197.0>:dist_manager:read_address_config_from_path:82]Reading ip config from "/opt/couchbase/var/lib/couchbase/ip_start"
[ns_server:info,2022-09-08T12:07:38.596Z,nonode@nohost:dist_manager<0.197.0>:dist_manager:read_address_config_from_path:82]Reading ip config from "/opt/couchbase/var/lib/couchbase/ip"
[error_logger:info,2022-09-08T12:07:38.604Z,nonode@nohost:inet_gethost_native_sup<0.198.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,inet_gethost_native_sup}
    started: [{pid,<0.199.0>},{mfa,{inet_gethost_native,init,[[]]}}]

[error_logger:info,2022-09-08T12:07:38.605Z,nonode@nohost:kernel_safe_sup<0.67.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,kernel_safe_sup}
    started: [{pid,<0.198.0>},
              {id,inet_gethost_native_sup},
              {mfargs,{inet_gethost_native,start_link,[]}},
              {restart_type,temporary},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:info,2022-09-08T12:07:38.633Z,nonode@nohost:dist_manager<0.197.0>:dist_manager:bringup:244]Attempting to bring up net_kernel with name 'ns_1@127.0.0.1'
[error_logger:info,2022-09-08T12:07:38.651Z,nonode@nohost:ssl_dist_admin_sup<0.202.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ssl_dist_admin_sup}
    started: [{pid,<0.203.0>},
              {id,ssl_pem_cache_dist},
              {mfargs,{ssl_pem_cache,start_link_dist,[[]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,4000},
              {child_type,worker}]

[error_logger:info,2022-09-08T12:07:38.651Z,nonode@nohost:ssl_dist_admin_sup<0.202.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ssl_dist_admin_sup}
    started: [{pid,<0.204.0>},
              {id,ssl_dist_manager},
              {mfargs,{ssl_manager,start_link_dist,[[]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,4000},
              {child_type,worker}]

[error_logger:info,2022-09-08T12:07:38.652Z,nonode@nohost:ssl_dist_sup<0.201.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ssl_dist_sup}
    started: [{pid,<0.202.0>},
              {id,ssl_dist_admin_sup},
              {mfargs,{ssl_dist_admin_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,4000},
              {child_type,supervisor}]

[error_logger:info,2022-09-08T12:07:38.654Z,nonode@nohost:tls_dist_sup<0.205.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,tls_dist_sup}
    started: [{pid,<0.206.0>},
              {id,dist_tls_connection},
              {mfargs,{tls_connection_sup,start_link_dist,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,4000},
              {child_type,supervisor}]

[error_logger:info,2022-09-08T12:07:38.657Z,nonode@nohost:tls_dist_server_sup<0.207.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,tls_dist_server_sup}
    started: [{pid,<0.208.0>},
              {id,dist_tls_socket},
              {mfargs,{ssl_listen_tracker_sup,start_link_dist,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,4000},
              {child_type,supervisor}]

[error_logger:info,2022-09-08T12:07:38.657Z,nonode@nohost:tls_dist_server_sup<0.207.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,tls_dist_server_sup}
    started: [{pid,<0.209.0>},
              {id,dist_tls_server_session_ticket},
              {mfargs,{tls_server_session_ticket_sup,start_link_dist,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,4000},
              {child_type,supervisor}]

[error_logger:info,2022-09-08T12:07:38.657Z,nonode@nohost:tls_dist_server_sup<0.207.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,tls_dist_server_sup}
    started: [{pid,<0.210.0>},
              {id,dist_ssl_server_session_cache_sup},
              {mfargs,
                  {ssl_upgrade_server_session_cache_sup,start_link_dist,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,4000},
              {child_type,supervisor}]

[error_logger:info,2022-09-08T12:07:38.657Z,nonode@nohost:tls_dist_sup<0.205.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,tls_dist_sup}
    started: [{pid,<0.207.0>},
              {id,dist_tls_server_sup},
              {mfargs,{tls_dist_server_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,4000},
              {child_type,supervisor}]

[error_logger:info,2022-09-08T12:07:38.658Z,nonode@nohost:ssl_dist_sup<0.201.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ssl_dist_sup}
    started: [{pid,<0.205.0>},
              {id,tls_dist_sup},
              {mfargs,{tls_dist_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,4000},
              {child_type,supervisor}]

[error_logger:info,2022-09-08T12:07:38.658Z,nonode@nohost:net_sup<0.200.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,net_sup}
    started: [{pid,<0.201.0>},
              {id,ssl_dist_sup},
              {mfargs,{ssl_dist_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2022-09-08T12:07:38.659Z,nonode@nohost:cb_dist<0.211.0>:cb_dist:info_msg:872]cb_dist: Starting cb_dist with config []
[error_logger:info,2022-09-08T12:07:38.674Z,nonode@nohost:net_sup<0.200.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,net_sup}
    started: [{pid,<0.211.0>},
              {id,cb_dist},
              {mfargs,{cb_dist,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,worker}]

[error_logger:info,2022-09-08T12:07:38.676Z,nonode@nohost:net_sup<0.200.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,net_sup}
    started: [{pid,<0.212.0>},
              {id,cb_epmd},
              {mfargs,{cb_epmd,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,2000},
              {child_type,worker}]

[error_logger:info,2022-09-08T12:07:38.678Z,nonode@nohost:net_sup<0.200.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,net_sup}
    started: [{pid,<0.213.0>},
              {id,auth},
              {mfargs,{auth,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,2000},
              {child_type,worker}]

[ns_server:debug,2022-09-08T12:07:38.680Z,nonode@nohost:cb_dist<0.211.0>:cb_dist:info_msg:872]cb_dist: Initial protos: [{external,inet_tcp_dist}], required protos: [{external,
                                                                        inet_tcp_dist}]
[ns_server:debug,2022-09-08T12:07:38.680Z,nonode@nohost:cb_dist<0.211.0>:cb_dist:info_msg:872]cb_dist: Starting {external,inet_tcp_dist} listener on 21100...
[ns_server:debug,2022-09-08T12:07:38.681Z,nonode@nohost:cb_dist<0.211.0>:cb_dist:info_msg:872]cb_dist: Started listener: inet_tcp_dist
[ns_server:debug,2022-09-08T12:07:38.727Z,ns_1@127.0.0.1:cb_dist<0.211.0>:cb_dist:info_msg:872]cb_dist: Started acceptor inet_tcp_dist: <0.216.0>
[error_logger:info,2022-09-08T12:07:38.727Z,ns_1@127.0.0.1:net_sup<0.200.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,net_sup}
    started: [{pid,<0.214.0>},
              {id,net_kernel},
              {mfargs,{net_kernel,start_link,
                                  [['ns_1@127.0.0.1',longnames],
                                   false,net_sup_dynamic]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,2000},
              {child_type,worker}]

[error_logger:info,2022-09-08T12:07:38.727Z,ns_1@127.0.0.1:kernel_sup<0.49.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,kernel_sup}
    started: [{pid,<0.200.0>},
              {id,net_sup_dynamic},
              {mfargs,
                  {erl_distribution,start_link,
                      [['ns_1@127.0.0.1',longnames],false,net_sup_dynamic]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,supervisor}]

[ns_server:debug,2022-09-08T12:07:38.727Z,ns_1@127.0.0.1:dist_manager<0.197.0>:dist_manager:configure_net_kernel:290]Set net_kernel vebosity to 10 -> 0
[ns_server:info,2022-09-08T12:07:38.733Z,ns_1@127.0.0.1:dist_manager<0.197.0>:dist_manager:save_node:158]saving node to "/opt/couchbase/var/lib/couchbase/couchbase-server.node"
[ns_server:debug,2022-09-08T12:07:38.753Z,ns_1@127.0.0.1:dist_manager<0.197.0>:dist_manager:bringup:260]Attempted to save node name to disk: ok
[ns_server:debug,2022-09-08T12:07:38.757Z,ns_1@127.0.0.1:dist_manager<0.197.0>:dist_manager:wait_for_node:267]Waiting for connection to node 'babysitter_of_ns_1@cb.local' to be established
[error_logger:info,2022-09-08T12:07:38.757Z,ns_1@127.0.0.1:net_kernel<0.214.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{connect,normal,'babysitter_of_ns_1@cb.local'}}
[ns_server:debug,2022-09-08T12:07:38.757Z,ns_1@127.0.0.1:net_kernel<0.214.0>:cb_dist:info_msg:872]cb_dist: Setting up new connection to 'babysitter_of_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2022-09-08T12:07:38.757Z,ns_1@127.0.0.1:cb_dist<0.211.0>:cb_dist:info_msg:872]cb_dist: Added connection {con,#Ref<0.1778819075.971767811.231191>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2022-09-08T12:07:38.757Z,ns_1@127.0.0.1:cb_dist<0.211.0>:cb_dist:info_msg:872]cb_dist: Updated connection: {con,#Ref<0.1778819075.971767811.231191>,
                                  inet_tcp_dist,<0.218.0>,
                                  #Ref<0.1778819075.971767811.231194>}
[ns_server:debug,2022-09-08T12:07:38.768Z,ns_1@127.0.0.1:dist_manager<0.197.0>:dist_manager:wait_for_node:279]Observed node 'babysitter_of_ns_1@cb.local' to come up
[ns_server:info,2022-09-08T12:07:38.769Z,ns_1@127.0.0.1:dist_manager<0.197.0>:dist_manager:save_address_config:145]Deleting irrelevant ip file "/opt/couchbase/var/lib/couchbase/ip_start": {error,
                                                                          enoent}
[ns_server:info,2022-09-08T12:07:38.769Z,ns_1@127.0.0.1:dist_manager<0.197.0>:dist_manager:save_address_config:146]saving ip config to "/opt/couchbase/var/lib/couchbase/ip"
[ns_server:info,2022-09-08T12:07:38.791Z,ns_1@127.0.0.1:dist_manager<0.197.0>:dist_manager:save_address_config:149]Persisted the address successfully
[error_logger:info,2022-09-08T12:07:38.799Z,ns_1@127.0.0.1:root_sup<0.196.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,root_sup}
    started: [{pid,<0.197.0>},
              {id,dist_manager},
              {mfargs,{dist_manager,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2022-09-08T12:07:38.805Z,ns_1@127.0.0.1:ns_server_cluster_sup<0.220.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_cluster_sup}
    started: [{pid,<0.221.0>},
              {id,local_tasks},
              {mfargs,{local_tasks,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[ns_server:info,2022-09-08T12:07:38.811Z,ns_1@127.0.0.1:ns_server_cluster_sup<0.220.0>:log_os_info:start_link:19]OS type: {unix,linux} Version: {5,10,16}
Runtime info: [{otp_release,"24"},
               {erl_version,"12.1.5"},
               {erl_version_long,
                   "Erlang/OTP 24 [erts-12.1.5] [source-38e24c5] [64-bit] [smp:8:8] [ds:8:8:10] [async-threads:16] [jit]\n"},
               {system_arch_raw,"x86_64-pc-linux-gnu"},
               {system_arch,"x86_64-pc-linux-gnu"},
               {localtime,{{2022,9,8},{12,7,38}}},
               {memory,
                   [{total,45229296},
                    {processes,11621144},
                    {processes_used,11612632},
                    {system,33608152},
                    {atom,532681},
                    {atom_used,501442},
                    {binary,103952},
                    {code,10393198},
                    {ets,2618088}]},
               {loaded,
                   [ns_info,log_os_info,local_tasks,restartable,
                    ns_server_cluster_sup,ns_cluster,dist_util,ns_node_disco,
                    crypto,re,auth,base64,pubkey_pem,public_key,
                    tls_dist_server_sup,tls_dist_sup,ssl_dist_admin_sup,
                    ssl_dist_sup,inet_tls_dist,inet_tcp_dist,inet_tcp,gen_tcp,
                    erl_epmd,cb_epmd,gen_udp,ale_error_logger_handler,
                    inet_hosts,dist_manager,root_sup,path_config,cb_dist,
                    ns_server_stats,calendar,ale_default_formatter,
                    'ale_logger-metakv','ale_logger-rebalance',
                    'ale_logger-chronicle','ale_logger-menelaus',
                    'ale_logger-stats','ale_logger-json_rpc',
                    'ale_logger-access','ale_logger-ns_server',
                    'ale_logger-user','ale_logger-ns_doctor',
                    'ale_logger-cluster','ale_logger-xdcr',erl_bits,
                    otp_internal,cb_log_counter_sink,ns_log_sink,
                    ale_disk_sink,misc,timer,couch_util,ns_server,cpu_sup,
                    filelib,memsup,disksup,os_mon,unicode_util,string,io,
                    release_handler,alarm_handler,sasl,httpd_sup,
                    httpc_handler_sup,httpc_cookie,inets_trace,httpc_manager,
                    httpc,httpc_profile_sup,httpc_sup,inets_sup,inets_app,ssl,
                    lhttpc_manager,lhttpc_sup,lhttpc,
                    dtls_server_session_cache_sup,dtls_listener_sup,
                    dtls_server_sup,dtls_connection_sup,dtls_sup,
                    ssl_upgrade_server_session_cache_sup,
                    ssl_server_session_cache_sup,
                    tls_server_session_ticket_sup,ssl_listen_tracker_sup,
                    tls_server_sup,tls_connection_sup,tls_sup,
                    ssl_connection_sup,tls_client_ticket_store,
                    ssl_client_session_cache_db,ssl_config,ssl_manager,
                    ssl_pkix_db,ssl_pem_cache,ssl_admin_sup,ssl_sup,
                    logger_h_common,logger_std_h,ssl_logger,ssl_app,
                    'ale_logger-ale_logger','ale_logger-error_logger',
                    beam_opcodes,beam_dict,beam_asm,beam_z,beam_flatten,
                    beam_trim,beam_clean,beam_peep,beam_block,beam_utils,
                    beam_jump,beam_a,beam_validator,beam_ssa_codegen,
                    beam_ssa_pre_codegen,beam_ssa_throw,beam_ssa_dead,
                    beam_call_types,beam_types,beam_ssa_type,beam_ssa_bc_size,
                    beam_ssa_opt,beam_ssa_funs,beam_ssa_bsm,beam_ssa_recv,
                    beam_ssa_share,beam_ssa_bool,beam_ssa,beam_kernel_to_ssa,
                    v3_kernel,sys_core_bsm,sys_core_alias,erl_bifs,
                    cerl_clauses,sets,sys_core_fold,sys_core_inline,
                    cerl_trees,core_lib,cerl,v3_core,erl_expand_records,sofs,
                    erl_internal,ordsets,compile,dynamic_compile,ale_utils,
                    io_lib_pretty,io_lib_format,io_lib,ale_codegen,dict,ale,
                    ale_dynamic_sup,ale_sup,ale_app,ns_bootstrap,child_erlang,
                    raw_file_io,orddict,c,erl_signal_handler,maps,
                    logger_handler_watcher,logger_sup,kernel_refc,
                    kernel_config,user_io,user_sup,supervisor_bridge,
                    standard_error,global_group,erl_distribution,net_kernel,
                    global,rpc,epp,inet_gethost_native,inet_parse,inet,
                    inet_udp,inet_config,inet_db,unicode,os,gb_trees,gb_sets,
                    binary,erl_anno,proplists,erl_scan,queue,logger_olp,
                    logger_proxy,error_handler,code_server,application_master,
                    application,file_server,error_logger,file_io_server,code,
                    heart,file,logger_filters,kernel,logger_config,filename,
                    ets,logger_backend,logger_server,logger_simple_h,logger,
                    gen_event,erl_eval,application_controller,proc_lib,
                    gen_server,gen,supervisor,lists,erl_parse,erl_lint,
                    persistent_term,counters,atomics,
                    erts_dirty_process_signal_handler,
                    erts_literal_area_collector,erl_tracer,erts_internal,
                    erlang,erl_prim_loader,prim_zip,prim_net,prim_socket,
                    socket_registry,zlib,prim_file,prim_inet,prim_eval,
                    prim_buffer,init,erl_init,erts_code_purger]},
               {applications,
                   [{lhttpc,"Lightweight HTTP Client","1.3.0"},
                    {sasl,"SASL  CXC 138 11","4.1"},
                    {asn1,"The Erlang ASN1 compiler version 5.0.17","5.0.17"},
                    {public_key,"Public key infrastructure","1.11.3"},
                    {crypto,"CRYPTO","5.0.4"},
                    {ssl,"Erlang/OTP SSL application","10.5.3"},
                    {os_mon,"CPO  CXC 138 46","2.7.1"},
                    {inets,"INETS  CXC 138 49","7.4.2"},
                    {stdlib,"ERTS  CXC 138 10","3.16.1"},
                    {kernel,"ERTS  CXC 138 10","8.1.3"},
                    {ns_server,"Couchbase server","7.1.1-3175-enterprise"},
                    {ale,"Another Logger for Erlang","0.0.0"}]},
               {pre_loaded,
                   [persistent_term,counters,atomics,
                    erts_dirty_process_signal_handler,
                    erts_literal_area_collector,erl_tracer,erts_internal,
                    erlang,erl_prim_loader,prim_zip,prim_net,prim_socket,
                    socket_registry,zlib,prim_file,prim_inet,prim_eval,
                    prim_buffer,init,erl_init,erts_code_purger]},
               {process_count,152},
               {node,'ns_1@127.0.0.1'},
               {nodes,[]},
               {registered,
                   [httpd_sup,ssl_dist_admin_sup,tls_server_sup,
                    ssl_connection_sup,sasl_safe_sup,cb_dist,
                    global_name_server,ssl_upgrade_server_session_cache_sup,
                    user,kernel_refc,inets_sup,tls_client_ticket_store,
                    'sink-disk_default',ns_server_cluster_sup,
                    tls_dist_connection_sup,dtls_server_session_cache_sup,
                    httpc_manager,tls_sup,'sink-disk_stats',rex,
                    tls_server_session_ticket_sup_dist,net_kernel,
                    standard_error_sup,local_tasks,'sink-disk_metakv',
                    root_sup,cpu_sup,ssl_dist_sup,ssl_sup,global_group,
                    dist_manager,file_server_2,ssl_listen_tracker_sup,
                    logger_sup,ale,dtls_sup,tls_dist_sup,
                    tls_server_session_ticket_sup,'sink-disk_xdcr',kernel_sup,
                    code_server,ssl_listen_tracker_sup_dist,ssl_manager,
                    erts_code_purger,'sink-disk_access_int',
                    inet_gethost_native,ale_sup,disksup,os_mon_sup,
                    httpc_profile_sup,erl_signal_server,standard_error,
                    ssl_pem_cache_dist,logger_proxy,'sink-cb_log_counter',
                    ssl_manager_dist,erl_prim_loader,'sink-disk_debug',
                    ssl_admin_sup,application_controller,socket_registry,
                    release_handler,'sink-ns_log',httpc_sup,
                    ssl_server_session_cache_sup,timer_server,inet_db,
                    'sink-disk_access',inet_gethost_native_sup,auth,
                    dtls_server_sup,init,lhttpc_manager,kernel_safe_sup,
                    ale_stats_events,alarm_handler,httpc_handler_sup,
                    tls_dist_server_sup,logger_handler_watcher,net_sup,
                    ssl_pem_cache,erl_epmd,dtls_connection_sup,
                    tls_connection_sup,logger,sasl_sup,memsup,
                    'sink-disk_error',lhttpc_sup,'sink-disk_reports',
                    ssl_upgrade_server_session_cache_sup_dist,ale_dynamic_sup,
                    logger_std_h_ssl_handler,dtls_listener_sup,
                    'sink-disk_json_rpc']},
               {cookie,nocookie},
               {wordsize,8},
               {wall_clock,4}]
[ns_server:info,2022-09-08T12:07:38.821Z,ns_1@127.0.0.1:ns_server_cluster_sup<0.220.0>:log_os_info:start_link:21]Manifest:
["<manifest>",
 "  <remote name=\"blevesearch\" fetch=\"https://github.com/blevesearch/\" />",
 "  <remote name=\"couchbase\" fetch=\"https://github.com/couchbase/\" review=\"review.couchbase.org\" />",
 "  <remote name=\"couchbase-priv\" fetch=\"ssh://git@github.com/couchbase/\" review=\"review.couchbase.org\" />",
 "  <remote name=\"couchbasedeps\" fetch=\"https://github.com/couchbasedeps/\" review=\"review.couchbase.org\" />",
 "  <remote name=\"couchbaselabs\" fetch=\"https://github.com/couchbaselabs/\" review=\"review.couchbase.org\" />",
 "  ","  <default remote=\"couchbase\" revision=\"master\" />","  ",
 "  <project name=\"HdrHistogram_c\" path=\"third_party/HdrHistogram_c\" remote=\"couchbasedeps\" revision=\"caed837aa163421a637222157b3f6353b4ca831a\" groups=\"kv\" />",
 "  <project name=\"analytics-dcp-client\" path=\"analytics/java-dcp-client\" revision=\"28a01945b085e906bdd25c5d35e92d3c8c87ff1a\" upstream=\"neo\" dest-branch=\"neo\" groups=\"notdefault,enterprise,analytics\" />",
 "  <project name=\"asterixdb\" path=\"analytics/asterixdb\" revision=\"344eeb2ebb60f453d6422aadc1bd310bc1339d1b\" groups=\"notdefault,enterprise,analytics\" />",
 "  <project name=\"backup\" remote=\"couchbase-priv\" revision=\"c0b230eba0c54681116f93d5806457089b24da2a\" groups=\"backup,notdefault,enterprise\" />",
 "  <project name=\"bbolt\" path=\"godeps/src/go.etcd.io/bbolt\" remote=\"couchbasedeps\" revision=\"68cc10a767ea1c6b9e8dcb9847317ff192d6d974\" />",
 "  <project name=\"bitset\" path=\"godeps/src/github.com/willf/bitset\" remote=\"couchbasedeps\" revision=\"28a4168144bb8ac95454e1f51c84da1933681ad4\" />",
 "  <project name=\"blance\" path=\"godeps/src/github.com/couchbase/blance\" revision=\"b2ec44a33677b3c64c516ed9e9c49721e266a58a\" />",
 "  <project name=\"bolt\" path=\"godeps/src/github.com/boltdb/bolt\" remote=\"couchbasedeps\" revision=\"51f99c862475898df9773747d3accd05a7ca33c1\" />",
 "  <project name=\"build\" path=\"cbbuild\" revision=\"e5d21de4a5e4ce048b7b330765529f71c6ff7e08\" upstream=\"neo\" dest-branch=\"neo\" groups=\"notdefault,build\">",
 "    <annotation name=\"RELEASE\" value=\"neo\" />",
 "    <annotation name=\"PRODUCT\" value=\"couchbase-server\" />",
 "    <annotation name=\"BLD_NUM\" value=\"3175\" />",
 "    <annotation name=\"VERSION\" value=\"7.1.1\" />",
 "    <annotation name=\"BSL_PRODUCT\" value=\"Couchbase Server\" />",
 "    <annotation name=\"BSL_VERSION\" value=\"7.1\" />",
 "    <annotation name=\"BSL_CHANGE_DATE\" value=\"April 1, 2026\" />",
 "  </project>",
 "  <project name=\"cbas\" path=\"goproj/src/github.com/couchbase/cbas\" remote=\"couchbase-priv\" revision=\"cf4628ae5f8b69a3402764b83aeae1668a9d7659\" upstream=\"neo\" dest-branch=\"neo\" groups=\"notdefault,enterprise,analytics\" />",
 "  <project name=\"cbas-core\" path=\"analytics\" remote=\"couchbase-priv\" revision=\"c41c19f8b112cff101a03e4b047738b7c2704d33\" groups=\"notdefault,enterprise,analytics\" />",
 "  <project name=\"cbas-ui\" revision=\"b6986a61412d59e297a62a93df4ac75bf8085dbe\" groups=\"notdefault,enterprise,analytics\" />",
 "  <project name=\"cbauth\" path=\"goproj/src/github.com/couchbase/cbauth\" revision=\"694763b234d64faaac75fbec427fc13132ccbcfe\" upstream=\"neo\" dest-branch=\"neo\" groups=\"backup\" />",
 "  <project name=\"cbbs\" remote=\"couchbase-priv\" revision=\"cf90ef81e93f36a09405211ee31661bbb78163ad\" groups=\"backup,notdefault,enterprise\" />",
 "  <project name=\"cbft\" revision=\"1e7554bde9f77b09429c7a2a506a2c7cbc215d8e\" upstream=\"7.1.1\" dest-branch=\"7.1.1\" />",
 "  <project name=\"cbftx\" remote=\"couchbase-priv\" revision=\"64469e336f363ed4d7b0fd51c6f54d0cd804752a\" groups=\"notdefault,enterprise\" />",
 "  <project name=\"cbgt\" revision=\"b7cd3e62474ad834adb44d9c4f0758cec77cd6d7\" />",
 "  <project name=\"cbsummary\" path=\"goproj/src/github.com/couchbase/cbsummary\" revision=\"083dc4dd2fd992caaaf3c9f1412ca3a8e87d0183\" />",
 "  <project name=\"chronicle\" path=\"ns_server/deps/chronicle\" revision=\"f53f803d409988d1bf3cdd2d7a5e9f55fb9dc2df\" />",
 "  <project name=\"clog\" path=\"godeps/src/github.com/couchbase/clog\" revision=\"f935d1fdfc36541b505cf86fea4822e4067f9c39\" />",
 "  <project name=\"cobra\" path=\"godeps/src/github.com/spf13/cobra\" remote=\"couchbasedeps\" revision=\"0f056af21f5f368e5b0646079d0094a2c64150f7\" />",
 "  <project name=\"context\" path=\"godeps/src/github.com/gorilla/context\" remote=\"couchbasedeps\" revision=\"215affda49addc4c8ef7e2534915df2c8c35c6cd\" />",
 "  <project name=\"couchbase-cli\" revision=\"7682cc259e6e129c80a583ffcb658495f7a7abd3\" groups=\"kv\" />",
 "  <project name=\"couchdb\" revision=\"27d1470742daa7829209763389f7d3ab1d1c8443\" />",
 "  <project name=\"couchdbx-app\" revision=\"64bdc899ba72d021a3c1dde1a1aa5b698f42ee06\" groups=\"notdefault,packaging\" />",
 "  <project name=\"couchstore\" revision=\"56b0f7b7a890a4896f321c5114c87412606120c4\" upstream=\"neo\" dest-branch=\"neo\" groups=\"kv\" />",
 "  <project name=\"crypto\" path=\"godeps/src/golang.org/x/crypto\" remote=\"couchbasedeps\" revision=\"bd6f299fb381e4c3393d1c4b1f0b94f5e77650c8\" />",
 "  <project name=\"cuckoofilter\" path=\"godeps/src/github.com/seiflotfy/cuckoofilter\" remote=\"couchbasedeps\" revision=\"d04838794ab86926d32b124345777e55e6f43974\" />",
 "  <project name=\"docloader\" path=\"goproj/src/github.com/couchbase/docloader\" revision=\"2e6af0c097c8bb98a596cbc81cdf6e169ae5b3cc\" />",
 "  <project name=\"errors\" path=\"godeps/src/github.com/pkg/errors\" remote=\"couchbasedeps\" revision=\"30136e27e2ac8d167177e8a583aa4c3fea5be833\" />",
 "  <project name=\"eventing\" path=\"goproj/src/github.com/couchbase/eventing\" revision=\"2f3487bea6ff76c09cc6acaa9fb5b03dd12e4c39\" upstream=\"7.1.1\" dest-branch=\"7.1.1\" groups=\"notdefault,enterprise\" />",
 "  <project name=\"eventing-ee\" path=\"goproj/src/github.com/couchbase/eventing-ee\" remote=\"couchbase-priv\" revision=\"14fd48dd7c506efac0607f3baa7e293646ed3fd0\" groups=\"notdefault,enterprise\" />",
 "  <project name=\"flatbuffers\" path=\"godeps/src/github.com/google/flatbuffers\" remote=\"couchbasedeps\" revision=\"1a8968225130caeddd16e227678e6f8af1926303\" />",
 "  <project name=\"forestdb\" revision=\"48c31dcb979ca7e152c2db570af49149c6d3e2a7\" groups=\"backup\" />",
 "  <project name=\"fwd\" path=\"godeps/src/github.com/philhofer/fwd\" remote=\"couchbasedeps\" revision=\"bb6d471dc95d4fe11e432687f8b70ff496cf3136\" />",
 "  <project name=\"geocouch\" revision=\"68f3b9d36630682d17ca5232770f1693b9b8fa18\" />",
 "  <project name=\"ghistogram\" path=\"godeps/src/github.com/couchbase/ghistogram\" revision=\"4ae3f06d0ac7b02081e33c1ec309daa22838d207\" />",
 "  <project name=\"go-bindata-assetfs\" path=\"godeps/src/github.com/elazarl/go-bindata-assetfs\" remote=\"couchbasedeps\" revision=\"57eb5e1fc594ad4b0b1dbea7b286d299e0cb43c2\" />",
 "  <project name=\"go-couchbase\" path=\"goproj/src/github.com/couchbase/go-couchbase\" revision=\"959eaf944140a6c660990f38b1db310ddd6d8e42\" groups=\"backup\" />",
 "  <project name=\"go-curl\" path=\"godeps/src/github.com/andelf/go-curl\" remote=\"couchbasedeps\" revision=\"f0b2afc926ec79be5d7f30393b3485352781a705\" upstream=\"20161221-couchbase\" dest-branch=\"20161221-couchbase\" />",
 "  <project name=\"go-curl\" path=\"godeps/src/github.com/couchbasedeps/go-curl\" remote=\"couchbasedeps\" revision=\"f0b2afc926ec79be5d7f30393b3485352781a705\" upstream=\"20161221-couchbase\" dest-branch=\"20161221-couchbase\" />",
 "  <project name=\"go-genproto\" path=\"godeps/src/google.golang.org/genproto\" remote=\"couchbasedeps\" revision=\"2b5a72b8730b0b16380010cfe5286c42108d88e7\" />",
 "  <project name=\"go-jsonpointer\" path=\"godeps/src/github.com/dustin/go-jsonpointer\" remote=\"couchbasedeps\" revision=\"75939f54b39e7dafae879e61f65438dadc5f288c\" />",
 "  <project name=\"go-metrics\" path=\"godeps/src/github.com/rcrowley/go-metrics\" remote=\"couchbasedeps\" revision=\"dee209f2455f101a5e4e593dea94872d2c62d85d\" />",
 "  <project name=\"go-runewidth\" path=\"godeps/src/github.com/mattn/go-runewidth\" remote=\"couchbasedeps\" revision=\"703b5e6b11ae25aeb2af9ebb5d5fdf8fa2575211\" />",
 "  <project name=\"go-slab\" path=\"godeps/src/github.com/couchbase/go-slab\" revision=\"e47646b420b3c9eb344cef022236a54e2554d40b\" groups=\"bsl\" />",
 "  <project name=\"go-unsnap-stream\" path=\"godeps/src/github.com/glycerine/go-unsnap-stream\" remote=\"couchbasedeps\" revision=\"62a9a9eb44fd8932157b1a8ace2149eff5971af6\" />",
 "  <project name=\"go-zookeeper\" path=\"godeps/src/github.com/samuel/go-zookeeper\" remote=\"couchbasedeps\" revision=\"fa6674abf3f4580b946a01bf7a1ce4ba8766205b\" />",
 "  <project name=\"go_json\" path=\"goproj/src/github.com/couchbase/go_json\" revision=\"39c6c3c3e21c5c0a0be9f696bdb9e496fde773f1\" />",
 "  <project name=\"go_n1ql\" path=\"godeps/src/github.com/couchbase/go_n1ql\" revision=\"0ed4bf93e31de2371f9180e424942bd3d5235397\" groups=\"bsl\" />",
 "  <project name=\"gocb\" path=\"godeps/src/github.com/couchbase/gocb/v2\" revision=\"40020eef484873e507745107edbf99c421927a93\" upstream=\"refs/tags/v2.2.5\" dest-branch=\"refs/tags/v2.2.5\" />",
 "  <project name=\"gocb\" path=\"godeps/src/gopkg.in/couchbase/gocb.v1\" revision=\"01c846cb025ddd50a2ef4c82a27992b40c230dbb\" upstream=\"refs/tags/v1.4.2\" dest-branch=\"refs/tags/v1.4.2\" />",
 "  <project name=\"gocbconnstr\" path=\"godeps/src/gopkg.in/couchbaselabs/gocbconnstr.v1\" remote=\"couchbaselabs\" revision=\"8f9a894d174b836c6362de9af75545cf585fc278\" />",
 "  <project name=\"gocbcore\" path=\"godeps/src/gopkg.in/couchbase/gocbcore.v7\" revision=\"441cb91f01ce26932514ec10d9e59e568ee27722\" upstream=\"refs/tags/v7.1.14\" dest-branch=\"refs/tags/v7.1.14\" />",
 "  <project name=\"gocbcore\" path=\"godeps/src/github.com/couchbase/gocbcore/v9\" revision=\"0ece206041d8cf5f5fcd919767446603691bdb69\" upstream=\"refs/tags/v9.1.6\" dest-branch=\"refs/tags/v9.1.6\" />",
 "  <project name=\"godbc\" path=\"goproj/src/github.com/couchbase/godbc\" revision=\"938b768d5e33f7b70c183db527b460d1648f5c52\" />",
 "  <project name=\"gofarmhash\" path=\"godeps/src/github.com/leemcloughlin/gofarmhash\" remote=\"couchbasedeps\" revision=\"0a055c5b87a8c55ce83459cbf2776b563822a942\" />",
 "  <project name=\"goforestdb\" path=\"godeps/src/github.com/couchbase/goforestdb\" revision=\"0b501227de0e8c55d99ed14e900eea1a1dbaf899\" />",
 "  <project name=\"gojson\" path=\"godeps/src/github.com/dustin/gojson\" remote=\"couchbasedeps\" revision=\"af16e0e771e2ed110f2785564ae33931de8829e4\" />",
 "  <project name=\"gojsonsm\" path=\"godeps/src/github.com/couchbaselabs/gojsonsm\" remote=\"couchbaselabs\" revision=\"eec4953dcb855282c483b8cd4fe03a8074e2f7a1\" />",
 "  <project name=\"golang-pkg-pcre\" path=\"godeps/src/github.com/glenn-brown/golang-pkg-pcre\" remote=\"couchbasedeps\" revision=\"48bb82a8b8ceea98f4e97825b43870f6ba1970d6\" />",
 "  <project name=\"golang-snappy\" path=\"godeps/src/github.com/golang/snappy\" remote=\"couchbasedeps\" revision=\"723cc1e459b8eea2dea4583200fd60757d40097a\" />",
 "  <project name=\"golang-tools\" path=\"godeps/src/golang.org/x/tools\" remote=\"couchbasedeps\" revision=\"a28dfb48e06b2296b66678872c2cb638f0304f20\" />",
 "  <project name=\"goleveldb\" path=\"godeps/src/github.com/syndtr/goleveldb\" remote=\"couchbasedeps\" revision=\"fa5b5c78794bc5c18f330361059f871ae8c2b9d6\" />",
 "  <project name=\"gomemcached\" path=\"goproj/src/github.com/couchbase/gomemcached\" revision=\"9ae3e1a53ee2393526383f0d37dab16cd936d92f\" groups=\"backup\" />",
 "  <project name=\"gometa\" path=\"goproj/src/github.com/couchbase/gometa\" revision=\"f26942c60986380e757967002c365d6f2b7fb219\" />",
 "  <project name=\"goskiplist\" path=\"godeps/src/github.com/ryszard/goskiplist\" remote=\"couchbasedeps\" revision=\"2dfbae5fcf46374f166f8969cb07e167f1be6273\" />",
 "  <project name=\"gosnappy\" path=\"godeps/src/github.com/syndtr/gosnappy\" remote=\"couchbasedeps\" revision=\"156a073208e131d7d2e212cb749feae7c339e846\" />",
 "  <project name=\"goutils\" path=\"goproj/src/github.com/couchbase/goutils\" revision=\"73dda2bf44424b5c588579948399e86e5de4be6c\" groups=\"bsl\" />",
 "  <project name=\"goxdcr\" path=\"goproj/src/github.com/couchbase/goxdcr\" revision=\"86f3852dbd814c861bf82de1bd3a5e98761e3746\" groups=\"bsl\" />",
 "  <project name=\"grpc-go\" path=\"godeps/src/google.golang.org/grpc\" remote=\"couchbasedeps\" revision=\"df014850f6dee74ba2fc94874043a9f3f75fbfd8\" upstream=\"refs/tags/v1.17.0\" dest-branch=\"refs/tags/v1.17.0\" />",
 "  <project name=\"gsl-lite\" path=\"third_party/gsl-lite\" remote=\"couchbasedeps\" revision=\"e1c381746c2625a76227255f999ae9f14a062208\" upstream=\"refs/tags/v0.38.1\" dest-branch=\"refs/tags/v0.38.1\" groups=\"kv\" />",
 "  <project name=\"gtreap\" path=\"godeps/src/github.com/steveyen/gtreap\" remote=\"couchbasedeps\" revision=\"0abe01ef9be25c4aedc174758ec2d917314d6d70\" />",
 "  <project name=\"httprouter\" path=\"godeps/src/github.com/julienschmidt/httprouter\" remote=\"couchbasedeps\" revision=\"975b5c4c7c21c0e3d2764200bf2aa8e34657ae6e\" />",
 "  <project name=\"indexing\" path=\"goproj/src/github.com/couchbase/indexing\" revision=\"ab452e223878afe5313d78c63a56c1132afdd688\" groups=\"bsl\" />",
 "  <project name=\"json-iterator-go\" path=\"godeps/src/github.com/json-iterator/go\" remote=\"couchbasedeps\" revision=\"f7279a603edee96fe7764d3de9c6ff8cf9970994\" />",
 "  <project name=\"jsonparser\" path=\"godeps/src/github.com/buger/jsonparser\" remote=\"couchbasedeps\" revision=\"df3ea76ece10095374fd1c9a22a4fb85a44efc42\" />",
 "  <project name=\"jsonschema\" path=\"godeps/src/github.com/santhosh-tekuri/jsonschema\" remote=\"couchbasedeps\" revision=\"137f44a49015e5060a447c331aa37de6e0f50267\" />",
 "  <project name=\"jsonx\" path=\"godeps/src/gopkg.in/couchbaselabs/jsonx.v1\" remote=\"couchbaselabs\" revision=\"03f375ceefb769799cfa0d64352fdcc9f1192368\" />",
 "  <project name=\"kv_engine\" revision=\"84f34ca1acbcfc79e1434d4695ebd83c8bff56bc\" groups=\"kv,bsl\" />",
 "  <project name=\"libcouchbase\" revision=\"e4de408c96550b48745b3a142d9827c898d4e96f\" upstream=\"refs/tags/3.2.5\" dest-branch=\"refs/tags/3.2.5\" />",
 "  <project name=\"liner\" path=\"godeps/src/github.com/peterh/liner\" remote=\"couchbasedeps\" revision=\"6f820f8f90ce9482ffbd40bb15f9ea9932f4942d\" />",
 "  <project name=\"liner\" path=\"godeps/src/github.com/sbinet/liner\" remote=\"couchbasedeps\" revision=\"d9335eee40a45a4f5d74524c90040d6fe6013d50\" />",
 "  <project name=\"logstats\" path=\"godeps/src/github.com/couchbase/logstats\" revision=\"24ba9753289f155ab6d43a9a2585b9248da79791\" groups=\"bsl\" />",
 "  <project name=\"magma\" remote=\"couchbase-priv\" revision=\"91b88afd959db2835ae94a9a5d622fd28f3cf56b\" groups=\"notdefault,enterprise,kv_ee\" />",
 "  <project name=\"mmap-go\" path=\"godeps/src/github.com/blevesearch/mmap-go\" remote=\"blevesearch\" revision=\"99940f54c59671cf69e10b2e4041fabce88eb9b2\" />",
 "  <project name=\"mmap-go\" path=\"godeps/src/github.com/edsrzf/mmap-go\" remote=\"couchbasedeps\" revision=\"935e0e8a636ca4ba70b713f3e38a19e1b77739e8\" />",
 "  <project name=\"moss\" path=\"godeps/src/github.com/couchbase/moss\" revision=\"4fae7b31078a3e2bd5848a7029754885cdc495e0\" groups=\"bsl\" />",
 "  <project name=\"mossScope\" path=\"godeps/src/github.com/couchbase/mossScope\" revision=\"9e34f3688e0abd1b057ea2196f02e45f830506f8\" groups=\"bsl\" />",
 "  <project name=\"mousetrap\" path=\"godeps/src/github.com/inconshreveable/mousetrap\" remote=\"couchbasedeps\" revision=\"76626ae9c91c4f2a10f34cad8ce83ea42c93bb75\" />",
 "  <project name=\"msgp\" path=\"godeps/src/github.com/tinylib/msgp\" remote=\"couchbasedeps\" revision=\"5bb5e1aed7ba5bcc93307153b020e7ffe79b0509\" />",
 "  <project name=\"mux\" path=\"godeps/src/github.com/gorilla/mux\" remote=\"couchbasedeps\" revision=\"043ee6597c29786140136a5747b6a886364f5282\" />",
 "  <project name=\"n1fty\" path=\"goproj/src/github.com/couchbase/n1fty\" revision=\"ea4db331c572f6f0157d3947084141fb5e6d6243\" upstream=\"7.1.1\" dest-branch=\"7.1.1\" groups=\"bsl\" />",
 "  <project name=\"net\" path=\"godeps/src/golang.org/x/net\" remote=\"couchbasedeps\" revision=\"44b7c21cbf19450f38b337eb6b6fe4f6496fb5b3\" />",
 "  <project name=\"nitro\" path=\"goproj/src/github.com/couchbase/nitro\" revision=\"2575ec52bf5cf4c7ccc2cbc161eb38e46ce7b4a8\" groups=\"bsl\" />",
 "  <project name=\"npipe\" path=\"godeps/src/github.com/natefinch/npipe\" remote=\"couchbasedeps\" revision=\"272c8150302e83f23d32a355364578c9c13ab20f\" />",
 "  <project name=\"ns_server\" revision=\"954eaa16df8ec2378baa16b80850dab69d230289\" groups=\"bsl\" />",
 "  <project name=\"opentracing-go\" path=\"godeps/src/github.com/opentracing/opentracing-go\" remote=\"couchbasedeps\" revision=\"1949ddbfd147afd4d964a9f00b24eb291e0e7c38\" />",
 "  <project name=\"participle\" path=\"godeps/src/github.com/alecthomas/participle\" remote=\"couchbasedeps\" revision=\"d638c6e1953ed899e05a34da3935146790c60e46\" />",
 "  <project name=\"pflag\" path=\"godeps/src/github.com/spf13/pflag\" remote=\"couchbasedeps\" revision=\"a232f6d9f87afaaa08bafaff5da685f974b83313\" />",
 "  <project name=\"phosphor\" revision=\"2eb6c244d6910baf2834513ece579ba88e4f9b9d\" groups=\"bsl,kv\" />",
 "  <project name=\"pierrec-lz4\" path=\"godeps/src/github.com/pierrec/lz4\" remote=\"couchbasedeps\" revision=\"ed8d4cc3b461464e69798080a0092bd028910298\" />",
 "  <project name=\"pierrec-xxHash\" path=\"godeps/src/github.com/pierrec/xxHash\" remote=\"couchbasedeps\" revision=\"a0006b13c722f7f12368c00a3d3c2ae8a999a0c6\" />",
 "  <project name=\"pkcs8\" path=\"godeps/src/github.com/youmark/pkcs8\" remote=\"couchbasedeps\" revision=\"1be2e3e5546da8a58903ff4adcfab015022538ea\" upstream=\"refs/tags/v1.1\" dest-branch=\"refs/tags/v1.1\" />",
 "  <project name=\"plasma\" path=\"goproj/src/github.com/couchbase/plasma\" remote=\"couchbase-priv\" revision=\"38d8516ac67737cb6f2ad727258184b979713156\" groups=\"notdefault,enterprise\" />",
 "  <project name=\"platform\" revision=\"ebae7a1442bec210dddab6a2c98722476f669a50\" upstream=\"neo\" dest-branch=\"neo\" groups=\"bsl,kv\" />",
 "  <project name=\"product-metadata\" revision=\"ff7c24047435e393698847ec5147716017048636\" groups=\"notdefault,packaging\" />",
 "  <project name=\"product-texts\" revision=\"7ca232481faa1a9240564da4907b6d51ef8d7883\" upstream=\"neo\" dest-branch=\"neo\" />",
 "  <project name=\"protobuf\" path=\"godeps/src/github.com/golang/protobuf\" remote=\"couchbasedeps\" revision=\"ddf22928ea3c56eb4292a0adbbf5001b1e8e7d0d\" />",
 "  <project name=\"query\" path=\"goproj/src/github.com/couchbase/query\" revision=\"7bb63a13eef84a5b1143310186944d19dc12cbff\" upstream=\"7.1.1\" dest-branch=\"7.1.1\" groups=\"bsl\" />",
 "  <project name=\"query-ee\" path=\"goproj/src/github.com/couchbase/query-ee\" remote=\"couchbase-priv\" revision=\"69e90e12caa6084e0d78139d354e55fdc606e04a\" upstream=\"7.1.1\" dest-branch=\"7.1.1\" groups=\"notdefault,enterprise\" />",
 "  <project name=\"query-ui\" revision=\"96b9be252e5068a19f10ae5e956ea4c707a71b4b\" groups=\"bsl\" />",
 "  <project name=\"retriever\" path=\"godeps/src/github.com/couchbase/retriever\" revision=\"295b11134f91d9451c3ae21895f5615fc7a61e31\" groups=\"bsl\" />",
 "  <project name=\"roaring\" path=\"godeps/src/github.com/RoaringBitmap/roaring\" remote=\"couchbasedeps\" revision=\"4208ad825dda03a6a3d2197df8ec57948aebcc12\" />",
 "  <project name=\"sigar\" revision=\"e66221577adc083bfce0b17ea2f833fb49f28081\" groups=\"kv\" />",
 "  <project name=\"subjson\" revision=\"0820f83427d69c6eb737876eb2f2cf6aefa45802\" upstream=\"neo\" dest-branch=\"neo\" groups=\"bsl,kv\" />",
 "  <project name=\"sys\" path=\"godeps/src/golang.org/x/sys\" remote=\"couchbasedeps\" revision=\"12a6c2dcc1e4cb348b57847c73987099e261714b\" />",
 "  <project name=\"testrunner\" revision=\"5e88163df228ac2c4b7bb264260953cef2a26907\" upstream=\"master\" dest-branch=\"master\" />",
 "  <project name=\"text\" path=\"godeps/src/golang.org/x/text\" remote=\"couchbasedeps\" revision=\"22f1617af38ed4cd65b3b96e02bab267e560155c\" upstream=\"refs/tags/v0.3.4\" dest-branch=\"refs/tags/v0.3.4\" />",
 "  <project name=\"tlm\" revision=\"3623279137a5e48ccb3bbe6b22fec22af926f473\" upstream=\"neo\" dest-branch=\"neo\" groups=\"bsl,kv\">",
 "    <copyfile src=\"GNUmakefile\" dest=\"GNUmakefile\" />",
 "    <copyfile src=\"Makefile\" dest=\"Makefile\" />",
 "    <copyfile src=\"CMakeLists.txt\" dest=\"CMakeLists.txt\" />",
 "    <copyfile src=\"dot-clang-format\" dest=\".clang-format\" />",
 "    <copyfile src=\"dot-clang-tidy\" dest=\".clang-tidy\" />",
 "    <copyfile src=\"third-party-CMakeLists.txt\" dest=\"third_party/CMakeLists.txt\" />",
 "  </project>",
 "  <project name=\"udf-api\" path=\"goproj/src/github.com/couchbase/udf-api\" revision=\"b2788ae3d412356a330b36d7f38ad2c66edb5879\" />",
 "  <project name=\"uuid\" path=\"godeps/src/github.com/google/uuid\" remote=\"couchbasedeps\" revision=\"dec09d789f3dba190787f8b4454c7d3c936fed9e\" />",
 "  <project name=\"vbmap\" revision=\"946986b5cf372f16c25ac5e67136ceac3b720fd2\" upstream=\"neo\" dest-branch=\"neo\" />",
 "  <project name=\"voltron\" remote=\"couchbase-priv\" revision=\"43a9e30bc8371f6904a7d944120a403270fe53a6\" upstream=\"neo\" dest-branch=\"neo\" groups=\"notdefault,packaging\" />",
 "  <project name=\"zstd\" path=\"godeps/src/github.com/DataDog/zstd\" remote=\"couchbasedeps\" revision=\"aebefd9fcb99f22cd691ef778a12ed68f0e6a1ab\" />",
 "</manifest>"]

[error_logger:info,2022-09-08T12:07:38.828Z,ns_1@127.0.0.1:ns_server_cluster_sup<0.220.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_cluster_sup}
    started: [{pid,<0.222.0>},
              {id,timeout_diag_logger},
              {mfargs,{timeout_diag_logger,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2022-09-08T12:07:38.831Z,ns_1@127.0.0.1:ns_server_cluster_sup<0.220.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_cluster_sup}
    started: [{pid,<0.223.0>},
              {id,ns_cookie_manager},
              {mfargs,{ns_cookie_manager,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2022-09-08T12:07:38.838Z,ns_1@127.0.0.1:chronicle_local<0.224.0>:chronicle_local:init:54]Ensure chronicle is started
[error_logger:info,2022-09-08T12:07:38.872Z,ns_1@127.0.0.1:chronicle_sup<0.229.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,chronicle_sup}
    started: [{pid,<0.230.0>},
              {id,chronicle_events},
              {mfargs,{chronicle_events,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2022-09-08T12:07:38.872Z,ns_1@127.0.0.1:chronicle_sup<0.229.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,chronicle_sup}
    started: [{pid,<0.231.0>},
              {id,chronicle_external_events},
              {mfargs,{gen_event,start_link,
                                 [{local,chronicle_external_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2022-09-08T12:07:38.875Z,ns_1@127.0.0.1:chronicle_sup<0.229.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,chronicle_sup}
    started: [{pid,<0.232.0>},
              {id,chronicle_ets},
              {mfargs,{chronicle_ets,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2022-09-08T12:07:38.878Z,ns_1@127.0.0.1:chronicle_sup<0.229.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,chronicle_sup}
    started: [{pid,<0.233.0>},
              {id,chronicle_settings},
              {mfargs,{chronicle_settings,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2022-09-08T12:07:38.883Z,ns_1@127.0.0.1:chronicle_agent_sup<0.234.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,chronicle_agent_sup}
    started: [{pid,<0.235.0>},
              {id,chronicle_snapshot_mgr},
              {mfargs,{chronicle_snapshot_mgr,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2022-09-08T12:07:38.883Z,ns_1@127.0.0.1:chronicle_agent_sup<0.234.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,chronicle_agent_sup}
    started: [{pid,<0.236.0>},
              {id,chronicle_rsm_events},
              {mfargs,{chronicle_events,start_link,[chronicle_rsm_events]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2022-09-08T12:07:39.018Z,ns_1@127.0.0.1:chronicle_agent_sup<0.234.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,chronicle_agent_sup}
    started: [{pid,<0.237.0>},
              {id,chronicle_agent},
              {mfargs,{chronicle_agent,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2022-09-08T12:07:39.019Z,ns_1@127.0.0.1:chronicle_sup<0.229.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,chronicle_sup}
    started: [{pid,<0.234.0>},
              {id,chronicle_agent_sup},
              {mfargs,{chronicle_agent_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2022-09-08T12:07:39.028Z,ns_1@127.0.0.1:<0.240.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.240.0>,dynamic_supervisor}
    started: [{pid,<0.241.0>},
              {id,chronicle_leader},
              {mfargs,{chronicle_leader,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2022-09-08T12:07:39.036Z,ns_1@127.0.0.1:chronicle_secondary_restartable_sup<0.242.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,chronicle_secondary_restartable_sup}
    started: [{pid,<0.243.0>},
              {id,chronicle_status},
              {mfargs,{chronicle_status,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2022-09-08T12:07:39.039Z,ns_1@127.0.0.1:chronicle_secondary_restartable_sup<0.242.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,chronicle_secondary_restartable_sup}
    started: [{pid,<0.244.0>},
              {id,chronicle_failover},
              {mfargs,{chronicle_failover,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2022-09-08T12:07:39.039Z,ns_1@127.0.0.1:<0.240.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.240.0>,dynamic_supervisor}
    started: [{pid,<0.242.0>},
              {id,chronicle_secondary_restartable_sup},
              {mfargs,{chronicle_secondary_restartable_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2022-09-08T12:07:39.042Z,ns_1@127.0.0.1:<0.240.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.240.0>,dynamic_supervisor}
    started: [{pid,<0.245.0>},
              {id,chronicle_server},
              {mfargs,{chronicle_server,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2022-09-08T12:07:39.103Z,ns_1@127.0.0.1:<0.248.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.248.0>,chronicle_single_rsm_sup}
    started: [{pid,<0.249.0>},
              {id,chronicle_config_rsm},
              {mfargs,{chronicle_rsm,start_link,
                                     [chronicle_config_rsm,
                                      <<"fb54a3882f4ab57cf6f0be39357bb948">>,
                                      chronicle_config_rsm,[]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[chronicle:debug,2022-09-08T12:07:39.103Z,ns_1@127.0.0.1:chronicle_server<0.245.0>:chronicle_server:handle_register_rsm:361]Registering RSM chronicle_config_rsm with pid <0.249.0>
[error_logger:info,2022-09-08T12:07:39.103Z,ns_1@127.0.0.1:<0.247.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.247.0>,dynamic_supervisor}
    started: [{pid,<0.248.0>},
              {id,chronicle_config_rsm},
              {mfargs,
                  {chronicle_single_rsm_sup,start_link,
                      [chronicle_config_rsm,
                       <<"fb54a3882f4ab57cf6f0be39357bb948">>,
                       chronicle_config_rsm,[]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2022-09-08T12:07:39.107Z,ns_1@127.0.0.1:<0.251.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.251.0>,chronicle_single_rsm_sup}
    started: [{pid,<0.252.0>},
              {id,'kv-events'},
              {mfargs,{gen_event,start_link,[{local,'kv-events'}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2022-09-08T12:07:39.157Z,ns_1@127.0.0.1:<0.251.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.251.0>,chronicle_single_rsm_sup}
    started: [{pid,<0.253.0>},
              {id,kv},
              {mfargs,{chronicle_rsm,start_link,
                                     [kv,
                                      <<"fb54a3882f4ab57cf6f0be39357bb948">>,
                                      chronicle_kv,[]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[chronicle:debug,2022-09-08T12:07:39.157Z,ns_1@127.0.0.1:chronicle_server<0.245.0>:chronicle_server:handle_register_rsm:361]Registering RSM kv with pid <0.253.0>
[error_logger:info,2022-09-08T12:07:39.157Z,ns_1@127.0.0.1:<0.247.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.247.0>,dynamic_supervisor}
    started: [{pid,<0.251.0>},
              {id,kv},
              {mfargs,
                  {chronicle_single_rsm_sup,start_link,
                      [kv,<<"fb54a3882f4ab57cf6f0be39357bb948">>,chronicle_kv,
                       []]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2022-09-08T12:07:39.157Z,ns_1@127.0.0.1:<0.240.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.240.0>,dynamic_supervisor}
    started: [{pid,<0.246.0>},
              {id,chronicle_rsm_sup},
              {mfargs,{chronicle_rsm_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2022-09-08T12:07:39.157Z,ns_1@127.0.0.1:chronicle_sup<0.229.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,chronicle_sup}
    started: [{pid,<0.239.0>},
              {id,chronicle_secondary_sup},
              {mfargs,{chronicle_secondary_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2022-09-08T12:07:39.158Z,ns_1@127.0.0.1:application_controller<0.44.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    application: chronicle
    started_at: 'ns_1@127.0.0.1'

[ns_server:debug,2022-09-08T12:07:39.160Z,ns_1@127.0.0.1:chronicle_local<0.224.0>:chronicle_local:init:58]Chronicle state is: provisioned
[error_logger:info,2022-09-08T12:07:39.161Z,ns_1@127.0.0.1:ns_server_cluster_sup<0.220.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_cluster_sup}
    started: [{pid,<0.224.0>},
              {id,chronicle_local},
              {mfargs,{chronicle_local,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[ns_server:info,2022-09-08T12:07:39.163Z,ns_1@127.0.0.1:ns_cluster<0.255.0>:ns_cluster:handle_info:519]Chronicle state is: provisioned
[error_logger:info,2022-09-08T12:07:39.163Z,ns_1@127.0.0.1:ns_server_cluster_sup<0.220.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_cluster_sup}
    started: [{pid,<0.255.0>},
              {id,ns_cluster},
              {mfargs,{ns_cluster,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[ns_server:info,2022-09-08T12:07:39.166Z,ns_1@127.0.0.1:sigar<0.257.0>:sigar:spawn_sigar:135]Spawing sigar process 'portsigar for ns_1@127.0.0.1'("/opt/couchbase/bin/sigar_port") with babysitter pid: 42
[error_logger:info,2022-09-08T12:07:39.167Z,ns_1@127.0.0.1:ns_server_cluster_sup<0.220.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_cluster_sup}
    started: [{pid,<0.257.0>},
              {id,sigar},
              {mfargs,{sigar,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[ns_server:info,2022-09-08T12:07:39.168Z,ns_1@127.0.0.1:ns_config_sup<0.258.0>:ns_config_sup:init:26]loading static ns_config from "/opt/couchbase/etc/couchbase/config"
[error_logger:info,2022-09-08T12:07:39.170Z,ns_1@127.0.0.1:ns_config_sup<0.258.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_config_sup}
    started: [{pid,<0.259.0>},
              {id,tombstone_keeper},
              {mfargs,{tombstone_keeper,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2022-09-08T12:07:39.170Z,ns_1@127.0.0.1:ns_config_sup<0.258.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_config_sup}
    started: [{pid,<0.260.0>},
              {id,ns_config_events},
              {mfargs,{gen_event,start_link,[{local,ns_config_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2022-09-08T12:07:39.171Z,ns_1@127.0.0.1:ns_config_sup<0.258.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_config_sup}
    started: [{pid,<0.261.0>},
              {id,ns_config_events_local},
              {mfargs,{gen_event,start_link,[{local,ns_config_events_local}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[chronicle:debug,2022-09-08T12:07:39.226Z,ns_1@127.0.0.1:chronicle_leader<0.241.0>:chronicle_leader:handle_state_timeout:608]State timeout when state is: {observer,true,false}
[chronicle:info,2022-09-08T12:07:39.226Z,ns_1@127.0.0.1:<0.264.0>:chronicle_leader:do_election_worker:892]Starting election.
History ID: <<"1933459bb8a062c61b619e931c483c80">>
Log position: {{4,'ns_1@127.0.0.1'},41}
Peers: ['ns_1@127.0.0.1']
Required quorum: {majority,{set,1,16,16,8,80,48,
                                {[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],
                                 []},
                                {{[],
                                  ['ns_1@127.0.0.1'],
                                  [],[],[],[],[],[],[],[],[],[],[],[],[],
                                  []}}}}
[chronicle:info,2022-09-08T12:07:39.226Z,ns_1@127.0.0.1:<0.264.0>:chronicle_leader:do_election_worker:901]I'm the only peer, so I'm the leader.
[chronicle:info,2022-09-08T12:07:39.226Z,ns_1@127.0.0.1:chronicle_leader<0.241.0>:chronicle_leader:handle_election_result:691]Going to become a leader in term {5,'ns_1@127.0.0.1'} (history id <<"1933459bb8a062c61b619e931c483c80">>)
[chronicle:debug,2022-09-08T12:07:39.237Z,ns_1@127.0.0.1:chronicle_agent<0.237.0>:chronicle_agent:handle_establish_term:1529]Accepted term {5,'ns_1@127.0.0.1'} in history <<"1933459bb8a062c61b619e931c483c80">>
[chronicle:debug,2022-09-08T12:07:39.237Z,ns_1@127.0.0.1:chronicle_proposer<0.265.0>:chronicle_proposer:establish_term_init:367]Going to establish term {5,'ns_1@127.0.0.1'} (history id <<"1933459bb8a062c61b619e931c483c80">>).
Quorum peers: ['ns_1@127.0.0.1']
Metadata:
{metadata,'ns_1@127.0.0.1',<<"fb54a3882f4ab57cf6f0be39357bb948">>,
          <<"1933459bb8a062c61b619e931c483c80">>,
          {4,'ns_1@127.0.0.1'},
          {4,'ns_1@127.0.0.1'},
          41,41,
          {log_entry,<<"1933459bb8a062c61b619e931c483c80">>,
                     {4,'ns_1@127.0.0.1'},
                     28,
                     {config,{<<"fb54a3882f4ab57cf6f0be39357bb948">>,0,1},
                             0,<<"875f103e9ad5d5bd2ce188db2441ff4a">>,
                             #{'ns_1@127.0.0.1' =>
                                   #{id =>
                                         <<"fb54a3882f4ab57cf6f0be39357bb948">>,
                                     role => voter}},
                             undefined,
                             #{chronicle_config_rsm =>
                                   {rsm_config,chronicle_config_rsm,[]},
                               kv => {rsm_config,chronicle_kv,[]}},
                             #{},undefined,
                             [{<<"1933459bb8a062c61b619e931c483c80">>,0}]}},
          {log_entry,<<"1933459bb8a062c61b619e931c483c80">>,
                     {4,'ns_1@127.0.0.1'},
                     28,
                     {config,{<<"fb54a3882f4ab57cf6f0be39357bb948">>,0,1},
                             0,<<"875f103e9ad5d5bd2ce188db2441ff4a">>,
                             #{'ns_1@127.0.0.1' =>
                                   #{id =>
                                         <<"fb54a3882f4ab57cf6f0be39357bb948">>,
                                     role => voter}},
                             undefined,
                             #{chronicle_config_rsm =>
                                   {rsm_config,chronicle_config_rsm,[]},
                               kv => {rsm_config,chronicle_kv,[]}},
                             #{},undefined,
                             [{<<"1933459bb8a062c61b619e931c483c80">>,0}]}},
          undefined}
[chronicle:debug,2022-09-08T12:07:39.237Z,ns_1@127.0.0.1:chronicle_proposer<0.265.0>:chronicle_proposer:establish_term_maybe_transition:686]Established term {5,'ns_1@127.0.0.1'} (history id <<"1933459bb8a062c61b619e931c483c80">>) successfully.
Votes: ['ns_1@127.0.0.1']
[chronicle:debug,2022-09-08T12:07:39.237Z,ns_1@127.0.0.1:chronicle_proposer<0.265.0>:chronicle_proposer:handle_state_enter:284]Starting recovery for term {5,'ns_1@127.0.0.1'} in history <<"1933459bb8a062c61b619e931c483c80">>
[chronicle:debug,2022-09-08T12:07:39.246Z,ns_1@127.0.0.1:chronicle_proposer<0.265.0>:chronicle_proposer:handle_state_enter:296]Proposer for term {5,'ns_1@127.0.0.1'} in history <<"1933459bb8a062c61b619e931c483c80">> is ready. Committed seqno: 42
[chronicle:info,2022-09-08T12:07:39.246Z,ns_1@127.0.0.1:chronicle_leader<0.241.0>:chronicle_leader:handle_note_term_status:596]Term {5,'ns_1@127.0.0.1'} established.
[ns_server:info,2022-09-08T12:07:39.265Z,ns_1@127.0.0.1:ns_config<0.262.0>:ns_config:load_config:1108]Loading static config from "/opt/couchbase/etc/couchbase/config"
[ns_server:info,2022-09-08T12:07:39.268Z,ns_1@127.0.0.1:ns_config<0.262.0>:ns_config:load_config:1122]Loading dynamic config from "/opt/couchbase/var/lib/couchbase/config/config.dat"
[ns_server:debug,2022-09-08T12:07:39.317Z,ns_1@127.0.0.1:ns_config<0.262.0>:ns_config:load_config:1130]Here's full dynamic config we loaded:
[[{{node,'ns_1@127.0.0.1',stats_scrape_dynamic_intervals},
   [{'_vclock',63829780449,
     [{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780929}}]}]},
  {{metakv,<<"/indexing/settings/config/features/PlasmaInMemoryCompression">>},
   [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780389}}]}|
    <<"{}">>]},
  {{metakv,<<"/indexing/settings/config">>},
   [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{6,63829780389}}]}|
    <<"{\"indexer.plasma.backIndex.enableInMemoryCompression\":true,\"indexer.plasma.backIndex.enablePageBloomFilter\":false,\"indexer.plasma.mainIndex.enableInMemoryCompression\":true,\"indexer.settings.allow_large_keys\":true,\"indexer.settings.bufferPoolBlockSize\":16384,\"indexer.settings.build.batch_size\":5,\"indexer.settings.compaction.abort_exceed_interval\":false,\"indexer.settings.compaction.check_period\":30,\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.compaction.min_frag\":30,\"indexer.settings.compaction.min_size\":5242880"...>>]},
  {{metakv,<<"/indexing/info/versionToken">>},
   [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780356}}]}|
    <<"{\"Version\":6}">>]},
  {{metakv,<<"/cbas/cluster/state/partitions/topology">>},
   [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780354}}]}|
    <<"{\"version\":1,\"revision\":0,\"balanced\":true,\"ccNodeId\":\"1678dfae96c38e07dff43c49b9f6967b\",\"metadataPartition\":-1,\"numReplicas\":0,\"partitions\":[{\"id\":\"0\",\"origin\":\"1678dfae96c38e07dff43c49b9f6967b\",\"master\":\"1678dfae96c38e07dff43c49b9f6967b\",\"replicas\":[]},{\"id\":\"-1\",\"origin\":\"1678dfae96c38e07dff43c49b9f6967b\",\"master\":\"1678dfae96c38e07dff43c49b9f6967b\",\"replicas\":[]}]}">>]},
  {vbucket_map_history,
   [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780348}}]},
    {[['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      ['ns_1@127.0.0.1'],
      [...]|...],
     [{replication_topology,star},
      {tags,undefined},
      {use_vbmap_greedy_optimization,true},
      {max_slaves,10}]}]},
  {{metakv,<<"/cbas/bootstrap/ensureCc/1678dfae96c38e07dff43c49b9f6967b">>},
   [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780345}}]}|
    <<"{}">>]},
  {{metakv,<<"/cbas/config/node/1678dfae96c38e07dff43c49b9f6967b">>},
   [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780341}}]}|
    <<"{\"configVersion\":1,\"config\":{\"additionalCcNodes\":[],\"address\":\"0.0.0.0\",\"analyticsCbHome\":\"/opt/couchbase\",\"analyticsCcHttpPort\":\"9111\",\"analyticsHttpAdminListenPort\":\"9110\",\"analyticsHttpListenAddress\":[\"127.0.0.1\",\"172.18.0.2\"],\"analyticsHttpListenPort\":\"8095\",\"analyticsHttpsListenAddress\":[\"127.0.0.1\",\"172.18.0.2\"],\"analyticsHttpsListenPort\":\"18095\",\"analyticsNodeName\":\"127.0.0.1:8091\",\"analyticsSslEnabled\":true,\"cbasAddress\":\"127.0.0.1\",\"cbasPort\":\"9122\",\"ccAddress\":\"0.0.0.0\",\"ccAnalyticsHttpListenPort\":\"9111\",\"ccAnalyticsMetadataPartitionId\":-1,\"ccClientListenPort\":\"9113\",\"ccClusterListenPort\":\"9112\",\"ccConsoleListenPort\":\"9114\",\"ccControllerId\":\"0\",\"ccHeartbeatMaxMi"...>>]},
  {{metakv,<<"/eventing/config/keepNodes">>},
   [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780339}}]}|
    <<"[\"1678dfae96c38e07dff43c49b9f6967b\"]">>]},
  {{metakv,<<"/cbas/topology">>},
   [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780339}}]}|
    <<"{\"nodes\":[{\"nodeId\":\"1678dfae96c38e07dff43c49b9f6967b\",\"priority\":1970329132007424,\"opaque\":{\"cbas-version\":\"7.1.1-3175\",\"cc-http-port\":\"9111\",\"controller-id\":\"0\",\"host\":\"127.0.0.1\",\"ns-server-port\":\"8091\",\"num-iodevices\":\"1\",\"starting-partition-id\":\"0\",\"svc-http-port\":\"8095\"}}],\"id\":\"bd9f7bc826257f1781c2ba1e03188aa1\",\"type\":\"topology-change-rebalance\",\"ccNodeId\":\"1678dfae96c38e07dff43c49b9f6967b\",\"metadataNodeId\":\"1678dfae96c38e07dff43c49b9f6967b\",\"metadataPartition\":-1,\"rev\":1,\"configVersion\":1,\"balanceState\":\"balanced-65\",\"keepNodesUpdated\":true,\"keepNodes\":[\"1678dfae96c38e07dff43c49b9f6967b\"],\"inPlaceNumReplicas\":0,\"balanceStateClusterCompat\":458753}">>]},
  {{metakv,<<"/cbas/nextControllerId">>},
   [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780339}}]}|
    <<"1">>]},
  {{metakv,<<"/cbas/nextPartitionId">>},
   [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780339}}]}|
    <<"1">>]},
  {{metakv,<<"/cbbs/internal/default_plans_loaded">>},
   [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780334}}]}|
    <<"true">>]},
  {{metakv,<<"/cbbs/plan/_hourly_backups">>},
   [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780334}}]}|
    <<"{\"name\":\"_hourly_backups\",\"description\":\"This plan does a backup every hour and merges the previous 6 backups every 6 hours. At the end of the week it merges the last week of backups together.\",\"services\":null,\"tasks\":[{\"name\":\"backup_hourly\",\"task_type\":\"BACKUP\",\"schedule\":{\"job_type\":\"BACKUP\",\"frequency\":1,\"period\":\"HOURS\",\"time\":\"00:00\"},\"full_backup\":false},{\"name\":\"merge_every_6_hours\",\"task_type\":\"MERGE\",\"schedule\":{\"job_type\":\"MERGE\",\"frequency\":6,\"period\":\"HOURS\",\"time\":\"00:30\"},\"merge_options\":{\"offset_start\":0,\"offset_end\":0},\"full_backup\":false},{\"name\":\"merge_week\",\"task_type\":\"MERGE\",\"schedule\":{\"job_type\":\"MERGE\",\"frequency\":1,\"period"...>>]},
  {{metakv,<<"/cbbs/plan/_daily_backups">>},
   [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780334}}]}|
    <<"{\"name\":\"_daily_backups\",\"description\":\"This plan does a backup a day and merges them at the end of the week. It then merges every four weeks together.\",\"services\":null,\"tasks\":[{\"name\":\"backup_monday_full\",\"task_type\":\"BACKUP\",\"schedule\":{\"job_type\":\"BACKUP\",\"frequency\":1,\"period\":\"MONDAY\",\"time\":\"22:00\"},\"full_backup\":true},{\"name\":\"backup_tuesday\",\"task_type\":\"BACKUP\",\"schedule\":{\"job_type\":\"BACKUP\",\"frequency\":1,\"period\":\"TUESDAY\",\"time\":\"22:00\"},\"full_backup\":false},{\"name\":\"backup_wednesday\",\"task_type\":\"BACKUP\",\"schedule\":{\"job_type\":\"BACKUP\",\"frequency\":1,\"period\":\"WEDNESDAY\",\"time\":\"22:00\"},\"full_backup\":false},{\"name\":\"backup_thursday"...>>]},
  {{metakv,<<"/cbbs/nodes/1678dfae96c38e07dff43c49b9f6967b">>},
   [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780334}}]}|
    <<"{\"node_id\":\"1678dfae96c38e07dff43c49b9f6967b\",\"host\":\"127.0.0.1\",\"http_port\":8097,\"grpc_port\":9124,\"status\":\"\",\"leader\":false}">>]},
  {{metakv,<<"/cbbs/leader">>},
   [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780334}}]}|
    <<"\"1678dfae96c38e07dff43c49b9f6967b\"">>]},
  {{metakv,
    <<"/fts/cbgt/cfg/nodeDefs-wanted/1678dfae96c38e07dff43c49b9f6967b">>},
   [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780331}}]}|
    <<31,139,8,0,0,0,0,0,0,255,132,145,223,111,211,48,28,196,255,21,116,207,
      38,196,107,243,203,175,3,109,15,32,69,109,197,11,65,147,99,127,157,26,
      101,118,176,157,110,83,213,255,29,53,108,98,20,164,189,222,125,244,181,
      239,238,136,121,182,26,2,235,162,82,170,206,101,217,152,85,113,213,24,
      48,56,175,233,35,153,8,113,4,47,171,90,27,73,77,169,86,53,229,149,54,
      102,189,82,235,166,111,76,217,148,85,127,102,246,62,166,214,135,4,1,
      126,85,101,121,150,103,92,212,121,179,6,123,121,229,205,51,12,246,126,
      26,191,82,136,214,59,8,20,89,153,229,96,72,114,136,16,223,96,136,52,24,
      ...>>]},
  {{metakv,
    <<"/fts/cbgt/cfg/nodeDefs-known/1678dfae96c38e07dff43c49b9f6967b">>},
   [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780331}}]}|
    <<31,139,8,0,0,0,0,0,0,255,132,145,79,111,212,48,16,197,191,10,122,103,
      19,98,118,243,207,215,130,218,3,72,209,238,138,11,65,149,99,143,179,70,
      169,29,108,103,219,106,181,223,29,37,180,162,20,36,174,239,253,52,51,
      111,222,25,243,108,53,4,182,220,108,250,178,40,229,182,202,11,83,107,
      48,56,175,233,3,153,8,113,6,47,171,90,27,73,77,169,54,53,229,149,54,
      102,187,81,219,166,111,76,217,148,85,191,48,71,31,83,235,67,130,0,127,
      95,101,121,150,103,92,212,121,179,5,123,222,242,223,49,12,246,110,26,
      191,80,136,214,59,8,20,89,153,229,96,72,114,136,16,95,97,136,150,...>>]},
  {{metakv,<<"/query/functions_cache/counter">>},
   [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780330}}]}|
    <<"[127.0.0.1:8091]0">>]},
  {{metakv,<<"/fts/cbgt/cfg/version">>},
   [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780329}}]}|
    <<"5.6.0">>]},
  {{metakv,<<"/eventing/settings/config">>},
   [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780329}}]}|
    <<"{\n \"enable_debugger\": false,\n \"ram_quota\": 256\n}">>]},
  {{metakv,<<"/cbbs/config/historyRotationSize">>},
   [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780329}}]}|
    <<"50">>]},
  {uuid,
   [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780329}}]}|
    <<"f7a8495a9eb390597cfca9b1032b2584">>]},
  {rest_creds,
   [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780329}}]}|
    {"<ud>admin</ud>",
     {auth,
      [{<<"plain">>,
        {sanitized,<<"mcPCh2CwRLArgh5PGoubNf43Umyo0hRlCWrpgFQXOVg=">>}},
       {<<"sha512">>,
        {[{<<"h">>,
           {sanitized,<<"U0RGkBHSXAG5dAsOvTH+QGobrx6orS5bdCBqgDyQ+BM=">>}},
          {<<"s">>,
           <<"UpTtwmbTZhnFLOkKwRU9RDn1G6Nr3oVYbVNJ+oayaNMdPzcjvoYDY0p8VEQ1tbuSWKwz2RlQlPyc5/c0qdE8Uw==">>},
          {<<"i">>,4000}]}},
       {<<"sha256">>,
        {[{<<"h">>,
           {sanitized,<<"Ru0lDlLhCS2xLqGsf8+eFR2FOQD8CQBcz5oQzOiYq2E=">>}},
          {<<"s">>,<<"2E26bewdxfGnlaB61iTF3UGV91wy8MaGmypQGeyTb7E=">>},
          {<<"i">>,4000}]}},
       {<<"sha1">>,
        {[{<<"h">>,
           {sanitized,<<"32H/xGHX51I6hfs1h89rVNzUuANXbLudifWbeUP4weA=">>}},
          {<<"s">>,<<"lMYLrzOTT/BNkybU5FRBrBsBbI0=">>},
          {<<"i">>,4000}]}}]}}]},
  {rest,[{port,8091}]},
  {memory_quota,
   [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780329}}]}|
    3739]},
  {cbas_memory_quota,
   [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780329}}]}|
    1488]},
  {fts_memory_quota,
   [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780329}}]}|
    256]},
  {settings,
   [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780329}}]},
    {stats,[{send_stats,true}]}]},
  {cluster_name,
   [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780329}}]},
    116,111,100,111]},
  {nodes_wanted,
   [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780157}}]}]},
  {buckets,
   [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780097}}]},
    {configs,[]}]},
  {auto_failover_cfg,
   [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780097}}]},
    {enabled,true},
    {timeout,120},
    {count,0},
    {max_count,1},
    {failed_over_server_groups,[]},
    {failover_on_data_disk_issues,[{enabled,false},{timePeriod,120}]},
    {failover_server_group,false},
    {can_abort_rebalance,true}]},
  {audit_decriptors,
   [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780097}}]},
    {8243,
     [{name,<<"mutate document">>},
      {description,<<"Document was mutated via the REST API">>},
      {enabled,true},
      {module,ns_server}]},
    {8255,
     [{name,<<"read document">>},
      {description,<<"Document was read via the REST API">>},
      {enabled,false},
      {module,ns_server}]},
    {8257,
     [{name,<<"alert email sent">>},
      {description,<<"An alert email was successfully sent">>},
      {enabled,true},
      {module,ns_server}]},
    {8265,
     [{name,<<"RBAC information retrieved">>},
      {description,<<"RBAC information was retrieved">>},
      {enabled,true},
      {module,ns_server}]},
    {20480,
     [{name,<<"opened DCP connection">>},
      {description,<<"opened DCP connection">>},
      {enabled,true},
      {module,memcached}]},
    {20482,
     [{name,<<"external memcached bucket flush">>},
      {description,
       <<"External user flushed the content of a memcached bucket">>},
      {enabled,true},
      {module,memcached}]},
    {20483,
     [{name,<<"invalid packet">>},
      {description,<<"Rejected an invalid packet">>},
      {enabled,true},
      {module,memcached}]},
    {20485,
     [{name,<<"authentication succeeded">>},
      {description,<<"Authentication to the cluster succeeded">>},
      {enabled,false},
      {module,memcached}]},
    {20488,
     [{name,<<"document read">>},
      {description,<<"Document was read">>},
      {enabled,false},
      {module,memcached}]},
    {20489,
     [{name,<<"document locked">>},
      {description,<<"Document was locked">>},
      {enabled,false},
      {module,memcached}]},
    {20490,
     [{name,<<"document modify">>},
      {description,<<"Document was modified">>},
      {enabled,false},
      {module,memcached}]},
    {20491,
     [{name,<<"document delete">>},
      {description,<<"Document was deleted">>},
      {enabled,false},
      {module,memcached}]},
    {20492,
     [{name,<<"select bucket">>},
      {description,<<"The specified bucket was selected">>},
      {enabled,true},
      {module,memcached}]},
    {20493,
     [{name,<<"session terminated">>},
      {description,<<"Session to the cluster has terminated">>},
      {enabled,false},
      {module,memcached}]},
    {20494,
     [{name,<<"tenant rate limited">>},
      {description,<<"The given tenant was rate limited">>},
      {enabled,true},
      {module,memcached}]},
    {28672,
     [{name,<<"SELECT statement">>},
      {description,<<"A N1QL SELECT statement was executed">>},
      {enabled,false},
      {module,n1ql}]},
    {28673,
     [{name,<<"EXPLAIN statement">>},
      {description,<<"A N1QL EXPLAIN statement was executed">>},
      {enabled,false},
      {module,n1ql}]},
    {28674,
     [{name,<<"PREPARE statement">>},
      {description,<<"A N1QL PREPARE statement was executed">>},
      {enabled,false},
      {module,n1ql}]},
    {28675,
     [{name,<<"INFER statement">>},
      {description,<<"A N1QL INFER statement was executed">>},
      {enabled,false},
      {module,n1ql}]},
    {28676,
     [{name,<<"INSERT statement">>},
      {description,<<"A N1QL INSERT statement was executed">>},
      {enabled,false},
      {module,n1ql}]},
    {28677,
     [{name,<<"UPSERT statement">>},
      {description,<<"A N1QL UPSERT statement was executed">>},
      {enabled,false},
      {module,n1ql}]},
    {28678,
     [{name,<<"DELETE statement">>},
      {description,<<"A N1QL DELETE statement was executed">>},
      {enabled,false},
      {module,n1ql}]},
    {28679,
     [{name,<<"UPDATE statement">>},
      {description,<<"A N1QL UPDATE statement was executed">>},
      {enabled,false},
      {module,n1ql}]},
    {28680,
     [{name,<<"MERGE statement">>},
      {description,<<"A N1QL MERGE statement was executed">>},
      {enabled,false},
      {module,n1ql}]},
    {28681,
     [{name,<<"CREATE INDEX statement">>},
      {description,<<"A N1QL CREATE INDEX statement was executed">>},
      {enabled,false},
      {module,n1ql}]},
    {28682,
     [{name,<<"DROP INDEX statement">>},
      {description,<<"A N1QL DROP INDEX statement was executed">>},
      {enabled,false},
      {module,n1ql}]},
    {28683,
     [{name,<<"ALTER INDEX statement">>},
      {description,<<"A N1QL ALTER INDEX statement was executed">>},
      {enabled,false},
      {module,n1ql}]},
    {28684,
     [{name,<<"BUILD INDEX statement">>},
      {description,<<"A N1QL BUILD INDEX statement was executed">>},
      {enabled,false},
      {module,n1ql}]},
    {28685,
     [{name,<<"GRANT ROLE statement">>},
      {description,<<"A N1QL GRANT ROLE statement was executed">>},
      {enabled,false},
      {module,n1ql}]},
    {28686,
     [{name,<<"REVOKE ROLE statement">>},
      {description,<<"A N1QL REVOKE ROLE statement was executed">>},
      {enabled,false},
      {module,n1ql}]},
    {28687,
     [{name,<<"UNRECOGNIZED statement">>},
      {description,
       <<"An unrecognized statement was received by the N1QL query engine">>},
      {enabled,false},
      {module,n1ql}]},
    {28688,
     [{name,<<"CREATE PRIMARY INDEX statement">>},
      {description,<<"A N1QL CREATE PRIMARY INDEX statement was executed">>},
      {enabled,false},
      {module,n1ql}]},
    {28689,
     [{name,<<"/admin/stats API request">>},
      {description,<<"An HTTP request was made to the API at /admin/stats.">>},
      {enabled,false},
      {module,n1ql}]},
    {28690,
     [{name,<<"/admin/vitals API request">>},
      {description,
       <<"An HTTP request was made to the API at /admin/vitals.">>},
      {enabled,false},
      {module,n1ql}]},
    {28691,
     [{name,<<"/admin/prepareds API request">>},
      {description,
       <<"An HTTP request was made to the API at /admin/prepareds.">>},
      {enabled,false},
      {module,n1ql}]},
    {28692,
     [{name,<<"/admin/active_requests API request">>},
      {description,
       <<"An HTTP request was made to the API at /admin/active_requests.">>},
      {enabled,false},
      {module,n1ql}]},
    {28693,
     [{name,<<"/admin/indexes/prepareds API request">>},
      {description,
       <<"An HTTP request was made to the API at /admin/indexes/prepareds.">>},
      {enabled,false},
      {module,n1ql}]},
    {28694,
     [{name,<<"/admin/indexes/active_requests API request">>},
      {description,
       <<"An HTTP request was made to the API at /admin/indexes/active_requests.">>},
      {enabled,false},
      {module,n1ql}]},
    {28695,
     [{name,<<"/admin/indexes/completed_requests API request">>},
      {description,
       <<"An HTTP request was made to the API at /admin/indexes/completed_requests.">>},
      {enabled,false},
      {module,n1ql}]},
    {28697,
     [{name,<<"/admin/ping API request">>},
      {description,<<"An HTTP request was made to the API at /admin/ping.">>},
      {enabled,false},
      {module,n1ql}]},
    {28698,
     [{name,<<"/admin/config API request">>},
      {description,
       <<"An HTTP request was made to the API at /admin/config.">>},
      {enabled,false},
      {module,n1ql}]},
    {28699,
     [{name,<<"/admin/ssl_cert API request">>},
      {description,
       <<"An HTTP request was made to the API at /admin/ssl_cert.">>},
      {enabled,false},
      {module,n1ql}]},
    {28700,
     [{name,<<"/admin/settings API request">>},
      {description,
       <<"An HTTP request was made to the API at /admin/settings.">>},
      {enabled,false},
      {module,n1ql}]},
    {28701,
     [{name,<<"/admin/clusters API request">>},
      {description,
       <<"An HTTP request was made to the API at /admin/clusters.">>},
      {enabled,false},
      {module,n1ql}]},
    {28702,
     [{name,<<"/admin/completed_requests API request">>},
      {description,
       <<"An HTTP request was made to the API at /admin/completed_requests.">>},
      {enabled,false},
      {module,n1ql}]},
    {28704,
     [{name,<<"/admin/functions API request">>},
      {description,
       <<"An HTTP request was made to the API at /admin/functions.">>},
      {enabled,false},
      {module,n1ql}]},
    {28705,
     [{name,<<"/admin/indexes/functions API request">>},
      {description,
       <<"An HTTP request was made to the API at /admin/indexes/functions.">>},
      {enabled,false},
      {module,n1ql}]},
    {28706,
     [{name,<<"CREATE FUNCTION statement">>},
      {description,<<"A N1QL CREATE FUNCTION statement was executed">>},
      {enabled,false},
      {module,n1ql}]},
    {28707,
     [{name,<<"DROP FUNCTION statement">>},
      {description,<<"A N1QL DROP FUNCTION statement was executed">>},
      {enabled,false},
      {module,n1ql}]},
    {28708,
     [{name,<<"EXECUTE FUNCTION statement">>},
      {description,<<"A N1QL EXECUTE FUNCTION statement was executed">>},
      {enabled,false},
      {module,n1ql}]},
    {28709,
     [{name,<<"/admin/tasks API request">>},
      {description,<<"An HTTP request was made to the API at /admin/tasks.">>},
      {enabled,false},
      {module,n1ql}]},
    {28710,
     [{name,<<"/admin/indexes/tasks API request">>},
      {description,
       <<"An HTTP request was made to the API at /admin/indexes/tasks.">>},
      {enabled,false},
      {module,n1ql}]},
    {28711,
     [{name,<<"/admin/dictionary_cache API request">>},
      {description,
       <<"An HTTP request was made to the API at /admin/dictionary_cache.">>},
      {enabled,false},
      {module,n1ql}]},
    {28712,
     [{name,<<"/admin/indexes/dictionary_cache API request">>},
      {description,
       <<"An HTTP request was made to the API at /admin/indexes/dictionary_cache.">>},
      {enabled,false},
      {module,n1ql}]},
    {28713,
     [{name,<<"CREATE SCOPE statement">>},
      {description,<<"A N1QL CREATE SCOPE statement was executed">>},
      {enabled,false},
      {module,n1ql}]},
    {28714,
     [{name,<<"DROP SCOPE statement">>},
      {description,<<"A N1QL DROP SCOPE statement was executed">>},
      {enabled,false},
      {module,n1ql}]},
    {28715,
     [{name,<<"CREATE COLLECTION statement">>},
      {description,<<"A N1QL CREATE COLLECTION statement was executed">>},
      {enabled,false},
      {module,n1ql}]},
    {28716,
     [{name,<<"DROP COLLECTION statement">>},
      {description,<<"A N1QL DROP COLLECTION statement was executed">>},
      {enabled,false},
      {module,n1ql}]},
    {28717,
     [{name,<<"FLUSH COLLECTION statement">>},
      {description,<<"A N1QL FLUSH COLLECTION statement was executed">>},
      {enabled,false},
      {module,n1ql}]},
    {28718,
     [{name,<<"UPDATE STATISTICS statement">>},
      {description,<<"A N1QL UPDATE STATISTICS statement was executed">>},
      {enabled,false},
      {module,n1ql}]},
    {28719,
     [{name,<<"ADVISE statement">>},
      {description,<<"A N1QL ADVISE statement was executed">>},
      {enabled,false},
      {module,n1ql}]},
    {28720,
     [{name,<<"START TRANSACTION statement">>},
      {description,<<"A N1QL START TRANSACTION statement was executed">>},
      {enabled,false},
      {module,n1ql}]},
    {28721,
     [{name,<<"COMMIT TRANSACTION statement">>},
      {description,<<"A N1QL COMMIT TRANSACTION statement was executed">>},
      {enabled,false},
      {module,n1ql}]},
    {28722,
     [{name,<<"ROLLBACK TRANSACTION statement">>},
      {description,<<"A N1QL ROLLBACK TRANSACTION statement was executed">>},
      {enabled,false},
      {module,n1ql}]},
    {28723,
     [{name,<<"ROLLBACK TRANSACTION TO SAVEPOINT statement">>},
      {description,
       <<"A N1QL ROLLBACK TRANSACTION TO SAVEPOINT statement was executed">>},
      {enabled,false},
      {module,n1ql}]},
    {28724,
     [{name,<<"SET TRANSACTION ISOLATION statement">>},
      {description,
       <<"A N1QL SET TRANSACTION ISOLATION statement was executed">>},
      {enabled,false},
      {module,n1ql}]},
    {28725,
     [{name,<<"SAVEPOINT statement">>},
      {description,<<"A N1QL SAVEPOINT statement was executed">>},
      {enabled,false},
      {module,n1ql}]},
    {28726,
     [{name,<<"/admin/transactions API request">>},
      {description,
       <<"An HTTP request was made to the API at /admin/transactions.">>},
      {enabled,false},
      {module,n1ql}]},
    {28727,
     [{name,<<"/admin/indexes/transactions API request">>},
      {description,
       <<"An HTTP request was made to the API at /admin/indexes/transactions.">>},
      {enabled,false},
      {module,n1ql}]},
    {28728,
     [{name,<<"N1QL backup / restore API request">>},
      {description,
       <<"An HTTP request was made to archive or restore N1QL metadata ">>},
      {enabled,false},
      {module,n1ql}]},
    {28729,
     [{name,<<"/admin/shutdown API request">>},
      {description,
       <<"An HTTP request was made to initate graceful shutdown">>},
      {enabled,false},
      {module,n1ql}]},
    {32768,
     [{name,<<"Create Function">>},
      {description,
       <<"Request to create or update eventing function definition">>},
      {enabled,true},
      {module,eventing}]},
    {32769,
     [{name,<<"Delete Function">>},
      {description,<<"Request to delete eventing function definition">>},
      {enabled,true},
      {module,eventing}]},
    {32770,
     [{name,<<"Fetch Functions">>},
      {description,<<"Request to fetch eventing function definition">>},
      {enabled,false},
      {module,eventing}]},
    {32771,
     [{name,<<"List Deployed">>},
      {description,<<"Request to fetch eventing deployed functions list">>},
      {enabled,false},
      {module,eventing}]},
    {32772,
     [{name,<<"Fetch Drafts">>},
      {description,<<"Request to fetch eventing function draft definitions">>},
      {enabled,false},
      {module,eventing}]},
    {32773,
     [{name,<<"Delete Drafts">>},
      {description,
       <<"Request to delete eventing function draft definitions">>},
      {enabled,true},
      {module,eventing}]},
    {32774,
     [{name,<<"Save Draft">>},
      {description,<<"Request to save a draft definition">>},
      {enabled,true},
      {module,eventing}]},
    {32775,
     [{name,<<"Start Debug">>},
      {description,<<"Request to start eventing function debugger">>},
      {enabled,true},
      {module,eventing}]},
    {32776,
     [{name,<<"Stop Debug">>},
      {description,<<"Request to stop eventing function debugger">>},
      {enabled,true},
      {module,eventing}]},
    {32777,
     [{name,<<"Start Tracing">>},
      {description,<<"Request to start tracing eventing function execution">>},
      {enabled,true},
      {module,eventing}]},
    {32778,
     [{name,<<"Stop Tracing">>},
      {description,<<"Request to stop tracing eventing function execution">>},
      {enabled,true},
      {module,eventing}]},
    {32779,
     [{name,<<"Set Settings">>},
      {description,<<"Request to save settings for an eventing function">>},
      {enabled,true},
      {module,eventing}]},
    {32780,
     [{name,<<"Fetch Config">>},
      {description,<<"Request to fetch eventing config">>},
      {enabled,false},
      {module,eventing}]},
    {32781,
     [{name,<<"Save Config">>},
      {description,<<"Request to save eventing config">>},
      {enabled,true},
      {module,eventing}]},
    {32783,
     [{name,<<"Get Settings">>},
      {description,<<"Request to fetch eventing function settings">>},
      {enabled,false},
      {module,eventing}]},
    {32784,
     [{name,<<"Import Functions">>},
      {description,<<"Request to import one or more eventing functions">>},
      {enabled,true},
      {module,eventing}]},
    {32785,
     [{name,<<"Export Functions">>},
      {description,<<"Request to export all eventing functions">>},
      {enabled,false},
      {module,eventing}]},
    {32786,
     [{name,<<"List Running">>},
      {description,<<"Request to fetch eventing running function list">>},
      {enabled,false},
      {module,eventing}]},
    {32789,
     [{name,<<"Deploy Function">>},
      {description,<<"Request to deploy eventing function">>},
      {enabled,true},
      {module,eventing}]},
    {32790,
     [{name,<<"Undeploy Function">>},
      {description,<<"Request to undeploy eventing function">>},
      {enabled,true},
      {module,eventing}]},
    {32791,
     [{name,<<"Pause Function">>},
      {description,<<"Request to pause eventing function">>},
      {enabled,true},
      {module,eventing}]},
    {32792,
     [{name,<<"Resume Function">>},
      {description,<<"Request to resume eventing function">>},
      {enabled,true},
      {module,eventing}]},
    {32793,
     [{name,<<"Backup Functions">>},
      {description,<<"Request to backup one or more eventing functions">>},
      {enabled,false},
      {module,eventing}]},
    {32794,
     [{name,<<"Restore Functions">>},
      {description,
       <<"Request to restore one or more eventing functions from a backup">>},
      {enabled,true},
      {module,eventing}]},
    {32795,
     [{name,<<"List Function">>},
      {description,<<"Request to fetch eventing functions">>},
      {enabled,false},
      {module,eventing}]},
    {32796,
     [{name,<<"Function Status">>},
      {description,<<"Request to fetch eventing function status">>},
      {enabled,false},
      {module,eventing}]},
    {32797,
     [{name,<<"Clear Stats">>},
      {description,<<"Request to reset eventing function stats">>},
      {enabled,true},
      {module,eventing}]},
    {32798,
     [{name,<<"Fetch Stats">>},
      {description,<<"Request to fetch eventing function stats">>},
      {enabled,false},
      {module,eventing}]},
    {32799,
     [{name,<<"Eventing Cluster Stats">>},
      {description,<<"Request to fetch eventing cluster stats">>},
      {enabled,false},
      {module,eventing}]},
    {32801,
     [{name,<<"Eventing System Event">>},
      {description,<<"Request to execute eventing node related functions">>},
      {enabled,false},
      {module,eventing}]},
    {32802,
     [{name,<<"Get User Info">>},
      {description,<<"Request to get user eventing permissions">>},
      {enabled,false},
      {module,eventing}]},
    {36865,
     [{name,<<"Service configuration change">>},
      {description,<<"A successful service configuration change was made.">>},
      {enabled,true},
      {module,analytics}]},
    {36866,
     [{name,<<"Node configuration change">>},
      {description,<<"A successful node configuration change was made.">>},
      {enabled,true},
      {module,analytics}]},
    {36867,
     [{name,<<"SELECT statement">>},
      {description,<<"A N1QL SELECT statement was executed">>},
      {enabled,false},
      {module,analytics}]},
    {36868,
     [{name,<<"CREATE DATAVERSE statement">>},
      {description,<<"A N1QL CREATE DATAVERSE statement was executed">>},
      {enabled,false},
      {module,analytics}]},
    {36869,
     [{name,<<"DROP DATAVERSE statement">>},
      {description,<<"A N1QL DROP DATAVERSE statement was executed">>},
      {enabled,false},
      {module,analytics}]},
    {36870,
     [{name,<<"CREATE DATASET statement">>},
      {description,<<"A N1QL CREATE DATASET statement was executed">>},
      {enabled,false},
      {module,analytics}]},
    {36871,
     [{name,<<"DROP DATASET statement">>},
      {description,<<"A N1QL DROP DATASET statement was executed">>},
      {enabled,false},
      {module,analytics}]},
    {36872,
     [{name,<<"CREATE INDEX statement">>},
      {description,<<"A N1QL CREATE INDEX statement was executed">>},
      {enabled,false},
      {module,analytics}]},
    {36873,
     [{name,<<"DROP INDEX statement">>},
      {description,<<"A N1QL DROP INDEX statement was executed">>},
      {enabled,false},
      {module,analytics}]},
    {36877,
     [{name,<<"CONNECT LINK statement">>},
      {description,<<"A N1QL CONNECT LINK statement was executed">>},
      {enabled,false},
      {module,analytics}]},
    {36878,
     [{name,<<"DISCONNECT LINK statement">>},
      {description,<<"A N1QL DISCONNECT LINK statement was executed">>},
      {enabled,false},
      {module,analytics}]},
    {36879,
     [{name,<<"UNRECOGNIZED statement">>},
      {description,<<"An UNRECOGNIZED N1QL statement was encountered">>},
      {enabled,false},
      {module,analytics}]},
    {36880,
     [{name,<<"ALTER COLLECTION statement">>},
      {description,<<"A N1QL ALTER COLLECTION statement was executed">>},
      {enabled,false},
      {module,analytics}]},
    {40960,
     [{name,<<"Create Design Doc">>},
      {description,<<"Design Doc is Created">>},
      {enabled,true},
      {module,view_engine}]},
    {40961,
     [{name,<<"Delete Design Doc">>},
      {description,<<"Design Doc is Deleted">>},
      {enabled,true},
      {module,view_engine}]},
    {40962,
     [{name,<<"Query DDoc Meta Data">>},
      {description,<<"Design Doc Meta Data Query Request">>},
      {enabled,true},
      {module,view_engine}]},
    {40963,
     [{name,<<"View Query">>},
      {description,<<"View Query Request">>},
      {enabled,false},
      {module,view_engine}]},
    {40964,
     [{name,<<"Update Design Doc">>},
      {description,<<"Design Doc is Updated">>},
      {enabled,true},
      {module,view_engine}]},
    {40966,
     [{name,<<"Access denied">>},
      {description,
       <<"Access denied to the REST API due to invalid permissions or credenti"...>>},
      {enabled,true},
      {module,view_engine}]},
    {45056,
     [{name,<<"Modify configuration">>},
      {description,<<"Backup service configuration was modified">>},
      {enabled,true},
      {module,backup}]},
    {45057,
     [{name,<<"Fetch configuration">>},
      {description,<<"Backup service configuration was retrieved">>},
      {enabled,false},
      {module,backup}]},
    {45058,
     [{name,<<"Add plan">>},
      {description,<<"A new backup plan was added">>},
      {enabled,true},
      {module,backup}]},
    {45059,
     [{name,<<"Modify plan">>},
      {description,<<"Existing backup plan was modified">>},
      {enabled,true},
      {module,backup}]},
    {45060,
     [{name,<<"Delete plan">>},
      {description,<<"A backup plan was removed">>},
      {enabled,true},
      {module,backup}]},
    {45061,
     [{name,<<"Fetch plan">>},
      {description,<<"One or more backup plans where fetched">>},
      {enabled,false},
      {module,backup}]},
    {45062,
     [{name,<<"Add repository">>},
      {description,<<"A new active backup repository was added">>},
      {enabled,true},
      {module,backup}]},
    {45063,
     [{name,<<"Archive repository">>},
      {description,<<"An active repository was archived">>},
      {enabled,true},
      {module,backup}]},
    {45064,
     [{name,<<"Pause repository">>},
      {description,<<"An active repository was paused">>},
      {enabled,true},
      {module,backup}]},
    {45065,
     [{name,<<"Resume repository">>},
      {description,<<"An active repository was res"...>>},
      {enabled,true},
      {module,backup}]},
    {45066,
     [{name,<<"Fetch repository">>},
      {description,<<"A repository was fetched">>},
      {enabled,false},
      {module,backup}]},
    {45067,
     [{name,<<"Restore repository">>},
      {description,<<"The repository data "...>>},
      {enabled,true},
      {module,backup}]},
    {45068,
     [{name,<<"Backup repository">>},
      {description,<<"A manual backup "...>>},
      {enabled,true},
      {module,backup}]},
    {45069,
     [{name,<<"Merge repository">>},
      {description,<<"A manual mer"...>>},
      {enabled,true},
      {module,backup}]},
    {45070,
     [{name,<<"Info reposit"...>>},
      {description,<<"Informat"...>>},
      {enabled,false},
      {module,backup}]},
    {45071,
     [{name,<<"Examine "...>>},
      {description,<<"A do"...>>},
      {enabled,true},
      {module,...}]},
    {45072,[{name,<<"Dele"...>>},{description,<<...>>},{enabled,...},{...}]},
    {45073,[{name,<<...>>},{description,...},{...}|...]},
    {45074,[{name,...},{...}|...]}]},
  {cluster_compat_version,
   [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{4,63829780097}}]},
    7,1]},
  {otp,
   [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780097}}]},
    {cookie,{sanitized,<<"6jw2YfUfDxiQCs0fCwoUSmHbOdRC4E/A03fIDwJPR3g=">>}}]},
  {{node,'ns_1@127.0.0.1',prometheus_auth_info},
   [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|
    {"@prometheus",
     {auth,
      [{<<"plain">>,
        {sanitized,<<"M19CN2uT3IM7to60JvPLLFWe15QE+xqQj4ae4pILIQY=">>}},
       {<<"sha512">>,
        {[{<<"h">>,
           {sanitized,<<"ogcmvOcIDb3trVZUf8MQ924hUlHGPZQI7aFQbckkSIo=">>}},
          {<<"s">>,
           <<"XsQ6FZpbB9BqHVSzMNqrHDzDO0uulN8mxg267iifMDsUbKxifdxVg+Lc+pAhc3YVdmMhCop0SLTuJ+7LUuQAWw==">>},
          {<<"i">>,4000}]}},
       {<<"sha256">>,
        {[{<<"h">>,
           {sanitized,<<"Z1mj6nW/ZuHa02+s9poaXdfNdF/M/zUY/YJjIP5h0XQ=">>}},
          {<<"s">>,<<"ciebqiheHmvTVsOoEf7zCJiMybfXrUIx2RqHrbKMOF0=">>},
          {<<"i">>,4000}]}},
       {<<"sha1">>,
        {[{<<"h">>,
           {sanitized,<<"OrBaLmtLgYmmtpuHjerlFrjJQFp2/h97H+MvyWAZUtY=">>}},
          {<<"s">>,<<"0mqqcO1V739RDCcAsk84uGUFIGA=">>},
          {<<"i">>,4000}]}}]}}]},
  {{node,'ns_1@127.0.0.1',eventing_dir},
   [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]},
    47,111,112,116,47,99,111,117,99,104,98,97,115,101,47,118,97,114,47,108,
    105,98,47,99,111,117,99,104,98,97,115,101,47,100,97,116,97]},
  {{node,'ns_1@127.0.0.1',cbas_dirs},
   [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]},
    "/opt/couchbase/var/lib/couchbase/data"]},
  {{node,'ns_1@127.0.0.1',node_cert},
   [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]},
    {subject,<<"CN=Couchbase Server Node (127.0.0.1)">>},
    {not_after,64691827199},
    {verified_with,
     <<48,185,21,78,241,90,231,242,183,201,180,2,229,131,239,69>>},
    {type,generated},
    {load_timestamp,63829780092},
    {ca,
     <<"-----BEGIN CERTIFICATE-----\nMIIDITCCAgmgAwIBAgIIFxKaUqQBFTwwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciA5NGQ2NGQ0NTAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgOTRkNjRk\nNDUwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQDCrzpi1QtF4PyrbI2Y\nQB0augORV0Ro18YRvmfImOFnIZpNAmUYZRU68E39LZL794EOVdu05NB0tXNOfVrG\nB6gQNweW7UL1RiCL3OhoZ/VBsR9qSG54l/rFSddbrw3cNMKoZyYaPFoNhJogPOP4\n0IIUZhUjKgA7yKA/L0AqKn516TbfuA9oX/3y5Pm1Dycg0oRmDRenKl/cpORSvwaL\nqb4A35vbvA31mBswu3iLKhpn6L7uu/FPoPqRO"...>>},
    {pem,
     <<"-----BEGIN CERTIFICATE-----\nMIIDOTCCAiGgAwIBAgIIFxKaUsnSLBwwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciA5NGQ2NGQ0NTAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCwxKjAoBgNVBAMTIUNvdWNoYmFzZSBTZXJ2ZXIgTm9kZSAo\nMTI3LjAuMC4xKTCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBALESQvmu\nbrV8JXPnQh3zLduahBDrsIVoTPN0LJWCDoH6bNJOn7QdE6oSpVYqgG/rDVmX67GV\nEl3Cbf4bLB4oK3aYh+v5xlz4ZR4EHtdlBQs7KXT26b8fjtVyCizWAJUXL0iFDrd5\niwmcnK237ql+zfGqCU6/WdaKM0CX083pGiAkURuv9pK8pbmyiBkNaGxAmezfSHLL\n/qo2+rAlvLk11j96RcRcf6dCXCkbNi7JN"...>>},
    {pkey_passphrase_settings,[]},
    {certs_epoch,0},
    {hostname,"127.0.0.1"}]},
  {cert_and_pkey,
   [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780092}}]}|
    {<<"-----BEGIN CERTIFICATE-----\nMIIDITCCAgmgAwIBAgIIFxKaUqQBFTwwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciA5NGQ2NGQ0NTAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgOTRkNjRk\nNDUwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQDCrzpi1QtF4PyrbI2Y\nQB0augORV0Ro18YRvmfImOFnIZpNAmUYZRU68E39LZL794EOVdu05NB0tXNOfVrG\nB6gQNweW7UL1RiCL3OhoZ/VBsR9qSG54l/rFSddbrw3cNMKoZyYaPFoNhJogPOP4\n0IIUZhUjKgA7yKA/L0AqKn516TbfuA9oX/3y5Pm1Dycg0oRmDRenKl/cpORSvwaL\nqb4A35vbvA31mBswu3iLKhpn6L7uu/FPoPqROxTbBY6pHfu53IWOmzGbv"...>>,
     {sanitized,<<"rUyVs4dBK1Fcwmw1hpHxsD1qPAlzmuJ/tb4d6NhlhWU=">>}}]},
  {{node,'ns_1@127.0.0.1',erl_external_listeners},
   [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]},
    {inet,false}]},
  {{node,'ns_1@127.0.0.1',node_encryption},
   [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|
    false]},
  {{node,'ns_1@127.0.0.1',address_family},
   [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|
    inet]},
  {alert_limits,
   [{max_overhead_perc,50},{max_disk_used,90},{max_indexer_ram,75}]},
  {audit,
   [{auditd_enabled,false},
    {rotate_interval,86400},
    {rotate_size,20971520},
    {disabled,[]},
    {enabled,[]},
    {disabled_users,[]},
    {sync,[]},
    {log_path,"/opt/couchbase/var/lib/couchbase/logs"}]},
  {autocompaction,
   [{database_fragmentation_threshold,{30,undefined}},
    {view_fragmentation_threshold,{30,undefined}}]},
  {client_cert_auth,[{state,"disable"},{prefixes,[]}]},
  {email_alerts,
   [{recipients,["root@localhost"]},
    {sender,"couchbase@localhost"},
    {enabled,false},
    {email_server,
     [{user,[]},{pass,"*****"},{host,"localhost"},{port,25},{encrypt,false}]},
    {alerts,
     [auto_failover_node,auto_failover_maximum_reached,
      auto_failover_other_nodes_down,auto_failover_cluster_too_small,
      auto_failover_disabled,ip,disk,overhead,ep_oom_errors,
      ep_item_commit_failed,audit_dropped_events,indexer_ram_max_usage,
      ep_clock_cas_drift_threshold_exceeded,communication_issue,
      time_out_of_sync,disk_usage_analyzer_stuck]},
    {pop_up_alerts,
     [auto_failover_node,auto_failover_maximum_reached,
      auto_failover_other_nodes_down,auto_failover_cluster_too_small,
      auto_failover_disabled,ip,disk,overhead,ep_oom_errors,
      ep_item_commit_failed,audit_dropped_events,indexer_ram_max_usage,
      ep_clock_cas_drift_threshold_exceeded,communication_issue,
      time_out_of_sync,disk_usage_analyzer_stuck]}]},
  {index_aware_rebalance_disabled,false},
  {log_redaction_default_cfg,[{redact_level,none}]},
  {max_bucket_count,30},
  {memcached,[]},
  {password_policy,[{min_length,6},{must_present,[]}]},
  {quorum_nodes,
   [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780328}}]},
    'ns_1@127.0.0.1']},
  {remote_clusters,[]},
  {replication,[{enabled,true}]},
  {retry_rebalance,[{enabled,false},{after_time_period,300},{max_attempts,1}]},
  {scramsha_fallback_salt,<<"ïyñ-ÞÃ¼zÌÿ^\b">>},
  {secure_headers,[]},
  {set_view_update_daemon,
   [{update_interval,5000},
    {update_min_changes,5000},
    {replica_update_min_changes,5000}]},
  {{couchdb,max_parallel_indexers},4},
  {{couchdb,max_parallel_replica_indexers},2},
  {{metakv,<<"/analytics/settings/config">>},
   <<"{\"analytics.settings.num_replicas\":0}">>},
  {{metakv,<<"/query/settings/config">>},
   [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780097}}]}|
    <<"{\"timeout\":0,\"numatrs\":1024,\"n1ql-feat-ctrl\":76,\"max-parallelism\":1,\"query.settings.curl_whitelist\":{\"all_access\":false,\"allowed_urls\":[],\"disallowed_urls\":[]},\"query.settings.tmp_space_dir\":\"/opt/couchbase/var/lib/couchbase/tmp\",\"completed-limit\":4000,\"pipeline-batch\":16,\"prepared-limit\":16384,\"pipeline-cap\":512,\"memory-quota\":0,\"cleanupwindow\":\"60s\",\"use-cbo\":true,\"scan-cap\":512,\"query.settings.tmp_space_size\":5120,\"completed-threshold\":1000,"...>>]},
  {{node,'ns_1@127.0.0.1',audit},
   [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}]},
  {{node,'ns_1@127.0.0.1',backup_grpc_port},
   [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|
    9124]},
  {{node,'ns_1@127.0.0.1',backup_http_port},
   [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|
    8097]},
  {{node,'ns_1@127.0.0.1',backup_https_port},
   [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|
    18097]},
  {{node,'ns_1@127.0.0.1',capi_port},
   [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|
    8092]},
  {{node,'ns_1@127.0.0.1',cbas_admin_port},
   [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|
    9110]},
  {{node,'ns_1@127.0.0.1',cbas_cc_client_port},
   [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|
    9113]},
  {{node,'ns_1@127.0.0.1',cbas_cc_cluster_port},
   [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|
    9112]},
  {{node,'ns_1@127.0.0.1',cbas_cc_http_port},
   [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|
    9111]},
  {{node,'ns_1@127.0.0.1',cbas_cluster_port},
   [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|
    9115]},
  {{node,'ns_1@127.0.0.1',cbas_console_port},
   [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|
    9114]},
  {{node,'ns_1@127.0.0.1',cbas_data_port},
   [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|
    9116]},
  {{node,'ns_1@127.0.0.1',cbas_debug_port},
   [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|
    -1]},
  {{node,'ns_1@127.0.0.1',cbas_http_port},
   [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|
    8095]},
  {{node,'ns_1@127.0.0.1',cbas_messaging_port},
   [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|
    9118]},
  {{node,'ns_1@127.0.0.1',cbas_metadata_callback_port},
   [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|
    9119]},
  {{node,'ns_1@127.0.0.1',cbas_metadata_port},
   [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|
    9121]},
  {{node,'ns_1@127.0.0.1',cbas_parent_port},
   [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|
    9122]},
  {{node,'ns_1@127.0.0.1',cbas_replication_port},
   [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|
    9120]},
  {{node,'ns_1@127.0.0.1',cbas_result_port},
   [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|
    9117]},
  {{node,'ns_1@127.0.0.1',cbas_ssl_port},
   [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|
    18095]},
  {{node,'ns_1@127.0.0.1',compaction_daemon},
   [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]},
    {check_interval,30},
    {min_db_file_size,131072},
    {min_view_file_size,20971520}]},
  {{node,'ns_1@127.0.0.1',config_version},
   [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|
    {7,1}]},
  {{node,'ns_1@127.0.0.1',event_log},
   [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]},
    {filename,"/opt/couchbase/var/lib/couchbase/event_log"}]},
  {{node,'ns_1@127.0.0.1',eventing_debug_port},
   [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|
    9140]},
  {{node,'ns_1@127.0.0.1',eventing_http_port},
   [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|
    8096]},
  {{node,'ns_1@127.0.0.1',eventing_https_port},
   [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|
    18096]},
  {{node,'ns_1@127.0.0.1',fts_grpc_port},
   [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|
    9130]},
  {{node,'ns_1@127.0.0.1',fts_grpc_ssl_port},
   [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|
    19130]},
  {{node,'ns_1@127.0.0.1',fts_http_port},
   [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|
    8094]},
  {{node,'ns_1@127.0.0.1',fts_ssl_port},
   [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|
    18094]},
  {{node,'ns_1@127.0.0.1',indexer_admin_port},
   [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|
    9100]},
  {{node,'ns_1@127.0.0.1',indexer_http_port},
   [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|
    9102]},
  {{node,'ns_1@127.0.0.1',indexer_https_port},
   [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|
    19102]},
  {{node,'ns_1@127.0.0.1',indexer_scan_port},
   [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|
    9101]},
  {{node,'ns_1@127.0.0.1',indexer_stcatchup_port},
   [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|
    9104]},
  {{node,'ns_1@127.0.0.1',indexer_stinit_port},
   [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|
    9103]},
  {{node,'ns_1@127.0.0.1',indexer_stmaint_port},
   [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|
    9105]},
  {{node,'ns_1@127.0.0.1',is_enterprise},
   [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|
    true]},
  {{node,'ns_1@127.0.0.1',isasl},
   [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]},
    {path,"/opt/couchbase/var/lib/couchbase/isasl.pw"}]},
  {{node,'ns_1@127.0.0.1',memcached},
   [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]},
    {port,11210},
    {dedicated_port,11209},
    {dedicated_ssl_port,11206},
    {ssl_port,11207},
    {admin_user,"@ns_server"},
    {other_users,
     ["@cbq-engine","@projector","@goxdcr","@index","@fts","@eventing",
      "@cbas","@backup"]},
    {admin_pass,"*****"},
    {engines,
     [{membase,
       [{engine,"/opt/couchbase/lib/memcached/ep.so"},
        {static_config_string,"failpartialwarmup=false"}]},
      {memcached,
       [{engine,"/opt/couchbase/lib/memcached/default_engine.so"},
        {static_config_string,"vb0=true"}]}]},
    {config_path,"/opt/couchbase/var/lib/couchbase/config/memcached.json"},
    {audit_file,"/opt/couchbase/var/lib/couchbase/config/audit.json"},
    {rbac_file,"/opt/couchbase/var/lib/couchbase/config/memcached.rbac"},
    {log_path,"/opt/couchbase/var/lib/couchbase/logs"},
    {log_prefix,"memcached.log"},
    {log_generations,20},
    {log_cyclesize,10485760},
    {log_rotation_period,39003}]},
  {{node,'ns_1@127.0.0.1',memcached_config},
   [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|
    {[{interfaces,{memcached_config_mgr,get_interfaces,[]}},
      {client_cert_auth,{memcached_config_mgr,client_cert_auth,[]}},
      {connection_idle_time,connection_idle_time},
      {privilege_debug,privilege_debug},
      {breakpad,
       {[{enabled,breakpad_enabled},
         {minidump_dir,{memcached_config_mgr,get_minidump_dir,[]}}]}},
      {admin,{"~s",[admin_user]}},
      {verbosity,verbosity},
      {audit_file,{"~s",[audit_file]}},
      {rbac_file,{"~s",[rbac_file]}},
      {dedupe_nmvb_maps,dedupe_nmvb_maps},
      {tracing_enabled,tracing_enabled},
      {datatype_snappy,{memcached_config_mgr,is_snappy_enabled,[]}},
      {xattr_enabled,true},
      {scramsha_fallback_salt,{memcached_config_mgr,get_fallback_salt,[]}},
      {collections_enabled,{memcached_config_mgr,collections_enabled,[]}},
      {enforce_tenant_limits_enabled,
       {memcached_config_mgr,should_enforce_limits,[]}},
      {max_connections,max_connections},
      {system_connections,system_connections},
      {num_reader_threads,num_reader_threads},
      {num_writer_threads,num_writer_threads},
      {num_auxio_threads,num_auxio_threads},
      {num_nonio_threads,num_nonio_threads},
      {num_storage_threads,num_storage_threads},
      {logger,
       {[{filename,{"~s/~s",[log_path,log_prefix]}},
         {cyclesize,log_cyclesize}]}},
      {external_auth_service,
       {memcached_config_mgr,get_external_auth_service,[]}},
      {active_external_users_push_interval,
       {memcached_config_mgr,get_external_users_push_interval,[]}},
      {prometheus,{memcached_config_mgr,prometheus_cfg,[]}}]}]},
  {{node,'ns_1@127.0.0.1',memcached_dedicated_ssl_port},
   [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|
    11206]},
  {{node,'ns_1@127.0.0.1',memcached_defaults},
   [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]},
    {max_connections,65000},
    {system_connections,5000},
    {connection_idle_time,0},
    {verbosity,0},
    {privilege_debug,false},
    {breakpad_enabled,true},
    {breakpad_minidump_dir_path,"/opt/couchbase/var/lib/couchbase/crash"},
    {dedupe_nmvb_maps,false},
    {je_malloc_conf,undefined},
    {tracing_enabled,true},
    {datatype_snappy,true},
    {num_reader_threads,<<"default">>},
    {num_writer_threads,<<"default">>},
    {num_auxio_threads,<<"default">>},
    {num_nonio_threads,<<"default">>},
    {num_storage_threads,<<"default">>}]},
  {{node,'ns_1@127.0.0.1',memcached_prometheus},
   [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|
    11280]},
  {{node,'ns_1@127.0.0.1',ns_log},
   [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]},
    {filename,"/opt/couchbase/var/lib/couchbase/ns_log"}]},
  {{node,'ns_1@127.0.0.1',port_servers},
   [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}]},
  {{node,'ns_1@127.0.0.1',projector_port},
   [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|
    9999]},
  {{node,'ns_1@127.0.0.1',projector_ssl_port},
   [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|
    9999]},
  {{node,'ns_1@127.0.0.1',prometheus_http_port},
   [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|
    9123]},
  {{node,'ns_1@127.0.0.1',query_port},
   [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|
    8093]},
  {{node,'ns_1@127.0.0.1',rest},
   [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]},
    {port,8091},
    {port_meta,global}]},
  {{node,'ns_1@127.0.0.1',saslauthd_enabled},
   [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|
    true]},
  {{node,'ns_1@127.0.0.1',ssl_capi_port},
   [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|
    18092]},
  {{node,'ns_1@127.0.0.1',ssl_query_port},
   [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|
    18093]},
  {{node,'ns_1@127.0.0.1',ssl_rest_port},
   [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|
    18091]},
  {{node,'ns_1@127.0.0.1',uuid},
   [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|
    <<"1678dfae96c38e07dff43c49b9f6967b">>]},
  {{node,'ns_1@127.0.0.1',xdcr_rest_port},
   [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|
    9998]},
  {{node,'ns_1@127.0.0.1',{project_intact,is_vulnerable}},
   [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|
    false]},
  {{local_changes_count,<<"1678dfae96c38e07dff43c49b9f6967b">>},
   [{'_vclock',[{<<49,54,55,56,100,102,97,101,...>>,...}]}]}]]
[ns_server:info,2022-09-08T12:07:39.360Z,ns_1@127.0.0.1:ns_config<0.262.0>:ns_config:load_config:1152]Here's full dynamic config we loaded + static & default config:
[{auto_reprovision_cfg,[{enabled,true},{max_nodes,1},{count,0}]},
 {{node,'ns_1@127.0.0.1',membership},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829858059}}]}|
   active]},
 {server_groups,
  [[{uuid,<<"0">>},{name,<<"Group 1">>},{nodes,['ns_1@127.0.0.1']}]]},
 {{local_changes_count,<<"1678dfae96c38e07dff43c49b9f6967b">>},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{52,63829780929}}]}]},
 {{node,'ns_1@127.0.0.1',{project_intact,is_vulnerable}},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|
   false]},
 {{node,'ns_1@127.0.0.1',xdcr_rest_port},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|
   9998]},
 {{node,'ns_1@127.0.0.1',uuid},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|
   <<"1678dfae96c38e07dff43c49b9f6967b">>]},
 {{node,'ns_1@127.0.0.1',ssl_rest_port},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|
   18091]},
 {{node,'ns_1@127.0.0.1',ssl_query_port},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|
   18093]},
 {{node,'ns_1@127.0.0.1',ssl_capi_port},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|
   18092]},
 {{node,'ns_1@127.0.0.1',saslauthd_enabled},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|
   true]},
 {{node,'ns_1@127.0.0.1',rest},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]},
   {port,8091},
   {port_meta,global}]},
 {{node,'ns_1@127.0.0.1',query_port},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|
   8093]},
 {{node,'ns_1@127.0.0.1',prometheus_http_port},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|
   9123]},
 {{node,'ns_1@127.0.0.1',projector_ssl_port},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|
   9999]},
 {{node,'ns_1@127.0.0.1',projector_port},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|
   9999]},
 {{node,'ns_1@127.0.0.1',port_servers},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}]},
 {{node,'ns_1@127.0.0.1',ns_log},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]},
   {filename,"/opt/couchbase/var/lib/couchbase/ns_log"}]},
 {{node,'ns_1@127.0.0.1',memcached_prometheus},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|
   11280]},
 {{node,'ns_1@127.0.0.1',memcached_defaults},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]},
   {max_connections,65000},
   {system_connections,5000},
   {connection_idle_time,0},
   {verbosity,0},
   {privilege_debug,false},
   {breakpad_enabled,true},
   {breakpad_minidump_dir_path,"/opt/couchbase/var/lib/couchbase/crash"},
   {dedupe_nmvb_maps,false},
   {je_malloc_conf,undefined},
   {tracing_enabled,true},
   {datatype_snappy,true},
   {num_reader_threads,<<"default">>},
   {num_writer_threads,<<"default">>},
   {num_auxio_threads,<<"default">>},
   {num_nonio_threads,<<"default">>},
   {num_storage_threads,<<"default">>}]},
 {{node,'ns_1@127.0.0.1',memcached_dedicated_ssl_port},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|
   11206]},
 {{node,'ns_1@127.0.0.1',memcached_config},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|
   {[{interfaces,{memcached_config_mgr,get_interfaces,[]}},
     {client_cert_auth,{memcached_config_mgr,client_cert_auth,[]}},
     {connection_idle_time,connection_idle_time},
     {privilege_debug,privilege_debug},
     {breakpad,
      {[{enabled,breakpad_enabled},
        {minidump_dir,{memcached_config_mgr,get_minidump_dir,[]}}]}},
     {admin,{"~s",[admin_user]}},
     {verbosity,verbosity},
     {audit_file,{"~s",[audit_file]}},
     {rbac_file,{"~s",[rbac_file]}},
     {dedupe_nmvb_maps,dedupe_nmvb_maps},
     {tracing_enabled,tracing_enabled},
     {datatype_snappy,{memcached_config_mgr,is_snappy_enabled,[]}},
     {xattr_enabled,true},
     {scramsha_fallback_salt,{memcached_config_mgr,get_fallback_salt,[]}},
     {collections_enabled,{memcached_config_mgr,collections_enabled,[]}},
     {enforce_tenant_limits_enabled,
      {memcached_config_mgr,should_enforce_limits,[]}},
     {max_connections,max_connections},
     {system_connections,system_connections},
     {num_reader_threads,num_reader_threads},
     {num_writer_threads,num_writer_threads},
     {num_auxio_threads,num_auxio_threads},
     {num_nonio_threads,num_nonio_threads},
     {num_storage_threads,num_storage_threads},
     {logger,
      {[{filename,{"~s/~s",[log_path,log_prefix]}},
        {cyclesize,log_cyclesize}]}},
     {external_auth_service,
      {memcached_config_mgr,get_external_auth_service,[]}},
     {active_external_users_push_interval,
      {memcached_config_mgr,get_external_users_push_interval,[]}},
     {prometheus,{memcached_config_mgr,prometheus_cfg,[]}}]}]},
 {{node,'ns_1@127.0.0.1',memcached},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]},
   {port,11210},
   {dedicated_port,11209},
   {dedicated_ssl_port,11206},
   {ssl_port,11207},
   {admin_user,"@ns_server"},
   {other_users,
    ["@cbq-engine","@projector","@goxdcr","@index","@fts","@eventing","@cbas",
     "@backup"]},
   {admin_pass,"*****"},
   {engines,
    [{membase,
      [{engine,"/opt/couchbase/lib/memcached/ep.so"},
       {static_config_string,"failpartialwarmup=false"}]},
     {memcached,
      [{engine,"/opt/couchbase/lib/memcached/default_engine.so"},
       {static_config_string,"vb0=true"}]}]},
   {config_path,"/opt/couchbase/var/lib/couchbase/config/memcached.json"},
   {audit_file,"/opt/couchbase/var/lib/couchbase/config/audit.json"},
   {rbac_file,"/opt/couchbase/var/lib/couchbase/config/memcached.rbac"},
   {log_path,"/opt/couchbase/var/lib/couchbase/logs"},
   {log_prefix,"memcached.log"},
   {log_generations,20},
   {log_cyclesize,10485760},
   {log_rotation_period,39003}]},
 {{node,'ns_1@127.0.0.1',isasl},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]},
   {path,"/opt/couchbase/var/lib/couchbase/isasl.pw"}]},
 {{node,'ns_1@127.0.0.1',is_enterprise},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|
   true]},
 {{node,'ns_1@127.0.0.1',indexer_stmaint_port},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|
   9105]},
 {{node,'ns_1@127.0.0.1',indexer_stinit_port},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|
   9103]},
 {{node,'ns_1@127.0.0.1',indexer_stcatchup_port},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|
   9104]},
 {{node,'ns_1@127.0.0.1',indexer_scan_port},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|
   9101]},
 {{node,'ns_1@127.0.0.1',indexer_https_port},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|
   19102]},
 {{node,'ns_1@127.0.0.1',indexer_http_port},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|
   9102]},
 {{node,'ns_1@127.0.0.1',indexer_admin_port},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|
   9100]},
 {{node,'ns_1@127.0.0.1',fts_ssl_port},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|
   18094]},
 {{node,'ns_1@127.0.0.1',fts_http_port},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|
   8094]},
 {{node,'ns_1@127.0.0.1',fts_grpc_ssl_port},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|
   19130]},
 {{node,'ns_1@127.0.0.1',fts_grpc_port},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|
   9130]},
 {{node,'ns_1@127.0.0.1',eventing_https_port},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|
   18096]},
 {{node,'ns_1@127.0.0.1',eventing_http_port},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|
   8096]},
 {{node,'ns_1@127.0.0.1',eventing_debug_port},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|
   9140]},
 {{node,'ns_1@127.0.0.1',event_log},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]},
   {filename,"/opt/couchbase/var/lib/couchbase/event_log"}]},
 {{node,'ns_1@127.0.0.1',config_version},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|
   {7,1}]},
 {{node,'ns_1@127.0.0.1',compaction_daemon},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]},
   {check_interval,30},
   {min_db_file_size,131072},
   {min_view_file_size,20971520}]},
 {{node,'ns_1@127.0.0.1',cbas_ssl_port},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|
   18095]},
 {{node,'ns_1@127.0.0.1',cbas_result_port},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|
   9117]},
 {{node,'ns_1@127.0.0.1',cbas_replication_port},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|
   9120]},
 {{node,'ns_1@127.0.0.1',cbas_parent_port},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|
   9122]},
 {{node,'ns_1@127.0.0.1',cbas_metadata_port},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|
   9121]},
 {{node,'ns_1@127.0.0.1',cbas_metadata_callback_port},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|
   9119]},
 {{node,'ns_1@127.0.0.1',cbas_messaging_port},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|
   9118]},
 {{node,'ns_1@127.0.0.1',cbas_http_port},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|
   8095]},
 {{node,'ns_1@127.0.0.1',cbas_debug_port},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|-1]},
 {{node,'ns_1@127.0.0.1',cbas_data_port},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|
   9116]},
 {{node,'ns_1@127.0.0.1',cbas_console_port},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|
   9114]},
 {{node,'ns_1@127.0.0.1',cbas_cluster_port},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|
   9115]},
 {{node,'ns_1@127.0.0.1',cbas_cc_http_port},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|
   9111]},
 {{node,'ns_1@127.0.0.1',cbas_cc_cluster_port},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|
   9112]},
 {{node,'ns_1@127.0.0.1',cbas_cc_client_port},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|
   9113]},
 {{node,'ns_1@127.0.0.1',cbas_admin_port},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|
   9110]},
 {{node,'ns_1@127.0.0.1',capi_port},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|
   8092]},
 {{node,'ns_1@127.0.0.1',backup_https_port},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|
   18097]},
 {{node,'ns_1@127.0.0.1',backup_http_port},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|
   8097]},
 {{node,'ns_1@127.0.0.1',backup_grpc_port},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|
   9124]},
 {{node,'ns_1@127.0.0.1',audit},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}]},
 {{metakv,<<"/query/settings/config">>},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780097}}]}|
   <<"{\"timeout\":0,\"numatrs\":1024,\"n1ql-feat-ctrl\":76,\"max-parallelism\":1,\"query.settings.curl_whitelist\":{\"all_access\":false,\"allowed_urls\":[],\"disallowed_urls\":[]},\"query.settings.tmp_space_dir\":\"/opt/couchbase/var/lib/couchbase/tmp\",\"completed-limit\":4000,\"pipeline-batch\":16,\"prepared-limit\":16384,\"pipeline-cap\":512,\"memory-quota\":0,\"cleanupwindow\":\"60s\",\"use-cbo\":true,\"scan-cap\":512,\"query.settings.tmp_space_size\":5120,\"completed-threshold\":1000,\"loglevel\":\"info\",\"cleanuplostattempts\":true,\"cleanupclientattempts\":true,\"txtimeout\":\"0ms\"}">>]},
 {{metakv,<<"/analytics/settings/config">>},
  <<"{\"analytics.settings.num_replicas\":0}">>},
 {{couchdb,max_parallel_replica_indexers},2},
 {{couchdb,max_parallel_indexers},4},
 {set_view_update_daemon,
  [{update_interval,5000},
   {update_min_changes,5000},
   {replica_update_min_changes,5000}]},
 {secure_headers,[]},
 {scramsha_fallback_salt,<<"ïyñ-ÞÃ¼zÌÿ^\b">>},
 {retry_rebalance,[{enabled,false},{after_time_period,300},{max_attempts,1}]},
 {replication,[{enabled,true}]},
 {remote_clusters,[]},
 {quorum_nodes,
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780328}}]},
   'ns_1@127.0.0.1']},
 {password_policy,[{min_length,6},{must_present,[]}]},
 {memcached,[]},
 {max_bucket_count,30},
 {log_redaction_default_cfg,[{redact_level,none}]},
 {index_aware_rebalance_disabled,false},
 {email_alerts,
  [{recipients,["root@localhost"]},
   {sender,"couchbase@localhost"},
   {enabled,false},
   {email_server,
    [{user,[]},{pass,"*****"},{host,"localhost"},{port,25},{encrypt,false}]},
   {alerts,
    [auto_failover_node,auto_failover_maximum_reached,
     auto_failover_other_nodes_down,auto_failover_cluster_too_small,
     auto_failover_disabled,ip,disk,overhead,ep_oom_errors,
     ep_item_commit_failed,audit_dropped_events,indexer_ram_max_usage,
     ep_clock_cas_drift_threshold_exceeded,communication_issue,
     time_out_of_sync,disk_usage_analyzer_stuck]},
   {pop_up_alerts,
    [auto_failover_node,auto_failover_maximum_reached,
     auto_failover_other_nodes_down,auto_failover_cluster_too_small,
     auto_failover_disabled,ip,disk,overhead,ep_oom_errors,
     ep_item_commit_failed,audit_dropped_events,indexer_ram_max_usage,
     ep_clock_cas_drift_threshold_exceeded,communication_issue,
     time_out_of_sync,disk_usage_analyzer_stuck]}]},
 {client_cert_auth,[{state,"disable"},{prefixes,[]}]},
 {autocompaction,
  [{database_fragmentation_threshold,{30,undefined}},
   {view_fragmentation_threshold,{30,undefined}}]},
 {audit,
  [{auditd_enabled,false},
   {rotate_interval,86400},
   {rotate_size,20971520},
   {disabled,[]},
   {enabled,[]},
   {disabled_users,[]},
   {sync,[]},
   {log_path,"/opt/couchbase/var/lib/couchbase/logs"}]},
 {alert_limits,
  [{max_overhead_perc,50},{max_disk_used,90},{max_indexer_ram,75}]},
 {{node,'ns_1@127.0.0.1',address_family},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|
   inet]},
 {{node,'ns_1@127.0.0.1',node_encryption},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|
   false]},
 {{node,'ns_1@127.0.0.1',erl_external_listeners},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]},
   {inet,false}]},
 {cert_and_pkey,
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780092}}]}|
   {<<"-----BEGIN CERTIFICATE-----\nMIIDITCCAgmgAwIBAgIIFxKaUqQBFTwwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciA5NGQ2NGQ0NTAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgOTRkNjRk\nNDUwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQDCrzpi1QtF4PyrbI2Y\nQB0augORV0Ro18YRvmfImOFnIZpNAmUYZRU68E39LZL794EOVdu05NB0tXNOfVrG\nB6gQNweW7UL1RiCL3OhoZ/VBsR9qSG54l/rFSddbrw3cNMKoZyYaPFoNhJogPOP4\n0IIUZhUjKgA7yKA/L0AqKn516TbfuA9oX/3y5Pm1Dycg0oRmDRenKl/cpORSvwaL\nqb4A35vbvA31mBswu3iLKhpn6L7uu/FPoPqROxTbBY6pHfu53IWOmzGbv3PFhHDR\noNKQI34ZliCL9NP06H6+8ZubL6C7J+XfG+zZkH/ZXda4SFCY3GOiZpPpdEOchSBw\nxi5pAgMBAAGjVzBVMA4GA1UdDwEB/wQEAwICpDATBgNVHSUEDDAKBggrBgEFBQcD\nATAPBgNVHR"...>>,
    {sanitized,<<"rUyVs4dBK1Fcwmw1hpHxsD1qPAlzmuJ/tb4d6NhlhWU=">>}}]},
 {{node,'ns_1@127.0.0.1',node_cert},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]},
   {subject,<<"CN=Couchbase Server Node (127.0.0.1)">>},
   {not_after,64691827199},
   {verified_with,
    <<48,185,21,78,241,90,231,242,183,201,180,2,229,131,239,69>>},
   {type,generated},
   {load_timestamp,63829780092},
   {ca,
    <<"-----BEGIN CERTIFICATE-----\nMIIDITCCAgmgAwIBAgIIFxKaUqQBFTwwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciA5NGQ2NGQ0NTAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgOTRkNjRk\nNDUwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQDCrzpi1QtF4PyrbI2Y\nQB0augORV0Ro18YRvmfImOFnIZpNAmUYZRU68E39LZL794EOVdu05NB0tXNOfVrG\nB6gQNweW7UL1RiCL3OhoZ/VBsR9qSG54l/rFSddbrw3cNMKoZyYaPFoNhJogPOP4\n0IIUZhUjKgA7yKA/L0AqKn516TbfuA9oX/3y5Pm1Dycg0oRmDRenKl/cpORSvwaL\nqb4A35vbvA31mBswu3iLKhpn6L7uu/FPoPqROxTbBY6pHfu53IWOmzGbv3PFhHDR\noNKQI34ZliCL9NP06H6+8ZubL6C7J+XfG+zZkH/ZXda4SFCY3GOiZpPpdEOchSBw\nxi5pAgMBAAGjVzBVMA4GA1UdDwEB/wQEAwICpDATBgNVHSU"...>>},
   {pem,
    <<"-----BEGIN CERTIFICATE-----\nMIIDOTCCAiGgAwIBAgIIFxKaUsnSLBwwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciA5NGQ2NGQ0NTAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCwxKjAoBgNVBAMTIUNvdWNoYmFzZSBTZXJ2ZXIgTm9kZSAo\nMTI3LjAuMC4xKTCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBALESQvmu\nbrV8JXPnQh3zLduahBDrsIVoTPN0LJWCDoH6bNJOn7QdE6oSpVYqgG/rDVmX67GV\nEl3Cbf4bLB4oK3aYh+v5xlz4ZR4EHtdlBQs7KXT26b8fjtVyCizWAJUXL0iFDrd5\niwmcnK237ql+zfGqCU6/WdaKM0CX083pGiAkURuv9pK8pbmyiBkNaGxAmezfSHLL\n/qo2+rAlvLk11j96RcRcf6dCXCkbNi7JNRTRomjwyp0+ZjYcmSMCrTDoE6yQ6tEo\nbbDEqb3uXMolkTEL+af4QYkzCWC+3CaPaYzg70sThu1F58b4AUoUyoOW+7CKDTAM\n8G8G85m+KnzVmK8CAwEAAaNnMGUwDgYDVR0PAQH/BAQ"...>>},
   {pkey_passphrase_settings,[]},
   {certs_epoch,0},
   {hostname,"127.0.0.1"}]},
 {{node,'ns_1@127.0.0.1',cbas_dirs},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]},
   "/opt/couchbase/var/lib/couchbase/data"]},
 {{node,'ns_1@127.0.0.1',eventing_dir},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]},
   47,111,112,116,47,99,111,117,99,104,98,97,115,101,47,118,97,114,47,108,105,
   98,47,99,111,117,99,104,98,97,115,101,47,100,97,116,97]},
 {{node,'ns_1@127.0.0.1',prometheus_auth_info},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780328}}]}|
   {"@prometheus",
    {auth,
     [{<<"plain">>,
       {sanitized,<<"M19CN2uT3IM7to60JvPLLFWe15QE+xqQj4ae4pILIQY=">>}},
      {<<"sha512">>,
       {[{<<"h">>,
          {sanitized,<<"ogcmvOcIDb3trVZUf8MQ924hUlHGPZQI7aFQbckkSIo=">>}},
         {<<"s">>,
          <<"XsQ6FZpbB9BqHVSzMNqrHDzDO0uulN8mxg267iifMDsUbKxifdxVg+Lc+pAhc3YVdmMhCop0SLTuJ+7LUuQAWw==">>},
         {<<"i">>,4000}]}},
      {<<"sha256">>,
       {[{<<"h">>,
          {sanitized,<<"Z1mj6nW/ZuHa02+s9poaXdfNdF/M/zUY/YJjIP5h0XQ=">>}},
         {<<"s">>,<<"ciebqiheHmvTVsOoEf7zCJiMybfXrUIx2RqHrbKMOF0=">>},
         {<<"i">>,4000}]}},
      {<<"sha1">>,
       {[{<<"h">>,
          {sanitized,<<"OrBaLmtLgYmmtpuHjerlFrjJQFp2/h97H+MvyWAZUtY=">>}},
         {<<"s">>,<<"0mqqcO1V739RDCcAsk84uGUFIGA=">>},
         {<<"i">>,4000}]}}]}}]},
 {otp,
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780097}}]},
   {cookie,{sanitized,<<"6jw2YfUfDxiQCs0fCwoUSmHbOdRC4E/A03fIDwJPR3g=">>}}]},
 {cluster_compat_version,
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{4,63829780097}}]},
   7,1]},
 {audit_decriptors,
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780097}}]},
   {8243,
    [{name,<<"mutate document">>},
     {description,<<"Document was mutated via the REST API">>},
     {enabled,true},
     {module,ns_server}]},
   {8255,
    [{name,<<"read document">>},
     {description,<<"Document was read via the REST API">>},
     {enabled,false},
     {module,ns_server}]},
   {8257,
    [{name,<<"alert email sent">>},
     {description,<<"An alert email was successfully sent">>},
     {enabled,true},
     {module,ns_server}]},
   {8265,
    [{name,<<"RBAC information retrieved">>},
     {description,<<"RBAC information was retrieved">>},
     {enabled,true},
     {module,ns_server}]},
   {20480,
    [{name,<<"opened DCP connection">>},
     {description,<<"opened DCP connection">>},
     {enabled,true},
     {module,memcached}]},
   {20482,
    [{name,<<"external memcached bucket flush">>},
     {description,
      <<"External user flushed the content of a memcached bucket">>},
     {enabled,true},
     {module,memcached}]},
   {20483,
    [{name,<<"invalid packet">>},
     {description,<<"Rejected an invalid packet">>},
     {enabled,true},
     {module,memcached}]},
   {20485,
    [{name,<<"authentication succeeded">>},
     {description,<<"Authentication to the cluster succeeded">>},
     {enabled,false},
     {module,memcached}]},
   {20488,
    [{name,<<"document read">>},
     {description,<<"Document was read">>},
     {enabled,false},
     {module,memcached}]},
   {20489,
    [{name,<<"document locked">>},
     {description,<<"Document was locked">>},
     {enabled,false},
     {module,memcached}]},
   {20490,
    [{name,<<"document modify">>},
     {description,<<"Document was modified">>},
     {enabled,false},
     {module,memcached}]},
   {20491,
    [{name,<<"document delete">>},
     {description,<<"Document was deleted">>},
     {enabled,false},
     {module,memcached}]},
   {20492,
    [{name,<<"select bucket">>},
     {description,<<"The specified bucket was selected">>},
     {enabled,true},
     {module,memcached}]},
   {20493,
    [{name,<<"session terminated">>},
     {description,<<"Session to the cluster has terminated">>},
     {enabled,false},
     {module,memcached}]},
   {20494,
    [{name,<<"tenant rate limited">>},
     {description,<<"The given tenant was rate limited">>},
     {enabled,true},
     {module,memcached}]},
   {28672,
    [{name,<<"SELECT statement">>},
     {description,<<"A N1QL SELECT statement was executed">>},
     {enabled,false},
     {module,n1ql}]},
   {28673,
    [{name,<<"EXPLAIN statement">>},
     {description,<<"A N1QL EXPLAIN statement was executed">>},
     {enabled,false},
     {module,n1ql}]},
   {28674,
    [{name,<<"PREPARE statement">>},
     {description,<<"A N1QL PREPARE statement was executed">>},
     {enabled,false},
     {module,n1ql}]},
   {28675,
    [{name,<<"INFER statement">>},
     {description,<<"A N1QL INFER statement was executed">>},
     {enabled,false},
     {module,n1ql}]},
   {28676,
    [{name,<<"INSERT statement">>},
     {description,<<"A N1QL INSERT statement was executed">>},
     {enabled,false},
     {module,n1ql}]},
   {28677,
    [{name,<<"UPSERT statement">>},
     {description,<<"A N1QL UPSERT statement was executed">>},
     {enabled,false},
     {module,n1ql}]},
   {28678,
    [{name,<<"DELETE statement">>},
     {description,<<"A N1QL DELETE statement was executed">>},
     {enabled,false},
     {module,n1ql}]},
   {28679,
    [{name,<<"UPDATE statement">>},
     {description,<<"A N1QL UPDATE statement was executed">>},
     {enabled,false},
     {module,n1ql}]},
   {28680,
    [{name,<<"MERGE statement">>},
     {description,<<"A N1QL MERGE statement was executed">>},
     {enabled,false},
     {module,n1ql}]},
   {28681,
    [{name,<<"CREATE INDEX statement">>},
     {description,<<"A N1QL CREATE INDEX statement was executed">>},
     {enabled,false},
     {module,n1ql}]},
   {28682,
    [{name,<<"DROP INDEX statement">>},
     {description,<<"A N1QL DROP INDEX statement was executed">>},
     {enabled,false},
     {module,n1ql}]},
   {28683,
    [{name,<<"ALTER INDEX statement">>},
     {description,<<"A N1QL ALTER INDEX statement was executed">>},
     {enabled,false},
     {module,n1ql}]},
   {28684,
    [{name,<<"BUILD INDEX statement">>},
     {description,<<"A N1QL BUILD INDEX statement was executed">>},
     {enabled,false},
     {module,n1ql}]},
   {28685,
    [{name,<<"GRANT ROLE statement">>},
     {description,<<"A N1QL GRANT ROLE statement was executed">>},
     {enabled,false},
     {module,n1ql}]},
   {28686,
    [{name,<<"REVOKE ROLE statement">>},
     {description,<<"A N1QL REVOKE ROLE statement was executed">>},
     {enabled,false},
     {module,n1ql}]},
   {28687,
    [{name,<<"UNRECOGNIZED statement">>},
     {description,
      <<"An unrecognized statement was received by the N1QL query engine">>},
     {enabled,false},
     {module,n1ql}]},
   {28688,
    [{name,<<"CREATE PRIMARY INDEX statement">>},
     {description,<<"A N1QL CREATE PRIMARY INDEX statement was executed">>},
     {enabled,false},
     {module,n1ql}]},
   {28689,
    [{name,<<"/admin/stats API request">>},
     {description,<<"An HTTP request was made to the API at /admin/stats.">>},
     {enabled,false},
     {module,n1ql}]},
   {28690,
    [{name,<<"/admin/vitals API request">>},
     {description,<<"An HTTP request was made to the API at /admin/vitals.">>},
     {enabled,false},
     {module,n1ql}]},
   {28691,
    [{name,<<"/admin/prepareds API request">>},
     {description,
      <<"An HTTP request was made to the API at /admin/prepareds.">>},
     {enabled,false},
     {module,n1ql}]},
   {28692,
    [{name,<<"/admin/active_requests API request">>},
     {description,
      <<"An HTTP request was made to the API at /admin/active_requests.">>},
     {enabled,false},
     {module,n1ql}]},
   {28693,
    [{name,<<"/admin/indexes/prepareds API request">>},
     {description,
      <<"An HTTP request was made to the API at /admin/indexes/prepareds.">>},
     {enabled,false},
     {module,n1ql}]},
   {28694,
    [{name,<<"/admin/indexes/active_requests API request">>},
     {description,
      <<"An HTTP request was made to the API at /admin/indexes/active_requests.">>},
     {enabled,false},
     {module,n1ql}]},
   {28695,
    [{name,<<"/admin/indexes/completed_requests API request">>},
     {description,
      <<"An HTTP request was made to the API at /admin/indexes/completed_requests.">>},
     {enabled,false},
     {module,n1ql}]},
   {28697,
    [{name,<<"/admin/ping API request">>},
     {description,<<"An HTTP request was made to the API at /admin/ping.">>},
     {enabled,false},
     {module,n1ql}]},
   {28698,
    [{name,<<"/admin/config API request">>},
     {description,<<"An HTTP request was made to the API at /admin/config.">>},
     {enabled,false},
     {module,n1ql}]},
   {28699,
    [{name,<<"/admin/ssl_cert API request">>},
     {description,
      <<"An HTTP request was made to the API at /admin/ssl_cert.">>},
     {enabled,false},
     {module,n1ql}]},
   {28700,
    [{name,<<"/admin/settings API request">>},
     {description,
      <<"An HTTP request was made to the API at /admin/settings.">>},
     {enabled,false},
     {module,n1ql}]},
   {28701,
    [{name,<<"/admin/clusters API request">>},
     {description,
      <<"An HTTP request was made to the API at /admin/clusters.">>},
     {enabled,false},
     {module,n1ql}]},
   {28702,
    [{name,<<"/admin/completed_requests API request">>},
     {description,
      <<"An HTTP request was made to the API at /admin/completed_requests.">>},
     {enabled,false},
     {module,n1ql}]},
   {28704,
    [{name,<<"/admin/functions API request">>},
     {description,
      <<"An HTTP request was made to the API at /admin/functions.">>},
     {enabled,false},
     {module,n1ql}]},
   {28705,
    [{name,<<"/admin/indexes/functions API request">>},
     {description,
      <<"An HTTP request was made to the API at /admin/indexes/functions.">>},
     {enabled,false},
     {module,n1ql}]},
   {28706,
    [{name,<<"CREATE FUNCTION statement">>},
     {description,<<"A N1QL CREATE FUNCTION statement was executed">>},
     {enabled,false},
     {module,n1ql}]},
   {28707,
    [{name,<<"DROP FUNCTION statement">>},
     {description,<<"A N1QL DROP FUNCTION statement was executed">>},
     {enabled,false},
     {module,n1ql}]},
   {28708,
    [{name,<<"EXECUTE FUNCTION statement">>},
     {description,<<"A N1QL EXECUTE FUNCTION statement was executed">>},
     {enabled,false},
     {module,n1ql}]},
   {28709,
    [{name,<<"/admin/tasks API request">>},
     {description,<<"An HTTP request was made to the API at /admin/tasks.">>},
     {enabled,false},
     {module,n1ql}]},
   {28710,
    [{name,<<"/admin/indexes/tasks API request">>},
     {description,
      <<"An HTTP request was made to the API at /admin/indexes/tasks.">>},
     {enabled,false},
     {module,n1ql}]},
   {28711,
    [{name,<<"/admin/dictionary_cache API request">>},
     {description,
      <<"An HTTP request was made to the API at /admin/dictionary_cache.">>},
     {enabled,false},
     {module,n1ql}]},
   {28712,
    [{name,<<"/admin/indexes/dictionary_cache API request">>},
     {description,
      <<"An HTTP request was made to the API at /admin/indexes/dictionary_cache.">>},
     {enabled,false},
     {module,n1ql}]},
   {28713,
    [{name,<<"CREATE SCOPE statement">>},
     {description,<<"A N1QL CREATE SCOPE statement was executed">>},
     {enabled,false},
     {module,n1ql}]},
   {28714,
    [{name,<<"DROP SCOPE statement">>},
     {description,<<"A N1QL DROP SCOPE statement was executed">>},
     {enabled,false},
     {module,n1ql}]},
   {28715,
    [{name,<<"CREATE COLLECTION statement">>},
     {description,<<"A N1QL CREATE COLLECTION statement was executed">>},
     {enabled,false},
     {module,n1ql}]},
   {28716,
    [{name,<<"DROP COLLECTION statement">>},
     {description,<<"A N1QL DROP COLLECTION statement was executed">>},
     {enabled,false},
     {module,n1ql}]},
   {28717,
    [{name,<<"FLUSH COLLECTION statement">>},
     {description,<<"A N1QL FLUSH COLLECTION statement was executed">>},
     {enabled,false},
     {module,n1ql}]},
   {28718,
    [{name,<<"UPDATE STATISTICS statement">>},
     {description,<<"A N1QL UPDATE STATISTICS statement was executed">>},
     {enabled,false},
     {module,n1ql}]},
   {28719,
    [{name,<<"ADVISE statement">>},
     {description,<<"A N1QL ADVISE statement was executed">>},
     {enabled,false},
     {module,n1ql}]},
   {28720,
    [{name,<<"START TRANSACTION statement">>},
     {description,<<"A N1QL START TRANSACTION statement was executed">>},
     {enabled,false},
     {module,n1ql}]},
   {28721,
    [{name,<<"COMMIT TRANSACTION statement">>},
     {description,<<"A N1QL COMMIT TRANSACTION statement was executed">>},
     {enabled,false},
     {module,n1ql}]},
   {28722,
    [{name,<<"ROLLBACK TRANSACTION statement">>},
     {description,<<"A N1QL ROLLBACK TRANSACTION statement was executed">>},
     {enabled,false},
     {module,n1ql}]},
   {28723,
    [{name,<<"ROLLBACK TRANSACTION TO SAVEPOINT statement">>},
     {description,
      <<"A N1QL ROLLBACK TRANSACTION TO SAVEPOINT statement was executed">>},
     {enabled,false},
     {module,n1ql}]},
   {28724,
    [{name,<<"SET TRANSACTION ISOLATION statement">>},
     {description,
      <<"A N1QL SET TRANSACTION ISOLATION statement was executed">>},
     {enabled,false},
     {module,n1ql}]},
   {28725,
    [{name,<<"SAVEPOINT statement">>},
     {description,<<"A N1QL SAVEPOINT statement was executed">>},
     {enabled,false},
     {module,n1ql}]},
   {28726,
    [{name,<<"/admin/transactions API request">>},
     {description,
      <<"An HTTP request was made to the API at /admin/transactions.">>},
     {enabled,false},
     {module,n1ql}]},
   {28727,
    [{name,<<"/admin/indexes/transactions API request">>},
     {description,
      <<"An HTTP request was made to the API at /admin/indexes/transactions.">>},
     {enabled,false},
     {module,n1ql}]},
   {28728,
    [{name,<<"N1QL backup / restore API request">>},
     {description,
      <<"An HTTP request was made to archive or restore N1QL metadata ">>},
     {enabled,false},
     {module,n1ql}]},
   {28729,
    [{name,<<"/admin/shutdown API request">>},
     {description,<<"An HTTP request was made to initate graceful shutdown">>},
     {enabled,false},
     {module,n1ql}]},
   {32768,
    [{name,<<"Create Function">>},
     {description,
      <<"Request to create or update eventing function definition">>},
     {enabled,true},
     {module,eventing}]},
   {32769,
    [{name,<<"Delete Function">>},
     {description,<<"Request to delete eventing function definition">>},
     {enabled,true},
     {module,eventing}]},
   {32770,
    [{name,<<"Fetch Functions">>},
     {description,<<"Request to fetch eventing function definition">>},
     {enabled,false},
     {module,eventing}]},
   {32771,
    [{name,<<"List Deployed">>},
     {description,<<"Request to fetch eventing deployed functions list">>},
     {enabled,false},
     {module,eventing}]},
   {32772,
    [{name,<<"Fetch Drafts">>},
     {description,<<"Request to fetch eventing function draft definitions">>},
     {enabled,false},
     {module,eventing}]},
   {32773,
    [{name,<<"Delete Drafts">>},
     {description,<<"Request to delete eventing function draft definitions">>},
     {enabled,true},
     {module,eventing}]},
   {32774,
    [{name,<<"Save Draft">>},
     {description,<<"Request to save a draft definition">>},
     {enabled,true},
     {module,eventing}]},
   {32775,
    [{name,<<"Start Debug">>},
     {description,<<"Request to start eventing function debugger">>},
     {enabled,true},
     {module,eventing}]},
   {32776,
    [{name,<<"Stop Debug">>},
     {description,<<"Request to stop eventing function debugger">>},
     {enabled,true},
     {module,eventing}]},
   {32777,
    [{name,<<"Start Tracing">>},
     {description,<<"Request to start tracing eventing function execution">>},
     {enabled,true},
     {module,eventing}]},
   {32778,
    [{name,<<"Stop Tracing">>},
     {description,<<"Request to stop tracing eventing function execution">>},
     {enabled,true},
     {module,eventing}]},
   {32779,
    [{name,<<"Set Settings">>},
     {description,<<"Request to save settings for an eventing function">>},
     {enabled,true},
     {module,eventing}]},
   {32780,
    [{name,<<"Fetch Config">>},
     {description,<<"Request to fetch eventing config">>},
     {enabled,false},
     {module,eventing}]},
   {32781,
    [{name,<<"Save Config">>},
     {description,<<"Request to save eventing config">>},
     {enabled,true},
     {module,eventing}]},
   {32783,
    [{name,<<"Get Settings">>},
     {description,<<"Request to fetch eventing function settings">>},
     {enabled,false},
     {module,eventing}]},
   {32784,
    [{name,<<"Import Functions">>},
     {description,<<"Request to import one or more eventing functions">>},
     {enabled,true},
     {module,eventing}]},
   {32785,
    [{name,<<"Export Functions">>},
     {description,<<"Request to export all eventing functions">>},
     {enabled,false},
     {module,eventing}]},
   {32786,
    [{name,<<"List Running">>},
     {description,<<"Request to fetch eventing running function list">>},
     {enabled,false},
     {module,eventing}]},
   {32789,
    [{name,<<"Deploy Function">>},
     {description,<<"Request to deploy eventing function">>},
     {enabled,true},
     {module,eventing}]},
   {32790,
    [{name,<<"Undeploy Function">>},
     {description,<<"Request to undeploy eventing function">>},
     {enabled,true},
     {module,eventing}]},
   {32791,
    [{name,<<"Pause Function">>},
     {description,<<"Request to pause eventing function">>},
     {enabled,true},
     {module,eventing}]},
   {32792,
    [{name,<<"Resume Function">>},
     {description,<<"Request to resume eventing function">>},
     {enabled,true},
     {module,eventing}]},
   {32793,
    [{name,<<"Backup Functions">>},
     {description,<<"Request to backup one or more eventing functions">>},
     {enabled,false},
     {module,eventing}]},
   {32794,
    [{name,<<"Restore Functions">>},
     {description,
      <<"Request to restore one or more eventing functions from a backup">>},
     {enabled,true},
     {module,eventing}]},
   {32795,
    [{name,<<"List Function">>},
     {description,<<"Request to fetch eventing functions">>},
     {enabled,false},
     {module,eventing}]},
   {32796,
    [{name,<<"Function Status">>},
     {description,<<"Request to fetch eventing function status">>},
     {enabled,false},
     {module,eventing}]},
   {32797,
    [{name,<<"Clear Stats">>},
     {description,<<"Request to reset eventing function stats">>},
     {enabled,true},
     {module,eventing}]},
   {32798,
    [{name,<<"Fetch Stats">>},
     {description,<<"Request to fetch eventing function stats">>},
     {enabled,false},
     {module,eventing}]},
   {32799,
    [{name,<<"Eventing Cluster Stats">>},
     {description,<<"Request to fetch eventing cluster stats">>},
     {enabled,false},
     {module,eventing}]},
   {32801,
    [{name,<<"Eventing System Event">>},
     {description,<<"Request to execute eventing node related functions">>},
     {enabled,false},
     {module,eventing}]},
   {32802,
    [{name,<<"Get User Info">>},
     {description,<<"Request to get user eventing permissions">>},
     {enabled,false},
     {module,eventing}]},
   {36865,
    [{name,<<"Service configuration change">>},
     {description,<<"A successful service configuration change was made.">>},
     {enabled,true},
     {module,analytics}]},
   {36866,
    [{name,<<"Node configuration change">>},
     {description,<<"A successful node configuration change was made.">>},
     {enabled,true},
     {module,analytics}]},
   {36867,
    [{name,<<"SELECT statement">>},
     {description,<<"A N1QL SELECT statement was executed">>},
     {enabled,false},
     {module,analytics}]},
   {36868,
    [{name,<<"CREATE DATAVERSE statement">>},
     {description,<<"A N1QL CREATE DATAVERSE statement was executed">>},
     {enabled,false},
     {module,analytics}]},
   {36869,
    [{name,<<"DROP DATAVERSE statement">>},
     {description,<<"A N1QL DROP DATAVERSE statement was executed">>},
     {enabled,false},
     {module,analytics}]},
   {36870,
    [{name,<<"CREATE DATASET statement">>},
     {description,<<"A N1QL CREATE DATASET statement was executed">>},
     {enabled,false},
     {module,analytics}]},
   {36871,
    [{name,<<"DROP DATASET statement">>},
     {description,<<"A N1QL DROP DATASET statement was executed">>},
     {enabled,false},
     {module,analytics}]},
   {36872,
    [{name,<<"CREATE INDEX statement">>},
     {description,<<"A N1QL CREATE INDEX statement was executed">>},
     {enabled,false},
     {module,analytics}]},
   {36873,
    [{name,<<"DROP INDEX statement">>},
     {description,<<"A N1QL DROP INDEX statement was executed">>},
     {enabled,false},
     {module,analytics}]},
   {36877,
    [{name,<<"CONNECT LINK statement">>},
     {description,<<"A N1QL CONNECT LINK statement was executed">>},
     {enabled,false},
     {module,analytics}]},
   {36878,
    [{name,<<"DISCONNECT LINK statement">>},
     {description,<<"A N1QL DISCONNECT LINK statement was executed">>},
     {enabled,false},
     {module,analytics}]},
   {36879,
    [{name,<<"UNRECOGNIZED statement">>},
     {description,<<"An UNRECOGNIZED N1QL statement was encountered">>},
     {enabled,false},
     {module,analytics}]},
   {36880,
    [{name,<<"ALTER COLLECTION statement">>},
     {description,<<"A N1QL ALTER COLLECTION statement was executed">>},
     {enabled,false},
     {module,analytics}]},
   {40960,
    [{name,<<"Create Design Doc">>},
     {description,<<"Design Doc is Created">>},
     {enabled,true},
     {module,view_engine}]},
   {40961,
    [{name,<<"Delete Design Doc">>},
     {description,<<"Design Doc is Deleted">>},
     {enabled,true},
     {module,view_engine}]},
   {40962,
    [{name,<<"Query DDoc Meta Data">>},
     {description,<<"Design Doc Meta Data Query Request">>},
     {enabled,true},
     {module,view_engine}]},
   {40963,
    [{name,<<"View Query">>},
     {description,<<"View Query Request">>},
     {enabled,false},
     {module,view_engine}]},
   {40964,
    [{name,<<"Update Design Doc">>},
     {description,<<"Design Doc is Updated">>},
     {enabled,true},
     {module,view_engine}]},
   {40966,
    [{name,<<"Access denied">>},
     {description,
      <<"Access denied to the REST API due to invalid permissions or credentials">>},
     {enabled,true},
     {module,view_engine}]},
   {45056,
    [{name,<<"Modify configuration">>},
     {description,<<"Backup service configuration was modified">>},
     {enabled,true},
     {module,backup}]},
   {45057,
    [{name,<<"Fetch configuration">>},
     {description,<<"Backup service configuration was retrieved">>},
     {enabled,false},
     {module,backup}]},
   {45058,
    [{name,<<"Add plan">>},
     {description,<<"A new backup plan was added">>},
     {enabled,true},
     {module,backup}]},
   {45059,
    [{name,<<"Modify plan">>},
     {description,<<"Existing backup plan was modified">>},
     {enabled,true},
     {module,backup}]},
   {45060,
    [{name,<<"Delete plan">>},
     {description,<<"A backup plan was removed">>},
     {enabled,true},
     {module,backup}]},
   {45061,
    [{name,<<"Fetch plan">>},
     {description,<<"One or more backup plans where fetched">>},
     {enabled,false},
     {module,backup}]},
   {45062,
    [{name,<<"Add repository">>},
     {description,<<"A new active backup repository was added">>},
     {enabled,true},
     {module,backup}]},
   {45063,
    [{name,<<"Archive repository">>},
     {description,<<"An active repository was archived">>},
     {enabled,true},
     {module,backup}]},
   {45064,
    [{name,<<"Pause repository">>},
     {description,<<"An active repository was paused">>},
     {enabled,true},
     {module,backup}]},
   {45065,
    [{name,<<"Resume repository">>},
     {description,<<"An active repository was resumed">>},
     {enabled,true},
     {module,backup}]},
   {45066,
    [{name,<<"Fetch repository">>},
     {description,<<"A repository was fetched">>},
     {enabled,false},
     {module,backup}]},
   {45067,
    [{name,<<"Restore repository">>},
     {description,<<"The repository data was restored">>},
     {enabled,true},
     {module,backup}]},
   {45068,
    [{name,<<"Backup repository">>},
     {description,<<"A manual backup was triggered on an active repository">>},
     {enabled,true},
     {module,backup}]},
   {45069,
    [{name,<<"Merge repository">>},
     {description,<<"A manual merge was triggered on an active repository">>},
     {enabled,true},
     {module,backup}]},
   {45070,
    [{name,<<"Info repository">>},
     {description,
      <<"Information about the structure and contents of the backup repository was fetched.">>},
     {enabled,false},
     {module,backup}]},
   {45071,
    [{name,<<"Examine repository">>},
     {description,<<"A document was retrieved from the repository backups">>},
     {enabled,true},
     {module,backup}]},
   {45072,
    [{name,<<"Delete repository">>},
     {description,<<"A repository was deleted">>},
     {enabled,true},
     {module,backup}]},
   {45073,
    [{name,<<"Delete backup">>},
     {description,<<"An active repository backup was deleted">>},
     {enabled,true},
     {module,backup}]},
   {45074,
    [{name,<<"Access denied">>},
     {description,
      <<"A user has been denied access to the REST API due to invalid permissions or credenti"...>>},
     {enabled,true},
     {module,backup}]}]},
 {auto_failover_cfg,
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780097}}]},
   {enabled,true},
   {timeout,120},
   {count,0},
   {max_count,1},
   {failed_over_server_groups,[]},
   {failover_on_data_disk_issues,[{enabled,false},{timePeriod,120}]},
   {failover_server_group,false},
   {can_abort_rebalance,true}]},
 {buckets,
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780097}}]},
   {configs,[]}]},
 {nodes_wanted,
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780157}}]}]},
 {cluster_name,
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780329}}]},
   116,111,100,111]},
 {settings,
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780329}}]},
   {stats,[{send_stats,true}]}]},
 {fts_memory_quota,
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780329}}]}|
   256]},
 {cbas_memory_quota,
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780329}}]}|
   1488]},
 {memory_quota,
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780329}}]}|
   3739]},
 {rest,[{port,8091}]},
 {rest_creds,
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780329}}]}|
   {"<ud>admin</ud>",
    {auth,
     [{<<"plain">>,
       {sanitized,<<"mcPCh2CwRLArgh5PGoubNf43Umyo0hRlCWrpgFQXOVg=">>}},
      {<<"sha512">>,
       {[{<<"h">>,
          {sanitized,<<"U0RGkBHSXAG5dAsOvTH+QGobrx6orS5bdCBqgDyQ+BM=">>}},
         {<<"s">>,
          <<"UpTtwmbTZhnFLOkKwRU9RDn1G6Nr3oVYbVNJ+oayaNMdPzcjvoYDY0p8VEQ1tbuSWKwz2RlQlPyc5/c0qdE8Uw==">>},
         {<<"i">>,4000}]}},
      {<<"sha256">>,
       {[{<<"h">>,
          {sanitized,<<"Ru0lDlLhCS2xLqGsf8+eFR2FOQD8CQBcz5oQzOiYq2E=">>}},
         {<<"s">>,<<"2E26bewdxfGnlaB61iTF3UGV91wy8MaGmypQGeyTb7E=">>},
         {<<"i">>,4000}]}},
      {<<"sha1">>,
       {[{<<"h">>,
          {sanitized,<<"32H/xGHX51I6hfs1h89rVNzUuANXbLudifWbeUP4weA=">>}},
         {<<"s">>,<<"lMYLrzOTT/BNkybU5FRBrBsBbI0=">>},
         {<<"i">>,4000}]}}]}}]},
 {uuid,
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780329}}]}|
   <<"f7a8495a9eb390597cfca9b1032b2584">>]},
 {{metakv,<<"/cbbs/config/historyRotationSize">>},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780329}}]}|
   <<"50">>]},
 {{metakv,<<"/eventing/settings/config">>},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780329}}]}|
   <<"{\n \"enable_debugger\": false,\n \"ram_quota\": 256\n}">>]},
 {{metakv,<<"/fts/cbgt/cfg/version">>},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780329}}]}|
   <<"5.6.0">>]},
 {{metakv,<<"/query/functions_cache/counter">>},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780330}}]}|
   <<"[127.0.0.1:8091]0">>]},
 {{metakv,<<"/fts/cbgt/cfg/nodeDefs-known/1678dfae96c38e07dff43c49b9f6967b">>},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780331}}]}|
   <<31,139,8,0,0,0,0,0,0,255,132,145,79,111,212,48,16,197,191,10,122,103,19,
     98,118,243,207,215,130,218,3,72,209,238,138,11,65,149,99,143,179,70,169,
     29,108,103,219,106,181,223,29,37,180,162,20,36,174,239,253,52,51,111,
     222,25,243,108,53,4,182,220,108,250,178,40,229,182,202,11,83,107,48,56,
     175,233,3,153,8,113,6,47,171,90,27,73,77,169,54,53,229,149,54,102,187,
     81,219,166,111,76,217,148,85,191,48,71,31,83,235,67,130,0,127,95,101,
     121,150,103,92,212,121,179,5,123,222,242,223,49,12,246,110,26,191,80,
     136,214,59,8,20,89,153,229,...>>]},
 {{metakv,
   <<"/fts/cbgt/cfg/nodeDefs-wanted/1678dfae96c38e07dff43c49b9f6967b">>},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{2,63829780331}}]}|
   <<31,139,8,0,0,0,0,0,0,255,132,145,223,111,211,48,28,196,255,21,116,207,
     38,196,107,243,203,175,3,109,15,32,69,109,197,11,65,147,99,127,157,26,
     101,118,176,157,110,83,213,255,29,53,108,98,20,164,189,222,125,244,181,
     239,238,136,121,182,26,2,235,162,82,170,206,101,217,152,85,113,213,24,
     48,56,175,233,35,153,8,113,4,47,171,90,27,73,77,169,86,53,229,149,54,
     102,189,82,235,166,111,76,217,148,85,127,102,246,62,166,214,135,4,1,126,
     85,101,121,150,103,92,212,121,179,6,123,121,229,205,51,12,246,126,26,
     191,82,136,214,59,8,20,89,153,...>>]},
 {{metakv,<<"/cbbs/leader">>},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780334}}]}|
   <<"\"1678dfae96c38e07dff43c49b9f6967b\"">>]},
 {{metakv,<<"/cbbs/nodes/1678dfae96c38e07dff43c49b9f6967b">>},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780334}}]}|
   <<"{\"node_id\":\"1678dfae96c38e07dff43c49b9f6967b\",\"host\":\"127.0.0.1\",\"http_port\":8097,\"grpc_port\":9124,\"status\":\"\",\"leader\":false}">>]},
 {{metakv,<<"/cbbs/plan/_daily_backups">>},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780334}}]}|
   <<"{\"name\":\"_daily_backups\",\"description\":\"This plan does a backup a day and merges them at the end of the week. It then merges every four weeks together.\",\"services\":null,\"tasks\":[{\"name\":\"backup_monday_full\",\"task_type\":\"BACKUP\",\"schedule\":{\"job_type\":\"BACKUP\",\"frequency\":1,\"period\":\"MONDAY\",\"time\":\"22:00\"},\"full_backup\":true},{\"name\":\"backup_tuesday\",\"task_type\":\"BACKUP\",\"schedule\":{\"job_type\":\"BACKUP\",\"frequency\":1,\"period\":\"TUESDAY\",\"time\":\"22:00\"},\"full_backup\":false},{\"name\":\"backup_wednesday\",\"task_type\":\"BACKUP\",\"schedule\":{\"job_type\":\"BACKUP\",\"frequency\":1,\"period\":\"WEDN"...>>]},
 {{metakv,<<"/cbbs/plan/_hourly_backups">>},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780334}}]}|
   <<"{\"name\":\"_hourly_backups\",\"description\":\"This plan does a backup every hour and merges the previous 6 backups every 6 hours. At the end of the week it merges the last week of backups together.\",\"services\":null,\"tasks\":[{\"name\":\"backup_hourly\",\"task_type\":\"BACKUP\",\"schedule\":{\"job_type\":\"BACKUP\",\"frequency\":1,\"period\":\"HOURS\",\"time\":\"00:00\"},\"full_backup\":false},{\"name\":\"merge_every_6_hours\",\"task_type\":\"MERGE\",\"schedule\":{\"job_type\":\"MERGE\",\"frequency\":6,\"period\":\"HOURS\",\"time\":\"00:30\"},\"merge_options\":{\"offset_start\":0,\"offset_end\":0},\"full_backup\":false},{\"name\":\"merge_we"...>>]},
 {{metakv,<<"/cbbs/internal/default_plans_loaded">>},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780334}}]}|
   <<"true">>]},
 {{metakv,<<"/cbas/nextPartitionId">>},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780339}}]}|
   <<"1">>]},
 {{metakv,<<"/cbas/nextControllerId">>},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780339}}]}|
   <<"1">>]},
 {{metakv,<<"/cbas/topology">>},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780339}}]}|
   <<"{\"nodes\":[{\"nodeId\":\"1678dfae96c38e07dff43c49b9f6967b\",\"priority\":1970329132007424,\"opaque\":{\"cbas-version\":\"7.1.1-3175\",\"cc-http-port\":\"9111\",\"controller-id\":\"0\",\"host\":\"127.0.0.1\",\"ns-server-port\":\"8091\",\"num-iodevices\":\"1\",\"starting-partition-id\":\"0\",\"svc-http-port\":\"8095\"}}],\"id\":\"bd9f7bc826257f1781c2ba1e03188aa1\",\"type\":\"topology-change-rebalance\",\"ccNodeId\":\"1678dfae96c38e07dff43c49b9f6967b\",\"metadataNodeId\":\"1678dfae96c38e07dff43c49b9f6967b\",\"metadataPartition\":-1,\"rev\":1,\"configVersion\":1,\"balanceState\":\"balanced-65\",\"keepNodesUpdated\":true,\"keepNode"...>>]},
 {{metakv,<<"/eventing/config/keepNodes">>},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780339}}]}|
   <<"[\"1678dfae96c38e07dff43c49b9f6967b\"]">>]},
 {{metakv,<<"/cbas/config/node/1678dfae96c38e07dff43c49b9f6967b">>},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780341}}]}|
   <<"{\"configVersion\":1,\"config\":{\"additionalCcNodes\":[],\"address\":\"0.0.0.0\",\"analyticsCbHome\":\"/opt/couchbase\",\"analyticsCcHttpPort\":\"9111\",\"analyticsHttpAdminListenPort\":\"9110\",\"analyticsHttpListenAddress\":[\"127.0.0.1\",\"172.18.0.2\"],\"analyticsHttpListenPort\":\"8095\",\"analyticsHttpsListenAddress\":[\"127.0.0.1\",\"172.18.0.2\"],\"analyticsHttpsListenPort\":\"18095\",\"analyticsNodeName\":\"127.0.0.1:8091\",\"analyticsSslEnabled\":true,\"cbasAddress\":\"127.0.0.1\",\"cbasPort\":\"9122\",\"ccAddress\":\"0.0.0.0\",\"ccAnalyticsHttpListenPort\":\"9111\",\"ccAnalyticsMetadataPartitionId\":-1,"...>>]},
 {{metakv,<<"/cbas/bootstrap/ensureCc/1678dfae96c38e07dff43c49b9f6967b">>},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780345}}]}|
   <<"{}">>]},
 {vbucket_map_history,
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780348}}]},
   {[['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     ['ns_1@127.0.0.1'],
     [...]|...],
    [{replication_topology,star},
     {tags,undefined},
     {use_vbmap_greedy_optimization,true},
     {max_slaves,10}]}]},
 {{metakv,<<"/cbas/cluster/state/partitions/topology">>},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780354}}]}|
   <<"{\"version\":1,\"revision\":0,\"balanced\":true,\"ccNodeId\":\"1678dfae96c38e07dff43c49b9f6967b\",\"metadataPartition\":-1,\"numReplicas\":0,\"partitions\":[{\"id\":\"0\",\"origin\":\"1678dfae96c38e07dff43c49b9f6967b\",\"master\":\"1678dfae96c38e07dff43c49b9f6967b\",\"replicas\":[]},{\"id\":\"-1\",\"origin\":\"1678dfae96c38e07dff43c49b9f6967b\",\"master\":\"1678dfae96c38e07dff43c49b9f6967b\",\"replicas\":[]}]}">>]},
 {{metakv,<<"/indexing/info/versionToken">>},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780356}}]}|
   <<"{\"Version\":6}">>]},
 {{metakv,<<"/indexing/settings/config">>},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{6,63829780389}}]}|
   <<"{\"indexer.plasma.backIndex.enableInMemoryCompression\":true,\"indexer.plasma.backIndex.enablePageBloomFilter\":false,\"indexer.plasma.mainIndex.enableInMemoryCompression\":true,\"indexer.settings.allow_large_keys\":true,\"indexer.settings.bufferPoolBlockSize\":16384,\"indexer.settings.build.batch_size\":5,\"indexer.settings.compaction.abort_exceed_interval\":false,\"indexer.settings.compaction.check_period\":30,\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thu"...>>]},
 {{metakv,<<"/indexing/settings/config/features/PlasmaInMemoryCompression">>},
  [{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780389}}]}|
   <<"{}">>]},
 {{node,'ns_1@127.0.0.1',stats_scrape_dynamic_intervals},
  [{'_vclock',63829780449,
    [{<<"1678dfae96c38e07dff43c49b9f6967b">>,{1,63829780929}}]}]}]
[error_logger:info,2022-09-08T12:07:39.393Z,ns_1@127.0.0.1:ns_config_sup<0.258.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_config_sup}
    started: [{pid,<0.262.0>},
              {id,ns_config},
              {mfargs,{ns_config,start_link,
                                 ["/opt/couchbase/etc/couchbase/config",
                                  ns_config_default]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2022-09-08T12:07:39.395Z,ns_1@127.0.0.1:ns_config_sup<0.258.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_config_sup}
    started: [{pid,<0.268.0>},
              {id,ns_config_remote},
              {mfargs,{ns_config_replica,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2022-09-08T12:07:39.396Z,ns_1@127.0.0.1:ns_config_sup<0.258.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_config_sup}
    started: [{pid,<0.269.0>},
              {id,ns_config_log},
              {mfargs,{ns_config_log,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2022-09-08T12:07:39.396Z,ns_1@127.0.0.1:ns_server_cluster_sup<0.220.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_cluster_sup}
    started: [{pid,<0.258.0>},
              {id,ns_config_sup},
              {mfargs,{ns_config_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2022-09-08T12:07:39.399Z,ns_1@127.0.0.1:ns_config_log<0.269.0>:ns_config_log:log_common:275]config change:
{local_changes_count,<<"1678dfae96c38e07dff43c49b9f6967b">>} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{53,63829858059}}]}]
[error_logger:info,2022-09-08T12:07:39.399Z,ns_1@127.0.0.1:ns_server_cluster_sup<0.220.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_cluster_sup}
    started: [{pid,<0.271.0>},
              {id,netconfig_updater},
              {mfargs,{netconfig_updater,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2022-09-08T12:07:39.401Z,ns_1@127.0.0.1:ns_server_cluster_sup<0.220.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_cluster_sup}
    started: [{pid,<0.273.0>},
              {id,json_rpc_connection_sup},
              {mfargs,{json_rpc_connection_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2022-09-08T12:07:39.410Z,ns_1@127.0.0.1:ns_server_nodes_sup<0.276.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.277.0>},
              {name,chronicle_compat_events},
              {mfargs,{chronicle_compat_events,start_link,[]}},
              {restart_type,permanent},
              {shutdown,5000},
              {child_type,worker}]
[error_logger:info,2022-09-08T12:07:39.411Z,ns_1@127.0.0.1:ns_server_nodes_sup<0.276.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.282.0>},
              {name,remote_monitors},
              {mfargs,{remote_monitors,start_link,[]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[ns_server:debug,2022-09-08T12:07:39.412Z,ns_1@127.0.0.1:menelaus_barrier<0.283.0>:one_shot_barrier:barrier_body:52]Barrier menelaus_barrier has started
[error_logger:info,2022-09-08T12:07:39.412Z,ns_1@127.0.0.1:ns_server_nodes_sup<0.276.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.283.0>},
              {name,menelaus_barrier},
              {mfargs,{menelaus_sup,barrier_start_link,[]}},
              {restart_type,temporary},
              {shutdown,1000},
              {child_type,worker}]
[error_logger:info,2022-09-08T12:07:39.413Z,ns_1@127.0.0.1:ns_server_nodes_sup<0.276.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.284.0>},
              {name,rest_lhttpc_pool},
              {mfargs,{lhttpc_manager,start_link,
                                      [[{name,rest_lhttpc_pool},
                                        {connection_timeout,120000},
                                        {pool_size,20}]]}},
              {restart_type,{permanent,1}},
              {shutdown,1000},
              {child_type,worker}]
[error_logger:info,2022-09-08T12:07:39.419Z,ns_1@127.0.0.1:ns_server_nodes_sup<0.276.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.285.0>},
              {name,memcached_refresh},
              {mfargs,{memcached_refresh,start_link,[]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[error_logger:info,2022-09-08T12:07:39.422Z,ns_1@127.0.0.1:ns_server_nodes_sup<0.276.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.286.0>},
              {name,ns_secrets},
              {mfargs,{ns_secrets,start_link,[]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[error_logger:info,2022-09-08T12:07:39.425Z,ns_1@127.0.0.1:ns_ssl_services_sup<0.287.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_ssl_services_sup}
    started: [{pid,<0.288.0>},
              {id,ssl_service_events},
              {mfargs,{gen_event,start_link,[{local,ssl_service_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2022-09-08T12:07:39.435Z,ns_1@127.0.0.1:ns_ssl_services_setup<0.289.0>:ns_ssl_services_setup:maybe_store_ca_certs:672]Considering to store CA certs
[error_logger:info,2022-09-08T12:07:39.471Z,ns_1@127.0.0.1:ns_ssl_services_sup<0.287.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_ssl_services_sup}
    started: [{pid,<0.289.0>},
              {id,ns_ssl_services_setup},
              {mfargs,{ns_ssl_services_setup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:info,2022-09-08T12:07:39.474Z,ns_1@127.0.0.1:ns_ssl_services_setup<0.289.0>:ns_ssl_services_setup:validate_pkey:856]Private key passphrase validation suceeded
[ns_server:info,2022-09-08T12:07:39.489Z,ns_1@127.0.0.1:<0.294.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:129]Loaded pluggable UI specification for backup from "/opt/couchbase/etc/couchbase/pluggable-ui-backup.json"
[ns_server:info,2022-09-08T12:07:39.489Z,ns_1@127.0.0.1:<0.294.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:129]Loaded pluggable UI specification for cbas from "/opt/couchbase/etc/couchbase/pluggable-ui-cbas.json"
[ns_server:info,2022-09-08T12:07:39.490Z,ns_1@127.0.0.1:<0.294.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:129]Loaded pluggable UI specification for eventing from "/opt/couchbase/etc/couchbase/pluggable-ui-eventing.json"
[ns_server:info,2022-09-08T12:07:39.490Z,ns_1@127.0.0.1:<0.294.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:129]Loaded pluggable UI specification for fts from "/opt/couchbase/etc/couchbase/pluggable-ui-fts.json"
[ns_server:info,2022-09-08T12:07:39.490Z,ns_1@127.0.0.1:<0.294.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:129]Loaded pluggable UI specification for n1ql from "/opt/couchbase/etc/couchbase/pluggable-ui-query.json"
[ns_server:info,2022-09-08T12:07:39.520Z,ns_1@127.0.0.1:<0.294.0>:menelaus_web:maybe_start_http_server:126]Started web service with options:
[{ip,"0.0.0.0"},
 [{keyfile,"/opt/couchbase/var/lib/couchbase/config/certs/pkey.pem"},
  {certfile,"/opt/couchbase/var/lib/couchbase/config/certs/chain.pem"},
  {versions,['tlsv1.2','tlsv1.3']},
  {cacerts,[<<48,130,3,33,48,130,2,9,160,3,2,1,2,2,8,23,18,154,82,164,1,21,
              60,48,13,6,9,42,134,72,134,247,13,1,1,11,5,0,48,36,49,34,48,32,
              6,3,85,4,3,19,25,67,111,117,99,104,98,97,115,101,32,83,101,114,
              118,101,114,32,57,52,100,54,52,100,52,53,48,30,23,13,49,51,48,
              49,48,49,48,48,48,48,48,48,90,23,13,52,57,49,50,51,49,50,51,53,
              57,53,57,90,48,36,49,34,48,32,6,3,85,4,3,19,25,67,111,117,99,
              104,98,97,115,101,32,83,101,114,118,101,114,32,57,52,100,54,52,
              100,52,53,48,130,1,34,48,13,6,9,42,134,72,134,247,13,1,1,1,5,0,
              3,130,1,15,0,48,130,1,10,2,130,1,1,0,194,175,58,98,213,11,69,
              224,252,171,108,141,152,64,29,26,186,3,145,87,68,104,215,198,
              17,190,103,200,152,225,103,33,154,77,2,101,24,101,21,58,240,77,
              253,45,146,251,247,129,14,85,219,180,228,208,116,181,115,78,
              125,90,198,7,168,16,55,7,150,237,66,245,70,32,139,220,232,104,
              103,245,65,177,31,106,72,110,120,151,250,197,73,215,91,175,13,
              220,52,194,168,103,38,26,60,90,13,132,154,32,60,227,248,208,
              130,20,102,21,35,42,0,59,200,160,63,47,64,42,42,126,117,233,54,
              223,184,15,104,95,253,242,228,249,181,15,39,32,210,132,102,13,
              23,167,42,95,220,164,228,82,191,6,139,169,190,0,223,155,219,
              188,13,245,152,27,48,187,120,139,42,26,103,232,190,238,187,241,
              79,160,250,145,59,20,219,5,142,169,29,251,185,220,133,142,155,
              49,155,191,115,197,132,112,209,160,210,144,35,126,25,150,32,
              139,244,211,244,232,126,190,241,155,155,47,160,187,39,229,223,
              27,236,217,144,127,217,93,214,184,72,80,152,220,99,162,102,147,
              233,116,67,156,133,32,112,198,46,105,2,3,1,0,1,163,87,48,85,48,
              14,6,3,85,29,15,1,1,255,4,4,3,2,2,164,48,19,6,3,85,29,37,4,12,
              48,10,6,8,43,6,1,5,5,7,3,1,48,15,6,3,85,29,19,1,1,255,4,5,48,3,
              1,1,255,48,29,6,3,85,29,14,4,22,4,20,26,62,215,165,145,243,120,
              59,142,217,59,224,230,37,149,26,147,152,18,136,48,13,6,9,42,
              134,72,134,247,13,1,1,11,5,0,3,130,1,1,0,75,201,225,9,128,150,
              65,117,74,201,239,161,49,117,149,221,111,121,254,46,56,20,44,
              215,53,72,21,97,155,106,187,190,52,189,22,217,122,255,61,75,17,
              115,57,144,77,227,111,247,253,175,36,226,26,104,126,16,85,98,0,
              83,54,183,38,69,236,89,157,164,52,166,213,142,23,103,2,116,161,
              20,86,33,226,146,49,227,200,165,194,227,135,194,150,0,12,241,
              208,44,8,56,108,176,101,158,239,144,179,231,21,58,226,99,160,
              31,227,96,122,178,108,38,165,81,36,60,232,52,211,27,63,188,131,
              246,50,242,200,250,91,139,169,137,255,138,88,27,7,158,108,179,
              241,87,134,231,130,126,229,203,90,0,116,187,60,128,244,113,169,
              146,19,38,91,169,55,122,218,31,17,122,237,2,4,182,252,217,173,
              164,24,141,61,235,200,42,49,168,94,140,32,130,90,168,71,189,
              146,149,28,198,91,62,48,122,221,237,203,241,28,223,158,236,176,
              99,91,183,59,184,31,73,192,215,55,210,146,64,72,11,232,58,130,
              57,242,47,185,118,10,199,112,44,101,57,194,120,18,234,209,13,
              216,190,53,243,209,52>>]},
  {dh,<<48,130,1,8,2,130,1,1,0,152,202,99,248,92,201,35,238,246,5,77,93,120,
        10,118,129,36,52,111,193,167,220,49,229,106,105,152,133,121,157,73,
        158,232,153,197,197,21,171,140,30,207,52,165,45,8,221,162,21,199,183,
        66,211,247,51,224,102,214,190,130,96,253,218,193,35,43,139,145,89,200,
        250,145,92,50,80,134,135,188,205,254,148,122,136,237,220,186,147,187,
        104,159,36,147,217,117,74,35,163,145,249,175,242,18,221,124,54,140,16,
        246,169,84,252,45,47,99,136,30,60,189,203,61,86,225,117,255,4,91,46,
        110,167,173,106,51,65,10,248,94,225,223,73,40,232,140,26,11,67,170,
        118,190,67,31,127,233,39,68,88,132,171,224,62,187,207,160,189,209,101,
        74,8,205,174,146,173,80,105,144,246,25,153,86,36,24,178,163,64,202,
        221,95,184,110,244,32,226,217,34,55,188,230,55,16,216,247,173,246,139,
        76,187,66,211,159,17,46,20,18,48,80,27,250,96,189,29,214,234,241,34,
        69,254,147,103,220,133,40,164,84,8,44,241,61,164,151,9,135,41,60,75,4,
        202,133,173,72,6,69,167,89,112,174,40,229,171,2,1,2>>},
  {ciphers,[{any,aes_256_gcm,aead,sha384},
            {any,aes_128_gcm,aead,sha256},
            {any,chacha20_poly1305,aead,sha256},
            {any,aes_128_ccm,aead,sha256},
            {any,aes_128_ccm_8,aead,sha256},
            {ecdhe_ecdsa,aes_256_gcm,aead,sha384},
            {ecdhe_rsa,aes_256_gcm,aead,sha384},
            {ecdhe_ecdsa,aes_256_ccm,aead},
            {ecdhe_ecdsa,aes_256_ccm_8,aead},
            {ecdhe_ecdsa,chacha20_poly1305,aead,sha256},
            {ecdhe_rsa,chacha20_poly1305,aead,sha256},
            {ecdhe_ecdsa,aes_128_gcm,aead,sha256},
            {ecdhe_rsa,aes_128_gcm,aead,sha256},
            {ecdhe_ecdsa,aes_128_ccm,aead},
            {ecdhe_ecdsa,aes_128_ccm_8,aead},
            {ecdh_ecdsa,aes_256_gcm,aead,sha384},
            {ecdh_rsa,aes_256_gcm,aead,sha384},
            {ecdh_ecdsa,aes_128_gcm,aead,sha256},
            {ecdh_rsa,aes_128_gcm,aead,sha256},
            {dhe_rsa,aes_256_gcm,aead,sha384},
            {dhe_dss,aes_256_gcm,aead,sha384},
            {dhe_rsa,aes_128_gcm,aead,sha256},
            {dhe_dss,aes_128_gcm,aead,sha256},
            {dhe_rsa,chacha20_poly1305,aead,sha256},
            {rsa_psk,aes_256_gcm,aead,sha384},
            {rsa_psk,aes_128_gcm,aead,sha256},
            {rsa,aes_256_gcm,aead,sha384},
            {rsa,aes_128_gcm,aead,sha256}]},
  {honor_cipher_order,true},
  {secure_renegotiate,true},
  {client_renegotiation,false},
  {password,"********"}],
 {ssl,true},
 {port,18091}]
[error_logger:info,2022-09-08T12:07:39.521Z,ns_1@127.0.0.1:<0.294.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.294.0>,menelaus_web}
    started: [{pid,<0.296.0>},
              {id,menelaus_web_ipv4},
              {mfargs,
                  {menelaus_web,http_server,
                      [[{afamily,inet},
                        {ssl,true},
                        {name,menelaus_web_ssl},
                        {ssl_opts_fun,#Fun<ns_ssl_services_setup.6.133565730>},
                        {port,18091},
                        {nodelay,true},
                        {approot,
                            "/opt/couchbase/lib/ns_server/erlang/lib/ns_server/priv/public"}]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[ns_server:info,2022-09-08T12:07:39.524Z,ns_1@127.0.0.1:<0.294.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:129]Loaded pluggable UI specification for backup from "/opt/couchbase/etc/couchbase/pluggable-ui-backup.json"
[ns_server:info,2022-09-08T12:07:39.524Z,ns_1@127.0.0.1:<0.294.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:129]Loaded pluggable UI specification for cbas from "/opt/couchbase/etc/couchbase/pluggable-ui-cbas.json"
[ns_server:info,2022-09-08T12:07:39.524Z,ns_1@127.0.0.1:<0.294.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:129]Loaded pluggable UI specification for eventing from "/opt/couchbase/etc/couchbase/pluggable-ui-eventing.json"
[ns_server:info,2022-09-08T12:07:39.524Z,ns_1@127.0.0.1:<0.294.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:129]Loaded pluggable UI specification for fts from "/opt/couchbase/etc/couchbase/pluggable-ui-fts.json"
[ns_server:info,2022-09-08T12:07:39.524Z,ns_1@127.0.0.1:<0.294.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:129]Loaded pluggable UI specification for n1ql from "/opt/couchbase/etc/couchbase/pluggable-ui-query.json"
[ns_server:info,2022-09-08T12:07:39.527Z,ns_1@127.0.0.1:<0.294.0>:menelaus_web:maybe_start_http_server:126]Started web service with options:
[{ip,"::"},
 [{keyfile,"/opt/couchbase/var/lib/couchbase/config/certs/pkey.pem"},
  {certfile,"/opt/couchbase/var/lib/couchbase/config/certs/chain.pem"},
  {versions,['tlsv1.2','tlsv1.3']},
  {cacerts,[<<48,130,3,33,48,130,2,9,160,3,2,1,2,2,8,23,18,154,82,164,1,21,
              60,48,13,6,9,42,134,72,134,247,13,1,1,11,5,0,48,36,49,34,48,32,
              6,3,85,4,3,19,25,67,111,117,99,104,98,97,115,101,32,83,101,114,
              118,101,114,32,57,52,100,54,52,100,52,53,48,30,23,13,49,51,48,
              49,48,49,48,48,48,48,48,48,90,23,13,52,57,49,50,51,49,50,51,53,
              57,53,57,90,48,36,49,34,48,32,6,3,85,4,3,19,25,67,111,117,99,
              104,98,97,115,101,32,83,101,114,118,101,114,32,57,52,100,54,52,
              100,52,53,48,130,1,34,48,13,6,9,42,134,72,134,247,13,1,1,1,5,0,
              3,130,1,15,0,48,130,1,10,2,130,1,1,0,194,175,58,98,213,11,69,
              224,252,171,108,141,152,64,29,26,186,3,145,87,68,104,215,198,
              17,190,103,200,152,225,103,33,154,77,2,101,24,101,21,58,240,77,
              253,45,146,251,247,129,14,85,219,180,228,208,116,181,115,78,
              125,90,198,7,168,16,55,7,150,237,66,245,70,32,139,220,232,104,
              103,245,65,177,31,106,72,110,120,151,250,197,73,215,91,175,13,
              220,52,194,168,103,38,26,60,90,13,132,154,32,60,227,248,208,
              130,20,102,21,35,42,0,59,200,160,63,47,64,42,42,126,117,233,54,
              223,184,15,104,95,253,242,228,249,181,15,39,32,210,132,102,13,
              23,167,42,95,220,164,228,82,191,6,139,169,190,0,223,155,219,
              188,13,245,152,27,48,187,120,139,42,26,103,232,190,238,187,241,
              79,160,250,145,59,20,219,5,142,169,29,251,185,220,133,142,155,
              49,155,191,115,197,132,112,209,160,210,144,35,126,25,150,32,
              139,244,211,244,232,126,190,241,155,155,47,160,187,39,229,223,
              27,236,217,144,127,217,93,214,184,72,80,152,220,99,162,102,147,
              233,116,67,156,133,32,112,198,46,105,2,3,1,0,1,163,87,48,85,48,
              14,6,3,85,29,15,1,1,255,4,4,3,2,2,164,48,19,6,3,85,29,37,4,12,
              48,10,6,8,43,6,1,5,5,7,3,1,48,15,6,3,85,29,19,1,1,255,4,5,48,3,
              1,1,255,48,29,6,3,85,29,14,4,22,4,20,26,62,215,165,145,243,120,
              59,142,217,59,224,230,37,149,26,147,152,18,136,48,13,6,9,42,
              134,72,134,247,13,1,1,11,5,0,3,130,1,1,0,75,201,225,9,128,150,
              65,117,74,201,239,161,49,117,149,221,111,121,254,46,56,20,44,
              215,53,72,21,97,155,106,187,190,52,189,22,217,122,255,61,75,17,
              115,57,144,77,227,111,247,253,175,36,226,26,104,126,16,85,98,0,
              83,54,183,38,69,236,89,157,164,52,166,213,142,23,103,2,116,161,
              20,86,33,226,146,49,227,200,165,194,227,135,194,150,0,12,241,
              208,44,8,56,108,176,101,158,239,144,179,231,21,58,226,99,160,
              31,227,96,122,178,108,38,165,81,36,60,232,52,211,27,63,188,131,
              246,50,242,200,250,91,139,169,137,255,138,88,27,7,158,108,179,
              241,87,134,231,130,126,229,203,90,0,116,187,60,128,244,113,169,
              146,19,38,91,169,55,122,218,31,17,122,237,2,4,182,252,217,173,
              164,24,141,61,235,200,42,49,168,94,140,32,130,90,168,71,189,
              146,149,28,198,91,62,48,122,221,237,203,241,28,223,158,236,176,
              99,91,183,59,184,31,73,192,215,55,210,146,64,72,11,232,58,130,
              57,242,47,185,118,10,199,112,44,101,57,194,120,18,234,209,13,
              216,190,53,243,209,52>>]},
  {dh,<<48,130,1,8,2,130,1,1,0,152,202,99,248,92,201,35,238,246,5,77,93,120,
        10,118,129,36,52,111,193,167,220,49,229,106,105,152,133,121,157,73,
        158,232,153,197,197,21,171,140,30,207,52,165,45,8,221,162,21,199,183,
        66,211,247,51,224,102,214,190,130,96,253,218,193,35,43,139,145,89,200,
        250,145,92,50,80,134,135,188,205,254,148,122,136,237,220,186,147,187,
        104,159,36,147,217,117,74,35,163,145,249,175,242,18,221,124,54,140,16,
        246,169,84,252,45,47,99,136,30,60,189,203,61,86,225,117,255,4,91,46,
        110,167,173,106,51,65,10,248,94,225,223,73,40,232,140,26,11,67,170,
        118,190,67,31,127,233,39,68,88,132,171,224,62,187,207,160,189,209,101,
        74,8,205,174,146,173,80,105,144,246,25,153,86,36,24,178,163,64,202,
        221,95,184,110,244,32,226,217,34,55,188,230,55,16,216,247,173,246,139,
        76,187,66,211,159,17,46,20,18,48,80,27,250,96,189,29,214,234,241,34,
        69,254,147,103,220,133,40,164,84,8,44,241,61,164,151,9,135,41,60,75,4,
        202,133,173,72,6,69,167,89,112,174,40,229,171,2,1,2>>},
  {ciphers,[{any,aes_256_gcm,aead,sha384},
            {any,aes_128_gcm,aead,sha256},
            {any,chacha20_poly1305,aead,sha256},
            {any,aes_128_ccm,aead,sha256},
            {any,aes_128_ccm_8,aead,sha256},
            {ecdhe_ecdsa,aes_256_gcm,aead,sha384},
            {ecdhe_rsa,aes_256_gcm,aead,sha384},
            {ecdhe_ecdsa,aes_256_ccm,aead},
            {ecdhe_ecdsa,aes_256_ccm_8,aead},
            {ecdhe_ecdsa,chacha20_poly1305,aead,sha256},
            {ecdhe_rsa,chacha20_poly1305,aead,sha256},
            {ecdhe_ecdsa,aes_128_gcm,aead,sha256},
            {ecdhe_rsa,aes_128_gcm,aead,sha256},
            {ecdhe_ecdsa,aes_128_ccm,aead},
            {ecdhe_ecdsa,aes_128_ccm_8,aead},
            {ecdh_ecdsa,aes_256_gcm,aead,sha384},
            {ecdh_rsa,aes_256_gcm,aead,sha384},
            {ecdh_ecdsa,aes_128_gcm,aead,sha256},
            {ecdh_rsa,aes_128_gcm,aead,sha256},
            {dhe_rsa,aes_256_gcm,aead,sha384},
            {dhe_dss,aes_256_gcm,aead,sha384},
            {dhe_rsa,aes_128_gcm,aead,sha256},
            {dhe_dss,aes_128_gcm,aead,sha256},
            {dhe_rsa,chacha20_poly1305,aead,sha256},
            {rsa_psk,aes_256_gcm,aead,sha384},
            {rsa_psk,aes_128_gcm,aead,sha256},
            {rsa,aes_256_gcm,aead,sha384},
            {rsa,aes_128_gcm,aead,sha256}]},
  {honor_cipher_order,true},
  {secure_renegotiate,true},
  {client_renegotiation,false},
  {password,"********"}],
 {ssl,true},
 {port,18091}]
[error_logger:info,2022-09-08T12:07:39.527Z,ns_1@127.0.0.1:<0.294.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.294.0>,menelaus_web}
    started: [{pid,<0.315.0>},
              {id,menelaus_web_ipv6},
              {mfargs,
                  {menelaus_web,http_server,
                      [[{afamily,inet6},
                        {ssl,true},
                        {name,menelaus_web_ssl},
                        {ssl_opts_fun,#Fun<ns_ssl_services_setup.6.133565730>},
                        {port,18091},
                        {nodelay,true},
                        {approot,
                            "/opt/couchbase/lib/ns_server/erlang/lib/ns_server/priv/public"}]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[ns_server:debug,2022-09-08T12:07:39.528Z,ns_1@127.0.0.1:<0.293.0>:restartable:start_child:92]Started child process <0.294.0>
  MFA: {ns_ssl_services_setup,start_link_rest_service,[]}
[error_logger:info,2022-09-08T12:07:39.528Z,ns_1@127.0.0.1:ns_ssl_services_sup<0.287.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_ssl_services_sup}
    started: [{pid,<0.293.0>},
              {id,ns_rest_ssl_service},
              {mfargs,
                  {restartable,start_link,
                      [{ns_ssl_services_setup,start_link_rest_service,[]},
                       1000]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,worker}]

[error_logger:info,2022-09-08T12:07:39.528Z,ns_1@127.0.0.1:ns_server_nodes_sup<0.276.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.287.0>},
              {name,ns_ssl_services_sup},
              {mfargs,{ns_ssl_services_sup,start_link,[]}},
              {restart_type,permanent},
              {shutdown,infinity},
              {child_type,supervisor}]
[error_logger:info,2022-09-08T12:07:39.536Z,ns_1@127.0.0.1:ns_server_nodes_sup<0.276.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.334.0>},
              {name,ldap_auth_cache},
              {mfargs,{ldap_auth_cache,start_link,[]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[error_logger:info,2022-09-08T12:07:39.538Z,ns_1@127.0.0.1:users_sup<0.336.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,users_sup}
    started: [{pid,<0.337.0>},
              {id,user_storage_events},
              {mfargs,{gen_event,start_link,[{local,user_storage_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2022-09-08T12:07:39.544Z,ns_1@127.0.0.1:users_storage_sup<0.338.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,users_storage_sup}
    started: [{pid,<0.339.0>},
              {id,users_replicator},
              {mfargs,{menelaus_users,start_replicator,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2022-09-08T12:07:39.545Z,ns_1@127.0.0.1:users_replicator<0.339.0>:replicated_storage:wait_for_startup:46]Start waiting for startup
[ns_server:debug,2022-09-08T12:07:39.548Z,ns_1@127.0.0.1:users_storage<0.340.0>:replicated_storage:announce_startup:60]Announce my startup to <0.339.0>
[ns_server:debug,2022-09-08T12:07:39.548Z,ns_1@127.0.0.1:users_storage<0.340.0>:replicated_dets:open:148]Opening file "/opt/couchbase/var/lib/couchbase/config/users.dets"
[ns_server:debug,2022-09-08T12:07:39.548Z,ns_1@127.0.0.1:users_replicator<0.339.0>:replicated_storage:wait_for_startup:49]Received replicated storage registration from <0.340.0>
[error_logger:info,2022-09-08T12:07:39.548Z,ns_1@127.0.0.1:users_storage_sup<0.338.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,users_storage_sup}
    started: [{pid,<0.340.0>},
              {id,users_storage},
              {mfargs,{menelaus_users,start_storage,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2022-09-08T12:07:39.548Z,ns_1@127.0.0.1:users_sup<0.336.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,users_sup}
    started: [{pid,<0.338.0>},
              {id,users_storage_sup},
              {mfargs,{users_storage_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2022-09-08T12:07:39.554Z,ns_1@127.0.0.1:compiled_roles_cache<0.342.0>:versioned_cache:init:41]Starting versioned cache compiled_roles_cache
[error_logger:info,2022-09-08T12:07:39.554Z,ns_1@127.0.0.1:users_sup<0.336.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,users_sup}
    started: [{pid,<0.342.0>},
              {id,compiled_roles_cache},
              {mfargs,{menelaus_roles,start_compiled_roles_cache,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2022-09-08T12:07:39.558Z,ns_1@127.0.0.1:users_sup<0.336.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,users_sup}
    started: [{pid,<0.345.0>},
              {id,roles_cache},
              {mfargs,{roles_cache,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2022-09-08T12:07:39.558Z,ns_1@127.0.0.1:ns_server_nodes_sup<0.276.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.336.0>},
              {name,users_sup},
              {mfargs,{users_sup,start_link,[]}},
              {restart_type,permanent},
              {shutdown,infinity},
              {child_type,supervisor}]
[error_logger:info,2022-09-08T12:07:39.559Z,ns_1@127.0.0.1:kernel_safe_sup<0.67.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,kernel_safe_sup}
    started: [{pid,<0.349.0>},
              {id,dets_sup},
              {mfargs,{dets_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,supervisor}]

[error_logger:info,2022-09-08T12:07:39.559Z,ns_1@127.0.0.1:kernel_safe_sup<0.67.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,kernel_safe_sup}
    started: [{pid,<0.350.0>},
              {id,dets},
              {mfargs,{dets_server,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,2000},
              {child_type,worker}]

[error_logger:info,2022-09-08T12:07:39.571Z,ns_1@127.0.0.1:ns_server_nodes_sup<0.276.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.353.0>},
              {name,start_couchdb_node},
              {mfargs,{ns_server_nodes_sup,start_couchdb_node,[]}},
              {restart_type,{permanent,5}},
              {shutdown,86400000},
              {child_type,worker}]
[ns_server:debug,2022-09-08T12:07:39.571Z,ns_1@127.0.0.1:wait_link_to_couchdb_node<0.354.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:160]Waiting for ns_couchdb node to start
[error_logger:info,2022-09-08T12:07:39.582Z,ns_1@127.0.0.1:net_kernel<0.214.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {1,#Ref<0.1778819075.971898882.231430>}}}
[ns_server:debug,2022-09-08T12:07:39.582Z,ns_1@127.0.0.1:net_kernel<0.214.0>:cb_dist:info_msg:872]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2022-09-08T12:07:39.582Z,ns_1@127.0.0.1:cb_dist<0.211.0>:cb_dist:info_msg:872]cb_dist: Added connection {con,#Ref<0.1778819075.971767809.232054>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2022-09-08T12:07:39.582Z,ns_1@127.0.0.1:cb_dist<0.211.0>:cb_dist:info_msg:872]cb_dist: Updated connection: {con,#Ref<0.1778819075.971767809.232054>,
                                  inet_tcp_dist,<0.356.0>,
                                  #Ref<0.1778819075.971767809.232056>}
[ns_server:debug,2022-09-08T12:07:39.583Z,ns_1@127.0.0.1:cb_dist<0.211.0>:cb_dist:info_msg:872]cb_dist: Connection down: {con,#Ref<0.1778819075.971767809.232054>,
                               inet_tcp_dist,<0.356.0>,
                               #Ref<0.1778819075.971767809.232056>}
[error_logger:info,2022-09-08T12:07:39.583Z,ns_1@127.0.0.1:net_kernel<0.214.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.356.0>,shutdown}}
[error_logger:info,2022-09-08T12:07:39.583Z,ns_1@127.0.0.1:net_kernel<0.214.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1157,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2022-09-08T12:07:39.583Z,ns_1@127.0.0.1:<0.355.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:174]ns_couchdb is not ready: {badrpc,nodedown}
[ns_server:debug,2022-09-08T12:07:39.586Z,ns_1@127.0.0.1:users_storage<0.340.0>:replicated_dets:select_from_table:281][dets] Starting select with {users_storage,
                                [{{docv2,'_','_','_'},[],['$_']}],
                                100}
[ns_server:debug,2022-09-08T12:07:39.587Z,ns_1@127.0.0.1:users_storage<0.340.0>:replicated_dets:init_after_ack:141]Loading 0 items, 305 words took 38ms
[ns_server:debug,2022-09-08T12:07:39.587Z,ns_1@127.0.0.1:users_storage<0.340.0>:replicated_dets:select_from_table:281][ets] Starting select with {users_storage,
                               [{{docv2,{user,{'_',local}},'_','_'},
                                 [],
                                 ['$_']}],
                               100}
[ns_server:debug,2022-09-08T12:07:39.587Z,ns_1@127.0.0.1:roles_cache<0.345.0>:active_cache:renew:238]Starting roles_cache cache renewal
[ns_server:debug,2022-09-08T12:07:39.587Z,ns_1@127.0.0.1:roles_cache<0.345.0>:active_cache:renew:244]roles_cache cache renewal process originated 0 queries
[ns_server:debug,2022-09-08T12:07:39.596Z,ns_1@127.0.0.1:compiled_roles_cache<0.342.0>:versioned_cache:handle_info:89]Flushing cache compiled_roles_cache due to version change from undefined to {[7,
                                                                              1],
                                                                             {0,
                                                                              2994238951},
                                                                             {0,
                                                                              2994238951},
                                                                             true,
                                                                             [{"todo",
                                                                               <<"1d5e549a39e5fbca838b18cff4b8f22b">>,
                                                                               0}]}
[error_logger:info,2022-09-08T12:07:39.784Z,ns_1@127.0.0.1:net_kernel<0.214.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {2,#Ref<0.1778819075.971898882.231430>}}}
[ns_server:debug,2022-09-08T12:07:39.784Z,ns_1@127.0.0.1:net_kernel<0.214.0>:cb_dist:info_msg:872]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2022-09-08T12:07:39.784Z,ns_1@127.0.0.1:cb_dist<0.211.0>:cb_dist:info_msg:872]cb_dist: Added connection {con,#Ref<0.1778819075.971767809.232081>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2022-09-08T12:07:39.784Z,ns_1@127.0.0.1:cb_dist<0.211.0>:cb_dist:info_msg:872]cb_dist: Updated connection: {con,#Ref<0.1778819075.971767809.232081>,
                                  inet_tcp_dist,<0.358.0>,
                                  #Ref<0.1778819075.971767809.232084>}
[error_logger:info,2022-09-08T12:07:39.789Z,ns_1@127.0.0.1:net_kernel<0.214.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.358.0>,shutdown}}
[ns_server:debug,2022-09-08T12:07:39.789Z,ns_1@127.0.0.1:cb_dist<0.211.0>:cb_dist:info_msg:872]cb_dist: Connection down: {con,#Ref<0.1778819075.971767809.232081>,
                               inet_tcp_dist,<0.358.0>,
                               #Ref<0.1778819075.971767809.232084>}
[error_logger:info,2022-09-08T12:07:39.789Z,ns_1@127.0.0.1:net_kernel<0.214.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1157,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2022-09-08T12:07:39.789Z,ns_1@127.0.0.1:<0.355.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:174]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2022-09-08T12:07:39.990Z,ns_1@127.0.0.1:net_kernel<0.214.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {3,#Ref<0.1778819075.971898882.231430>}}}
[ns_server:debug,2022-09-08T12:07:39.990Z,ns_1@127.0.0.1:net_kernel<0.214.0>:cb_dist:info_msg:872]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2022-09-08T12:07:39.990Z,ns_1@127.0.0.1:cb_dist<0.211.0>:cb_dist:info_msg:872]cb_dist: Added connection {con,#Ref<0.1778819075.971767809.232093>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2022-09-08T12:07:39.990Z,ns_1@127.0.0.1:cb_dist<0.211.0>:cb_dist:info_msg:872]cb_dist: Updated connection: {con,#Ref<0.1778819075.971767809.232093>,
                                  inet_tcp_dist,<0.360.0>,
                                  #Ref<0.1778819075.971767809.232096>}
[error_logger:info,2022-09-08T12:07:40.042Z,ns_1@127.0.0.1:net_kernel<0.214.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.360.0>,{recv_challenge_ack_failed,{error,closed}}}}
[ns_server:debug,2022-09-08T12:07:40.042Z,ns_1@127.0.0.1:cb_dist<0.211.0>:cb_dist:info_msg:872]cb_dist: Connection down: {con,#Ref<0.1778819075.971767809.232093>,
                               inet_tcp_dist,<0.360.0>,
                               #Ref<0.1778819075.971767809.232096>}
[error_logger:info,2022-09-08T12:07:40.042Z,ns_1@127.0.0.1:net_kernel<0.214.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1157,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2022-09-08T12:07:40.042Z,ns_1@127.0.0.1:<0.355.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:174]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2022-09-08T12:07:40.243Z,ns_1@127.0.0.1:net_kernel<0.214.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {4,#Ref<0.1778819075.971898882.231430>}}}
[ns_server:debug,2022-09-08T12:07:40.243Z,ns_1@127.0.0.1:net_kernel<0.214.0>:cb_dist:info_msg:872]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2022-09-08T12:07:40.243Z,ns_1@127.0.0.1:cb_dist<0.211.0>:cb_dist:info_msg:872]cb_dist: Added connection {con,#Ref<0.1778819075.971767812.231051>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2022-09-08T12:07:40.243Z,ns_1@127.0.0.1:cb_dist<0.211.0>:cb_dist:info_msg:872]cb_dist: Updated connection: {con,#Ref<0.1778819075.971767812.231051>,
                                  inet_tcp_dist,<0.362.0>,
                                  #Ref<0.1778819075.971767811.231508>}
[ns_server:info,2022-09-08T12:07:40.252Z,ns_1@127.0.0.1:ns_couchdb_port<0.353.0>:ns_port_server:log:226]ns_couchdb<0.353.0>: =ERROR REPORT==== 8-Sep-2022::12:07:40.041077 ===
ns_couchdb<0.353.0>: ** Connection attempt from node 'ns_1@127.0.0.1' rejected. Invalid challenge reply. **
ns_couchdb<0.353.0>: 

[ns_server:debug,2022-09-08T12:07:40.254Z,ns_1@127.0.0.1:cb_dist<0.211.0>:cb_dist:info_msg:872]cb_dist: Connection down: {con,#Ref<0.1778819075.971767812.231051>,
                               inet_tcp_dist,<0.362.0>,
                               #Ref<0.1778819075.971767811.231508>}
[error_logger:info,2022-09-08T12:07:40.255Z,ns_1@127.0.0.1:net_kernel<0.214.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.362.0>,{recv_challenge_ack_failed,{error,closed}}}}
[error_logger:info,2022-09-08T12:07:40.255Z,ns_1@127.0.0.1:net_kernel<0.214.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1157,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2022-09-08T12:07:40.255Z,ns_1@127.0.0.1:<0.355.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:174]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2022-09-08T12:07:40.456Z,ns_1@127.0.0.1:net_kernel<0.214.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {5,#Ref<0.1778819075.971898882.231430>}}}
[ns_server:debug,2022-09-08T12:07:40.456Z,ns_1@127.0.0.1:net_kernel<0.214.0>:cb_dist:info_msg:872]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2022-09-08T12:07:40.456Z,ns_1@127.0.0.1:cb_dist<0.211.0>:cb_dist:info_msg:872]cb_dist: Added connection {con,#Ref<0.1778819075.971767812.231065>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2022-09-08T12:07:40.456Z,ns_1@127.0.0.1:cb_dist<0.211.0>:cb_dist:info_msg:872]cb_dist: Updated connection: {con,#Ref<0.1778819075.971767812.231065>,
                                  inet_tcp_dist,<0.364.0>,
                                  #Ref<0.1778819075.971767812.231068>}
[error_logger:info,2022-09-08T12:07:40.463Z,ns_1@127.0.0.1:net_kernel<0.214.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.364.0>,{recv_challenge_ack_failed,{error,closed}}}}
[ns_server:debug,2022-09-08T12:07:40.463Z,ns_1@127.0.0.1:cb_dist<0.211.0>:cb_dist:info_msg:872]cb_dist: Connection down: {con,#Ref<0.1778819075.971767812.231065>,
                               inet_tcp_dist,<0.364.0>,
                               #Ref<0.1778819075.971767812.231068>}
[error_logger:info,2022-09-08T12:07:40.463Z,ns_1@127.0.0.1:net_kernel<0.214.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1157,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2022-09-08T12:07:40.464Z,ns_1@127.0.0.1:<0.355.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:174]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2022-09-08T12:07:40.664Z,ns_1@127.0.0.1:net_kernel<0.214.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {6,#Ref<0.1778819075.971898882.231430>}}}
[ns_server:debug,2022-09-08T12:07:40.664Z,ns_1@127.0.0.1:net_kernel<0.214.0>:cb_dist:info_msg:872]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2022-09-08T12:07:40.664Z,ns_1@127.0.0.1:cb_dist<0.211.0>:cb_dist:info_msg:872]cb_dist: Added connection {con,#Ref<0.1778819075.971767809.232112>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2022-09-08T12:07:40.664Z,ns_1@127.0.0.1:cb_dist<0.211.0>:cb_dist:info_msg:872]cb_dist: Updated connection: {con,#Ref<0.1778819075.971767809.232112>,
                                  inet_tcp_dist,<0.366.0>,
                                  #Ref<0.1778819075.971767809.232115>}
[ns_server:debug,2022-09-08T12:07:40.670Z,ns_1@127.0.0.1:cb_dist<0.211.0>:cb_dist:info_msg:872]cb_dist: Connection down: {con,#Ref<0.1778819075.971767809.232112>,
                               inet_tcp_dist,<0.366.0>,
                               #Ref<0.1778819075.971767809.232115>}
[error_logger:info,2022-09-08T12:07:40.670Z,ns_1@127.0.0.1:net_kernel<0.214.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.366.0>,{recv_challenge_ack_failed,{error,closed}}}}
[error_logger:info,2022-09-08T12:07:40.670Z,ns_1@127.0.0.1:net_kernel<0.214.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1157,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2022-09-08T12:07:40.670Z,ns_1@127.0.0.1:<0.355.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:174]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2022-09-08T12:07:40.871Z,ns_1@127.0.0.1:net_kernel<0.214.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {7,#Ref<0.1778819075.971898882.231430>}}}
[ns_server:debug,2022-09-08T12:07:40.871Z,ns_1@127.0.0.1:net_kernel<0.214.0>:cb_dist:info_msg:872]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2022-09-08T12:07:40.871Z,ns_1@127.0.0.1:cb_dist<0.211.0>:cb_dist:info_msg:872]cb_dist: Added connection {con,#Ref<0.1778819075.971767811.231528>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2022-09-08T12:07:40.871Z,ns_1@127.0.0.1:cb_dist<0.211.0>:cb_dist:info_msg:872]cb_dist: Updated connection: {con,#Ref<0.1778819075.971767811.231528>,
                                  inet_tcp_dist,<0.368.0>,
                                  #Ref<0.1778819075.971767811.231531>}
[ns_server:debug,2022-09-08T12:07:40.874Z,ns_1@127.0.0.1:cb_dist<0.211.0>:cb_dist:info_msg:872]cb_dist: Connection down: {con,#Ref<0.1778819075.971767811.231528>,
                               inet_tcp_dist,<0.368.0>,
                               #Ref<0.1778819075.971767811.231531>}
[error_logger:info,2022-09-08T12:07:40.874Z,ns_1@127.0.0.1:net_kernel<0.214.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.368.0>,{recv_challenge_ack_failed,{error,closed}}}}
[error_logger:info,2022-09-08T12:07:40.874Z,ns_1@127.0.0.1:net_kernel<0.214.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1157,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2022-09-08T12:07:40.874Z,ns_1@127.0.0.1:<0.355.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:174]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2022-09-08T12:07:41.075Z,ns_1@127.0.0.1:net_kernel<0.214.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {8,#Ref<0.1778819075.971898882.231430>}}}
[ns_server:debug,2022-09-08T12:07:41.075Z,ns_1@127.0.0.1:net_kernel<0.214.0>:cb_dist:info_msg:872]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2022-09-08T12:07:41.075Z,ns_1@127.0.0.1:cb_dist<0.211.0>:cb_dist:info_msg:872]cb_dist: Added connection {con,#Ref<0.1778819075.971767810.231448>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2022-09-08T12:07:41.075Z,ns_1@127.0.0.1:cb_dist<0.211.0>:cb_dist:info_msg:872]cb_dist: Updated connection: {con,#Ref<0.1778819075.971767810.231448>,
                                  inet_tcp_dist,<0.370.0>,
                                  #Ref<0.1778819075.971767810.231451>}
[ns_server:debug,2022-09-08T12:07:41.078Z,ns_1@127.0.0.1:cb_dist<0.211.0>:cb_dist:info_msg:872]cb_dist: Connection down: {con,#Ref<0.1778819075.971767810.231448>,
                               inet_tcp_dist,<0.370.0>,
                               #Ref<0.1778819075.971767810.231451>}
[error_logger:info,2022-09-08T12:07:41.078Z,ns_1@127.0.0.1:net_kernel<0.214.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.370.0>,{recv_challenge_ack_failed,{error,closed}}}}
[error_logger:info,2022-09-08T12:07:41.079Z,ns_1@127.0.0.1:net_kernel<0.214.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1157,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2022-09-08T12:07:41.079Z,ns_1@127.0.0.1:<0.355.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:174]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2022-09-08T12:07:41.279Z,ns_1@127.0.0.1:net_kernel<0.214.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {9,#Ref<0.1778819075.971898882.231430>}}}
[ns_server:debug,2022-09-08T12:07:41.280Z,ns_1@127.0.0.1:net_kernel<0.214.0>:cb_dist:info_msg:872]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2022-09-08T12:07:41.280Z,ns_1@127.0.0.1:cb_dist<0.211.0>:cb_dist:info_msg:872]cb_dist: Added connection {con,#Ref<0.1778819075.971767810.231454>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2022-09-08T12:07:41.280Z,ns_1@127.0.0.1:cb_dist<0.211.0>:cb_dist:info_msg:872]cb_dist: Updated connection: {con,#Ref<0.1778819075.971767810.231454>,
                                  inet_tcp_dist,<0.372.0>,
                                  #Ref<0.1778819075.971767810.231457>}
[ns_server:debug,2022-09-08T12:07:41.285Z,ns_1@127.0.0.1:<0.355.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:174]ns_couchdb is not ready: false
[ns_server:info,2022-09-08T12:07:41.653Z,ns_1@127.0.0.1:ns_couchdb_port<0.353.0>:ns_port_server:log:226]ns_couchdb<0.353.0>: Apache CouchDB v4.5.1-290-g27d1470 (LogLevel=info) is starting.

[error_logger:info,2022-09-08T12:07:41.928Z,ns_1@127.0.0.1:ns_server_nodes_sup<0.276.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.354.0>},
              {name,wait_for_couchdb_node},
              {mfargs,{erlang,apply,
                              [#Fun<ns_server_nodes_sup.0.18460070>,[]]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[ns_server:debug,2022-09-08T12:07:41.935Z,ns_1@127.0.0.1:ns_server_nodes_sup<0.276.0>:ns_storage_conf:setup_db_and_ix_paths:59]Initialize db_and_ix_paths variable with [{db_path,
                                           "/opt/couchbase/var/lib/couchbase/data"},
                                          {index_path,
                                           "/opt/couchbase/var/lib/couchbase/data"}]
[error_logger:info,2022-09-08T12:07:41.938Z,ns_1@127.0.0.1:ns_server_sup<0.376.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.377.0>},
              {name,ns_disksup},
              {mfargs,{ns_disksup,start_link,[]}},
              {restart_type,{permanent,4}},
              {shutdown,1000},
              {child_type,worker}]
[error_logger:info,2022-09-08T12:07:41.939Z,ns_1@127.0.0.1:ns_server_sup<0.376.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.378.0>},
              {name,diag_handler_worker},
              {mfargs,{work_queue,start_link,[diag_handler_worker]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[ns_server:info,2022-09-08T12:07:41.940Z,ns_1@127.0.0.1:ns_server_sup<0.376.0>:dir_size:start_link:33]Starting quick version of dir_size with program name: godu
[error_logger:info,2022-09-08T12:07:41.940Z,ns_1@127.0.0.1:ns_server_sup<0.376.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.379.0>},
              {name,dir_size},
              {mfargs,{dir_size,start_link,[]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[error_logger:info,2022-09-08T12:07:41.941Z,ns_1@127.0.0.1:ns_server_sup<0.376.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.380.0>},
              {name,request_tracker},
              {mfargs,{request_tracker,start_link,[]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[error_logger:info,2022-09-08T12:07:41.942Z,ns_1@127.0.0.1:ns_server_sup<0.376.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.381.0>},
              {name,chronicle_kv_log},
              {mfargs,{chronicle_kv_log,start_link,[]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[error_logger:info,2022-09-08T12:07:41.952Z,ns_1@127.0.0.1:ns_server_sup<0.376.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.383.0>},
              {name,ns_log},
              {mfargs,{ns_log,start_link,[]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[error_logger:info,2022-09-08T12:07:41.952Z,ns_1@127.0.0.1:ns_server_sup<0.376.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.384.0>},
              {name,event_log_events},
              {mfargs,{gen_event,start_link,[{local,event_log_events}]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[error_logger:info,2022-09-08T12:07:41.957Z,ns_1@127.0.0.1:ns_server_sup<0.376.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.385.0>},
              {name,event_log_server},
              {mfargs,{event_log_server,start_link,[]}},
              {restart_type,permanent},
              {shutdown,5000},
              {child_type,worker}]
[error_logger:info,2022-09-08T12:07:41.974Z,ns_1@127.0.0.1:ns_server_sup<0.376.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.387.0>},
              {name,initargs_updater},
              {mfargs,{initargs_updater,start_link,[]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[error_logger:info,2022-09-08T12:07:41.976Z,ns_1@127.0.0.1:ns_server_sup<0.376.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.389.0>},
              {name,timer_lag_recorder},
              {mfargs,{timer_lag_recorder,start_link,[]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[error_logger:info,2022-09-08T12:07:41.977Z,ns_1@127.0.0.1:ns_server_sup<0.376.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.390.0>},
              {name,ns_babysitter_log_consumer},
              {mfargs,{ns_log,start_link_babysitter_log_consumer,[]}},
              {restart_type,{permanent,4}},
              {shutdown,1000},
              {child_type,worker}]
[ns_server:info,2022-09-08T12:07:41.999Z,ns_1@127.0.0.1:ns_couchdb_port<0.353.0>:ns_port_server:log:226]ns_couchdb<0.353.0>: Apache CouchDB has started. Time to relax.
ns_couchdb<0.353.0>: 219: Booted. Waiting for shutdown request
ns_couchdb<0.353.0>: working as port

[ns_server:debug,2022-09-08T12:07:42.026Z,ns_1@127.0.0.1:prometheus_cfg<0.391.0>:prometheus_cfg:ensure_prometheus_config:811]Updating prometheus config file: /opt/couchbase/var/lib/couchbase/config/prometheus.yml
[ns_server:debug,2022-09-08T12:07:42.044Z,ns_1@127.0.0.1:prometheus_cfg<0.391.0>:prometheus_cfg:ensure_prometheus_config:811]Updating prometheus config file: /opt/couchbase/var/lib/couchbase/config/prometheus_rules.yml
[ns_server:debug,2022-09-08T12:07:42.112Z,ns_1@127.0.0.1:ns_config_log<0.269.0>:ns_config_log:log_common:275]config change:
{local_changes_count,<<"1678dfae96c38e07dff43c49b9f6967b">>} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{54,63829858062}}]}]
[ns_server:debug,2022-09-08T12:07:42.113Z,ns_1@127.0.0.1:ns_config_log<0.269.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',prometheus_auth_info} ->
[{'_vclock',[{<<"1678dfae96c38e07dff43c49b9f6967b">>,{3,63829858062}}]}|
 {"@prometheus",
  {auth,
   [{<<"plain">>,
     {sanitized,<<"AzSNoOfhFEdXHEZl/9m9/GdFvRyDxRpVxrVTCEw85PU=">>}},
    {<<"sha512">>,
     {[{<<"h">>,
        {sanitized,<<"shBVEvfrvmMXC0/qyinOBJo3Y8FS6e2wJEWjqkXipXA=">>}},
       {<<"s">>,
        <<"/8H3B+tThhyNixdpVJMoQ5PxPDjxkvf3UR20z/Vz+CG9YV6vahFN5W6o7ARhhey+VbafHIQs7Tq8KOAlIYbDRA==">>},
       {<<"i">>,4000}]}},
    {<<"sha256">>,
     {[{<<"h">>,
        {sanitized,<<"0dlIctyKw4w1+sB/HoaE642ZknlaacWVkDDooxVhCJw=">>}},
       {<<"s">>,<<"KY6VgBC5UpAAeHHWfSZQXjduPFRvPKsJHxVaF/pPvUU=">>},
       {<<"i">>,4000}]}},
    {<<"sha1">>,
     {[{<<"h">>,
        {sanitized,<<"+aZDEO+3P67y0DUV27Vjr97RL4shs3+W4uTug0qtLfU=">>}},
       {<<"s">>,<<"EJxPj7hBsWN8IUqs0ZwbOY6za/c=">>},
       {<<"i">>,4000}]}}]}}]
[ns_server:debug,2022-09-08T12:07:42.132Z,ns_1@127.0.0.1:prometheus_cfg<0.391.0>:prometheus_cfg:apply_config:626]Restarting Prometheus as the start specs have changed
[error_logger:info,2022-09-08T12:07:42.212Z,ns_1@127.0.0.1:ale_dynamic_sup<0.77.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ale_dynamic_sup}
    started: [{pid,<0.395.0>},
              {id,'sink-prometheus'},
              {mfargs,
                  {ale_dynamic_sup,delay_death,
                      [{ale_disk_sink,start_link,
                           ['sink-prometheus',
                            "/opt/couchbase/var/lib/couchbase/logs/prometheus.log",
                            [{rotation,
                                 [{compress,true},
                                  {size,41943040},
                                  {num_files,10},
                                  {buffer_size_max,52428800}]}]]},
                       1000]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2022-09-08T12:07:42.305Z,ns_1@127.0.0.1:ns_server_sup<0.376.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.391.0>},
              {name,prometheus_cfg},
              {mfargs,{prometheus_cfg,start_link,[]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[ns_server:debug,2022-09-08T12:07:42.310Z,ns_1@127.0.0.1:memcached_passwords<0.401.0>:memcached_cfg:init:58]Init config writer for memcached_passwords, "/opt/couchbase/var/lib/couchbase/isasl.pw"
[ns_server:debug,2022-09-08T12:07:42.312Z,ns_1@127.0.0.1:memcached_passwords<0.401.0>:memcached_cfg:write_cfg:148]Writing config file for: "/opt/couchbase/var/lib/couchbase/isasl.pw"
[ns_server:debug,2022-09-08T12:07:42.348Z,ns_1@127.0.0.1:memcached_passwords<0.401.0>:replicated_dets:select_from_table:281][ets] Starting select with {users_storage,
                               [{{docv2,{auth,{'_',local}},'_','_'},
                                 [],
                                 ['$_']}],
                               100}
[ns_server:debug,2022-09-08T12:07:42.350Z,ns_1@127.0.0.1:memcached_refresh<0.285.0>:memcached_refresh:handle_call:49]File rename from "/opt/couchbase/var/lib/couchbase/isasl.pw.tmp" to "/opt/couchbase/var/lib/couchbase/isasl.pw" is requested
[ns_server:debug,2022-09-08T12:07:42.353Z,ns_1@127.0.0.1:memcached_passwords<0.401.0>:memcached_cfg:rename_and_refresh:170]Successfully renamed "/opt/couchbase/var/lib/couchbase/isasl.pw.tmp" to "/opt/couchbase/var/lib/couchbase/isasl.pw"
[ns_server:debug,2022-09-08T12:07:42.353Z,ns_1@127.0.0.1:memcached_refresh<0.285.0>:memcached_refresh:handle_cast:55]Refresh of isasl requested
[error_logger:info,2022-09-08T12:07:42.353Z,ns_1@127.0.0.1:ns_server_sup<0.376.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.401.0>},
              {name,memcached_passwords},
              {mfargs,{memcached_passwords,start_link,[]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[ns_server:warn,2022-09-08T12:07:42.360Z,ns_1@127.0.0.1:memcached_refresh<0.285.0>:ns_memcached:connect:1237]Unable to connect: {error,{badmatch,[{inet,{error,econnrefused}}]}}.
[ns_server:debug,2022-09-08T12:07:42.360Z,ns_1@127.0.0.1:memcached_refresh<0.285.0>:memcached_refresh:handle_info:93]Refresh of [isasl] failed. Retry in 1000 ms.
[ns_server:debug,2022-09-08T12:07:42.361Z,ns_1@127.0.0.1:memcached_permissions<0.404.0>:memcached_cfg:init:58]Init config writer for memcached_permissions, "/opt/couchbase/var/lib/couchbase/config/memcached.rbac"
[ns_server:debug,2022-09-08T12:07:42.362Z,ns_1@127.0.0.1:memcached_permissions<0.404.0>:memcached_cfg:write_cfg:148]Writing config file for: "/opt/couchbase/var/lib/couchbase/config/memcached.rbac"
[ns_server:debug,2022-09-08T12:07:42.365Z,ns_1@127.0.0.1:memcached_permissions<0.404.0>:replicated_dets:select_from_table:281][ets] Starting select with {users_storage,
                               [{{docv2,{user,{'_',local}},'_','_'},
                                 [],
                                 ['$_']}],
                               100}
[ns_server:debug,2022-09-08T12:07:42.366Z,ns_1@127.0.0.1:memcached_refresh<0.285.0>:memcached_refresh:handle_call:49]File rename from "/opt/couchbase/var/lib/couchbase/config/memcached.rbac.tmp" to "/opt/couchbase/var/lib/couchbase/config/memcached.rbac" is requested
[ns_server:debug,2022-09-08T12:07:42.369Z,ns_1@127.0.0.1:memcached_permissions<0.404.0>:memcached_cfg:rename_and_refresh:170]Successfully renamed "/opt/couchbase/var/lib/couchbase/config/memcached.rbac.tmp" to "/opt/couchbase/var/lib/couchbase/config/memcached.rbac"
[ns_server:debug,2022-09-08T12:07:42.370Z,ns_1@127.0.0.1:memcached_refresh<0.285.0>:memcached_refresh:handle_cast:55]Refresh of rbac requested
[error_logger:info,2022-09-08T12:07:42.370Z,ns_1@127.0.0.1:ns_server_sup<0.376.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.404.0>},
              {name,memcached_permissions},
              {mfargs,{memcached_permissions,start_link,[]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[ns_server:warn,2022-09-08T12:07:42.371Z,ns_1@127.0.0.1:memcached_refresh<0.285.0>:ns_memcached:connect:1237]Unable to connect: {error,{badmatch,[{inet,{error,econnrefused}}]}}.
[ns_server:debug,2022-09-08T12:07:42.371Z,ns_1@127.0.0.1:memcached_refresh<0.285.0>:memcached_refresh:handle_info:93]Refresh of [rbac,isasl] failed. Retry in 1000 ms.
[error_logger:info,2022-09-08T12:07:42.371Z,ns_1@127.0.0.1:ns_server_sup<0.376.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.407.0>},
              {name,ns_email_alert},
              {mfargs,{ns_email_alert,start_link,[]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[error_logger:info,2022-09-08T12:07:42.373Z,ns_1@127.0.0.1:ns_node_disco_sup<0.408.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_node_disco_sup}
    started: [{pid,<0.409.0>},
              {id,ns_node_disco_events},
              {mfargs,{gen_event,start_link,[{local,ns_node_disco_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2022-09-08T12:07:42.373Z,ns_1@127.0.0.1:ns_node_disco<0.410.0>:ns_node_disco:init:112]Initting ns_node_disco with []
[ns_server:debug,2022-09-08T12:07:42.374Z,ns_1@127.0.0.1:ns_cookie_manager<0.223.0>:ns_cookie_manager:do_cookie_sync:101]ns_cookie_manager do_cookie_sync
[error_logger:info,2022-09-08T12:07:42.374Z,ns_1@127.0.0.1:ns_node_disco_sup<0.408.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_node_disco_sup}
    started: [{pid,<0.410.0>},
              {id,ns_node_disco},
              {mfargs,{ns_node_disco,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[user:info,2022-09-08T12:07:42.374Z,ns_1@127.0.0.1:ns_cookie_manager<0.223.0>:ns_cookie_manager:do_cookie_sync:122]Node 'ns_1@127.0.0.1' synchronized otp cookie {sanitized,
                                               <<"6jw2YfUfDxiQCs0fCwoUSmHbOdRC4E/A03fIDwJPR3g=">>} from cluster
[ns_server:debug,2022-09-08T12:07:42.374Z,ns_1@127.0.0.1:<0.412.0>:ns_node_disco:do_nodes_wanted_updated_fun:224]ns_node_disco: nodes_wanted updated: ['ns_1@127.0.0.1'], with cookie: {sanitized,
                                                                       <<"6jw2YfUfDxiQCs0fCwoUSmHbOdRC4E/A03fIDwJPR3g=">>}
[error_logger:info,2022-09-08T12:07:42.375Z,ns_1@127.0.0.1:ns_node_disco_sup<0.408.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_node_disco_sup}
    started: [{pid,<0.413.0>},
              {id,ns_node_disco_log},
              {mfargs,{ns_node_disco_log,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2022-09-08T12:07:42.377Z,ns_1@127.0.0.1:<0.412.0>:ns_node_disco:do_nodes_wanted_updated_fun:227]ns_node_disco: nodes_wanted pong: ['ns_1@127.0.0.1'], with cookie: {sanitized,
                                                                    <<"6jw2YfUfDxiQCs0fCwoUSmHbOdRC4E/A03fIDwJPR3g=">>}
[error_logger:info,2022-09-08T12:07:42.381Z,ns_1@127.0.0.1:ns_config_rep_sup<0.414.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_config_rep_sup}
    started: [{pid,<0.415.0>},
              {id,ns_config_rep_merger},
              {mfargs,{ns_config_rep,start_link_merger,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[ns_server:debug,2022-09-08T12:07:42.381Z,ns_1@127.0.0.1:ns_config_rep<0.416.0>:ns_config_rep:init:80]init pulling
[ns_server:debug,2022-09-08T12:07:42.381Z,ns_1@127.0.0.1:ns_config_rep<0.416.0>:ns_config_rep:init:82]init pushing
[error_logger:info,2022-09-08T12:07:42.386Z,ns_1@127.0.0.1:ns_config_rep_sup<0.414.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_config_rep_sup}
    started: [{pid,<0.416.0>},
              {id,ns_config_rep},
              {mfargs,{ns_config_rep,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2022-09-08T12:07:42.387Z,ns_1@127.0.0.1:ns_node_disco_sup<0.408.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_node_disco_sup}
    started: [{pid,<0.414.0>},
              {id,ns_config_rep_sup},
              {mfargs,{ns_config_rep_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2022-09-08T12:07:42.387Z,ns_1@127.0.0.1:ns_server_sup<0.376.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.408.0>},
              {name,ns_node_disco_sup},
              {mfargs,{ns_node_disco_sup,start_link,[]}},
              {restart_type,permanent},
              {shutdown,infinity},
              {child_type,supervisor}]
[error_logger:info,2022-09-08T12:07:42.387Z,ns_1@127.0.0.1:ns_server_sup<0.376.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.426.0>},
              {name,tombstone_agent},
              {mfargs,{tombstone_agent,start_link,[]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[error_logger:info,2022-09-08T12:07:42.389Z,ns_1@127.0.0.1:ns_server_sup<0.376.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.428.0>},
              {name,vbucket_map_mirror},
              {mfargs,{vbucket_map_mirror,start_link,[]}},
              {restart_type,permanent},
              {shutdown,brutal_kill},
              {child_type,worker}]
[error_logger:info,2022-09-08T12:07:42.391Z,ns_1@127.0.0.1:ns_server_sup<0.376.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.430.0>},
              {name,capi_url_cache},
              {mfargs,{capi_url_cache,start_link,[]}},
              {restart_type,permanent},
              {shutdown,brutal_kill},
              {child_type,worker}]
[error_logger:info,2022-09-08T12:07:42.394Z,ns_1@127.0.0.1:ns_server_sup<0.376.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.432.0>},
              {name,bucket_info_cache},
              {mfargs,{bucket_info_cache,start_link,[]}},
              {restart_type,permanent},
              {shutdown,brutal_kill},
              {child_type,worker}]
[error_logger:info,2022-09-08T12:07:42.395Z,ns_1@127.0.0.1:ns_server_sup<0.376.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.435.0>},
              {name,ns_tick_event},
              {mfargs,{gen_event,start_link,[{local,ns_tick_event}]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[error_logger:info,2022-09-08T12:07:42.395Z,ns_1@127.0.0.1:ns_server_sup<0.376.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.436.0>},
              {name,buckets_events},
              {mfargs,{gen_event,start_link,[{local,buckets_events}]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[error_logger:info,2022-09-08T12:07:42.395Z,ns_1@127.0.0.1:ns_server_sup<0.376.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.437.0>},
              {name,ns_stats_event},
              {mfargs,{gen_event,start_link,[{local,ns_stats_event}]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[error_logger:info,2022-09-08T12:07:42.398Z,ns_1@127.0.0.1:ns_server_sup<0.376.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.438.0>},
              {name,samples_loader_tasks},
              {mfargs,{samples_loader_tasks,start_link,[]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[error_logger:info,2022-09-08T12:07:42.403Z,ns_1@127.0.0.1:ns_heart_sup<0.439.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_heart_sup}
    started: [{pid,<0.440.0>},
              {id,ns_heart},
              {mfargs,{ns_heart,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2022-09-08T12:07:42.404Z,ns_1@127.0.0.1:ns_heart_sup<0.439.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_heart_sup}
    started: [{pid,<0.442.0>},
              {id,ns_heart_slow_updater},
              {mfargs,{ns_heart,start_link_slow_updater,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2022-09-08T12:07:42.404Z,ns_1@127.0.0.1:ns_server_sup<0.376.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.439.0>},
              {name,ns_heart_sup},
              {mfargs,{ns_heart_sup,start_link,[]}},
              {restart_type,permanent},
              {shutdown,infinity},
              {child_type,supervisor}]
[error_logger:info,2022-09-08T12:07:42.406Z,ns_1@127.0.0.1:ns_doctor_sup<0.446.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_doctor_sup}
    started: [{pid,<0.447.0>},
              {id,ns_doctor_events},
              {mfargs,{gen_event,start_link,[{local,ns_doctor_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2022-09-08T12:07:42.410Z,ns_1@127.0.0.1:ns_doctor_sup<0.446.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_doctor_sup}
    started: [{pid,<0.448.0>},
              {id,ns_doctor},
              {mfargs,{ns_doctor,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2022-09-08T12:07:42.411Z,ns_1@127.0.0.1:<0.443.0>:restartable:start_child:92]Started child process <0.446.0>
  MFA: {ns_doctor_sup,start_link,[]}
[error_logger:info,2022-09-08T12:07:42.411Z,ns_1@127.0.0.1:ns_server_sup<0.376.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.443.0>},
              {name,ns_doctor_sup},
              {mfargs,{restartable,start_link,
                                   [{ns_doctor_sup,start_link,[]},infinity]}},
              {restart_type,permanent},
              {shutdown,infinity},
              {child_type,supervisor}]
[error_logger:info,2022-09-08T12:07:42.411Z,ns_1@127.0.0.1:ns_server_sup<0.376.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.451.0>},
              {name,master_activity_events},
              {mfargs,{gen_event,start_link,[{local,master_activity_events}]}},
              {restart_type,permanent},
              {shutdown,brutal_kill},
              {child_type,worker}]
[error_logger:info,2022-09-08T12:07:42.415Z,ns_1@127.0.0.1:ns_server_sup<0.376.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.454.0>},
              {name,xdcr_ckpt_store},
              {mfargs,{simple_store,start_link,[xdcr_ckpt_data]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[error_logger:info,2022-09-08T12:07:42.415Z,ns_1@127.0.0.1:ns_server_sup<0.376.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.455.0>},
              {name,metakv_worker},
              {mfargs,{work_queue,start_link,[metakv_worker]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[error_logger:info,2022-09-08T12:07:42.415Z,ns_1@127.0.0.1:ns_server_sup<0.376.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.456.0>},
              {name,index_events},
              {mfargs,{gen_event,start_link,[{local,index_events}]}},
              {restart_type,permanent},
              {shutdown,brutal_kill},
              {child_type,worker}]
[error_logger:info,2022-09-08T12:07:42.416Z,ns_1@127.0.0.1:ns_server_sup<0.376.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.457.0>},
              {name,index_settings_manager},
              {mfargs,{index_settings_manager,start_link,[]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[error_logger:info,2022-09-08T12:07:42.416Z,ns_1@127.0.0.1:ns_server_sup<0.376.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.459.0>},
              {name,query_settings_manager},
              {mfargs,{query_settings_manager,start_link,[]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[error_logger:info,2022-09-08T12:07:42.416Z,ns_1@127.0.0.1:ns_server_sup<0.376.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.461.0>},
              {name,eventing_settings_manager},
              {mfargs,{eventing_settings_manager,start_link,[]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[error_logger:info,2022-09-08T12:07:42.416Z,ns_1@127.0.0.1:ns_server_sup<0.376.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.463.0>},
              {name,analytics_settings_manager},
              {mfargs,{analytics_settings_manager,start_link,[]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[error_logger:info,2022-09-08T12:07:42.416Z,ns_1@127.0.0.1:ns_server_sup<0.376.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.465.0>},
              {name,audit_events},
              {mfargs,{gen_event,start_link,[{local,audit_events}]}},
              {restart_type,permanent},
              {shutdown,brutal_kill},
              {child_type,worker}]
[ns_server:debug,2022-09-08T12:07:42.420Z,ns_1@127.0.0.1:user_uuid_limits<0.466.0>:versioned_cache:init:41]Starting versioned cache user_uuid_limits
[error_logger:info,2022-09-08T12:07:42.420Z,ns_1@127.0.0.1:ns_server_sup<0.376.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.466.0>},
              {name,user_limits_cache},
              {mfargs,{user_request_throttler,start_limits_cache,[]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[ns_server:debug,2022-09-08T12:07:42.420Z,ns_1@127.0.0.1:user_request_throttler<0.468.0>:user_request_throttler:handle_info:286]Clearing all user stats {<0.470.0>,#Ref<0.1778819075.971767811.231606>}
[error_logger:info,2022-09-08T12:07:42.420Z,ns_1@127.0.0.1:ns_server_sup<0.376.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.468.0>},
              {name,user_request_throttler},
              {mfargs,{user_request_throttler,start_link,[]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[error_logger:info,2022-09-08T12:07:42.430Z,ns_1@127.0.0.1:menelaus_sup<0.471.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_sup}
    started: [{pid,<0.472.0>},
              {id,menelaus_ui_auth},
              {mfargs,{menelaus_ui_auth,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2022-09-08T12:07:42.430Z,ns_1@127.0.0.1:menelaus_sup<0.471.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_sup}
    started: [{pid,<0.474.0>},
              {id,scram_sha},
              {mfargs,{scram_sha,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2022-09-08T12:07:42.435Z,ns_1@127.0.0.1:menelaus_sup<0.471.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_sup}
    started: [{pid,<0.475.0>},
              {id,menelaus_local_auth},
              {mfargs,{menelaus_local_auth,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2022-09-08T12:07:42.445Z,ns_1@127.0.0.1:menelaus_sup<0.471.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_sup}
    started: [{pid,<0.476.0>},
              {id,menelaus_web_cache},
              {mfargs,{menelaus_web_cache,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2022-09-08T12:07:42.451Z,ns_1@127.0.0.1:menelaus_sup<0.471.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_sup}
    started: [{pid,<0.478.0>},
              {id,menelaus_stats_gatherer},
              {mfargs,{menelaus_stats_gatherer,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2022-09-08T12:07:42.451Z,ns_1@127.0.0.1:menelaus_sup<0.471.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_sup}
    started: [{pid,<0.479.0>},
              {id,json_rpc_events},
              {mfargs,{gen_event,start_link,[{local,json_rpc_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:error,2022-09-08T12:07:42.461Z,ns_1@127.0.0.1:<0.477.0>:prometheus:post_async:192]Prometheus http request failed:
URL: http://127.0.0.1:9123/api/v1/query
Body: query=%7Bname%3D~%60kv_curr_items%7Ckv_curr_items_tot%7Ckv_mem_used_bytes%7Ccouch_docs_actual_disk_size%7Ccouch_views_actual_disk_size%7Ckv_ep_db_data_size_bytes%7Ckv_ep_bg_fetched%60%7D+or+kv_vb_curr_items%7Bstate%3D%27replica%27%7D+or+kv_vb_num_non_resident%7Bstate%3D%27active%27%7D+or+label_replace%28sum+by+%28bucket%2C+name%29+%28irate%28kv_ops%7Bop%3D%60get%60%7D%5B1m%5D%29%29%2C+%60name%60%2C%60cmd_get%60%2C+%60%60%2C+%60%60%29+or+label_replace%28irate%28kv_ops%7Bop%3D%60get%60%2Cresult%3D%60hit%60%7D%5B1m%5D%29%2C%60name%60%2C%60get_hits%60%2C%60%60%2C%60%60%29+or+label_replace%28sum+by+%28bucket%29+%28irate%28kv_cmd_lookup%5B1m%5D%29+or+irate%28kv_ops%7Bop%3D~%60set%7Cincr%7Cdecr%7Cdelete%7Cdel_meta%7Cget_meta%7Cset_meta%7Cset_ret_meta%7Cdel_ret_meta%60%7D%5B1m%5D%29%29%2C+%60name%60%2C+%60ops%60%2C+%60%60%2C+%60%60%29+or+sum+by+%28bucket%2C+name%29+%28%7Bname%3D~%60index_data_size%7Cindex_disk_size%7Ccouch_spatial_data_size%7Ccouch_spatial_disk_size%7Ccouch_views_data_size%60%7D%29&timeout=5s
Reason: {failed_connect,[{to_address,{"127.0.0.1",9123}},
                         {inet,[inet],econnrefused}]}
[error_logger:info,2022-09-08T12:07:42.463Z,ns_1@127.0.0.1:menelaus_web_sup<0.480.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_web_sup}
    started: [{pid,<0.484.0>},
              {id,menelaus_event},
              {mfargs,{menelaus_event,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[ns_server:error,2022-09-08T12:07:42.463Z,ns_1@127.0.0.1:<0.483.0>:prometheus:post_async:192]Prometheus http request failed:
URL: http://127.0.0.1:9123/api/v1/query
Body: query=%7Bcategory%3D%60system-processes%60%2Cname%3D~%60sysproc_mem_resident%7Csysproc_mem_size%7Csysproc_cpu_utilization%7Csysproc_major_faults_raw%60%7D&timeout=5s
Reason: {failed_connect,[{to_address,{"127.0.0.1",9123}},
                         {inet,[inet],econnrefused}]}
[ns_server:info,2022-09-08T12:07:42.464Z,ns_1@127.0.0.1:<0.486.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:129]Loaded pluggable UI specification for backup from "/opt/couchbase/etc/couchbase/pluggable-ui-backup.json"
[ns_server:info,2022-09-08T12:07:42.464Z,ns_1@127.0.0.1:<0.486.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:129]Loaded pluggable UI specification for cbas from "/opt/couchbase/etc/couchbase/pluggable-ui-cbas.json"
[ns_server:info,2022-09-08T12:07:42.464Z,ns_1@127.0.0.1:<0.486.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:129]Loaded pluggable UI specification for eventing from "/opt/couchbase/etc/couchbase/pluggable-ui-eventing.json"
[ns_server:error,2022-09-08T12:07:42.464Z,ns_1@127.0.0.1:<0.489.0>:prometheus:post_async:192]Prometheus http request failed:
URL: http://127.0.0.1:9123/api/v1/query
Body: query=%7Bcategory%3D%60system%60%2Cname%3D~%60sys_cpu_utilization_rate%7Csys_cpu_stolen_rate%7Csys_swap_total%7Csys_swap_used%7Csys_mem_total%7Csys_mem_free%7Csys_mem_limit%7Csys_cpu_cores_available%7Csys_allocstall%60%7D&timeout=5s
Reason: {failed_connect,[{to_address,{"127.0.0.1",9123}},
                         {inet,[inet],econnrefused}]}
[ns_server:info,2022-09-08T12:07:42.464Z,ns_1@127.0.0.1:<0.486.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:129]Loaded pluggable UI specification for fts from "/opt/couchbase/etc/couchbase/pluggable-ui-fts.json"
[ns_server:info,2022-09-08T12:07:42.464Z,ns_1@127.0.0.1:<0.486.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:129]Loaded pluggable UI specification for n1ql from "/opt/couchbase/etc/couchbase/pluggable-ui-query.json"
[ns_server:info,2022-09-08T12:07:42.465Z,ns_1@127.0.0.1:<0.486.0>:menelaus_web:maybe_start_http_server:126]Started web service with options:
[{ip,"0.0.0.0"},{port,8091}]
[error_logger:info,2022-09-08T12:07:42.465Z,ns_1@127.0.0.1:<0.486.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.486.0>,menelaus_web}
    started: [{pid,<0.490.0>},
              {id,menelaus_web_ipv4},
              {mfargs,
                  {menelaus_web,http_server,
                      [[{afamily,inet},
                        {name,menelaus_web},
                        {port,8091},
                        {nodelay,true},
                        {approot,
                            "/opt/couchbase/lib/ns_server/erlang/lib/ns_server/priv/public"}]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[ns_server:info,2022-09-08T12:07:42.466Z,ns_1@127.0.0.1:<0.486.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:129]Loaded pluggable UI specification for backup from "/opt/couchbase/etc/couchbase/pluggable-ui-backup.json"
[ns_server:info,2022-09-08T12:07:42.466Z,ns_1@127.0.0.1:<0.486.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:129]Loaded pluggable UI specification for cbas from "/opt/couchbase/etc/couchbase/pluggable-ui-cbas.json"
[ns_server:info,2022-09-08T12:07:42.466Z,ns_1@127.0.0.1:<0.486.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:129]Loaded pluggable UI specification for eventing from "/opt/couchbase/etc/couchbase/pluggable-ui-eventing.json"
[ns_server:info,2022-09-08T12:07:42.466Z,ns_1@127.0.0.1:<0.486.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:129]Loaded pluggable UI specification for fts from "/opt/couchbase/etc/couchbase/pluggable-ui-fts.json"
[ns_server:info,2022-09-08T12:07:42.466Z,ns_1@127.0.0.1:<0.486.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:129]Loaded pluggable UI specification for n1ql from "/opt/couchbase/etc/couchbase/pluggable-ui-query.json"
[ns_server:info,2022-09-08T12:07:42.467Z,ns_1@127.0.0.1:<0.486.0>:menelaus_web:maybe_start_http_server:126]Started web service with options:
[{ip,"::"},{port,8091}]
[error_logger:info,2022-09-08T12:07:42.467Z,ns_1@127.0.0.1:<0.486.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.486.0>,menelaus_web}
    started: [{pid,<0.507.0>},
              {id,menelaus_web_ipv6},
              {mfargs,
                  {menelaus_web,http_server,
                      [[{afamily,inet6},
                        {name,menelaus_web},
                        {port,8091},
                        {nodelay,true},
                        {approot,
                            "/opt/couchbase/lib/ns_server/erlang/lib/ns_server/priv/public"}]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[ns_server:debug,2022-09-08T12:07:42.467Z,ns_1@127.0.0.1:<0.485.0>:restartable:start_child:92]Started child process <0.486.0>
  MFA: {menelaus_web,start_link,[]}
[error_logger:info,2022-09-08T12:07:42.467Z,ns_1@127.0.0.1:menelaus_web_sup<0.480.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_web_sup}
    started: [{pid,<0.485.0>},
              {id,menelaus_web},
              {mfargs,{restartable,start_link,
                                   [{menelaus_web,start_link,[]},infinity]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[user:info,2022-09-08T12:07:42.467Z,ns_1@127.0.0.1:menelaus_sup<0.471.0>:menelaus_web_sup:start_link:38]Couchbase Server has started on web port 8091 on node 'ns_1@127.0.0.1'. Version: "7.1.1-3175-enterprise".
[error_logger:info,2022-09-08T12:07:42.468Z,ns_1@127.0.0.1:menelaus_sup<0.471.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_sup}
    started: [{pid,<0.480.0>},
              {id,menelaus_web_sup},
              {mfargs,{menelaus_web_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2022-09-08T12:07:42.470Z,ns_1@127.0.0.1:menelaus_sup<0.471.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_sup}
    started: [{pid,<0.524.0>},
              {id,hot_keys_keeper},
              {mfargs,{hot_keys_keeper,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2022-09-08T12:07:42.471Z,ns_1@127.0.0.1:menelaus_sup<0.471.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_sup}
    started: [{pid,<0.525.0>},
              {id,menelaus_web_alerts_srv},
              {mfargs,{menelaus_web_alerts_srv,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2022-09-08T12:07:42.475Z,ns_1@127.0.0.1:menelaus_sup<0.471.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_sup}
    started: [{pid,<0.527.0>},
              {id,menelaus_cbauth},
              {mfargs,{menelaus_cbauth,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2022-09-08T12:07:42.475Z,ns_1@127.0.0.1:ns_server_sup<0.376.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.471.0>},
              {name,menelaus},
              {mfargs,{menelaus_sup,start_link,[]}},
              {restart_type,permanent},
              {shutdown,infinity},
              {child_type,supervisor}]
[error_logger:info,2022-09-08T12:07:42.475Z,ns_1@127.0.0.1:ns_server_sup<0.376.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.533.0>},
              {name,ns_ports_setup},
              {mfargs,{ns_ports_setup,start,[]}},
              {restart_type,{permanent,4}},
              {shutdown,brutal_kill},
              {child_type,worker}]
[error_logger:info,2022-09-08T12:07:42.480Z,ns_1@127.0.0.1:service_agent_sup<0.536.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,service_agent_sup}
    started: [{pid,<0.537.0>},
              {id,service_agent_children_sup},
              {mfargs,{supervisor,start_link,
                                  [{local,service_agent_children_sup},
                                   service_agent_sup,child]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2022-09-08T12:07:42.482Z,ns_1@127.0.0.1:ns_heart<0.440.0>:goxdcr_rest:get_from_goxdcr:137]Goxdcr is temporary not available. Return empty list.
[error_logger:info,2022-09-08T12:07:42.486Z,ns_1@127.0.0.1:service_agent_children_sup<0.537.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,service_agent_children_sup}
    started: [{pid,<0.540.0>},
              {id,{service_agent,backup}},
              {mfargs,{service_agent,start_link,[backup]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2022-09-08T12:07:42.486Z,ns_1@127.0.0.1:service_agent_children_sup<0.537.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,service_agent_children_sup}
    started: [{pid,<0.544.0>},
              {id,{service_agent,cbas}},
              {mfargs,{service_agent,start_link,[cbas]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2022-09-08T12:07:42.487Z,ns_1@127.0.0.1:service_agent_children_sup<0.537.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,service_agent_children_sup}
    started: [{pid,<0.548.0>},
              {id,{service_agent,eventing}},
              {mfargs,{service_agent,start_link,[eventing]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2022-09-08T12:07:42.489Z,ns_1@127.0.0.1:service_agent_children_sup<0.537.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,service_agent_children_sup}
    started: [{pid,<0.552.0>},
              {id,{service_agent,fts}},
              {mfargs,{service_agent,start_link,[fts]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2022-09-08T12:07:42.489Z,ns_1@127.0.0.1:ns_heart<0.440.0>:cluster_logs_collection_task:maybe_build_cluster_logs_task:43]Ignoring exception trying to read cluster_logs_collection_task_status table: error:badarg
[error_logger:info,2022-09-08T12:07:42.489Z,ns_1@127.0.0.1:service_agent_children_sup<0.537.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,service_agent_children_sup}
    started: [{pid,<0.556.0>},
              {id,{service_agent,index}},
              {mfargs,{service_agent,start_link,[index]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2022-09-08T12:07:42.490Z,ns_1@127.0.0.1:service_agent_children_sup<0.537.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,service_agent_children_sup}
    started: [{pid,<0.560.0>},
              {id,{service_agent,n1ql}},
              {mfargs,{service_agent,start_link,[n1ql]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2022-09-08T12:07:42.490Z,ns_1@127.0.0.1:service_agent_sup<0.536.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,service_agent_sup}
    started: [{pid,<0.538.0>},
              {id,service_agent_worker},
              {mfargs,{erlang,apply,[#Fun<service_agent_sup.0.32483565>,[]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2022-09-08T12:07:42.490Z,ns_1@127.0.0.1:ns_server_sup<0.376.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.536.0>},
              {name,service_agent_sup},
              {mfargs,{service_agent_sup,start_link,[]}},
              {restart_type,permanent},
              {shutdown,infinity},
              {child_type,supervisor}]
[error_logger:info,2022-09-08T12:07:42.497Z,ns_1@127.0.0.1:ns_server_sup<0.376.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.564.0>},
              {name,ns_memcached_sockets_pool},
              {mfargs,{ns_memcached_sockets_pool,start_link,[]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[ns_server:debug,2022-09-08T12:07:42.506Z,ns_1@127.0.0.1:memcached_auth_server<0.566.0>:memcached_auth_server:reconnect:234]Skipping creation of 'Auth provider' connection because external users are disabled
[error_logger:info,2022-09-08T12:07:42.506Z,ns_1@127.0.0.1:ns_server_sup<0.376.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.566.0>},
              {name,memcached_auth_server},
              {mfargs,{memcached_auth_server,start_link,[]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[ns_server:debug,2022-09-08T12:07:42.506Z,ns_1@127.0.0.1:ns_audit_cfg<0.572.0>:ns_audit_cfg:write_audit_json:237]Writing new content to "/opt/couchbase/var/lib/couchbase/config/audit.json", Params [{descriptors_path,
                                                                                      "/opt/couchbase/etc/security"},
                                                                                     {version,
                                                                                      2},
                                                                                     {uuid,
                                                                                      "55497325"},
                                                                                     {event_states,
                                                                                      {[]}},
                                                                                     {filtering_enabled,
                                                                                      true},
                                                                                     {disabled_userids,
                                                                                      []},
                                                                                     {auditd_enabled,
                                                                                      false},
                                                                                     {log_path,
                                                                                      "/opt/couchbase/var/lib/couchbase/logs"},
                                                                                     {rotate_interval,
                                                                                      86400},
                                                                                     {rotate_size,
                                                                                      20971520},
                                                                                     {sync,
                                                                                      []}]
[ns_server:debug,2022-09-08T12:07:42.507Z,ns_1@127.0.0.1:ns_ports_setup<0.533.0>:ns_ports_manager:set_dynamic_children:48]Setting children [memcached,saslauthd_port,backup,cbas,eventing,fts,goxdcr,
                  index,kv,n1ql]
[ns_server:error,2022-09-08T12:07:42.516Z,ns_1@127.0.0.1:<0.576.0>:prometheus:post_async:192]Prometheus http request failed:
URL: http://127.0.0.1:9123/api/v1/query
Body: query=cm_failover_safeness_level%7Bbucket%3D%60todo%60%7D%5B20s%5D&timeout=5s
Reason: {failed_connect,[{to_address,{"127.0.0.1",9123}},
                         {inet,[inet],econnrefused}]}
[ns_server:debug,2022-09-08T12:07:42.531Z,ns_1@127.0.0.1:ns_audit_cfg<0.572.0>:ns_audit_cfg:notify_memcached:151]Instruct memcached to reload audit config
[error_logger:info,2022-09-08T12:07:42.531Z,ns_1@127.0.0.1:ns_server_sup<0.376.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.572.0>},
              {name,ns_audit_cfg},
              {mfargs,{ns_audit_cfg,start_link,[]}},
              {restart_type,{permanent,4}},
              {shutdown,1000},
              {child_type,worker}]
[ns_server:warn,2022-09-08T12:07:42.532Z,ns_1@127.0.0.1:<0.578.0>:ns_memcached:connect:1240]Unable to connect: {error,{badmatch,[{inet,{error,econnrefused}}]}}, retrying.
[error_logger:info,2022-09-08T12:07:42.540Z,ns_1@127.0.0.1:ns_server_sup<0.376.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.579.0>},
              {name,ns_audit},
              {mfargs,{ns_audit,start_link,[]}},
              {restart_type,{permanent,4}},
              {shutdown,1000},
              {child_type,worker}]
[ns_server:debug,2022-09-08T12:07:42.540Z,ns_1@127.0.0.1:memcached_config_mgr<0.580.0>:memcached_config_mgr:init:54]waiting for completion of initial ns_ports_setup round
[error_logger:info,2022-09-08T12:07:42.540Z,ns_1@127.0.0.1:ns_server_sup<0.376.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.580.0>},
              {name,memcached_config_mgr},
              {mfargs,{memcached_config_mgr,start_link,[]}},
              {restart_type,{permanent,4}},
              {shutdown,1000},
              {child_type,worker}]
[ns_server:info,2022-09-08T12:07:42.546Z,ns_1@127.0.0.1:<0.582.0>:ns_memcached_log_rotator:init:36]Starting log rotator on "/opt/couchbase/var/lib/couchbase/logs"/"memcached.log"* with an initial period of 39003ms
[error_logger:info,2022-09-08T12:07:42.546Z,ns_1@127.0.0.1:ns_server_sup<0.376.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.582.0>},
              {name,ns_memcached_log_rotator},
              {mfargs,{ns_memcached_log_rotator,start_link,[]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[error_logger:info,2022-09-08T12:07:42.552Z,ns_1@127.0.0.1:ns_server_sup<0.376.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.583.0>},
              {name,testconditions_store},
              {mfargs,{simple_store,start_link,[testconditions]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[ns_server:error,2022-09-08T12:07:42.556Z,ns_1@127.0.0.1:<0.596.0>:prometheus:post_async:192]Prometheus http request failed:
URL: http://127.0.0.1:9123/api/v1/query
Body: query=%7Bname%3D~%60kv_curr_items%7Ckv_curr_items_tot%7Ckv_mem_used_bytes%7Ccouch_docs_actual_disk_size%7Ccouch_views_actual_disk_size%7Ckv_ep_db_data_size_bytes%7Ckv_ep_bg_fetched%60%7D+or+kv_vb_curr_items%7Bstate%3D%27replica%27%7D+or+kv_vb_num_non_resident%7Bstate%3D%27active%27%7D+or+label_replace%28sum+by+%28bucket%2C+name%29+%28irate%28kv_ops%7Bop%3D%60get%60%7D%5B1m%5D%29%29%2C+%60name%60%2C%60cmd_get%60%2C+%60%60%2C+%60%60%29+or+label_replace%28irate%28kv_ops%7Bop%3D%60get%60%2Cresult%3D%60hit%60%7D%5B1m%5D%29%2C%60name%60%2C%60get_hits%60%2C%60%60%2C%60%60%29+or+label_replace%28sum+by+%28bucket%29+%28irate%28kv_cmd_lookup%5B1m%5D%29+or+irate%28kv_ops%7Bop%3D~%60set%7Cincr%7Cdecr%7Cdelete%7Cdel_meta%7Cget_meta%7Cset_meta%7Cset_ret_meta%7Cdel_ret_meta%60%7D%5B1m%5D%29%29%2C+%60name%60%2C+%60ops%60%2C+%60%60%2C+%60%60%29+or+sum+by+%28bucket%2C+name%29+%28%7Bname%3D~%60index_data_size%7Cindex_disk_size%7Ccouch_spatial_data_size%7Ccouch_spatial_disk_size%7Ccouch_views_data_size%60%7D%29&timeout=5s
Reason: {failed_connect,[{to_address,{"127.0.0.1",9123}},
                         {inet,[inet],econnrefused}]}
[ns_server:error,2022-09-08T12:07:42.557Z,ns_1@127.0.0.1:<0.599.0>:prometheus:post_async:192]Prometheus http request failed:
URL: http://127.0.0.1:9123/api/v1/query
Body: query=%7Bcategory%3D%60system-processes%60%2Cname%3D~%60sysproc_mem_resident%7Csysproc_mem_size%7Csysproc_cpu_utilization%7Csysproc_major_faults_raw%60%7D&timeout=5s
Reason: {failed_connect,[{to_address,{"127.0.0.1",9123}},
                         {inet,[inet],econnrefused}]}
[ns_server:error,2022-09-08T12:07:42.559Z,ns_1@127.0.0.1:<0.602.0>:prometheus:post_async:192]Prometheus http request failed:
URL: http://127.0.0.1:9123/api/v1/query
Body: query=%7Bcategory%3D%60system%60%2Cname%3D~%60sys_cpu_utilization_rate%7Csys_cpu_stolen_rate%7Csys_swap_total%7Csys_swap_used%7Csys_mem_total%7Csys_mem_free%7Csys_mem_limit%7Csys_cpu_cores_available%7Csys_allocstall%60%7D&timeout=5s
Reason: {failed_connect,[{to_address,{"127.0.0.1",9123}},
                         {inet,[inet],econnrefused}]}
[error_logger:info,2022-09-08T12:07:42.559Z,ns_1@127.0.0.1:ns_server_sup<0.376.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.604.0>},
              {name,terse_cluster_info_uploader},
              {mfargs,{terse_cluster_info_uploader,start_link,[]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[ns_server:debug,2022-09-08T12:07:42.561Z,ns_1@127.0.0.1:ns_heart_slow_status_updater<0.442.0>:goxdcr_rest:get_from_goxdcr:137]Goxdcr is temporary not available. Return empty list.
[ns_server:debug,2022-09-08T12:07:42.561Z,ns_1@127.0.0.1:ns_heart_slow_status_updater<0.442.0>:cluster_logs_collection_task:maybe_build_cluster_logs_task:43]Ignoring exception trying to read cluster_logs_collection_task_status table: error:badarg
[ns_server:debug,2022-09-08T12:07:42.570Z,ns_1@127.0.0.1:terse_cluster_info_uploader<0.604.0>:terse_cluster_info_uploader:handle_info:42]Refreshing terse cluster info with <<"{\"rev\":94,\"nodesExt\":[{\"services\":{\"backupAPI\":8097,\"backupAPIHTTPS\":18097,\"backupGRPC\":9124,\"capi\":8092,\"capiSSL\":18092,\"cbas\":8095,\"cbasSSL\":18095,\"eventingAdminPort\":8096,\"eventingDebug\":9140,\"eventingSSL\":18096,\"fts\":8094,\"ftsGRPC\":9130,\"ftsGRPCSSL\":19130,\"ftsSSL\":18094,\"indexAdmin\":9100,\"indexHttp\":9102,\"indexHttps\":19102,\"indexScan\":9101,\"indexStreamCatchup\":9104,\"indexStreamInit\":9103,\"indexStreamMaint\":9105,\"kv\":11210,\"kvSSL\":11207,\"mgmt\":8091,\"mgmtSSL\":18091,\"n1ql\":8093,\"n1qlSSL\":18093,\"projector\":9999},\"thisNode\":true}],\"clusterCapabilitiesVer\":[1,0],\"clusterCapabilities\":{\"n1ql\":[\"enhancedPreparedStatements\"]},\"revEpoch\":1}">>
[ns_server:warn,2022-09-08T12:07:42.572Z,ns_1@127.0.0.1:<0.611.0>:ns_memcached:connect:1240]Unable to connect: {error,{badmatch,[{inet,{error,econnrefused}}]}}, retrying.
[error_logger:info,2022-09-08T12:07:42.576Z,ns_1@127.0.0.1:ns_bucket_worker_sup<0.609.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_bucket_worker_sup}
    started: [{pid,<0.614.0>},
              {id,ns_bucket_sup},
              {mfargs,{ns_bucket_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2022-09-08T12:07:42.579Z,ns_1@127.0.0.1:ns_bucket_worker<0.621.0>:ns_bucket_worker:start_one_bucket:101]Starting new bucket: "todo"
[error_logger:info,2022-09-08T12:07:42.579Z,ns_1@127.0.0.1:ns_bucket_worker_sup<0.609.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_bucket_worker_sup}
    started: [{pid,<0.621.0>},
              {id,ns_bucket_worker},
              {mfargs,{ns_bucket_worker,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2022-09-08T12:07:42.579Z,ns_1@127.0.0.1:ns_server_sup<0.376.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.609.0>},
              {name,ns_bucket_worker_sup},
              {mfargs,{ns_bucket_worker_sup,start_link,[]}},
              {restart_type,permanent},
              {shutdown,infinity},
              {child_type,supervisor}]
[error_logger:info,2022-09-08T12:07:42.580Z,ns_1@127.0.0.1:ns_server_sup<0.376.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.623.0>},
              {name,ns_server_stats},
              {mfargs,{ns_server_stats,start_link,[]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[ns_server:debug,2022-09-08T12:07:42.586Z,ns_1@127.0.0.1:single_bucket_kv_sup-todo<0.626.0>:single_bucket_kv_sup:sync_config_to_couchdb_node:64]Syncing config to couchdb node
[ns_server:debug,2022-09-08T12:07:42.586Z,ns_1@127.0.0.1:ns_config_rep<0.416.0>:ns_config_rep:handle_cast:173]Synchronized with merger in 36 us
[ns_server:debug,2022-09-08T12:07:42.587Z,ns_1@127.0.0.1:single_bucket_kv_sup-todo<0.626.0>:single_bucket_kv_sup:sync_config_to_couchdb_node:70]Synced config to couchdb node successfully
[ns_server:error,2022-09-08T12:07:42.594Z,ns_1@127.0.0.1:<0.620.0>:prometheus:post_async:192]Prometheus http request failed:
URL: http://127.0.0.1:9123/api/v1/query
Body: query=cm_failover_safeness_level%7Bbucket%3D%60todo%60%7D%5B20s%5D&timeout=5s
Reason: <<"Service Unavailable">>
[error_logger:info,2022-09-08T12:07:42.617Z,ns_1@127.0.0.1:ns_server_sup<0.376.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.635.0>},
              {name,{stats_reader,"@system"}},
              {mfargs,{stats_reader,start_link,["@system"]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[error_logger:info,2022-09-08T12:07:42.617Z,ns_1@127.0.0.1:ns_server_sup<0.376.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.637.0>},
              {name,{stats_reader,"@system-processes"}},
              {mfargs,{stats_reader,start_link,["@system-processes"]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[error_logger:info,2022-09-08T12:07:42.617Z,ns_1@127.0.0.1:ns_server_sup<0.376.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.639.0>},
              {name,{stats_reader,"@query"}},
              {mfargs,{stats_reader,start_link,["@query"]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[error_logger:info,2022-09-08T12:07:42.617Z,ns_1@127.0.0.1:ns_server_sup<0.376.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.641.0>},
              {name,{stats_reader,"@global"}},
              {mfargs,{stats_reader,start_link,["@global"]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[ns_server:debug,2022-09-08T12:07:42.619Z,ns_1@127.0.0.1:capi_doc_replicator-todo<0.643.0>:replicated_storage:wait_for_startup:46]Start waiting for startup
[error_logger:info,2022-09-08T12:07:42.619Z,ns_1@127.0.0.1:<0.632.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.632.0>,docs_kv_sup}
    started: [{pid,<0.643.0>},
              {id,doc_replicator},
              {mfargs,{capi_ddoc_manager,start_replicator,["todo"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2022-09-08T12:07:42.619Z,ns_1@127.0.0.1:capi_ddoc_replication_srv-todo<0.644.0>:replicated_storage:wait_for_startup:46]Start waiting for startup
[error_logger:info,2022-09-08T12:07:42.619Z,ns_1@127.0.0.1:<0.632.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.632.0>,docs_kv_sup}
    started: [{pid,<0.644.0>},
              {id,doc_replication_srv},
              {mfargs,{doc_replication_srv,start_link,["todo"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2022-09-08T12:07:42.631Z,ns_1@127.0.0.1:ns_server_sup<0.376.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.645.0>},
              {name,goxdcr_status_keeper},
              {mfargs,{goxdcr_status_keeper,start_link,[]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[ns_server:debug,2022-09-08T12:07:42.633Z,ns_1@127.0.0.1:goxdcr_status_keeper<0.645.0>:goxdcr_rest:get_from_goxdcr:137]Goxdcr is temporary not available. Return empty list.
[ns_server:debug,2022-09-08T12:07:42.634Z,ns_1@127.0.0.1:goxdcr_status_keeper<0.645.0>:goxdcr_rest:get_from_goxdcr:137]Goxdcr is temporary not available. Return empty list.
[error_logger:info,2022-09-08T12:07:42.639Z,ns_1@127.0.0.1:logger_proxy<0.70.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,'capi_ddoc_manager_sup-todo'}
    started: [{pid,<16171.376.0>},
              {id,capi_ddoc_manager_events},
              {mfargs,{capi_ddoc_manager,start_link_event_manager,["todo"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2022-09-08T12:07:42.640Z,ns_1@127.0.0.1:services_stats_sup<0.648.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,services_stats_sup}
    started: [{pid,<0.649.0>},
              {id,service_stats_children_sup},
              {mfargs,{supervisor,start_link,
                                  [{local,service_stats_children_sup},
                                   services_stats_sup,child]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2022-09-08T12:07:42.642Z,ns_1@127.0.0.1:capi_doc_replicator-todo<0.643.0>:replicated_storage:wait_for_startup:49]Received replicated storage registration from <16171.377.0>
[ns_server:debug,2022-09-08T12:07:42.642Z,ns_1@127.0.0.1:capi_ddoc_replication_srv-todo<0.644.0>:replicated_storage:wait_for_startup:49]Received replicated storage registration from <16171.377.0>
[error_logger:info,2022-09-08T12:07:42.642Z,ns_1@127.0.0.1:<0.632.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.632.0>,docs_kv_sup}
    started: [{pid,<16171.375.0>},
              {id,capi_ddoc_manager_sup},
              {mfargs,
                  {capi_ddoc_manager_sup,start_link_remote,
                      ['couchdb_ns_1@cb.local',"todo"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2022-09-08T12:07:42.642Z,ns_1@127.0.0.1:logger_proxy<0.70.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,'capi_ddoc_manager_sup-todo'}
    started: [{pid,<16171.377.0>},
              {id,capi_ddoc_manager},
              {mfargs,
                  {capi_ddoc_manager,start_link,["todo",<0.643.0>,<0.644.0>]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2022-09-08T12:07:42.644Z,ns_1@127.0.0.1:service_status_keeper_sup<0.651.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,service_status_keeper_sup}
    started: [{pid,<0.652.0>},
              {id,service_status_keeper_worker},
              {mfargs,{work_queue,start_link,[service_status_keeper_worker]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2022-09-08T12:07:42.653Z,ns_1@127.0.0.1:<0.632.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.632.0>,docs_kv_sup}
    started: [{pid,<16171.379.0>},
              {id,capi_set_view_manager},
              {mfargs,
                  {capi_set_view_manager,start_link_remote,
                      ['couchdb_ns_1@cb.local',"todo"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2022-09-08T12:07:42.655Z,ns_1@127.0.0.1:service_status_keeper_sup<0.651.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,service_status_keeper_sup}
    started: [{pid,<0.653.0>},
              {id,service_status_keeper_index},
              {mfargs,{service_index,start_keeper,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:error,2022-09-08T12:07:42.656Z,ns_1@127.0.0.1:service_status_keeper_worker<0.652.0>:rest_utils:get_json:62]Request to (indexer) getIndexStatus with headers [] failed: {error,
                                                             {econnrefused,
                                                              [{lhttpc_client,
                                                                send_request,
                                                                1,
                                                                [{file,
                                                                  "/home/couchbase/jenkins/workspace/couchbase-server-unix/couchdb/src/lhttpc/lhttpc_client.erl"},
                                                                 {line,220}]},
                                                               {lhttpc_client,
                                                                execute,9,
                                                                [{file,
                                                                  "/home/couchbase/jenkins/workspace/couchbase-server-unix/couchdb/src/lhttpc/lhttpc_client.erl"},
                                                                 {line,169}]},
                                                               {lhttpc_client,
                                                                request,9,
                                                                [{file,
                                                                  "/home/couchbase/jenkins/workspace/couchbase-server-unix/couchdb/src/lhttpc/lhttpc_client.erl"},
                                                                 {line,
                                                                  93}]}]}}
[ns_server:error,2022-09-08T12:07:42.656Z,ns_1@127.0.0.1:service_status_keeper-index<0.653.0>:service_status_keeper:handle_cast:103]Service service_index returned incorrect status
[error_logger:info,2022-09-08T12:07:42.661Z,ns_1@127.0.0.1:service_status_keeper_sup<0.651.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,service_status_keeper_sup}
    started: [{pid,<0.657.0>},
              {id,service_status_keeper_fts},
              {mfargs,{service_fts,start_keeper,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:error,2022-09-08T12:07:42.662Z,ns_1@127.0.0.1:service_status_keeper_worker<0.652.0>:rest_utils:get_json:62]Request to (fts) api/nsstatus with headers [] failed: {error,
                                                       {econnrefused,
                                                        [{lhttpc_client,
                                                          send_request,1,
                                                          [{file,
                                                            "/home/couchbase/jenkins/workspace/couchbase-server-unix/couchdb/src/lhttpc/lhttpc_client.erl"},
                                                           {line,220}]},
                                                         {lhttpc_client,
                                                          execute,9,
                                                          [{file,
                                                            "/home/couchbase/jenkins/workspace/couchbase-server-unix/couchdb/src/lhttpc/lhttpc_client.erl"},
                                                           {line,169}]},
                                                         {lhttpc_client,
                                                          request,9,
                                                          [{file,
                                                            "/home/couchbase/jenkins/workspace/couchbase-server-unix/couchdb/src/lhttpc/lhttpc_client.erl"},
                                                           {line,93}]}]}}
[ns_server:error,2022-09-08T12:07:42.662Z,ns_1@127.0.0.1:service_status_keeper-fts<0.657.0>:service_status_keeper:handle_cast:103]Service service_fts returned incorrect status
[error_logger:info,2022-09-08T12:07:42.662Z,ns_1@127.0.0.1:<0.632.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.632.0>,docs_kv_sup}
    started: [{pid,<16171.383.0>},
              {id,couch_stats_reader},
              {mfargs,
                  {couch_stats_reader,start_link_remote,
                      ['couchdb_ns_1@cb.local',"todo"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2022-09-08T12:07:42.664Z,ns_1@127.0.0.1:single_bucket_kv_sup-todo<0.626.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,'single_bucket_kv_sup-todo'}
    started: [{pid,<0.632.0>},
              {id,{docs_kv_sup,"todo"}},
              {mfargs,{docs_kv_sup,start_link,["todo"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2022-09-08T12:07:42.667Z,ns_1@127.0.0.1:service_status_keeper_sup<0.651.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,service_status_keeper_sup}
    started: [{pid,<0.661.0>},
              {id,service_status_keeper_eventing},
              {mfargs,{service_eventing,start_keeper,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2022-09-08T12:07:42.668Z,ns_1@127.0.0.1:services_stats_sup<0.648.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,services_stats_sup}
    started: [{pid,<0.651.0>},
              {id,service_status_keeper_sup},
              {mfargs,{service_status_keeper_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:error,2022-09-08T12:07:42.669Z,ns_1@127.0.0.1:service_status_keeper_worker<0.652.0>:rest_utils:get_json:62]Request to (eventing) api/v1/functions with headers [] failed: {error,
                                                                {econnrefused,
                                                                 [{lhttpc_client,
                                                                   send_request,
                                                                   1,
                                                                   [{file,
                                                                     "/home/couchbase/jenkins/workspace/couchbase-server-unix/couchdb/src/lhttpc/lhttpc_client.erl"},
                                                                    {line,
                                                                     220}]},
                                                                  {lhttpc_client,
                                                                   execute,9,
                                                                   [{file,
                                                                     "/home/couchbase/jenkins/workspace/couchbase-server-unix/couchdb/src/lhttpc/lhttpc_client.erl"},
                                                                    {line,
                                                                     169}]},
                                                                  {lhttpc_client,
                                                                   request,9,
                                                                   [{file,
                                                                     "/home/couchbase/jenkins/workspace/couchbase-server-unix/couchdb/src/lhttpc/lhttpc_client.erl"},
                                                                    {line,
                                                                     93}]}]}}
[ns_server:error,2022-09-08T12:07:42.669Z,ns_1@127.0.0.1:service_status_keeper-eventing<0.661.0>:service_status_keeper:handle_cast:103]Service service_eventing returned incorrect status
[ns_server:debug,2022-09-08T12:07:42.677Z,ns_1@127.0.0.1:ns_memcached-todo<0.668.0>:ns_memcached:init:148]Starting ns_memcached
[ns_server:debug,2022-09-08T12:07:42.677Z,ns_1@127.0.0.1:<0.669.0>:ns_memcached:run_connect_phase:174]Started 'connecting' phase of ns_memcached-todo. Parent is <0.668.0>
[error_logger:info,2022-09-08T12:07:42.677Z,ns_1@127.0.0.1:<0.667.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.667.0>,ns_memcached_sup}
    started: [{pid,<0.668.0>},
              {id,{ns_memcached,"todo"}},
              {mfargs,{ns_memcached,start_link,["todo"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,86400000},
              {child_type,worker}]

[ns_server:warn,2022-09-08T12:07:42.688Z,ns_1@127.0.0.1:<0.669.0>:ns_memcached:connect:1240]Unable to connect: {error,{badmatch,[{inet,{error,econnrefused}}]}}, retrying.
[error_logger:info,2022-09-08T12:07:42.691Z,ns_1@127.0.0.1:<0.667.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.667.0>,ns_memcached_sup}
    started: [{pid,<0.670.0>},
              {id,{terse_bucket_info_uploader,"todo"}},
              {mfargs,{terse_bucket_info_uploader,start_link,["todo"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2022-09-08T12:07:42.692Z,ns_1@127.0.0.1:single_bucket_kv_sup-todo<0.626.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,'single_bucket_kv_sup-todo'}
    started: [{pid,<0.667.0>},
              {id,{ns_memcached_sup,"todo"}},
              {mfargs,{ns_memcached_sup,start_link,["todo"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2022-09-08T12:07:42.694Z,ns_1@127.0.0.1:service_stats_children_sup<0.649.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,service_stats_children_sup}
    started: [{pid,<0.672.0>},
              {id,{service_cbas,stats_reader,"@cbas"}},
              {mfargs,{stats_reader,start_link,["@cbas"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2022-09-08T12:07:42.695Z,ns_1@127.0.0.1:service_stats_children_sup<0.649.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,service_stats_children_sup}
    started: [{pid,<0.674.0>},
              {id,{service_cbas,stats_reader,"todo"}},
              {mfargs,{stats_reader,start_link,["@cbas-todo"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2022-09-08T12:07:42.695Z,ns_1@127.0.0.1:service_stats_children_sup<0.649.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,service_stats_children_sup}
    started: [{pid,<0.676.0>},
              {id,{service_eventing,stats_reader,"@eventing"}},
              {mfargs,{stats_reader,start_link,["@eventing"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2022-09-08T12:07:42.696Z,ns_1@127.0.0.1:service_stats_children_sup<0.649.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,service_stats_children_sup}
    started: [{pid,<0.678.0>},
              {id,{service_eventing,stats_reader,"todo"}},
              {mfargs,{stats_reader,start_link,["@eventing-todo"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2022-09-08T12:07:42.696Z,ns_1@127.0.0.1:service_stats_children_sup<0.649.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,service_stats_children_sup}
    started: [{pid,<0.680.0>},
              {id,{service_fts,stats_reader,"@fts"}},
              {mfargs,{stats_reader,start_link,["@fts"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2022-09-08T12:07:42.696Z,ns_1@127.0.0.1:service_stats_children_sup<0.649.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,service_stats_children_sup}
    started: [{pid,<0.682.0>},
              {id,{service_fts,stats_reader,"todo"}},
              {mfargs,{stats_reader,start_link,["@fts-todo"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2022-09-08T12:07:42.696Z,ns_1@127.0.0.1:service_stats_children_sup<0.649.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,service_stats_children_sup}
    started: [{pid,<0.684.0>},
              {id,{service_index,stats_reader,"@index"}},
              {mfargs,{stats_reader,start_link,["@index"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2022-09-08T12:07:42.696Z,ns_1@127.0.0.1:service_stats_children_sup<0.649.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,service_stats_children_sup}
    started: [{pid,<0.686.0>},
              {id,{service_index,stats_reader,"todo"}},
              {mfargs,{stats_reader,start_link,["@index-todo"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2022-09-08T12:07:42.696Z,ns_1@127.0.0.1:services_stats_sup<0.648.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,services_stats_sup}
    started: [{pid,<0.665.0>},
              {id,service_stats_worker},
              {mfargs,{erlang,apply,
                              [#Fun<services_stats_sup.0.114823200>,[]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2022-09-08T12:07:42.697Z,ns_1@127.0.0.1:ns_server_sup<0.376.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.648.0>},
              {name,services_stats_sup},
              {mfargs,{services_stats_sup,start_link,[]}},
              {restart_type,permanent},
              {shutdown,infinity},
              {child_type,supervisor}]
[error_logger:info,2022-09-08T12:07:42.699Z,ns_1@127.0.0.1:single_bucket_kv_sup-todo<0.626.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,'single_bucket_kv_sup-todo'}
    started: [{pid,<0.688.0>},
              {id,{dcp_sup,"todo"}},
              {mfargs,{dcp_sup,start_link,["todo"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2022-09-08T12:07:42.699Z,ns_1@127.0.0.1:single_bucket_kv_sup-todo<0.626.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,'single_bucket_kv_sup-todo'}
    started: [{pid,<0.689.0>},
              {id,{dcp_replication_manager,"todo"}},
              {mfargs,{dcp_replication_manager,start_link,["todo"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2022-09-08T12:07:42.699Z,ns_1@127.0.0.1:single_bucket_kv_sup-todo<0.626.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,'single_bucket_kv_sup-todo'}
    started: [{pid,<0.690.0>},
              {id,{replication_manager,"todo"}},
              {mfargs,{replication_manager,start_link,["todo"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2022-09-08T12:07:42.718Z,ns_1@127.0.0.1:<0.695.0>:new_concurrency_throttle:init:109]init concurrent throttle process, pid: <0.695.0>, type: kv_throttle# of available token: 1
[error_logger:info,2022-09-08T12:07:42.722Z,ns_1@127.0.0.1:janitor_agent_sup-todo<0.693.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,'janitor_agent_sup-todo'}
    started: [{pid,<0.697.0>},
              {id,rebalance_subprocesses_registry},
              {mfargs,
                  {ns_process_registry,start_link,
                      ['rebalance_subprocesses_registry-todo',
                       [{terminate_command,kill}]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,86400000},
              {child_type,worker}]

[ns_server:debug,2022-09-08T12:07:42.723Z,ns_1@127.0.0.1:janitor_agent-todo<0.698.0>:dcp_sup:nuke:114]Nuking DCP replicators for bucket "todo":
[]
[error_logger:info,2022-09-08T12:07:42.723Z,ns_1@127.0.0.1:janitor_agent_sup-todo<0.693.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,'janitor_agent_sup-todo'}
    started: [{pid,<0.698.0>},
              {id,janitor_agent},
              {mfargs,{janitor_agent,start_link,["todo"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2022-09-08T12:07:42.723Z,ns_1@127.0.0.1:single_bucket_kv_sup-todo<0.626.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,'single_bucket_kv_sup-todo'}
    started: [{pid,<0.693.0>},
              {id,{janitor_agent_sup,"todo"}},
              {mfargs,{janitor_agent_sup,start_link,["todo"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2022-09-08T12:07:42.723Z,ns_1@127.0.0.1:single_bucket_kv_sup-todo<0.626.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,'single_bucket_kv_sup-todo'}
    started: [{pid,<0.699.0>},
              {id,{stats_reader,"todo"}},
              {mfargs,{stats_reader,start_link,["todo"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2022-09-08T12:07:42.723Z,ns_1@127.0.0.1:single_bucket_kv_sup-todo<0.626.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,'single_bucket_kv_sup-todo'}
    started: [{pid,<0.701.0>},
              {id,{goxdcr_stats_reader,"todo"}},
              {mfargs,{stats_reader,start_link,["@xdcr-todo"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2022-09-08T12:07:42.724Z,ns_1@127.0.0.1:ns_bucket_sup<0.614.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_bucket_sup}
    started: [{pid,<0.626.0>},
              {id,{single_bucket_kv_sup,"todo"}},
              {mfargs,{single_bucket_kv_sup,start_link,["todo"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2022-09-08T12:07:42.725Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[error_logger:info,2022-09-08T12:07:42.725Z,ns_1@127.0.0.1:ns_server_sup<0.376.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.691.0>},
              {name,compaction_daemon},
              {mfargs,{compaction_daemon,start_link,[]}},
              {restart_type,{permanent,4}},
              {shutdown,86400000},
              {child_type,worker}]
[ns_server:debug,2022-09-08T12:07:42.725Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:07:42.726Z,ns_1@127.0.0.1:<0.703.0>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:07:42.727Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_master) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:07:42.727Z,ns_1@127.0.0.1:<0.707.0>:compaction_daemon:spawn_master_db_compactor:901]Start compaction of master db for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[error_logger:info,2022-09-08T12:07:42.733Z,ns_1@127.0.0.1:cluster_logs_sup<0.708.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,cluster_logs_sup}
    started: [{pid,<0.709.0>},
              {id,ets_holder},
              {mfargs,{cluster_logs_collection_task,start_link_ets_holder,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2022-09-08T12:07:42.733Z,ns_1@127.0.0.1:ns_server_sup<0.376.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.708.0>},
              {name,cluster_logs_sup},
              {mfargs,{cluster_logs_sup,start_link,[]}},
              {restart_type,permanent},
              {shutdown,infinity},
              {child_type,supervisor}]
[error_logger:info,2022-09-08T12:07:42.734Z,ns_1@127.0.0.1:ns_server_sup<0.376.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.710.0>},
              {name,collections},
              {mfargs,{collections,start_link,[]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[error_logger:info,2022-09-08T12:07:42.735Z,ns_1@127.0.0.1:ns_server_sup<0.376.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.714.0>},
              {name,leader_events},
              {mfargs,{gen_event,start_link,[{local,leader_events}]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[ns_server:info,2022-09-08T12:07:42.735Z,ns_1@127.0.0.1:<0.706.0>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:07:42.735Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:07:42.735Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_master too soon. Next run will be in 3600s
[ns_server:debug,2022-09-08T12:07:42.736Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:07:42.736Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[error_logger:info,2022-09-08T12:07:42.748Z,ns_1@127.0.0.1:leader_leases_sup<0.719.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,leader_leases_sup}
    started: [{pid,<0.727.0>},
              {id,leader_activities},
              {mfargs,{leader_activities,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,10000},
              {child_type,worker}]

[ns_server:warn,2022-09-08T12:07:42.755Z,ns_1@127.0.0.1:leader_lease_agent<0.729.0>:leader_lease_agent:maybe_recover_persisted_lease:390]Found persisted lease [{node,'ns_1@127.0.0.1'},
                       {uuid,<<"c1569384114faab696b10950c44ef888">>},
                       {time_left,15000},
                       {status,active}]
[error_logger:info,2022-09-08T12:07:42.756Z,ns_1@127.0.0.1:leader_leases_sup<0.719.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,leader_leases_sup}
    started: [{pid,<0.729.0>},
              {id,leader_lease_agent},
              {mfargs,{leader_lease_agent,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2022-09-08T12:07:42.756Z,ns_1@127.0.0.1:leader_services_sup<0.717.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,leader_services_sup}
    started: [{pid,<0.719.0>},
              {id,leader_leases_sup},
              {mfargs,{leader_leases_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2022-09-08T12:07:42.762Z,ns_1@127.0.0.1:leader_registry_sup<0.731.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,leader_registry_sup}
    started: [{pid,<0.732.0>},
              {id,leader_registry},
              {mfargs,{leader_registry,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2022-09-08T12:07:42.766Z,ns_1@127.0.0.1:leader_registry_sup<0.731.0>:mb_master:check_master_takeover_needed:254]Sending master node question to the following nodes: []
[ns_server:debug,2022-09-08T12:07:42.767Z,ns_1@127.0.0.1:leader_registry_sup<0.731.0>:mb_master:check_master_takeover_needed:256]Got replies: []
[ns_server:debug,2022-09-08T12:07:42.767Z,ns_1@127.0.0.1:leader_registry_sup<0.731.0>:mb_master:check_master_takeover_needed:262]Was unable to discover master, not going to force mastership takeover
[ns_server:debug,2022-09-08T12:07:42.767Z,ns_1@127.0.0.1:mb_master<0.734.0>:mb_master:init:80]Heartbeat interval is 2000
[user:info,2022-09-08T12:07:42.767Z,ns_1@127.0.0.1:mb_master<0.734.0>:mb_master:init:85]I'm the only node, so I'm the master.
[ns_server:debug,2022-09-08T12:07:42.768Z,ns_1@127.0.0.1:leader_registry<0.732.0>:leader_registry:handle_new_leader:275]New leader is 'ns_1@127.0.0.1'. Invalidating name cache.
[ns_server:debug,2022-09-08T12:07:42.779Z,ns_1@127.0.0.1:mb_master<0.734.0>:master_activity_events:submit_cast:76]Failed to send master activity event {became_master,'ns_1@127.0.0.1'}: {error,
                                                                        badarg}
[error_logger:info,2022-09-08T12:07:42.783Z,ns_1@127.0.0.1:mb_master_sup<0.736.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,mb_master_sup}
    started: [{pid,<0.737.0>},
              {id,leader_lease_acquirer},
              {mfargs,{leader_lease_acquirer,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,10000},
              {child_type,worker}]

[ns_server:debug,2022-09-08T12:07:42.786Z,ns_1@127.0.0.1:leader_quorum_nodes_manager<0.739.0>:leader_quorum_nodes_manager:pull_config:99]Attempting to pull config from nodes:
[]
[ns_server:debug,2022-09-08T12:07:42.786Z,ns_1@127.0.0.1:leader_quorum_nodes_manager<0.739.0>:leader_quorum_nodes_manager:pull_config:104]Pulled config successfully.
[error_logger:info,2022-09-08T12:07:42.786Z,ns_1@127.0.0.1:mb_master_sup<0.736.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,mb_master_sup}
    started: [{pid,<0.739.0>},
              {id,leader_quorum_nodes_manager},
              {mfargs,{leader_quorum_nodes_manager,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:info,2022-09-08T12:07:42.792Z,ns_1@127.0.0.1:mb_master_sup<0.736.0>:misc:start_singleton:901]start_singleton(gen_server, start_link, [{via,leader_registry,ns_tick},
                                         ns_tick,[],[]]): started as <0.744.0> on 'ns_1@127.0.0.1'

[error_logger:info,2022-09-08T12:07:42.792Z,ns_1@127.0.0.1:mb_master_sup<0.736.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,mb_master_sup}
    started: [{pid,<0.744.0>},
              {id,ns_tick},
              {mfargs,{ns_tick,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,10},
              {child_type,worker}]

[ns_server:warn,2022-09-08T12:07:42.794Z,ns_1@127.0.0.1:<0.743.0>:leader_lease_acquire_worker:handle_lease_already_acquired:226]Failed to acquire lease from 'ns_1@127.0.0.1' because its already taken by {'ns_1@127.0.0.1',
                                                                            <<"c1569384114faab696b10950c44ef888">>} (valid for 14961ms)
[ns_server:debug,2022-09-08T12:07:42.800Z,ns_1@127.0.0.1:<0.747.0>:chronicle_master:do_init:141]Starting with SelfRef = #Ref<0.1778819075.971767809.232678>
[ns_server:info,2022-09-08T12:07:42.800Z,ns_1@127.0.0.1:mb_master_sup<0.736.0>:misc:start_singleton:901]start_singleton(gen_server2, start_link, [{via,leader_registry,
                                           chronicle_master},
                                          chronicle_master,[],[]]): started as <0.747.0> on 'ns_1@127.0.0.1'

[error_logger:info,2022-09-08T12:07:42.801Z,ns_1@127.0.0.1:mb_master_sup<0.736.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,mb_master_sup}
    started: [{pid,<0.747.0>},
              {id,chronicle_master},
              {mfargs,{chronicle_master,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2022-09-08T12:07:42.803Z,ns_1@127.0.0.1:ns_orchestrator_sup<0.750.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_orchestrator_sup}
    started: [{pid,<0.751.0>},
              {id,compat_mode_events},
              {mfargs,{gen_event,start_link,[{local,compat_mode_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2022-09-08T12:07:42.806Z,ns_1@127.0.0.1:ns_orchestrator_sup<0.750.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_orchestrator_sup}
    started: [{pid,<0.752.0>},
              {id,compat_mode_manager},
              {mfargs,{compat_mode_manager,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2022-09-08T12:07:42.811Z,ns_1@127.0.0.1:ns_orchestrator_child_sup<0.753.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_orchestrator_child_sup}
    started: [{pid,<0.754.0>},
              {id,ns_janitor_server},
              {mfargs,{ns_janitor_server,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:info,2022-09-08T12:07:42.815Z,ns_1@127.0.0.1:ns_orchestrator_child_sup<0.753.0>:misc:start_singleton:901]start_singleton(gen_server, start_link, [{via,leader_registry,
                                          auto_reprovision},
                                         auto_reprovision,[],[]]): started as <0.755.0> on 'ns_1@127.0.0.1'

[error_logger:info,2022-09-08T12:07:42.815Z,ns_1@127.0.0.1:ns_orchestrator_child_sup<0.753.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_orchestrator_child_sup}
    started: [{pid,<0.755.0>},
              {id,auto_reprovision},
              {mfargs,{auto_reprovision,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:info,2022-09-08T12:07:42.817Z,ns_1@127.0.0.1:ns_orchestrator_child_sup<0.753.0>:misc:start_singleton:901]start_singleton(gen_server, start_link, [{via,leader_registry,auto_rebalance},
                                         auto_rebalance,[],[]]): started as <0.756.0> on 'ns_1@127.0.0.1'

[error_logger:info,2022-09-08T12:07:42.818Z,ns_1@127.0.0.1:ns_orchestrator_child_sup<0.753.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_orchestrator_child_sup}
    started: [{pid,<0.756.0>},
              {id,auto_rebalance},
              {mfargs,{auto_rebalance,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:info,2022-09-08T12:07:42.818Z,ns_1@127.0.0.1:ns_orchestrator_child_sup<0.753.0>:misc:start_singleton:901]start_singleton(gen_statem, start_link, [{via,leader_registry,ns_orchestrator},
                                         ns_orchestrator,[],[]]): started as <0.757.0> on 'ns_1@127.0.0.1'

[error_logger:info,2022-09-08T12:07:42.818Z,ns_1@127.0.0.1:ns_orchestrator_child_sup<0.753.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_orchestrator_child_sup}
    started: [{pid,<0.757.0>},
              {id,ns_orchestrator},
              {mfargs,{ns_orchestrator,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2022-09-08T12:07:42.818Z,ns_1@127.0.0.1:ns_orchestrator_sup<0.750.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_orchestrator_sup}
    started: [{pid,<0.753.0>},
              {id,ns_orchestrator_child_sup},
              {mfargs,{ns_orchestrator_child_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2022-09-08T12:07:42.818Z,ns_1@127.0.0.1:<0.759.0>:auto_failover:init:192]init auto_failover.
[user:info,2022-09-08T12:07:42.818Z,ns_1@127.0.0.1:<0.759.0>:auto_failover:handle_call:223]Enabled auto-failover with timeout 120 and max count 1
[ns_server:info,2022-09-08T12:07:42.823Z,ns_1@127.0.0.1:ns_orchestrator_sup<0.750.0>:misc:start_singleton:901]start_singleton(gen_server, start_link, [{via,leader_registry,auto_failover},
                                         auto_failover,[],[]]): started as <0.759.0> on 'ns_1@127.0.0.1'

[error_logger:info,2022-09-08T12:07:42.823Z,ns_1@127.0.0.1:ns_orchestrator_sup<0.750.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_orchestrator_sup}
    started: [{pid,<0.759.0>},
              {id,auto_failover},
              {mfargs,{auto_failover,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2022-09-08T12:07:42.823Z,ns_1@127.0.0.1:mb_master_sup<0.736.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,mb_master_sup}
    started: [{pid,<0.750.0>},
              {id,ns_orchestrator_sup},
              {mfargs,{ns_orchestrator_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:info,2022-09-08T12:07:42.828Z,ns_1@127.0.0.1:mb_master_sup<0.736.0>:misc:start_singleton:901]start_singleton(gen_server, start_link, [{via,leader_registry,
                                          tombstone_purger},
                                         tombstone_purger,[],[]]): started as <0.761.0> on 'ns_1@127.0.0.1'

[error_logger:info,2022-09-08T12:07:42.828Z,ns_1@127.0.0.1:mb_master_sup<0.736.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,mb_master_sup}
    started: [{pid,<0.761.0>},
              {id,tombstone_purger},
              {mfargs,{tombstone_purger,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2022-09-08T12:07:42.832Z,ns_1@127.0.0.1:<0.763.0>:license_reporting:init:60]Starting license_reporting server
[ns_server:info,2022-09-08T12:07:42.832Z,ns_1@127.0.0.1:mb_master_sup<0.736.0>:misc:start_singleton:901]start_singleton(gen_server, start_link, [{via,leader_registry,
                                          license_reporting},
                                         license_reporting,[],[]]): started as <0.763.0> on 'ns_1@127.0.0.1'

[error_logger:info,2022-09-08T12:07:42.833Z,ns_1@127.0.0.1:mb_master_sup<0.736.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,mb_master_sup}
    started: [{pid,<0.763.0>},
              {id,license_reporting},
              {mfargs,{license_reporting,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2022-09-08T12:07:42.833Z,ns_1@127.0.0.1:leader_registry_sup<0.731.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,leader_registry_sup}
    started: [{pid,<0.734.0>},
              {id,mb_master},
              {mfargs,{mb_master,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2022-09-08T12:07:42.834Z,ns_1@127.0.0.1:leader_services_sup<0.717.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,leader_services_sup}
    started: [{pid,<0.731.0>},
              {id,leader_registry_sup},
              {mfargs,{leader_registry_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2022-09-08T12:07:42.834Z,ns_1@127.0.0.1:<0.716.0>:restartable:start_child:92]Started child process <0.717.0>
  MFA: {leader_services_sup,start_link,[]}
[error_logger:info,2022-09-08T12:07:42.834Z,ns_1@127.0.0.1:ns_server_sup<0.376.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.716.0>},
              {name,leader_services_sup},
              {mfargs,{restartable,start_link,
                                   [{leader_services_sup,start_link,[]},
                                    infinity]}},
              {restart_type,permanent},
              {shutdown,infinity},
              {child_type,supervisor}]
[error_logger:info,2022-09-08T12:07:42.840Z,ns_1@127.0.0.1:ns_server_sup<0.376.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.768.0>},
              {name,ns_tick_agent},
              {mfargs,{ns_tick_agent,start_link,[]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[error_logger:info,2022-09-08T12:07:42.840Z,ns_1@127.0.0.1:ns_server_sup<0.376.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.770.0>},
              {name,master_activity_events_ingress},
              {mfargs,{gen_event,start_link,
                                 [{local,master_activity_events_ingress}]}},
              {restart_type,permanent},
              {shutdown,brutal_kill},
              {child_type,worker}]
[error_logger:info,2022-09-08T12:07:42.841Z,ns_1@127.0.0.1:ns_server_sup<0.376.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.771.0>},
              {name,master_activity_events_timestamper},
              {mfargs,{master_activity_events,start_link_timestamper,[]}},
              {restart_type,permanent},
              {shutdown,brutal_kill},
              {child_type,worker}]
[error_logger:info,2022-09-08T12:07:42.844Z,ns_1@127.0.0.1:ns_server_sup<0.376.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.772.0>},
              {name,master_activity_events_pids_watcher},
              {mfargs,{master_activity_events_pids_watcher,start_link,[]}},
              {restart_type,permanent},
              {shutdown,brutal_kill},
              {child_type,worker}]
[error_logger:info,2022-09-08T12:07:42.848Z,ns_1@127.0.0.1:ns_server_sup<0.376.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.773.0>},
              {name,master_activity_events_keeper},
              {mfargs,{master_activity_events_keeper,start_link,[]}},
              {restart_type,permanent},
              {shutdown,brutal_kill},
              {child_type,worker}]
[error_logger:info,2022-09-08T12:07:42.861Z,ns_1@127.0.0.1:health_monitor_sup<0.775.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,health_monitor_sup}
    started: [{pid,<0.776.0>},
              {id,ns_server_monitor},
              {mfargs,{ns_server_monitor,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2022-09-08T12:07:42.861Z,ns_1@127.0.0.1:health_monitor_sup<0.775.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,health_monitor_sup}
    started: [{pid,<0.778.0>},
              {id,service_monitor_children_sup},
              {mfargs,{supervisor,start_link,
                                  [{local,service_monitor_children_sup},
                                   health_monitor_sup,child]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2022-09-08T12:07:42.866Z,ns_1@127.0.0.1:service_monitor_children_sup<0.778.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,service_monitor_children_sup}
    started: [{pid,<0.785.0>},
              {id,{kv,dcp_traffic_monitor}},
              {mfargs,{dcp_traffic_monitor,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2022-09-08T12:07:42.871Z,ns_1@127.0.0.1:service_monitor_children_sup<0.778.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,service_monitor_children_sup}
    started: [{pid,<0.787.0>},
              {id,{kv,kv_stats_monitor}},
              {mfargs,{kv_stats_monitor,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2022-09-08T12:07:42.879Z,ns_1@127.0.0.1:service_monitor_children_sup<0.778.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,service_monitor_children_sup}
    started: [{pid,<0.792.0>},
              {id,{kv,kv_monitor}},
              {mfargs,{kv_monitor,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2022-09-08T12:07:42.879Z,ns_1@127.0.0.1:health_monitor_sup<0.775.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,health_monitor_sup}
    started: [{pid,<0.779.0>},
              {id,service_monitor_worker},
              {mfargs,{erlang,apply,[#Fun<health_monitor_sup.0.70530162>,[]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:warn,2022-09-08T12:07:42.884Z,ns_1@127.0.0.1:<0.791.0>:ns_memcached:connect:1240]Unable to connect: {error,{badmatch,[{inet,{error,econnrefused}}]}}, retrying.
[ns_server:warn,2022-09-08T12:07:42.885Z,ns_1@127.0.0.1:kv_monitor<0.792.0>:kv_monitor:get_buckets_status:250]The following buckets are not ready: ["todo"]
[error_logger:info,2022-09-08T12:07:42.888Z,ns_1@127.0.0.1:health_monitor_sup<0.775.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,health_monitor_sup}
    started: [{pid,<0.798.0>},
              {id,node_monitor},
              {mfargs,{node_monitor,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2022-09-08T12:07:42.894Z,ns_1@127.0.0.1:health_monitor_sup<0.775.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,health_monitor_sup}
    started: [{pid,<0.804.0>},
              {id,node_status_analyzer},
              {mfargs,{node_status_analyzer,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2022-09-08T12:07:42.894Z,ns_1@127.0.0.1:ns_server_sup<0.376.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.775.0>},
              {name,health_monitor_sup},
              {mfargs,{health_monitor_sup,start_link,[]}},
              {restart_type,permanent},
              {shutdown,infinity},
              {child_type,supervisor}]
[error_logger:info,2022-09-08T12:07:42.901Z,ns_1@127.0.0.1:ns_server_sup<0.376.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.806.0>},
              {name,rebalance_agent},
              {mfargs,{rebalance_agent,start_link,[]}},
              {restart_type,permanent},
              {shutdown,5000},
              {child_type,worker}]
[error_logger:info,2022-09-08T12:07:42.916Z,ns_1@127.0.0.1:ns_server_sup<0.376.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.807.0>},
              {name,ns_rebalance_report_manager},
              {mfargs,{ns_rebalance_report_manager,start_link,[]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[error_logger:info,2022-09-08T12:07:42.916Z,ns_1@127.0.0.1:ns_server_nodes_sup<0.276.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.376.0>},
              {name,ns_server_sup},
              {mfargs,{ns_server_sup,start_link,[]}},
              {restart_type,permanent},
              {shutdown,infinity},
              {child_type,supervisor}]
[ns_server:debug,2022-09-08T12:07:42.916Z,ns_1@127.0.0.1:ns_server_nodes_sup<0.276.0>:one_shot_barrier:notify:21]Notifying on barrier menelaus_barrier
[ns_server:debug,2022-09-08T12:07:42.916Z,ns_1@127.0.0.1:menelaus_barrier<0.283.0>:one_shot_barrier:barrier_body:56]Barrier menelaus_barrier got notification from <0.276.0>
[ns_server:debug,2022-09-08T12:07:42.917Z,ns_1@127.0.0.1:ns_server_nodes_sup<0.276.0>:one_shot_barrier:notify:26]Successfuly notified on barrier menelaus_barrier
[ns_server:debug,2022-09-08T12:07:42.917Z,ns_1@127.0.0.1:<0.274.0>:restartable:start_child:92]Started child process <0.276.0>
  MFA: {ns_server_nodes_sup,start_link,[]}
[error_logger:info,2022-09-08T12:07:42.917Z,ns_1@127.0.0.1:ns_server_cluster_sup<0.220.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_cluster_sup}
    started: [{pid,<0.274.0>},
              {id,ns_server_nodes_sup},
              {mfargs,{restartable,start_link,
                                   [{ns_server_nodes_sup,start_link,[]},
                                    infinity]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2022-09-08T12:07:42.921Z,ns_1@127.0.0.1:compiled_roles_cache<0.342.0>:menelaus_roles:build_compiled_roles:998]Compile roles for user {"@",admin}
[error_logger:info,2022-09-08T12:07:42.923Z,ns_1@127.0.0.1:ns_server_cluster_sup<0.220.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_cluster_sup}
    started: [{pid,<0.809.0>},
              {id,remote_api},
              {mfargs,{remote_api,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2022-09-08T12:07:42.924Z,ns_1@127.0.0.1:root_sup<0.196.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,root_sup}
    started: [{pid,<0.220.0>},
              {id,ns_server_cluster_sup},
              {mfargs,{ns_server_cluster_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2022-09-08T12:07:42.924Z,ns_1@127.0.0.1:application_controller<0.44.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    application: ns_server
    started_at: 'ns_1@127.0.0.1'

[ns_server:debug,2022-09-08T12:07:42.924Z,ns_1@127.0.0.1:<0.9.0>:child_erlang:child_loop:128]159: Entered child_loop
[ns_server:debug,2022-09-08T12:07:42.934Z,ns_1@127.0.0.1:json_rpc_connection-saslauthd-saslauthd-port<0.810.0>:json_rpc_connection:init:68]Observed revrpc connection: label "saslauthd-saslauthd-port", handling process <0.810.0>
[ns_server:debug,2022-09-08T12:07:42.934Z,ns_1@127.0.0.1:json_rpc_connection-backup-cbauth<0.811.0>:json_rpc_connection:init:68]Observed revrpc connection: label "backup-cbauth", handling process <0.811.0>
[ns_server:debug,2022-09-08T12:07:42.935Z,ns_1@127.0.0.1:menelaus_cbauth<0.527.0>:menelaus_cbauth:handle_cast:101]Observed json rpc process {"backup-cbauth",<0.811.0>} started
[ns_server:debug,2022-09-08T12:07:42.947Z,ns_1@127.0.0.1:compiled_roles_cache<0.342.0>:menelaus_roles:build_compiled_roles:998]Compile roles for user {[],anonymous}
[ns_server:debug,2022-09-08T12:07:42.953Z,ns_1@127.0.0.1:compiled_roles_cache<0.342.0>:menelaus_roles:build_compiled_roles:998]Compile roles for user {"@backup",admin}
[ns_server:debug,2022-09-08T12:07:42.954Z,ns_1@127.0.0.1:ns_audit<0.579.0>:ns_audit:handle_call:148]Audit auth_failure: [{local,{[{ip,<<"127.0.0.1">>},{port,8091}]}},
                     {remote,{[{ip,<<"127.0.0.1">>},{port,45246}]}},
                     {real_userid,{[{domain,anonymous},
                                    {user,<<"<ud></ud>">>}]}},
                     {timestamp,<<"2022-09-08T12:07:42.954Z">>},
                     {raw_url,<<"<ud>/_event</ud>">>}]
[ns_server:warn,2022-09-08T12:07:42.956Z,ns_1@127.0.0.1:<0.817.0>:ns_memcached:connect:1240]Unable to connect: {error,{badmatch,[{inet,{error,econnrefused}}]}}, retrying.
[ns_server:debug,2022-09-08T12:07:42.959Z,ns_1@127.0.0.1:compiled_roles_cache<0.342.0>:menelaus_roles:build_compiled_roles:998]Compile roles for user {"@ns_server",admin}
[ns_server:debug,2022-09-08T12:07:43.067Z,ns_1@127.0.0.1:json_rpc_connection-cbas-cbauth<0.820.0>:json_rpc_connection:init:68]Observed revrpc connection: label "cbas-cbauth", handling process <0.820.0>
[ns_server:debug,2022-09-08T12:07:43.068Z,ns_1@127.0.0.1:menelaus_cbauth<0.527.0>:menelaus_cbauth:handle_cast:101]Observed json rpc process {"cbas-cbauth",<0.820.0>} started
[ns_server:debug,2022-09-08T12:07:43.072Z,ns_1@127.0.0.1:compiled_roles_cache<0.342.0>:menelaus_roles:build_compiled_roles:998]Compile roles for user {"@cbas",admin}
[ns_server:debug,2022-09-08T12:07:43.311Z,ns_1@127.0.0.1:json_rpc_connection-eventing-cbauth<0.825.0>:json_rpc_connection:init:68]Observed revrpc connection: label "eventing-cbauth", handling process <0.825.0>
[ns_server:debug,2022-09-08T12:07:43.311Z,ns_1@127.0.0.1:menelaus_cbauth<0.527.0>:menelaus_cbauth:handle_cast:101]Observed json rpc process {"eventing-cbauth",<0.825.0>} started
[ns_server:warn,2022-09-08T12:07:43.429Z,ns_1@127.0.0.1:memcached_refresh<0.285.0>:ns_memcached:connect:1237]Unable to connect: {error,{badmatch,[{inet,{error,econnrefused}}]}}.
[ns_server:debug,2022-09-08T12:07:43.429Z,ns_1@127.0.0.1:compiled_roles_cache<0.342.0>:menelaus_roles:build_compiled_roles:998]Compile roles for user {"@eventing",admin}
[ns_server:debug,2022-09-08T12:07:43.429Z,ns_1@127.0.0.1:memcached_refresh<0.285.0>:memcached_refresh:handle_info:93]Refresh of [rbac,isasl] failed. Retry in 1000 ms.
[ns_server:warn,2022-09-08T12:07:43.430Z,ns_1@127.0.0.1:memcached_refresh<0.285.0>:ns_memcached:connect:1237]Unable to connect: {error,{badmatch,[{inet,{error,econnrefused}}]}}.
[ns_server:debug,2022-09-08T12:07:43.430Z,ns_1@127.0.0.1:memcached_refresh<0.285.0>:memcached_refresh:handle_info:93]Refresh of [rbac,isasl] failed. Retry in 1000 ms.
[ns_server:warn,2022-09-08T12:07:43.534Z,ns_1@127.0.0.1:<0.578.0>:ns_memcached:connect:1240]Unable to connect: {error,{badmatch,[{inet,{error,econnrefused}}]}}, retrying.
[ns_server:debug,2022-09-08T12:07:43.550Z,ns_1@127.0.0.1:json_rpc_connection-fts-cbauth<0.830.0>:json_rpc_connection:init:68]Observed revrpc connection: label "fts-cbauth", handling process <0.830.0>
[ns_server:debug,2022-09-08T12:07:43.550Z,ns_1@127.0.0.1:menelaus_cbauth<0.527.0>:menelaus_cbauth:handle_cast:101]Observed json rpc process {"fts-cbauth",<0.830.0>} started
[ns_server:warn,2022-09-08T12:07:43.574Z,ns_1@127.0.0.1:<0.611.0>:ns_memcached:connect:1240]Unable to connect: {error,{badmatch,[{inet,{error,econnrefused}}]}}, retrying.
[ns_server:debug,2022-09-08T12:07:43.584Z,ns_1@127.0.0.1:compiled_roles_cache<0.342.0>:menelaus_roles:build_compiled_roles:998]Compile roles for user {"@fts",admin}
[ns_server:warn,2022-09-08T12:07:43.690Z,ns_1@127.0.0.1:<0.669.0>:ns_memcached:connect:1240]Unable to connect: {error,{badmatch,[{inet,{error,econnrefused}}]}}, retrying.
[ns_server:debug,2022-09-08T12:07:43.819Z,ns_1@127.0.0.1:json_rpc_connection-goxdcr-cbauth<0.839.0>:json_rpc_connection:init:68]Observed revrpc connection: label "goxdcr-cbauth", handling process <0.839.0>
[ns_server:debug,2022-09-08T12:07:43.819Z,ns_1@127.0.0.1:menelaus_cbauth<0.527.0>:menelaus_cbauth:handle_cast:101]Observed json rpc process {"goxdcr-cbauth",<0.839.0>} started
[ns_server:debug,2022-09-08T12:07:43.819Z,ns_1@127.0.0.1:<0.759.0>:auto_failover:log_down_nodes_reason:368]Node 'ns_1@127.0.0.1' is considered down. Reason:"The data service did not respond for the duration of the auto-failover threshold. Either none of the buckets have warmed up or there is an issue with the data service. "
[ns_server:debug,2022-09-08T12:07:43.820Z,ns_1@127.0.0.1:<0.759.0>:auto_failover_logic:log_master_activity:143]Incremented down state:
{node_state,{'ns_1@127.0.0.1',<<"1678dfae96c38e07dff43c49b9f6967b">>},
            0,new,undefined}
->{node_state,{'ns_1@127.0.0.1',<<"1678dfae96c38e07dff43c49b9f6967b">>},
              0,half_down,undefined}
[ns_server:debug,2022-09-08T12:07:43.844Z,ns_1@127.0.0.1:compiled_roles_cache<0.342.0>:menelaus_roles:build_compiled_roles:998]Compile roles for user {"@goxdcr",admin}
[ns_server:warn,2022-09-08T12:07:43.886Z,ns_1@127.0.0.1:<0.791.0>:ns_memcached:connect:1240]Unable to connect: {error,{badmatch,[{inet,{error,econnrefused}}]}}, retrying.
[ns_server:warn,2022-09-08T12:07:43.887Z,ns_1@127.0.0.1:kv_monitor<0.792.0>:kv_monitor:get_buckets_status:250]The following buckets are not ready: ["todo"]
[ns_server:debug,2022-09-08T12:07:43.931Z,ns_1@127.0.0.1:json_rpc_connection-index-cbauth<0.865.0>:json_rpc_connection:init:68]Observed revrpc connection: label "index-cbauth", handling process <0.865.0>
[ns_server:debug,2022-09-08T12:07:43.931Z,ns_1@127.0.0.1:menelaus_cbauth<0.527.0>:menelaus_cbauth:handle_cast:101]Observed json rpc process {"index-cbauth",<0.865.0>} started
[ns_server:warn,2022-09-08T12:07:43.958Z,ns_1@127.0.0.1:<0.817.0>:ns_memcached:connect:1240]Unable to connect: {error,{badmatch,[{inet,{error,econnrefused}}]}}, retrying.
[ns_server:debug,2022-09-08T12:07:43.984Z,ns_1@127.0.0.1:compiled_roles_cache<0.342.0>:menelaus_roles:build_compiled_roles:998]Compile roles for user {"@index",admin}
[ns_server:debug,2022-09-08T12:07:44.084Z,ns_1@127.0.0.1:json_rpc_connection-projector-cbauth<0.871.0>:json_rpc_connection:init:68]Observed revrpc connection: label "projector-cbauth", handling process <0.871.0>
[ns_server:debug,2022-09-08T12:07:44.084Z,ns_1@127.0.0.1:menelaus_cbauth<0.527.0>:menelaus_cbauth:handle_cast:101]Observed json rpc process {"projector-cbauth",<0.871.0>} started
[ns_server:debug,2022-09-08T12:07:44.105Z,ns_1@127.0.0.1:compiled_roles_cache<0.342.0>:menelaus_roles:build_compiled_roles:998]Compile roles for user {"@projector",admin}
[ns_server:debug,2022-09-08T12:07:44.173Z,ns_1@127.0.0.1:ns_ports_setup<0.533.0>:ns_ports_setup:set_children:60]Monitor ns_child_ports_sup <16170.129.0>
[ns_server:debug,2022-09-08T12:07:44.173Z,ns_1@127.0.0.1:memcached_config_mgr<0.580.0>:memcached_config_mgr:init:56]ns_ports_setup seems to be ready
[ns_server:debug,2022-09-08T12:07:44.183Z,ns_1@127.0.0.1:memcached_config_mgr<0.580.0>:memcached_config_mgr:find_port_pid_loop:156]Found memcached port <16170.135.0>
[ns_server:debug,2022-09-08T12:07:44.200Z,ns_1@127.0.0.1:memcached_config_mgr<0.580.0>:memcached_config_mgr:init:94]wrote memcached config to /opt/couchbase/var/lib/couchbase/config/memcached.json. Will activate memcached port server
[ns_server:debug,2022-09-08T12:07:44.201Z,ns_1@127.0.0.1:memcached_config_mgr<0.580.0>:memcached_config_mgr:init:98]activated memcached port server
[ns_server:info,2022-09-08T12:07:44.203Z,ns_1@127.0.0.1:memcached_config_mgr<0.580.0>:memcached_config_mgr:push_tls_config:225]Pushing TLS config to memcached:
{[{<<"private key">>,
   <<"/opt/couchbase/var/lib/couchbase/config/certs/pkey.pem">>},
  {<<"certificate chain">>,
   <<"/opt/couchbase/var/lib/couchbase/config/certs/chain.pem">>},
  {<<"CA file">>,<<"/opt/couchbase/var/lib/couchbase/config/certs/ca.pem">>},
  {<<"minimum version">>,<<"TLS 1.2">>},
  {<<"cipher list">>,
   {[{<<"TLS 1.2">>,<<"HIGH">>},
     {<<"TLS 1.3">>,
      <<"TLS_AES_256_GCM_SHA384:TLS_AES_128_GCM_SHA256:TLS_CHACHA20_POLY1305_SHA256">>}]}},
  {<<"cipher order">>,true},
  {<<"client cert auth">>,<<"disabled">>}]}
[ns_server:warn,2022-09-08T12:07:44.204Z,ns_1@127.0.0.1:<0.878.0>:ns_memcached:connect:1240]Unable to connect: {error,{badmatch,[{inet,{error,econnrefused}}]}}, retrying.
[ns_server:debug,2022-09-08T12:07:44.326Z,ns_1@127.0.0.1:json_rpc_connection-cbq-engine-cbauth<0.880.0>:json_rpc_connection:init:68]Observed revrpc connection: label "cbq-engine-cbauth", handling process <0.880.0>
[ns_server:debug,2022-09-08T12:07:44.326Z,ns_1@127.0.0.1:menelaus_cbauth<0.527.0>:menelaus_cbauth:handle_cast:101]Observed json rpc process {"cbq-engine-cbauth",<0.880.0>} started
[ns_server:debug,2022-09-08T12:07:44.336Z,ns_1@127.0.0.1:compiled_roles_cache<0.342.0>:menelaus_roles:build_compiled_roles:998]Compile roles for user {"@cbq-engine",admin}
[ns_server:debug,2022-09-08T12:07:44.520Z,ns_1@127.0.0.1:memcached_refresh<0.285.0>:memcached_refresh:handle_info:89]Refresh of [rbac,isasl] succeeded
[ns_server:debug,2022-09-08T12:07:44.567Z,ns_1@127.0.0.1:json_rpc_connection-cbas-service_api<0.913.0>:json_rpc_connection:init:68]Observed revrpc connection: label "cbas-service_api", handling process <0.913.0>
[ns_server:debug,2022-09-08T12:07:44.568Z,ns_1@127.0.0.1:service_agent-cbas<0.544.0>:service_agent:do_handle_connection:319]Observed new json rpc connection for cbas: <0.913.0>
[ns_server:debug,2022-09-08T12:07:44.568Z,ns_1@127.0.0.1:<0.547.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {json_rpc_events,<0.545.0>} exited with reason normal
[ns_server:debug,2022-09-08T12:07:44.674Z,ns_1@127.0.0.1:json_rpc_connection-eventing-service_api<0.1038.0>:json_rpc_connection:init:68]Observed revrpc connection: label "eventing-service_api", handling process <0.1038.0>
[ns_server:debug,2022-09-08T12:07:44.674Z,ns_1@127.0.0.1:<0.551.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {json_rpc_events,<0.549.0>} exited with reason normal
[ns_server:debug,2022-09-08T12:07:44.674Z,ns_1@127.0.0.1:service_agent-eventing<0.548.0>:service_agent:do_handle_connection:319]Observed new json rpc connection for eventing: <0.1038.0>
[ns_server:debug,2022-09-08T12:07:44.718Z,ns_1@127.0.0.1:json_rpc_connection-backup-service_api<0.1090.0>:json_rpc_connection:init:68]Observed revrpc connection: label "backup-service_api", handling process <0.1090.0>
[ns_server:debug,2022-09-08T12:07:44.718Z,ns_1@127.0.0.1:<0.543.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {json_rpc_events,<0.541.0>} exited with reason normal
[ns_server:debug,2022-09-08T12:07:44.718Z,ns_1@127.0.0.1:service_agent-backup<0.540.0>:service_agent:do_handle_connection:319]Observed new json rpc connection for backup: <0.1090.0>
[ns_server:debug,2022-09-08T12:07:44.820Z,ns_1@127.0.0.1:<0.759.0>:auto_failover_logic:log_master_activity:143]Incremented down state:
{node_state,{'ns_1@127.0.0.1',<<"1678dfae96c38e07dff43c49b9f6967b">>},
            0,half_down,undefined}
->{node_state,{'ns_1@127.0.0.1',<<"1678dfae96c38e07dff43c49b9f6967b">>},
              1,half_down,undefined}
[ns_server:debug,2022-09-08T12:07:44.854Z,ns_1@127.0.0.1:json_rpc_connection-fts-service_api<0.1171.0>:json_rpc_connection:init:68]Observed revrpc connection: label "fts-service_api", handling process <0.1171.0>
[ns_server:debug,2022-09-08T12:07:44.855Z,ns_1@127.0.0.1:service_agent-fts<0.552.0>:service_agent:do_handle_connection:319]Observed new json rpc connection for fts: <0.1171.0>
[ns_server:debug,2022-09-08T12:07:44.855Z,ns_1@127.0.0.1:<0.555.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {json_rpc_events,<0.553.0>} exited with reason normal
[ns_server:debug,2022-09-08T12:07:44.889Z,ns_1@127.0.0.1:<0.789.0>:kv_stats_monitor:get_latest_stats:231]Error {error,{select_bucket_failed,{memcached_error,key_enoent,undefined}}} while trying to read disk-failures stats for bucket "todo"
[ns_server:debug,2022-09-08T12:07:44.935Z,ns_1@127.0.0.1:ns_heart_slow_status_updater<0.442.0>:goxdcr_rest:get_from_goxdcr:137]Goxdcr is temporary not available. Return empty list.
[ns_server:debug,2022-09-08T12:07:44.954Z,ns_1@127.0.0.1:json_rpc_connection-index-service_api<0.1218.0>:json_rpc_connection:init:68]Observed revrpc connection: label "index-service_api", handling process <0.1218.0>
[ns_server:debug,2022-09-08T12:07:44.960Z,ns_1@127.0.0.1:service_agent-index<0.556.0>:service_agent:do_handle_connection:319]Observed new json rpc connection for index: <0.1218.0>
[ns_server:debug,2022-09-08T12:07:44.960Z,ns_1@127.0.0.1:<0.559.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {json_rpc_events,<0.557.0>} exited with reason normal
[ns_server:info,2022-09-08T12:07:45.222Z,ns_1@127.0.0.1:memcached_config_mgr<0.580.0>:memcached_config_mgr:push_tls_config:229]Successfully pushed TLS config to memcached
[ns_server:warn,2022-09-08T12:07:45.390Z,ns_1@127.0.0.1:kv_monitor<0.792.0>:kv_monitor:get_buckets_status:250]The following buckets are not ready: ["todo"]
[ns_server:debug,2022-09-08T12:07:45.823Z,ns_1@127.0.0.1:<0.759.0>:auto_failover_logic:log_master_activity:143]Incremented down state:
{node_state,{'ns_1@127.0.0.1',<<"1678dfae96c38e07dff43c49b9f6967b">>},
            1,half_down,undefined}
->{node_state,{'ns_1@127.0.0.1',<<"1678dfae96c38e07dff43c49b9f6967b">>},
              2,half_down,undefined}
[ns_server:debug,2022-09-08T12:07:45.904Z,ns_1@127.0.0.1:json_rpc_connection-cbq-engine-service_api<0.1435.0>:json_rpc_connection:init:68]Observed revrpc connection: label "cbq-engine-service_api", handling process <0.1435.0>
[ns_server:debug,2022-09-08T12:07:45.904Z,ns_1@127.0.0.1:service_agent-n1ql<0.560.0>:service_agent:do_handle_connection:319]Observed new json rpc connection for n1ql: <0.1435.0>
[ns_server:debug,2022-09-08T12:07:45.904Z,ns_1@127.0.0.1:<0.563.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {json_rpc_events,<0.561.0>} exited with reason normal
[ns_server:warn,2022-09-08T12:07:46.892Z,ns_1@127.0.0.1:kv_monitor<0.792.0>:kv_monitor:get_buckets_status:250]The following buckets are not ready: ["todo"]
[ns_server:debug,2022-09-08T12:07:46.893Z,ns_1@127.0.0.1:<0.759.0>:auto_failover_logic:log_master_activity:143]Incremented down state:
{node_state,{'ns_1@127.0.0.1',<<"1678dfae96c38e07dff43c49b9f6967b">>},
            2,half_down,undefined}
->{node_state,{'ns_1@127.0.0.1',<<"1678dfae96c38e07dff43c49b9f6967b">>},
              3,half_down,undefined}
[ns_server:info,2022-09-08T12:07:47.819Z,ns_1@127.0.0.1:<0.757.0>:ns_orchestrator:handle_event:494]Skipping janitor in state janitor_running
[ns_server:debug,2022-09-08T12:07:47.825Z,ns_1@127.0.0.1:<0.759.0>:auto_failover_logic:log_master_activity:143]Incremented down state:
{node_state,{'ns_1@127.0.0.1',<<"1678dfae96c38e07dff43c49b9f6967b">>},
            3,half_down,undefined}
->{node_state,{'ns_1@127.0.0.1',<<"1678dfae96c38e07dff43c49b9f6967b">>},
              4,half_down,undefined}
[ns_server:warn,2022-09-08T12:07:48.397Z,ns_1@127.0.0.1:kv_monitor<0.792.0>:kv_monitor:get_buckets_status:250]The following buckets are not ready: ["todo"]
[ns_server:debug,2022-09-08T12:07:48.826Z,ns_1@127.0.0.1:<0.759.0>:auto_failover_logic:log_master_activity:143]Incremented down state:
{node_state,{'ns_1@127.0.0.1',<<"1678dfae96c38e07dff43c49b9f6967b">>},
            4,half_down,undefined}
->{node_state,{'ns_1@127.0.0.1',<<"1678dfae96c38e07dff43c49b9f6967b">>},
              5,half_down,undefined}
[ns_server:debug,2022-09-08T12:07:48.992Z,ns_1@127.0.0.1:compiled_roles_cache<0.342.0>:menelaus_roles:build_compiled_roles:998]Compile roles for user {"@prometheus",stats_reader}
[ns_server:warn,2022-09-08T12:07:49.900Z,ns_1@127.0.0.1:kv_monitor<0.792.0>:kv_monitor:get_buckets_status:250]The following buckets are not ready: ["todo"]
[ns_server:debug,2022-09-08T12:07:49.902Z,ns_1@127.0.0.1:<0.759.0>:auto_failover_logic:log_master_activity:143]Incremented down state:
{node_state,{'ns_1@127.0.0.1',<<"1678dfae96c38e07dff43c49b9f6967b">>},
            5,half_down,undefined}
->{node_state,{'ns_1@127.0.0.1',<<"1678dfae96c38e07dff43c49b9f6967b">>},
              6,half_down,undefined}
[ns_server:debug,2022-09-08T12:07:50.830Z,ns_1@127.0.0.1:<0.759.0>:auto_failover_logic:log_master_activity:143]Incremented down state:
{node_state,{'ns_1@127.0.0.1',<<"1678dfae96c38e07dff43c49b9f6967b">>},
            6,half_down,undefined}
->{node_state,{'ns_1@127.0.0.1',<<"1678dfae96c38e07dff43c49b9f6967b">>},
              7,half_down,undefined}
[ns_server:warn,2022-09-08T12:07:51.402Z,ns_1@127.0.0.1:kv_monitor<0.792.0>:kv_monitor:get_buckets_status:250]The following buckets are not ready: ["todo"]
[ns_server:info,2022-09-08T12:07:51.411Z,ns_1@127.0.0.1:ns_memcached-todo<0.668.0>:ns_memcached:do_ensure_bucket:1365]Created bucket "todo" with config string "max_size=3920625664;dbname=/opt/couchbase/var/lib/couchbase/data/todo;backend=couchdb;couch_bucket=todo;max_vbuckets=1024;alog_path=/opt/couchbase/var/lib/couchbase/data/todo/access.log;data_traffic_enabled=false;max_num_workers=3;uuid=1d5e549a39e5fbca838b18cff4b8f22b;conflict_resolution_type=seqno;bucket_type=persistent;durability_min_level=none;pitr_enabled=false;pitr_granularity=600;pitr_max_history_age=86400;magma_fragmentation_percentage=50;magma_mem_quota_ratio=0.5;item_eviction_policy=value_only;persistent_metadata_purge_age=259200;max_ttl=0;ht_locks=47;compression_mode=passive;failpartialwarmup=false"
[ns_server:info,2022-09-08T12:07:51.412Z,ns_1@127.0.0.1:ns_memcached-todo<0.668.0>:ns_memcached:handle_info:706]Main ns_memcached connection established: {ok,#Port<0.149>}
[ns_server:debug,2022-09-08T12:07:51.418Z,ns_1@127.0.0.1:<0.705.0>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 0, disk size is 0
[ns_server:debug,2022-09-08T12:07:51.419Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:07:51.419Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 21s
[ns_server:info,2022-09-08T12:07:51.427Z,ns_1@127.0.0.1:janitor_agent-todo<0.698.0>:janitor_agent:read_flush_counter:915]Loading flushseq failed: {error,enoent}. Assuming it's equal to global config.
[ns_server:info,2022-09-08T12:07:51.427Z,ns_1@127.0.0.1:janitor_agent-todo<0.698.0>:janitor_agent:read_flush_counter_from_config:923]Initialized flushseq 0 from bucket config
[ns_server:debug,2022-09-08T12:07:51.831Z,ns_1@127.0.0.1:<0.759.0>:auto_failover_logic:log_master_activity:143]Incremented down state:
{node_state,{'ns_1@127.0.0.1',<<"1678dfae96c38e07dff43c49b9f6967b">>},
            7,half_down,undefined}
->{node_state,{'ns_1@127.0.0.1',<<"1678dfae96c38e07dff43c49b9f6967b">>},
              8,half_down,undefined}
[ns_server:warn,2022-09-08T12:07:52.404Z,ns_1@127.0.0.1:kv_monitor<0.792.0>:kv_monitor:get_buckets_status:250]The following buckets are not ready: ["todo"]
[ns_server:info,2022-09-08T12:07:52.821Z,ns_1@127.0.0.1:<0.757.0>:ns_orchestrator:handle_event:494]Skipping janitor in state janitor_running
[ns_server:debug,2022-09-08T12:07:52.832Z,ns_1@127.0.0.1:<0.759.0>:auto_failover_logic:log_master_activity:143]Incremented down state:
{node_state,{'ns_1@127.0.0.1',<<"1678dfae96c38e07dff43c49b9f6967b">>},
            8,half_down,undefined}
->{node_state,{'ns_1@127.0.0.1',<<"1678dfae96c38e07dff43c49b9f6967b">>},
              9,half_down,undefined}
[ns_server:warn,2022-09-08T12:07:53.406Z,ns_1@127.0.0.1:kv_monitor<0.792.0>:kv_monitor:get_buckets_status:250]The following buckets are not ready: ["todo"]
[ns_server:debug,2022-09-08T12:07:53.832Z,ns_1@127.0.0.1:<0.759.0>:auto_failover_logic:log_master_activity:143]Incremented down state:
{node_state,{'ns_1@127.0.0.1',<<"1678dfae96c38e07dff43c49b9f6967b">>},
            9,half_down,undefined}
->{node_state,{'ns_1@127.0.0.1',<<"1678dfae96c38e07dff43c49b9f6967b">>},
              10,half_down,undefined}
[ns_server:warn,2022-09-08T12:07:54.408Z,ns_1@127.0.0.1:kv_monitor<0.792.0>:kv_monitor:get_buckets_status:250]The following buckets are not ready: ["todo"]
[ns_server:debug,2022-09-08T12:07:54.834Z,ns_1@127.0.0.1:<0.759.0>:auto_failover_logic:log_master_activity:143]Incremented down state:
{node_state,{'ns_1@127.0.0.1',<<"1678dfae96c38e07dff43c49b9f6967b">>},
            10,half_down,undefined}
->{node_state,{'ns_1@127.0.0.1',<<"1678dfae96c38e07dff43c49b9f6967b">>},
              11,half_down,undefined}
[ns_server:warn,2022-09-08T12:07:55.410Z,ns_1@127.0.0.1:kv_monitor<0.792.0>:kv_monitor:get_buckets_status:250]The following buckets are not ready: ["todo"]
[ns_server:debug,2022-09-08T12:07:55.835Z,ns_1@127.0.0.1:<0.759.0>:auto_failover_logic:log_master_activity:143]Incremented down state:
{node_state,{'ns_1@127.0.0.1',<<"1678dfae96c38e07dff43c49b9f6967b">>},
            11,half_down,undefined}
->{node_state,{'ns_1@127.0.0.1',<<"1678dfae96c38e07dff43c49b9f6967b">>},
              12,half_down,undefined}
[ns_server:warn,2022-09-08T12:07:56.411Z,ns_1@127.0.0.1:kv_monitor<0.792.0>:kv_monitor:get_buckets_status:250]The following buckets are not ready: ["todo"]
[ns_server:debug,2022-09-08T12:07:56.835Z,ns_1@127.0.0.1:<0.759.0>:auto_failover_logic:log_master_activity:143]Incremented down state:
{node_state,{'ns_1@127.0.0.1',<<"1678dfae96c38e07dff43c49b9f6967b">>},
            12,half_down,undefined}
->{node_state,{'ns_1@127.0.0.1',<<"1678dfae96c38e07dff43c49b9f6967b">>},
              13,half_down,undefined}
[ns_server:warn,2022-09-08T12:07:57.413Z,ns_1@127.0.0.1:kv_monitor<0.792.0>:kv_monitor:get_buckets_status:250]The following buckets are not ready: ["todo"]
[ns_server:debug,2022-09-08T12:07:57.756Z,ns_1@127.0.0.1:leader_lease_agent<0.729.0>:leader_lease_agent:handle_lease_expired:277]Lease held by {lease_holder,<<"c1569384114faab696b10950c44ef888">>,
                            'ns_1@127.0.0.1'} expired. Starting expirer.
[ns_server:warn,2022-09-08T12:07:57.756Z,ns_1@127.0.0.1:<0.743.0>:leader_lease_acquire_worker:handle_lease_already_acquired:226]Failed to acquire lease from 'ns_1@127.0.0.1' because its already taken by {'ns_1@127.0.0.1',
                                                                            <<"c1569384114faab696b10950c44ef888">>} (valid for 0ms)
[ns_server:debug,2022-09-08T12:07:57.758Z,ns_1@127.0.0.1:leader_lease_agent<0.729.0>:leader_lease_agent:do_handle_acquire_lease:140]Granting lease to {lease_holder,<<"6739a7645d0d3cb96437d1c2d542cfcc">>,
                                'ns_1@127.0.0.1'} for 15000ms
[ns_server:info,2022-09-08T12:07:57.783Z,ns_1@127.0.0.1:<0.743.0>:leader_lease_acquire_worker:handle_fresh_lease_acquired:296]Acquired lease from node 'ns_1@127.0.0.1' (lease uuid: <<"6739a7645d0d3cb96437d1c2d542cfcc">>)
[ns_server:debug,2022-09-08T12:07:57.794Z,ns_1@127.0.0.1:<0.2762.0>:janitor_agent:query_vbuckets_loop_next_step:111]Waiting for "todo" on 'ns_1@127.0.0.1'
[ns_server:info,2022-09-08T12:07:57.822Z,ns_1@127.0.0.1:<0.757.0>:ns_orchestrator:handle_event:494]Skipping janitor in state janitor_running
[ns_server:debug,2022-09-08T12:07:57.836Z,ns_1@127.0.0.1:<0.759.0>:auto_failover_logic:log_master_activity:143]Incremented down state:
{node_state,{'ns_1@127.0.0.1',<<"1678dfae96c38e07dff43c49b9f6967b">>},
            13,half_down,undefined}
->{node_state,{'ns_1@127.0.0.1',<<"1678dfae96c38e07dff43c49b9f6967b">>},
              14,half_down,undefined}
[ns_server:warn,2022-09-08T12:07:58.416Z,ns_1@127.0.0.1:kv_monitor<0.792.0>:kv_monitor:get_buckets_status:250]The following buckets are not ready: ["todo"]
[ns_server:debug,2022-09-08T12:07:58.796Z,ns_1@127.0.0.1:<0.2762.0>:janitor_agent:query_vbuckets_loop_next_step:111]Waiting for "todo" on 'ns_1@127.0.0.1'
[ns_server:debug,2022-09-08T12:07:58.840Z,ns_1@127.0.0.1:<0.759.0>:auto_failover_logic:log_master_activity:143]Incremented down state:
{node_state,{'ns_1@127.0.0.1',<<"1678dfae96c38e07dff43c49b9f6967b">>},
            14,half_down,undefined}
->{node_state,{'ns_1@127.0.0.1',<<"1678dfae96c38e07dff43c49b9f6967b">>},
              15,half_down,undefined}
[ns_server:warn,2022-09-08T12:07:59.416Z,ns_1@127.0.0.1:kv_monitor<0.792.0>:kv_monitor:get_buckets_status:250]The following buckets are not ready: ["todo"]
[ns_server:debug,2022-09-08T12:07:59.797Z,ns_1@127.0.0.1:<0.2762.0>:janitor_agent:query_vbuckets_loop_next_step:111]Waiting for "todo" on 'ns_1@127.0.0.1'
[ns_server:debug,2022-09-08T12:07:59.840Z,ns_1@127.0.0.1:<0.759.0>:auto_failover_logic:log_master_activity:143]Incremented down state:
{node_state,{'ns_1@127.0.0.1',<<"1678dfae96c38e07dff43c49b9f6967b">>},
            15,half_down,undefined}
->{node_state,{'ns_1@127.0.0.1',<<"1678dfae96c38e07dff43c49b9f6967b">>},
              16,half_down,undefined}
[ns_server:warn,2022-09-08T12:08:00.418Z,ns_1@127.0.0.1:kv_monitor<0.792.0>:kv_monitor:get_buckets_status:250]The following buckets are not ready: ["todo"]
[user:info,2022-09-08T12:08:00.443Z,ns_1@127.0.0.1:ns_memcached-todo<0.668.0>:ns_memcached:handle_cast:676]Bucket "todo" loaded on node 'ns_1@127.0.0.1' in 9 seconds.
[ns_server:debug,2022-09-08T12:08:00.841Z,ns_1@127.0.0.1:<0.759.0>:auto_failover_logic:log_master_activity:143]Incremented down state:
{node_state,{'ns_1@127.0.0.1',<<"1678dfae96c38e07dff43c49b9f6967b">>},
            16,half_down,undefined}
->{node_state,{'ns_1@127.0.0.1',<<"1678dfae96c38e07dff43c49b9f6967b">>},
              17,half_down,undefined}
[ns_server:info,2022-09-08T12:08:00.908Z,ns_1@127.0.0.1:ns_memcached-todo<0.668.0>:ns_memcached:handle_call:294]Enabling traffic to bucket "todo"
[ns_server:info,2022-09-08T12:08:00.909Z,ns_1@127.0.0.1:ns_memcached-todo<0.668.0>:ns_memcached:handle_call:298]Bucket "todo" marked as warmed in 9 seconds
[ns_server:info,2022-09-08T12:08:01.155Z,ns_1@127.0.0.1:ns_doctor<0.448.0>:ns_doctor:update_status:309]The following buckets became ready on node 'ns_1@127.0.0.1': ["todo"]
[ns_server:debug,2022-09-08T12:08:01.842Z,ns_1@127.0.0.1:<0.759.0>:auto_failover_logic:log_master_activity:143]Incremented down state:
{node_state,{'ns_1@127.0.0.1',<<"1678dfae96c38e07dff43c49b9f6967b">>},
            17,half_down,undefined}
->{node_state,{'ns_1@127.0.0.1',<<"1678dfae96c38e07dff43c49b9f6967b">>},
              18,half_down,undefined}
[ns_server:debug,2022-09-08T12:08:02.843Z,ns_1@127.0.0.1:<0.759.0>:auto_failover_logic:log_master_activity:141]Transitioned node {'ns_1@127.0.0.1',<<"1678dfae96c38e07dff43c49b9f6967b">>} state half_down -> up
[ns_server:debug,2022-09-08T12:08:12.420Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:08:12.420Z,ns_1@127.0.0.1:<0.3485.0>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:08:12.422Z,ns_1@127.0.0.1:<0.3487.0>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 428783, disk size is 12650586
[ns_server:debug,2022-09-08T12:08:12.422Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:08:12.422Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:08:12.737Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:08:12.740Z,ns_1@127.0.0.1:<0.3515.0>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:08:12.740Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:08:12.740Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:08:42.423Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:08:42.424Z,ns_1@127.0.0.1:<0.4969.0>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:08:42.426Z,ns_1@127.0.0.1:<0.4971.0>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 428783, disk size is 12650586
[ns_server:debug,2022-09-08T12:08:42.426Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:08:42.426Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:08:42.741Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:08:42.744Z,ns_1@127.0.0.1:<0.5002.0>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:08:42.744Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:08:42.744Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:08:54.537Z,ns_1@127.0.0.1:ldap_auth_cache<0.334.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2022-09-08T12:09:12.428Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:09:12.429Z,ns_1@127.0.0.1:<0.6454.0>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:09:12.431Z,ns_1@127.0.0.1:<0.6456.0>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 428783, disk size is 12650586
[ns_server:debug,2022-09-08T12:09:12.431Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:09:12.431Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:09:12.745Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:09:12.750Z,ns_1@127.0.0.1:<0.6483.0>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:09:12.751Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:09:12.751Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:09:42.432Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:09:42.433Z,ns_1@127.0.0.1:<0.7940.0>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:09:42.435Z,ns_1@127.0.0.1:<0.7942.0>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 428783, disk size is 12650586
[ns_server:debug,2022-09-08T12:09:42.435Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:09:42.435Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:09:42.752Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:09:42.757Z,ns_1@127.0.0.1:<0.7973.0>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:09:42.758Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:09:42.758Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:09:48.221Z,ns_1@127.0.0.1:compiled_roles_cache<0.342.0>:menelaus_roles:build_compiled_roles:998]Compile roles for user {"<ud>admin</ud>",admin}
[ns_server:debug,2022-09-08T12:10:09.538Z,ns_1@127.0.0.1:ldap_auth_cache<0.334.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2022-09-08T12:10:12.436Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:10:12.436Z,ns_1@127.0.0.1:<0.9408.0>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:10:12.438Z,ns_1@127.0.0.1:<0.9410.0>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 428783, disk size is 12650586
[ns_server:debug,2022-09-08T12:10:12.439Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:10:12.439Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:10:12.759Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:10:12.762Z,ns_1@127.0.0.1:<0.9427.0>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:10:12.763Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:10:12.763Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:10:42.440Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:10:42.441Z,ns_1@127.0.0.1:<0.10892.0>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:10:42.525Z,ns_1@127.0.0.1:<0.10894.0>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 428783, disk size is 12650586
[ns_server:debug,2022-09-08T12:10:42.525Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:10:42.526Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:10:42.764Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:10:42.774Z,ns_1@127.0.0.1:<0.10899.0>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:10:42.775Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:10:42.775Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:11:12.526Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:11:12.526Z,ns_1@127.0.0.1:<0.12353.0>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:11:12.528Z,ns_1@127.0.0.1:<0.12355.0>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 428783, disk size is 12650586
[ns_server:debug,2022-09-08T12:11:12.529Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:11:12.529Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:11:12.776Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:11:12.778Z,ns_1@127.0.0.1:<0.12360.0>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:11:12.779Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:11:12.779Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:11:24.539Z,ns_1@127.0.0.1:ldap_auth_cache<0.334.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2022-09-08T12:11:42.529Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:11:42.530Z,ns_1@127.0.0.1:<0.13843.0>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:11:42.533Z,ns_1@127.0.0.1:<0.13845.0>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 428783, disk size is 12650586
[ns_server:debug,2022-09-08T12:11:42.534Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:11:42.534Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:11:42.780Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:11:42.786Z,ns_1@127.0.0.1:<0.13850.0>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:11:42.786Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:11:42.787Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:12:06.146Z,ns_1@127.0.0.1:ns_audit<0.579.0>:ns_audit:handle_call:148]Audit auth_failure: [{local,{[{ip,<<"172.18.0.2">>},{port,8091}]}},
                     {remote,{[{ip,<<"172.18.0.1">>},{port,46818}]}},
                     {timestamp,<<"2022-09-08T12:12:06.145Z">>},
                     {raw_url,<<"<ud>/pools</ud>">>}]
[ns_server:debug,2022-09-08T12:12:08.142Z,ns_1@127.0.0.1:ns_audit<0.579.0>:ns_audit:handle_call:148]Audit login_success: [{local,{[{ip,<<"172.18.0.2">>},{port,8091}]}},
                      {remote,{[{ip,<<"172.18.0.1">>},{port,46818}]}},
                      {sessionid,<<"8d72771539648f7a1bc57f49b878927e398368fd">>},
                      {real_userid,{[{domain,builtin},
                                     {user,<<"<ud>admin</ud>">>}]}},
                      {timestamp,<<"2022-09-08T12:12:08.142Z">>},
                      {roles,[<<"admin">>]}]
[ns_server:debug,2022-09-08T12:12:08.226Z,ns_1@127.0.0.1:users_storage<0.340.0>:replicated_storage:handle_call:107]Writing interactively saved doc {docv2,
                                    {ui_profile,{"<ud>admin</ud>",admin}},
                                    {[{<<"version">>,393221},
                                      {<<"scenarios">>,
                                       [{[{<<"name">>,<<"Cluster Overview">>},
                                          {<<"uiid">>,
                                           <<"mn-cluster-overview">>},
                                          {<<"desc">>,
                                           <<"Stats showing the general health of your cluster. Customize and/or make your own dashboard with \"new dashboard... \" below.">>},
                                          {<<"groups">>,
                                           [<<"z03w6u34a">>,<<"5soggmcam">>]},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"9aa0tpeid">>}]},
                                        {[{<<"name">>,<<"All Services">>},
                                          {<<"uiid">>,<<"mn-all-services">>},
                                          {<<"desc">>,
                                           <<"Most common stats, arranged per service. Customize and/or make your own dashboard with \"new dashboard... \" below.">>},
                                          {<<"groups">>,
                                           [<<"5yopd2vba">>,<<"uh1sdnzjt">>,
                                            <<"r0rpc6yj6">>,<<"okmfe0ifw">>,
                                            <<"mia8ah53c">>,<<"gpr13upik">>,
                                            <<"lhpc20944">>,<<"a4w6jt178">>,
                                            <<"zbs5mnwy7">>,<<"vcs53hxl7">>]},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"srbaku8z5">>}]}]},
                                      {<<"groups">>,
                                       [{[{<<"name">>,<<"Cluster Overview">>},
                                          {<<"uiid">>,
                                           <<"mn-cluster-overview-group">>},
                                          {<<"charts">>,
                                           [<<"teb05igr5">>,<<"cex3ykev4">>,
                                            <<"hwp4yryp6">>,<<"vr6vl8ss9">>,
                                            <<"9ga9z7pza">>,<<"4ixqvemq7">>,
                                            <<"n70ebcxia">>,<<"3in1fpudp">>]},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"z03w6u34a">>}]},
                                        {[{<<"name">>,<<"Node Resources">>},
                                          {<<"charts">>,
                                           [<<"xth91csu8">>,<<"o80p6mwi2">>,
                                            <<"eeglp0q4u">>,<<"ex2ldmqv0">>]},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"5soggmcam">>}]},
                                        {[{<<"name">>,
                                           <<"Data (Docs/Views/XDCR)">>},
                                          {<<"uiid">>,
                                           <<"mn-all-services-data-group">>},
                                          {<<"charts">>,
                                           [<<"qjlwt4ie9">>,<<"2t5k1n0xs">>,
                                            <<"rlurtvast">>,<<"vevb8bt8l">>,
                                            <<"7isc94m6c">>]},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"5yopd2vba">>}]},
                                        {[{<<"name">>,<<"Query">>},
                                          {<<"charts">>,
                                           [<<"yxu3zuscp">>,<<"hsd1dbu3l">>,
                                            <<"9yva4b019">>,<<"fgutu7xlw">>,
                                            <<"95q08wwol">>]},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"uh1sdnzjt">>}]},
                                        {[{<<"name">>,<<"Index">>},
                                          {<<"charts">>,
                                           [<<"6rdualle6">>,<<"t1ho94nbx">>,
                                            <<"qn1obgx8u">>,<<"idekze47i">>,
                                            <<"bp53oh5bg">>]},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"r0rpc6yj6">>}]},
                                        {[{<<"name">>,<<"Search">>},
                                          {<<"charts">>,
                                           [<<"g6acbjwse">>,<<"bbeahbtsm">>]},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"okmfe0ifw">>}]},
                                        {[{<<"name">>,<<"Analytics">>},
                                          {<<"enterprise">>,true},
                                          {<<"charts">>,
                                           [<<"bygxrjmc6">>,<<"d8u14qbwu">>,
                                            <<"rsuhq7a81">>,<<"3js5onprs">>,
                                            <<"a2z3ox5m6">>,<<"kk34rkxws">>,
                                            <<"g844qwsw9">>,<<"fi53d2hfo">>,
                                            <<"wc3i6sdm1">>,<<"qgqj33gyw">>,
                                            <<"e99ew9kg3">>,<<"lpog1z8x4">>,
                                            <<"cqv0gkhdn">>]},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"mia8ah53c">>}]},
                                        {[{<<"name">>,<<"Eventing">>},
                                          {<<"enterprise">>,true},
                                          {<<"charts">>,[<<"cut8tyr5r">>]},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"gpr13upik">>}]},
                                        {[{<<"name">>,<<"XDCR">>},
                                          {<<"charts">>,
                                           [<<"m34nefjv0">>,<<"43yauy9si">>,
                                            <<"edlacvk92">>,<<"fvr6ykof9">>]},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"lhpc20944">>}]},
                                        {[{<<"name">>,<<"vBucket Resources">>},
                                          {<<"charts">>,
                                           [<<"q7sun8qk3">>,<<"98w1wsckr">>,
                                            <<"9xjeyiw3n">>,<<"5l56udkwg">>,
                                            <<"gxt4lfbc4">>,<<"l7er1yzg2">>,
                                            <<"09yp5g7zc">>]},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"a4w6jt178">>}]},
                                        {[{<<"name">>,<<"DCP Queues">>},
                                          {<<"charts">>,
                                           [<<"4917rg4wy">>,<<"n430o4c55">>,
                                            <<"wlv9jsqqb">>,<<"ljvjah7xg">>,
                                            <<"88tjivg5r">>,<<"w42kpvdig">>]},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"zbs5mnwy7">>}]},
                                        {[{<<"name">>,<<"Disk Queues">>},
                                          {<<"charts">>,
                                           [<<"b43fw6j1x">>,<<"9bdcm2uu1">>,
                                            <<"hclcvm4as">>,<<"86zg6u6oa">>]},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"vcs53hxl7">>}]}]},
                                      {<<"charts">>,
                                       [{[{<<"stats">>,
                                           {[{<<"@kv-.ops">>,true},
                                             {<<"@query.query_requests">>,
                                              true},
                                             {<<"@fts-.@items.total_queries">>,
                                              true},
                                             {<<"@kv-.ep_tmp_oom_errors">>,
                                              true},
                                             {<<"@kv-.ep_cache_miss_rate">>,
                                              true},
                                             {<<"@kv-.cmd_get">>,true},
                                             {<<"@kv-.cmd_set">>,true},
                                             {<<"@kv-.delete_hits">>,true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"teb05igr5">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@kv-.mem_used">>,true},
                                             {<<"@kv-.ep_mem_low_wat">>,true},
                                             {<<"@kv-.ep_mem_high_wat">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"cex3ykev4">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@kv-.curr_items">>,true},
                                             {<<"@kv-.vb_replica_curr_items">>,
                                              true},
                                             {<<"@kv-.vb_active_resident_items_ratio">>,
                                              true},
                                             {<<"@kv-.vb_replica_resident_items_ratio">>,
                                              true},
                                             {<<"@kv-.couch_docs_fragmentation">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"hwp4yryp6">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@kv-.disk_write_queue">>,
                                              true},
                                             {<<"@kv-.couch_docs_actual_disk_size">>,
                                              true}]}},
                                          {<<"size">>,<<"small">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"vr6vl8ss9">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@kv-.ep_dcp_replica_items_remaining">>,
                                              true}]}},
                                          {<<"size">>,<<"small">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"9ga9z7pza">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@kv-.ep_data_read_failed">>,
                                              true},
                                             {<<"@kv-.ep_data_write_failed">>,
                                              true},
                                             {<<"@query.query_errors">>,true},
                                             {<<"@fts-.@items.total_queries_error">>,
                                              true},
                                             {<<"@eventing.eventing/failed_count">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"4ixqvemq7">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@query.query_requests_250ms">>,
                                              true},
                                             {<<"@query.query_requests_500ms">>,
                                              true},
                                             {<<"@query.query_requests_1000ms">>,
                                              true},
                                             {<<"@query.query_requests_5000ms">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"n70ebcxia">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@xdcr-.replication_changes_left">>,
                                              true},
                                             {<<"@index-.@items.num_docs_pending+queued">>,
                                              true},
                                             {<<"@fts-.@items.num_mutations_to_index">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"3in1fpudp">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@system.cpu_utilization_rate">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,true},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"xth91csu8">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@system.rest_requests">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,true},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"o80p6mwi2">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@system.mem_actual_free">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"eeglp0q4u">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@system.swap_used">>,true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,true},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"ex2ldmqv0">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@kv-.mem_used">>,true},
                                             {<<"@kv-.ep_mem_low_wat">>,true},
                                             {<<"@kv-.ep_mem_high_wat">>,true},
                                             {<<"@kv-.ep_kv_size">>,true},
                                             {<<"@kv-.ep_meta_data_memory">>,
                                              true},
                                             {<<"@kv-.vb_active_resident_items_ratio">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"qjlwt4ie9">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@kv-.ops">>,true},
                                             {<<"@kv-.ep_cache_miss_rate">>,
                                              true},
                                             {<<"@kv-.cmd_get">>,true},
                                             {<<"@kv-.cmd_set">>,true},
                                             {<<"@kv-.delete_hits">>,true},
                                             {<<"@kv-.ep_num_ops_set_meta">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"2t5k1n0xs">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@kv-.ep_dcp_views+indexes_items_remaining">>,
                                              true},
                                             {<<"@kv-.ep_dcp_cbas_items_remaining">>,
                                              true},
                                             {<<"@kv-.ep_dcp_replica_items_remaining">>,
                                              true},
                                             {<<"@kv-.ep_dcp_xdcr_items_remaining">>,
                                              true},
                                             {<<"@kv-.ep_dcp_eventing_items_remaining">>,
                                              true},
                                             {<<"@kv-.ep_dcp_other_items_remaining">>,
                                              true},
                                             {<<"@xdcr-.replication_changes_left">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"rlurtvast">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@kv-.ep_bg_fetched">>,true},
                                             {<<"@kv-.ep_data_read_failed">>,
                                              true},
                                             {<<"@kv-.ep_data_write_failed">>,
                                              true},
                                             {<<"@kv-.ep_ops_create">>,true},
                                             {<<"@kv-.ep_ops_update">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"vevb8bt8l">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@kv-.ep_diskqueue_items">>,
                                              true}]}},
                                          {<<"size">>,<<"small">>},
                                          {<<"specificStat">>,true},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"7isc94m6c">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@query.query_requests_1000ms">>,
                                              true},
                                             {<<"@query.query_requests_500ms">>,
                                              true},
                                             {<<"@query.query_requests_5000ms">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"yxu3zuscp">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@query.query_selects">>,true},
                                             {<<"@query.query_requests">>,
                                              true},
                                             {<<"@query.query_warnings">>,
                                              true},
                                             {<<"@query.query_invalid_requests">>,
                                              true},
                                             {<<"@query.query_errors">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"hsd1dbu3l">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@query.query_avg_req_time">>,
                                              true},
                                             {<<"@query.query_avg_svc_time">>,
                                              true}]}},
                                          {<<"size">>,<<"small">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"9yva4b019">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@query.query_avg_result_count">>,
                                              true}]}},
                                          {<<"size">>,<<"small">>},
                                          {<<"specificStat">>,true},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"fgutu7xlw">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@query.query_avg_response_size">>,
                                              true}]}},
                                          {<<"size">>,<<"small">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"95q08wwol">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@index-.index/num_rows_returned">>,
                                              true}]}},
                                          {<<"size">>,<<"small">>},
                                          {<<"specificStat">>,true},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"6rdualle6">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@index-.@items.num_docs_pending+queued">>,
                                              true}]}},
                                          {<<"size">>,<<"small">>},
                                          {<<"specificStat">>,true},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"t1ho94nbx">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@index-.index/data_size">>,
                                              true}]}},
                                          {<<"size">>,<<"small">>},
                                          {<<"specificStat">>,true},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"qn1obgx8u">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@index-.index/disk_size">>,
                                              true}]}},
                                          {<<"size">>,<<"small">>},
                                          {<<"specificStat">>,true},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"idekze47i">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@index.index_ram_percent">>,
                                              true},
                                             {<<"@index.index_remaining_ram">>,
                                              true},
                                             {<<"@index-.index/data_size">>,
                                              true},
                                             {<<"@index-.index/disk_size">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"bp53oh5bg">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@fts-.fts/num_bytes_used_disk">>,
                                              true},
                                             {<<"@fts.fts_num_bytes_used_ram">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"g6acbjwse">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@fts-.@items.total_queries">>,
                                              true},
                                             {<<"@fts-.@items.total_queries_error">>,
                                              true},
                                             {<<"@fts-.@items.total_queries_slow">>,
                                              true},
                                             {<<"@fts-.@items.total_queries_timeout">>,
                                              true},
                                             {<<"@fts.fts_total_queries_rejected_by_herder">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"bbeahbtsm">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@cbas-.cbas/incoming_records_count">>,
                                              true}]}},
                                          {<<"size">>,<<"small">>},
                                          {<<"specificStat">>,true},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"bygxrjmc6">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@cbas-.cbas_failed_to_parse_records_count">>,
                                              true}]}},
                                          {<<"size">>,<<"small">>},
                                          {<<"specificStat">>,true},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"d8u14qbwu">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@cbas-.cbas/failed_at_parser_records_count_total">>,
                                              true}]}},
                                          {<<"size">>,<<"small">>},
                                          {<<"specificStat">>,true},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"rsuhq7a81">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@cbas.cbas_heap_used">>,
                                              true}]}},
                                          {<<"size">>,<<"small">>},
                                          {<<"specificStat">>,true},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"3js5onprs">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@cbas.cbas_heap_memory_committed_bytes">>,
                                              true}]}},
                                          {<<"size">>,<<"small">>},
                                          {<<"specificStat">>,true},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"a2z3ox5m6">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@cbas.cbas_thread_count">>,
                                              true}]}},
                                          {<<"size">>,<<"small">>},
                                          {<<"specificStat">>,true},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"kk34rkxws">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@cbas.cbas_disk_used">>,
                                              true}]}},
                                          {<<"size">>,<<"small">>},
                                          {<<"specificStat">>,true},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"g844qwsw9">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@cbas.cbas_io_reads">>,
                                              true}]}},
                                          {<<"size">>,<<"small">>},
                                          {<<"specificStat">>,true},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"fi53d2hfo">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@cbas.cbas_io_writes">>,
                                              true}]}},
                                          {<<"size">>,<<"small">>},
                                          {<<"specificStat">>,true},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"wc3i6sdm1">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@cbas.cbas_system_load_average">>,
                                              true}]}},
                                          {<<"size">>,<<"small">>},
                                          {<<"specificStat">>,true},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"qgqj33gyw">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@cbas.cbas_pending_merge_ops">>,
                                              true}]}},
                                          {<<"size">>,<<"small">>},
                                          {<<"specificStat">>,true},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"e99ew9kg3">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@cbas.cbas_pending_flush_ops">>,
                                              true}]}},
                                          {<<"size">>,<<"small">>},
                                          {<<"specificStat">>,true},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"lpog1z8x4">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@system.sysproc_mem_resident_java_cbas">>,
                                              true}]}},
                                          {<<"size">>,<<"small">>},
                                          {<<"specificStat">>,true},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"cqv0gkhdn">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@eventing.eventing/failed_count">>,
                                              true},
                                             {<<"@eventing.eventing/timeout_count">>,
                                              true}]}},
                                          {<<"size">>,<<"small">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"cut8tyr5r">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@xdcr-.replication_changes_left">>,
                                              true}]}},
                                          {<<"size">>,<<"small">>},
                                          {<<"specificStat">>,true},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"m34nefjv0">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@xdcr-.@items.changes_left">>,
                                              true}]}},
                                          {<<"size">>,<<"small">>},
                                          {<<"specificStat">>,true},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"43yauy9si">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@xdcr-.@items.wtavg_docs_latency">>,
                                              true},
                                             {<<"@xdcr-.@items.wtavg_meta_latency">>,
                                              true}]}},
                                          {<<"size">>,<<"small">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"edlacvk92">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@xdcr-.@items.docs_failed_cr_source">>,
                                              true},
                                             {<<"@xdcr-.@items.xdcr_docs_failed_cr_target_total">>,
                                              true},
                                             {<<"@xdcr-.@items.docs_filtered">>,
                                              true}]}},
                                          {<<"size">>,<<"small">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"fvr6ykof9">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@kv-.vb_active_num">>,true},
                                             {<<"@kv-.vb_replica_num">>,true},
                                             {<<"@kv-.vb_pending_num">>,true},
                                             {<<"@kv-.ep_vb_total">>,true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"q7sun8qk3">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@kv-.curr_items">>,true},
                                             {<<"@kv-.vb_replica_curr_items">>,
                                              true},
                                             {<<"@kv-.vb_pending_curr_items">>,
                                              true},
                                             {<<"@kv-.curr_items_tot">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"98w1wsckr">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@kv-.vb_active_resident_items_ratio">>,
                                              true},
                                             {<<"@kv-.vb_replica_resident_items_ratio">>,
                                              true},
                                             {<<"@kv-.vb_pending_resident_items_ratio">>,
                                              true},
                                             {<<"@kv-.ep_resident_items_rate">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"9xjeyiw3n">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@kv-.vb_active_ops_create">>,
                                              true},
                                             {<<"@kv-.vb_replica_ops_create">>,
                                              true},
                                             {<<"@kv-.vb_pending_ops_create">>,
                                              true},
                                             {<<"@kv-.ep_ops_create">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"5l56udkwg">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@kv-.vb_active_eject">>,true},
                                             {<<"@kv-.vb_replica_eject">>,
                                              true},
                                             {<<"@kv-.vb_pending_eject">>,
                                              true},
                                             {<<"@kv-.ep_num_value_ejects">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"gxt4lfbc4">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@kv-.vb_active_itm_memory">>,
                                              true},
                                             {<<"@kv-.vb_replica_itm_memory">>,
                                              true},
                                             {<<"@kv-.vb_pending_itm_memory">>,
                                              true},
                                             {<<"@kv-.ep_kv_size">>,true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"l7er1yzg2">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@kv-.vb_active_meta_data_memory">>,
                                              true},
                                             {<<"@kv-.vb_replica_meta_data_memory">>,
                                              true},
                                             {<<"@kv-.vb_pending_meta_data_memory">>,
                                              true},
                                             {<<"@kv-.ep_meta_data_memory">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"09yp5g7zc">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@kv-.ep_dcp_views+indexes_count">>,
                                              true},
                                             {<<"@kv-.ep_dcp_cbas_count">>,
                                              true},
                                             {<<"@kv-.ep_dcp_replica_count">>,
                                              true},
                                             {<<"@kv-.ep_dcp_xdcr_count">>,
                                              true},
                                             {<<"@kv-.ep_dcp_eventing_count">>,
                                              true},
                                             {<<"@kv-.ep_dcp_other_count">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"4917rg4wy">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@kv-.ep_dcp_views+indexes_producer_count">>,
                                              true},
                                             {<<"@kv-.ep_dcp_cbas_producer_count">>,
                                              true},
                                             {<<"@kv-.ep_dcp_replica_producer_count">>,
                                              true},
                                             {<<"@kv-.ep_dcp_xdcr_producer_count">>,
                                              true},
                                             {<<"@kv-.ep_dcp_eventing_producer_count">>,
                                              true},
                                             {<<"@kv-.ep_dcp_other_producer_count">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"n430o4c55">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@kv-.ep_dcp_views+indexes_items_remaining">>,
                                              true},
                                             {<<"@kv-.ep_dcp_cbas_items_remaining">>,
                                              true},
                                             {<<"@kv-.ep_dcp_replica_items_remaining">>,
                                              true},
                                             {<<"@kv-.ep_dcp_xdcr_items_remaining">>,
                                              true},
                                             {<<"@kv-.ep_dcp_eventing_items_remaining">>,
                                              true},
                                             {<<"@kv-.ep_dcp_other_items_remaining">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"wlv9jsqqb">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@kv-.ep_dcp_views+indexes_items_sent">>,
                                              true},
                                             {<<"@kv-.ep_dcp_cbas_items_sent">>,
                                              true},
                                             {<<"@kv-.ep_dcp_replica_items_sent">>,
                                              true},
                                             {<<"@kv-.ep_dcp_xdcr_items_sent">>,
                                              true},
                                             {<<"@kv-.ep_dcp_eventing_items_sent">>,
                                              true},
                                             {<<"@kv-.ep_dcp_other_items_sent">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"ljvjah7xg">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@kv-.ep_dcp_views+indexes_total_bytes">>,
                                              true},
                                             {<<"@kv-.ep_dcp_cbas_total_bytes">>,
                                              true},
                                             {<<"@kv-.ep_dcp_replica_total_bytes">>,
                                              true},
                                             {<<"@kv-.ep_dcp_xdcr_total_bytes">>,
                                              true},
                                             {<<"@kv-.ep_dcp_eventing_total_bytes">>,
                                              true},
                                             {<<"@kv-.ep_dcp_other_total_bytes">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"88tjivg5r">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@kv-.ep_dcp_views+indexes_backoff">>,
                                              true},
                                             {<<"@kv-.ep_dcp_cbas_backoff">>,
                                              true},
                                             {<<"@kv-.ep_dcp_replica_backoff">>,
                                              true},
                                             {<<"@kv-.ep_dcp_xdcr_backoff">>,
                                              true},
                                             {<<"@kv-.ep_dcp_eventing_backoff">>,
                                              true},
                                             {<<"@kv-.ep_dcp_other_backoff">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"w42kpvdig">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@kv-.ep_diskqueue_fill">>,
                                              true},
                                             {<<"@kv-.ep_diskqueue_drain">>,
                                              true},
                                             {<<"@kv-.ep_diskqueue_items">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"b43fw6j1x">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@kv-.vb_active_queue_fill">>,
                                              true},
                                             {<<"@kv-.vb_active_queue_drain">>,
                                              true},
                                             {<<"@kv-.vb_active_queue_size">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"9bdcm2uu1">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@kv-.vb_replica_queue_fill">>,
                                              true},
                                             {<<"@kv-.vb_replica_queue_drain">>,
                                              true},
                                             {<<"@kv-.vb_replica_queue_size">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"hclcvm4as">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@kv-.vb_pending_queue_fill">>,
                                              true},
                                             {<<"@kv-.vb_pending_queue_drain">>,
                                              true},
                                             {<<"@kv-.vb_pending_queue_size">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"86zg6u6oa">>}]}]}]},
                                    [{rev,{1,<<35,185,184,148>>}},
                                     {deleted,false},
                                     {last_modified,1662639128221}]}
[ns_server:debug,2022-09-08T12:12:08.249Z,ns_1@127.0.0.1:ns_audit<0.579.0>:ns_audit:handle_call:148]Audit set_user_profile: [{local,{[{ip,<<"172.18.0.2">>},{port,8091}]}},
                         {remote,{[{ip,<<"172.18.0.1">>},{port,46818}]}},
                         {sessionid,<<"8d72771539648f7a1bc57f49b878927e398368fd">>},
                         {real_userid,{[{domain,builtin},
                                        {user,<<"<ud>admin</ud>">>}]}},
                         {timestamp,<<"2022-09-08T12:12:08.247Z">>},
                         {profile,{[{<<"version">>,393221},
                                    {<<"scenarios">>,
                                     [{[{<<"name">>,<<"Cluster Overview">>},
                                        {<<"uiid">>,<<"mn-cluster-overview">>},
                                        {<<"desc">>,
                                         <<"Stats showing the general health of your cluster. Customize and/or make your own dashboard with \"new dashboard... \" below.">>},
                                        {<<"groups">>,
                                         [<<"z03w6u34a">>,<<"5soggmcam">>]},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"9aa0tpeid">>}]},
                                      {[{<<"name">>,<<"All Services">>},
                                        {<<"uiid">>,<<"mn-all-services">>},
                                        {<<"desc">>,
                                         <<"Most common stats, arranged per service. Customize and/or make your own dashboard with \"new dashboard... \" below.">>},
                                        {<<"groups">>,
                                         [<<"5yopd2vba">>,<<"uh1sdnzjt">>,
                                          <<"r0rpc6yj6">>,<<"okmfe0ifw">>,
                                          <<"mia8ah53c">>,<<"gpr13upik">>,
                                          <<"lhpc20944">>,<<"a4w6jt178">>,
                                          <<"zbs5mnwy7">>,<<"vcs53hxl7">>]},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"srbaku8z5">>}]}]},
                                    {<<"groups">>,
                                     [{[{<<"name">>,<<"Cluster Overview">>},
                                        {<<"uiid">>,
                                         <<"mn-cluster-overview-group">>},
                                        {<<"charts">>,
                                         [<<"teb05igr5">>,<<"cex3ykev4">>,
                                          <<"hwp4yryp6">>,<<"vr6vl8ss9">>,
                                          <<"9ga9z7pza">>,<<"4ixqvemq7">>,
                                          <<"n70ebcxia">>,<<"3in1fpudp">>]},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"z03w6u34a">>}]},
                                      {[{<<"name">>,<<"Node Resources">>},
                                        {<<"charts">>,
                                         [<<"xth91csu8">>,<<"o80p6mwi2">>,
                                          <<"eeglp0q4u">>,<<"ex2ldmqv0">>]},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"5soggmcam">>}]},
                                      {[{<<"name">>,
                                         <<"Data (Docs/Views/XDCR)">>},
                                        {<<"uiid">>,
                                         <<"mn-all-services-data-group">>},
                                        {<<"charts">>,
                                         [<<"qjlwt4ie9">>,<<"2t5k1n0xs">>,
                                          <<"rlurtvast">>,<<"vevb8bt8l">>,
                                          <<"7isc94m6c">>]},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"5yopd2vba">>}]},
                                      {[{<<"name">>,<<"Query">>},
                                        {<<"charts">>,
                                         [<<"yxu3zuscp">>,<<"hsd1dbu3l">>,
                                          <<"9yva4b019">>,<<"fgutu7xlw">>,
                                          <<"95q08wwol">>]},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"uh1sdnzjt">>}]},
                                      {[{<<"name">>,<<"Index">>},
                                        {<<"charts">>,
                                         [<<"6rdualle6">>,<<"t1ho94nbx">>,
                                          <<"qn1obgx8u">>,<<"idekze47i">>,
                                          <<"bp53oh5bg">>]},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"r0rpc6yj6">>}]},
                                      {[{<<"name">>,<<"Search">>},
                                        {<<"charts">>,
                                         [<<"g6acbjwse">>,<<"bbeahbtsm">>]},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"okmfe0ifw">>}]},
                                      {[{<<"name">>,<<"Analytics">>},
                                        {<<"enterprise">>,true},
                                        {<<"charts">>,
                                         [<<"bygxrjmc6">>,<<"d8u14qbwu">>,
                                          <<"rsuhq7a81">>,<<"3js5onprs">>,
                                          <<"a2z3ox5m6">>,<<"kk34rkxws">>,
                                          <<"g844qwsw9">>,<<"fi53d2hfo">>,
                                          <<"wc3i6sdm1">>,<<"qgqj33gyw">>,
                                          <<"e99ew9kg3">>,<<"lpog1z8x4">>,
                                          <<"cqv0gkhdn">>]},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"mia8ah53c">>}]},
                                      {[{<<"name">>,<<"Eventing">>},
                                        {<<"enterprise">>,true},
                                        {<<"charts">>,[<<"cut8tyr5r">>]},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"gpr13upik">>}]},
                                      {[{<<"name">>,<<"XDCR">>},
                                        {<<"charts">>,
                                         [<<"m34nefjv0">>,<<"43yauy9si">>,
                                          <<"edlacvk92">>,<<"fvr6ykof9">>]},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"lhpc20944">>}]},
                                      {[{<<"name">>,<<"vBucket Resources">>},
                                        {<<"charts">>,
                                         [<<"q7sun8qk3">>,<<"98w1wsckr">>,
                                          <<"9xjeyiw3n">>,<<"5l56udkwg">>,
                                          <<"gxt4lfbc4">>,<<"l7er1yzg2">>,
                                          <<"09yp5g7zc">>]},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"a4w6jt178">>}]},
                                      {[{<<"name">>,<<"DCP Queues">>},
                                        {<<"charts">>,
                                         [<<"4917rg4wy">>,<<"n430o4c55">>,
                                          <<"wlv9jsqqb">>,<<"ljvjah7xg">>,
                                          <<"88tjivg5r">>,<<"w42kpvdig">>]},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"zbs5mnwy7">>}]},
                                      {[{<<"name">>,<<"Disk Queues">>},
                                        {<<"charts">>,
                                         [<<"b43fw6j1x">>,<<"9bdcm2uu1">>,
                                          <<"hclcvm4as">>,<<"86zg6u6oa">>]},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"vcs53hxl7">>}]}]},
                                    {<<"charts">>,
                                     [{[{<<"stats">>,
                                         {[{<<"@kv-.ops">>,true},
                                           {<<"@query.query_requests">>,true},
                                           {<<"@fts-.@items.total_queries">>,
                                            true},
                                           {<<"@kv-.ep_tmp_oom_errors">>,true},
                                           {<<"@kv-.ep_cache_miss_rate">>,
                                            true},
                                           {<<"@kv-.cmd_get">>,true},
                                           {<<"@kv-.cmd_set">>,true},
                                           {<<"@kv-.delete_hits">>,true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"teb05igr5">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.mem_used">>,true},
                                           {<<"@kv-.ep_mem_low_wat">>,true},
                                           {<<"@kv-.ep_mem_high_wat">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"cex3ykev4">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.curr_items">>,true},
                                           {<<"@kv-.vb_replica_curr_items">>,
                                            true},
                                           {<<"@kv-.vb_active_resident_items_ratio">>,
                                            true},
                                           {<<"@kv-.vb_replica_resident_items_ratio">>,
                                            true},
                                           {<<"@kv-.couch_docs_fragmentation">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"hwp4yryp6">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.disk_write_queue">>,true},
                                           {<<"@kv-.couch_docs_actual_disk_size">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"vr6vl8ss9">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.ep_dcp_replica_items_remaining">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"9ga9z7pza">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.ep_data_read_failed">>,
                                            true},
                                           {<<"@kv-.ep_data_write_failed">>,
                                            true},
                                           {<<"@query.query_errors">>,true},
                                           {<<"@fts-.@items.total_queries_error">>,
                                            true},
                                           {<<"@eventing.eventing/failed_count">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"4ixqvemq7">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@query.query_requests_250ms">>,
                                            true},
                                           {<<"@query.query_requests_500ms">>,
                                            true},
                                           {<<"@query.query_requests_1000ms">>,
                                            true},
                                           {<<"@query.query_requests_5000ms">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"n70ebcxia">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@xdcr-.replication_changes_left">>,
                                            true},
                                           {<<"@index-.@items.num_docs_pending+queued">>,
                                            true},
                                           {<<"@fts-.@items.num_mutations_to_index">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"3in1fpudp">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@system.cpu_utilization_rate">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,true},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"xth91csu8">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@system.rest_requests">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,true},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"o80p6mwi2">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@system.mem_actual_free">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"eeglp0q4u">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@system.swap_used">>,true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,true},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"ex2ldmqv0">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.mem_used">>,true},
                                           {<<"@kv-.ep_mem_low_wat">>,true},
                                           {<<"@kv-.ep_mem_high_wat">>,true},
                                           {<<"@kv-.ep_kv_size">>,true},
                                           {<<"@kv-.ep_meta_data_memory">>,
                                            true},
                                           {<<"@kv-.vb_active_resident_items_ratio">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"qjlwt4ie9">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.ops">>,true},
                                           {<<"@kv-.ep_cache_miss_rate">>,
                                            true},
                                           {<<"@kv-.cmd_get">>,true},
                                           {<<"@kv-.cmd_set">>,true},
                                           {<<"@kv-.delete_hits">>,true},
                                           {<<"@kv-.ep_num_ops_set_meta">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"2t5k1n0xs">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.ep_dcp_views+indexes_items_remaining">>,
                                            true},
                                           {<<"@kv-.ep_dcp_cbas_items_remaining">>,
                                            true},
                                           {<<"@kv-.ep_dcp_replica_items_remaining">>,
                                            true},
                                           {<<"@kv-.ep_dcp_xdcr_items_remaining">>,
                                            true},
                                           {<<"@kv-.ep_dcp_eventing_items_remaining">>,
                                            true},
                                           {<<"@kv-.ep_dcp_other_items_remaining">>,
                                            true},
                                           {<<"@xdcr-.replication_changes_left">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"rlurtvast">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.ep_bg_fetched">>,true},
                                           {<<"@kv-.ep_data_read_failed">>,
                                            true},
                                           {<<"@kv-.ep_data_write_failed">>,
                                            true},
                                           {<<"@kv-.ep_ops_create">>,true},
                                           {<<"@kv-.ep_ops_update">>,true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"vevb8bt8l">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.ep_diskqueue_items">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,true},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"7isc94m6c">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@query.query_requests_1000ms">>,
                                            true},
                                           {<<"@query.query_requests_500ms">>,
                                            true},
                                           {<<"@query.query_requests_5000ms">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"yxu3zuscp">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@query.query_selects">>,true},
                                           {<<"@query.query_requests">>,true},
                                           {<<"@query.query_warnings">>,true},
                                           {<<"@query.query_invalid_requests">>,
                                            true},
                                           {<<"@query.query_errors">>,true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"hsd1dbu3l">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@query.query_avg_req_time">>,
                                            true},
                                           {<<"@query.query_avg_svc_time">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"9yva4b019">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@query.query_avg_result_count">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,true},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"fgutu7xlw">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@query.query_avg_response_size">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"95q08wwol">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@index-.index/num_rows_returned">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,true},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"6rdualle6">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@index-.@items.num_docs_pending+queued">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,true},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"t1ho94nbx">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@index-.index/data_size">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,true},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"qn1obgx8u">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@index-.index/disk_size">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,true},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"idekze47i">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@index.index_ram_percent">>,
                                            true},
                                           {<<"@index.index_remaining_ram">>,
                                            true},
                                           {<<"@index-.index/data_size">>,
                                            true},
                                           {<<"@index-.index/disk_size">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"bp53oh5bg">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@fts-.fts/num_bytes_used_disk">>,
                                            true},
                                           {<<"@fts.fts_num_bytes_used_ram">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"g6acbjwse">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@fts-.@items.total_queries">>,
                                            true},
                                           {<<"@fts-.@items.total_queries_error">>,
                                            true},
                                           {<<"@fts-.@items.total_queries_slow">>,
                                            true},
                                           {<<"@fts-.@items.total_queries_timeout">>,
                                            true},
                                           {<<"@fts.fts_total_queries_rejected_by_herder">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"bbeahbtsm">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@cbas-.cbas/incoming_records_count">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,true},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"bygxrjmc6">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@cbas-.cbas_failed_to_parse_records_count">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,true},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"d8u14qbwu">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@cbas-.cbas/failed_at_parser_records_count_total">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,true},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"rsuhq7a81">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@cbas.cbas_heap_used">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,true},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"3js5onprs">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@cbas.cbas_heap_memory_committed_bytes">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,true},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"a2z3ox5m6">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@cbas.cbas_thread_count">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,true},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"kk34rkxws">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@cbas.cbas_disk_used">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,true},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"g844qwsw9">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@cbas.cbas_io_reads">>,true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,true},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"fi53d2hfo">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@cbas.cbas_io_writes">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,true},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"wc3i6sdm1">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@cbas.cbas_system_load_average">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,true},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"qgqj33gyw">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@cbas.cbas_pending_merge_ops">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,true},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"e99ew9kg3">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@cbas.cbas_pending_flush_ops">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,true},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"lpog1z8x4">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@system.sysproc_mem_resident_java_cbas">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,true},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"cqv0gkhdn">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@eventing.eventing/failed_count">>,
                                            true},
                                           {<<"@eventing.eventing/timeout_count">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"cut8tyr5r">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@xdcr-.replication_changes_left">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,true},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"m34nefjv0">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@xdcr-.@items.changes_left">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,true},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"43yauy9si">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@xdcr-.@items.wtavg_docs_latency">>,
                                            true},
                                           {<<"@xdcr-.@items.wtavg_meta_latency">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"edlacvk92">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@xdcr-.@items.docs_failed_cr_source">>,
                                            true},
                                           {<<"@xdcr-.@items.xdcr_docs_failed_cr_target_total">>,
                                            true},
                                           {<<"@xdcr-.@items.docs_filtered">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"fvr6ykof9">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.vb_active_num">>,true},
                                           {<<"@kv-.vb_replica_num">>,true},
                                           {<<"@kv-.vb_pending_num">>,true},
                                           {<<"@kv-.ep_vb_total">>,true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"q7sun8qk3">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.curr_items">>,true},
                                           {<<"@kv-.vb_replica_curr_items">>,
                                            true},
                                           {<<"@kv-.vb_pending_curr_items">>,
                                            true},
                                           {<<"@kv-.curr_items_tot">>,true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"98w1wsckr">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.vb_active_resident_items_ratio">>,
                                            true},
                                           {<<"@kv-.vb_replica_resident_items_ratio">>,
                                            true},
                                           {<<"@kv-.vb_pending_resident_items_ratio">>,
                                            true},
                                           {<<"@kv-.ep_resident_items_rate">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"9xjeyiw3n">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.vb_active_ops_create">>,
                                            true},
                                           {<<"@kv-.vb_replica_ops_create">>,
                                            true},
                                           {<<"@kv-.vb_pending_ops_create">>,
                                            true},
                                           {<<"@kv-.ep_ops_create">>,true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"5l56udkwg">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.vb_active_eject">>,true},
                                           {<<"@kv-.vb_replica_eject">>,true},
                                           {<<"@kv-.vb_pending_eject">>,true},
                                           {<<"@kv-.ep_num_value_ejects">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"gxt4lfbc4">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.vb_active_itm_memory">>,
                                            true},
                                           {<<"@kv-.vb_replica_itm_memory">>,
                                            true},
                                           {<<"@kv-.vb_pending_itm_memory">>,
                                            true},
                                           {<<"@kv-.ep_kv_size">>,true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"l7er1yzg2">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.vb_active_meta_data_memory">>,
                                            true},
                                           {<<"@kv-.vb_replica_meta_data_memory">>,
                                            true},
                                           {<<"@kv-.vb_pending_meta_data_memory">>,
                                            true},
                                           {<<"@kv-.ep_meta_data_memory">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"09yp5g7zc">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.ep_dcp_views+indexes_count">>,
                                            true},
                                           {<<"@kv-.ep_dcp_cbas_count">>,true},
                                           {<<"@kv-.ep_dcp_replica_count">>,
                                            true},
                                           {<<"@kv-.ep_dcp_xdcr_count">>,true},
                                           {<<"@kv-.ep_dcp_eventing_count">>,
                                            true},
                                           {<<"@kv-.ep_dcp_other_count">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"4917rg4wy">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.ep_dcp_views+indexes_producer_count">>,
                                            true},
                                           {<<"@kv-.ep_dcp_cbas_producer_count">>,
                                            true},
                                           {<<"@kv-.ep_dcp_replica_producer_count">>,
                                            true},
                                           {<<"@kv-.ep_dcp_xdcr_producer_count">>,
                                            true},
                                           {<<"@kv-.ep_dcp_eventing_producer_count">>,
                                            true},
                                           {<<"@kv-.ep_dcp_other_producer_count">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"n430o4c55">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.ep_dcp_views+indexes_items_remaining">>,
                                            true},
                                           {<<"@kv-.ep_dcp_cbas_items_remaining">>,
                                            true},
                                           {<<"@kv-.ep_dcp_replica_items_remaining">>,
                                            true},
                                           {<<"@kv-.ep_dcp_xdcr_items_remaining">>,
                                            true},
                                           {<<"@kv-.ep_dcp_eventing_items_remaining">>,
                                            true},
                                           {<<"@kv-.ep_dcp_other_items_remaining">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"wlv9jsqqb">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.ep_dcp_views+indexes_items_sent">>,
                                            true},
                                           {<<"@kv-.ep_dcp_cbas_items_sent">>,
                                            true},
                                           {<<"@kv-.ep_dcp_replica_items_sent">>,
                                            true},
                                           {<<"@kv-.ep_dcp_xdcr_items_sent">>,
                                            true},
                                           {<<"@kv-.ep_dcp_eventing_items_sent">>,
                                            true},
                                           {<<"@kv-.ep_dcp_other_items_sent">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"ljvjah7xg">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.ep_dcp_views+indexes_total_bytes">>,
                                            true},
                                           {<<"@kv-.ep_dcp_cbas_total_bytes">>,
                                            true},
                                           {<<"@kv-.ep_dcp_replica_total_bytes">>,
                                            true},
                                           {<<"@kv-.ep_dcp_xdcr_total_bytes">>,
                                            true},
                                           {<<"@kv-.ep_dcp_eventing_total_bytes">>,
                                            true},
                                           {<<"@kv-.ep_dcp_other_total_bytes">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"88tjivg5r">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.ep_dcp_views+indexes_backoff">>,
                                            true},
                                           {<<"@kv-.ep_dcp_cbas_backoff">>,
                                            true},
                                           {<<"@kv-.ep_dcp_replica_backoff">>,
                                            true},
                                           {<<"@kv-.ep_dcp_xdcr_backoff">>,
                                            true},
                                           {<<"@kv-.ep_dcp_eventing_backoff">>,
                                            true},
                                           {<<"@kv-.ep_dcp_other_backoff">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"w42kpvdig">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.ep_diskqueue_fill">>,true},
                                           {<<"@kv-.ep_diskqueue_drain">>,
                                            true},
                                           {<<"@kv-.ep_diskqueue_items">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"b43fw6j1x">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.vb_active_queue_fill">>,
                                            true},
                                           {<<"@kv-.vb_active_queue_drain">>,
                                            true},
                                           {<<"@kv-.vb_active_queue_size">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"9bdcm2uu1">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.vb_replica_queue_fill">>,
                                            true},
                                           {<<"@kv-.vb_replica_queue_drain">>,
                                            true},
                                           {<<"@kv-.vb_replica_queue_size">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"hclcvm4as">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.vb_pending_queue_fill">>,
                                            true},
                                           {<<"@kv-.vb_pending_queue_drain">>,
                                            true},
                                           {<<"@kv-.vb_pending_queue_size">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"86zg6u6oa">>}]}]}]}},
                         {identity,{[{domain,builtin},
                                     {user,<<"<ud>admin</ud>">>}]}}]
[ns_server:debug,2022-09-08T12:12:08.275Z,ns_1@127.0.0.1:users_storage<0.340.0>:replicated_storage:handle_call:107]Writing interactively saved doc {docv2,
                                    {ui_profile,{"<ud>admin</ud>",admin}},
                                    {[{<<"version">>,458752},
                                      {<<"scenarios">>,
                                       [{[{<<"name">>,<<"Cluster Overview">>},
                                          {<<"uiid">>,
                                           <<"mn-cluster-overview">>},
                                          {<<"desc">>,
                                           <<"Stats showing the general health of your cluster. Customize and/or make your own dashboard with \"new dashboard... \" below.">>},
                                          {<<"groups">>,
                                           [<<"11rx2q3dq">>,<<"0l17jptaz">>]},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"hq5gtf09a">>}]},
                                        {[{<<"name">>,<<"All Services">>},
                                          {<<"uiid">>,<<"mn-all-services">>},
                                          {<<"desc">>,
                                           <<"Most common stats, arranged per service. Customize and/or make your own dashboard with \"new dashboard... \" below.">>},
                                          {<<"groups">>,
                                           [<<"hoywddh35">>,<<"6hvu5jnx8">>,
                                            <<"ndtqn252p">>,<<"cf1bvg1sr">>,
                                            <<"fntj7a633">>,<<"afzuv5i81">>,
                                            <<"rmkds99te">>,<<"gwe664c43">>,
                                            <<"9pll8tqmu">>,<<"hlsgofi6p">>]},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"dl7q7bzx5">>}]}]},
                                      {<<"groups">>,
                                       [{[{<<"name">>,<<"Cluster Overview">>},
                                          {<<"uiid">>,
                                           <<"mn-cluster-overview-group">>},
                                          {<<"charts">>,
                                           [<<"9ogsfigym">>,<<"npt3v11wn">>,
                                            <<"mlzthzdj8">>,<<"kjqc7gszc">>,
                                            <<"77ebkjk67">>,<<"ztrhk4daf">>,
                                            <<"scgygdlbb">>,<<"in36w18a8">>]},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"11rx2q3dq">>}]},
                                        {[{<<"name">>,<<"Node Resources">>},
                                          {<<"charts">>,
                                           [<<"f3qns6gtf">>,<<"e75f64p1s">>,
                                            <<"g6cojdhji">>,<<"gk6oc5z0a">>]},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"0l17jptaz">>}]},
                                        {[{<<"name">>,
                                           <<"Data (Docs/Views/XDCR)">>},
                                          {<<"uiid">>,
                                           <<"mn-all-services-data-group">>},
                                          {<<"charts">>,
                                           [<<"fgmr2a75f">>,<<"ny2j9a7yr">>,
                                            <<"atm5mcjnv">>,<<"987tcav28">>,
                                            <<"9sqintxmy">>]},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"hoywddh35">>}]},
                                        {[{<<"name">>,<<"Query">>},
                                          {<<"charts">>,
                                           [<<"n9yk6ymll">>,<<"bfbbc4xz9">>,
                                            <<"rpfv85c41">>,<<"3tfm7aosv">>,
                                            <<"bhneyfyh0">>]},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"6hvu5jnx8">>}]},
                                        {[{<<"name">>,<<"Index">>},
                                          {<<"charts">>,
                                           [<<"gjd29lmz9">>,<<"jidz9ijfn">>,
                                            <<"nogoqkvvk">>,<<"n7p4o3vv8">>,
                                            <<"lv9ha6wam">>]},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"ndtqn252p">>}]},
                                        {[{<<"name">>,<<"Search">>},
                                          {<<"charts">>,
                                           [<<"hp8liq47s">>,<<"13xkxyd0f">>]},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"cf1bvg1sr">>}]},
                                        {[{<<"name">>,<<"Analytics">>},
                                          {<<"enterprise">>,true},
                                          {<<"charts">>,
                                           [<<"pgg7l1tz2">>,<<"ye7aqi8oo">>,
                                            <<"x4t0hojki">>,<<"yomkjapa3">>,
                                            <<"gd3nl721k">>,<<"abs0sq9qm">>,
                                            <<"olv1eleqi">>,<<"goknl7ind">>,
                                            <<"s1z7ypkgc">>,<<"22r3nufy5">>,
                                            <<"uqknixaoy">>,<<"bmqc6drl4">>,
                                            <<"hpe0a55tx">>]},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"fntj7a633">>}]},
                                        {[{<<"name">>,<<"Eventing">>},
                                          {<<"enterprise">>,true},
                                          {<<"charts">>,[<<"f3zcnd6rj">>]},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"afzuv5i81">>}]},
                                        {[{<<"name">>,<<"XDCR">>},
                                          {<<"charts">>,
                                           [<<"o2ts69dxs">>,<<"dblhe1for">>,
                                            <<"ju8mbgh0b">>,<<"pnpspa99w">>]},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"rmkds99te">>}]},
                                        {[{<<"name">>,<<"vBucket Resources">>},
                                          {<<"charts">>,
                                           [<<"deqbnnxp3">>,<<"cdryr96ee">>,
                                            <<"3tsnfxa13">>,<<"8fc2hkons">>,
                                            <<"mdbgi8bjj">>,<<"egmkytqyb">>,
                                            <<"b62ov7bzk">>]},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"gwe664c43">>}]},
                                        {[{<<"name">>,<<"DCP Queues">>},
                                          {<<"charts">>,
                                           [<<"xakh5iyng">>,<<"11874rd3s">>,
                                            <<"dwuy10do6">>,<<"au2z85yrz">>,
                                            <<"ldwxy7sqz">>,<<"zn3jkmlk4">>]},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"9pll8tqmu">>}]},
                                        {[{<<"name">>,<<"Disk Queues">>},
                                          {<<"charts">>,
                                           [<<"56zfuc9ht">>,<<"ativ0hd0v">>,
                                            <<"uel8bwemw">>,<<"lvam0tvln">>]},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"hlsgofi6p">>}]}]},
                                      {<<"charts">>,
                                       [{[{<<"stats">>,
                                           {[{<<"@kv-.kv_ops">>,true},
                                             {<<"@query.n1ql_requests">>,true},
                                             {<<"@fts-.@items.fts_total_queries">>,
                                              true},
                                             {<<"@kv-.kv_ep_tmp_oom_errors">>,
                                              true},
                                             {<<"@kv-.kv_ep_cache_miss_ratio">>,
                                              true},
                                             {<<"@kv-.kv_cmd_get">>,true},
                                             {<<"@kv-.kv_cmd_set">>,true},
                                             {<<"@kv-.kv_delete_hits">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"9ogsfigym">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@kv-.kv_mem_used_bytes">>,
                                              true},
                                             {<<"@kv-.kv_ep_mem_low_wat">>,
                                              true},
                                             {<<"@kv-.kv_ep_mem_high_wat">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"npt3v11wn">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@kv-.kv_curr_items">>,true},
                                             {<<"@kv-.kv_vb_replica_curr_items">>,
                                              true},
                                             {<<"@kv-.kv_vb_active_resident_items_ratio">>,
                                              true},
                                             {<<"@kv-.kv_vb_replica_resident_items_ratio">>,
                                              true},
                                             {<<"@kv-.couch_docs_fragmentation">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"mlzthzdj8">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@kv-.kv_disk_write_queue">>,
                                              true},
                                             {<<"@kv-.couch_docs_actual_disk_size">>,
                                              true}]}},
                                          {<<"size">>,<<"small">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"kjqc7gszc">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@kv-.kv_dcp_items_remaining_replication">>,
                                              true}]}},
                                          {<<"size">>,<<"small">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"77ebkjk67">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@kv-.kv_ep_data_read_failed">>,
                                              true},
                                             {<<"@kv-.kv_ep_data_write_failed">>,
                                              true},
                                             {<<"@query.n1ql_errors">>,true},
                                             {<<"@fts-.@items.fts_total_queries_error">>,
                                              true},
                                             {<<"@eventing.eventing_failed_count">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"ztrhk4daf">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@query.n1ql_requests_250ms">>,
                                              true},
                                             {<<"@query.n1ql_requests_500ms">>,
                                              true},
                                             {<<"@query.n1ql_requests_1000ms">>,
                                              true},
                                             {<<"@query.n1ql_requests_5000ms">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"scgygdlbb">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@xdcr-.xdcr_changes_left_total">>,
                                              true},
                                             {<<"@index-.@items.index_num_docs_pending_and_queued">>,
                                              true},
                                             {<<"@fts-.@items.fts_num_mutations_to_index">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"in36w18a8">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@system.sys_cpu_utilization_rate">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,true},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"f3qns6gtf">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@system.cm_rest_request_leaves_total">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,true},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"e75f64p1s">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@system.sys_mem_actual_free">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"g6cojdhji">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@system.sys_swap_used">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,true},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"gk6oc5z0a">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@kv-.kv_mem_used_bytes">>,
                                              true},
                                             {<<"@kv-.kv_ep_mem_low_wat">>,
                                              true},
                                             {<<"@kv-.kv_ep_mem_high_wat">>,
                                              true},
                                             {<<"@kv-.kv_memory_used_bytes">>,
                                              true},
                                             {<<"@kv-.kv_ep_meta_data_memory_bytes">>,
                                              true},
                                             {<<"@kv-.kv_vb_active_resident_items_ratio">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"fgmr2a75f">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@kv-.kv_ops">>,true},
                                             {<<"@kv-.kv_ep_cache_miss_ratio">>,
                                              true},
                                             {<<"@kv-.kv_cmd_get">>,true},
                                             {<<"@kv-.kv_cmd_set">>,true},
                                             {<<"@kv-.kv_delete_hits">>,true},
                                             {<<"@kv-.kv_ep_num_ops_set_meta">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"ny2j9a7yr">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@kv-.kv_dcp_items_remaining_views+indexes">>,
                                              true},
                                             {<<"@kv-.kv_dcp_items_remaining_cbas">>,
                                              true},
                                             {<<"@kv-.kv_dcp_items_remaining_replication">>,
                                              true},
                                             {<<"@kv-.kv_dcp_items_remaining_xdcr">>,
                                              true},
                                             {<<"@kv-.kv_dcp_items_remaining_eventing">>,
                                              true},
                                             {<<"@kv-.kv_dcp_items_remaining_other">>,
                                              true},
                                             {<<"@xdcr-.xdcr_changes_left_total">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"atm5mcjnv">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@kv-.kv_ep_bg_fetched">>,
                                              true},
                                             {<<"@kv-.kv_ep_data_read_failed">>,
                                              true},
                                             {<<"@kv-.kv_ep_data_write_failed">>,
                                              true},
                                             {<<"@kv-.kv_ep_ops_create">>,
                                              true},
                                             {<<"@kv-.kv_ep_ops_update">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"987tcav28">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@kv-.kv_ep_diskqueue_items">>,
                                              true}]}},
                                          {<<"size">>,<<"small">>},
                                          {<<"specificStat">>,true},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"9sqintxmy">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@query.n1ql_requests_1000ms">>,
                                              true},
                                             {<<"@query.n1ql_requests_500ms">>,
                                              true},
                                             {<<"@query.n1ql_requests_5000ms">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"n9yk6ymll">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@query.n1ql_selects">>,true},
                                             {<<"@query.n1ql_requests">>,true},
                                             {<<"@query.n1ql_warnings">>,true},
                                             {<<"@query.n1ql_invalid_requests">>,
                                              true},
                                             {<<"@query.n1ql_errors">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"bfbbc4xz9">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@query.n1ql_avg_req_time">>,
                                              true},
                                             {<<"@query.n1ql_avg_svc_time">>,
                                              true}]}},
                                          {<<"size">>,<<"small">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"rpfv85c41">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@query.n1ql_avg_result_count">>,
                                              true}]}},
                                          {<<"size">>,<<"small">>},
                                          {<<"specificStat">>,true},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"3tfm7aosv">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@query.n1ql_avg_response_size">>,
                                              true}]}},
                                          {<<"size">>,<<"small">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"bhneyfyh0">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@index-.index_num_rows_returned">>,
                                              true}]}},
                                          {<<"size">>,<<"small">>},
                                          {<<"specificStat">>,true},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"gjd29lmz9">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@index-.@items.index_num_docs_pending_and_queued">>,
                                              true}]}},
                                          {<<"size">>,<<"small">>},
                                          {<<"specificStat">>,true},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"jidz9ijfn">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@index-.index_data_size">>,
                                              true}]}},
                                          {<<"size">>,<<"small">>},
                                          {<<"specificStat">>,true},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"nogoqkvvk">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@index-.index_disk_size">>,
                                              true}]}},
                                          {<<"size">>,<<"small">>},
                                          {<<"specificStat">>,true},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"n7p4o3vv8">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@index.index_ram_percent">>,
                                              true},
                                             {<<"@index.index_remaining_ram">>,
                                              true},
                                             {<<"@index-.index_data_size">>,
                                              true},
                                             {<<"@index-.index_disk_size">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"lv9ha6wam">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@fts-.fts_num_bytes_used_disk">>,
                                              true},
                                             {<<"@fts.fts_num_bytes_used_ram">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"hp8liq47s">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@fts-.@items.fts_total_queries">>,
                                              true},
                                             {<<"@fts-.@items.fts_total_queries_error">>,
                                              true},
                                             {<<"@fts-.@items.fts_total_queries_slow">>,
                                              true},
                                             {<<"@fts-.@items.fts_total_queries_timeout">>,
                                              true},
                                             {<<"@fts.fts_total_queries_rejected_by_herder">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"13xkxyd0f">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@cbas-.cbas_incoming_records_count">>,
                                              true}]}},
                                          {<<"size">>,<<"small">>},
                                          {<<"specificStat">>,true},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"pgg7l1tz2">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@cbas-.cbas_failed_to_parse_records_count">>,
                                              true}]}},
                                          {<<"size">>,<<"small">>},
                                          {<<"specificStat">>,true},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"ye7aqi8oo">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@cbas-.cbas_failed_to_parse_records_count_total">>,
                                              true}]}},
                                          {<<"size">>,<<"small">>},
                                          {<<"specificStat">>,true},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"x4t0hojki">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@cbas.cbas_heap_memory_used_bytes">>,
                                              true}]}},
                                          {<<"size">>,<<"small">>},
                                          {<<"specificStat">>,true},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"yomkjapa3">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@cbas.cbas_heap_memory_committed_bytes">>,
                                              true}]}},
                                          {<<"size">>,<<"small">>},
                                          {<<"specificStat">>,true},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"gd3nl721k">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@cbas.cbas_thread_count">>,
                                              true}]}},
                                          {<<"size">>,<<"small">>},
                                          {<<"specificStat">>,true},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"abs0sq9qm">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@cbas.cbas_disk_used_bytes_total">>,
                                              true}]}},
                                          {<<"size">>,<<"small">>},
                                          {<<"specificStat">>,true},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"olv1eleqi">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@cbas.cbas_io_reads_total">>,
                                              true}]}},
                                          {<<"size">>,<<"small">>},
                                          {<<"specificStat">>,true},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"goknl7ind">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@cbas.cbas_io_writes_total">>,
                                              true}]}},
                                          {<<"size">>,<<"small">>},
                                          {<<"specificStat">>,true},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"s1z7ypkgc">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@cbas.cbas_system_load_average">>,
                                              true}]}},
                                          {<<"size">>,<<"small">>},
                                          {<<"specificStat">>,true},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"22r3nufy5">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@cbas.cbas_pending_merge_ops">>,
                                              true}]}},
                                          {<<"size">>,<<"small">>},
                                          {<<"specificStat">>,true},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"uqknixaoy">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@cbas.cbas_pending_flush_ops">>,
                                              true}]}},
                                          {<<"size">>,<<"small">>},
                                          {<<"specificStat">>,true},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"bmqc6drl4">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@system.sysproc_mem_resident_java_cbas">>,
                                              true}]}},
                                          {<<"size">>,<<"small">>},
                                          {<<"specificStat">>,true},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"hpe0a55tx">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@eventing.eventing_failed_count">>,
                                              true},
                                             {<<"@eventing.eventing_timeout_count">>,
                                              true}]}},
                                          {<<"size">>,<<"small">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"f3zcnd6rj">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@xdcr-.xdcr_changes_left_total">>,
                                              true}]}},
                                          {<<"size">>,<<"small">>},
                                          {<<"specificStat">>,true},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"o2ts69dxs">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@xdcr-.@items.xdcr_changes_left_total">>,
                                              true}]}},
                                          {<<"size">>,<<"small">>},
                                          {<<"specificStat">>,true},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"dblhe1for">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@xdcr-.@items.xdcr_wtavg_docs_latency_seconds">>,
                                              true},
                                             {<<"@xdcr-.@items.xdcr_wtavg_meta_latency_seconds">>,
                                              true}]}},
                                          {<<"size">>,<<"small">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"ju8mbgh0b">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@xdcr-.@items.xdcr_docs_failed_cr_source_total">>,
                                              true},
                                             {<<"@xdcr-.@items.xdcr_docs_failed_cr_target_total">>,
                                              true},
                                             {<<"@xdcr-.@items.xdcr_docs_filtered_total">>,
                                              true}]}},
                                          {<<"size">>,<<"small">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"pnpspa99w">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@kv-.kv_vb_active_num">>,
                                              true},
                                             {<<"@kv-.kv_vb_replica_num">>,
                                              true},
                                             {<<"@kv-.kv_vb_pending_num">>,
                                              true},
                                             {<<"@kv-.kv_ep_vb_total">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"deqbnnxp3">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@kv-.kv_curr_items">>,true},
                                             {<<"@kv-.kv_vb_replica_curr_items">>,
                                              true},
                                             {<<"@kv-.kv_vb_pending_curr_items">>,
                                              true},
                                             {<<"@kv-.kv_curr_items_tot">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"cdryr96ee">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@kv-.kv_vb_active_resident_items_ratio">>,
                                              true},
                                             {<<"@kv-.kv_vb_replica_resident_items_ratio">>,
                                              true},
                                             {<<"@kv-.kv_vb_pending_resident_items_ratio">>,
                                              true},
                                             {<<"@kv-.kv_ep_resident_items_ratio">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"3tsnfxa13">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@kv-.kv_vb_active_ops_create">>,
                                              true},
                                             {<<"@kv-.kv_vb_replica_ops_create">>,
                                              true},
                                             {<<"@kv-.kv_vb_pending_ops_create">>,
                                              true},
                                             {<<"@kv-.kv_ep_ops_create">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"8fc2hkons">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@kv-.kv_vb_active_eject">>,
                                              true},
                                             {<<"@kv-.kv_vb_replica_eject">>,
                                              true},
                                             {<<"@kv-.kv_vb_pending_eject">>,
                                              true},
                                             {<<"@kv-.kv_ep_num_value_ejects">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"mdbgi8bjj">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@kv-.kv_vb_active_itm_memory_bytes">>,
                                              true},
                                             {<<"@kv-.kv_vb_replica_itm_memory_bytes">>,
                                              true},
                                             {<<"@kv-.kv_vb_pending_itm_memory_bytes">>,
                                              true},
                                             {<<"@kv-.kv_memory_used_bytes">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"egmkytqyb">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@kv-.kv_vb_active_meta_data_memory_bytes">>,
                                              true},
                                             {<<"@kv-.kv_vb_replica_meta_data_memory_bytes">>,
                                              true},
                                             {<<"@kv-.kv_vb_pending_meta_data_memory_bytes">>,
                                              true},
                                             {<<"@kv-.kv_ep_meta_data_memory_bytes">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"b62ov7bzk">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@kv-.kv_dcp_connection_count_views+indexes">>,
                                              true},
                                             {<<"@kv-.kv_dcp_connection_count_cbas">>,
                                              true},
                                             {<<"@kv-.kv_dcp_connection_count_replication">>,
                                              true},
                                             {<<"@kv-.kv_dcp_connection_count_xdcr">>,
                                              true},
                                             {<<"@kv-.kv_dcp_connection_count_eventing">>,
                                              true},
                                             {<<"@kv-.kv_dcp_connection_count_other">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"xakh5iyng">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@kv-.kv_dcp_producer_count_views+indexes">>,
                                              true},
                                             {<<"@kv-.kv_dcp_producer_count_cbas">>,
                                              true},
                                             {<<"@kv-.kv_dcp_producer_count_replication">>,
                                              true},
                                             {<<"@kv-.kv_dcp_producer_count_xdcr">>,
                                              true},
                                             {<<"@kv-.kv_dcp_producer_count_eventing">>,
                                              true},
                                             {<<"@kv-.kv_dcp_producer_count_other">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"11874rd3s">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@kv-.kv_dcp_items_remaining_views+indexes">>,
                                              true},
                                             {<<"@kv-.kv_dcp_items_remaining_cbas">>,
                                              true},
                                             {<<"@kv-.kv_dcp_items_remaining_replication">>,
                                              true},
                                             {<<"@kv-.kv_dcp_items_remaining_xdcr">>,
                                              true},
                                             {<<"@kv-.kv_dcp_items_remaining_eventing">>,
                                              true},
                                             {<<"@kv-.kv_dcp_items_remaining_other">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"dwuy10do6">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@kv-.kv_dcp_items_sent_views+indexes">>,
                                              true},
                                             {<<"@kv-.kv_dcp_items_sent_cbas">>,
                                              true},
                                             {<<"@kv-.kv_dcp_items_sent_replication">>,
                                              true},
                                             {<<"@kv-.kv_dcp_items_sent_xdcr">>,
                                              true},
                                             {<<"@kv-.kv_dcp_items_sent_eventing">>,
                                              true},
                                             {<<"@kv-.kv_dcp_items_sent_other">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"au2z85yrz">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@kv-.kv_dcp_total_data_size_bytes_views+indexes">>,
                                              true},
                                             {<<"@kv-.kv_dcp_total_data_size_bytes_cbas">>,
                                              true},
                                             {<<"@kv-.kv_dcp_total_data_size_bytes_replication">>,
                                              true},
                                             {<<"@kv-.kv_dcp_total_data_size_bytes_xdcr">>,
                                              true},
                                             {<<"@kv-.kv_dcp_total_data_size_bytes_eventing">>,
                                              true},
                                             {<<"@kv-.kv_dcp_total_data_size_bytes_other">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"ldwxy7sqz">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@kv-.kv_dcp_backoff_views+indexes">>,
                                              true},
                                             {<<"@kv-.kv_dcp_backoff_cbas">>,
                                              true},
                                             {<<"@kv-.kv_dcp_backoff_replication">>,
                                              true},
                                             {<<"@kv-.kv_dcp_backoff_xdcr">>,
                                              true},
                                             {<<"@kv-.kv_dcp_backoff_eventing">>,
                                              true},
                                             {<<"@kv-.kv_dcp_backoff_other">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"zn3jkmlk4">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@kv-.kv_ep_diskqueue_fill">>,
                                              true},
                                             {<<"@kv-.kv_ep_diskqueue_drain">>,
                                              true},
                                             {<<"@kv-.kv_ep_diskqueue_items">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"56zfuc9ht">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@kv-.kv_vb_active_queue_fill">>,
                                              true},
                                             {<<"@kv-.kv_vb_active_queue_drain">>,
                                              true},
                                             {<<"@kv-.kv_vb_active_queue_size">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"ativ0hd0v">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@kv-.kv_vb_replica_queue_fill">>,
                                              true},
                                             {<<"@kv-.kv_vb_replica_queue_drain">>,
                                              true},
                                             {<<"@kv-.kv_vb_replica_queue_size">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"uel8bwemw">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@kv-.kv_vb_pending_queue_fill">>,
                                              true},
                                             {<<"@kv-.kv_vb_pending_queue_drain">>,
                                              true},
                                             {<<"@kv-.kv_vb_pending_queue_size">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"lvam0tvln">>}]}]}]},
                                    [{rev,{2,<<"©ÂÐ¢">>}},
                                     {deleted,false},
                                     {last_modified,1662639128273}]}
[ns_server:debug,2022-09-08T12:12:08.287Z,ns_1@127.0.0.1:ns_audit<0.579.0>:ns_audit:handle_call:148]Audit set_user_profile: [{local,{[{ip,<<"172.18.0.2">>},{port,8091}]}},
                         {remote,{[{ip,<<"172.18.0.1">>},{port,46818}]}},
                         {sessionid,<<"8d72771539648f7a1bc57f49b878927e398368fd">>},
                         {real_userid,{[{domain,builtin},
                                        {user,<<"<ud>admin</ud>">>}]}},
                         {timestamp,<<"2022-09-08T12:12:08.283Z">>},
                         {profile,{[{<<"version">>,458752},
                                    {<<"scenarios">>,
                                     [{[{<<"name">>,<<"Cluster Overview">>},
                                        {<<"uiid">>,<<"mn-cluster-overview">>},
                                        {<<"desc">>,
                                         <<"Stats showing the general health of your cluster. Customize and/or make your own dashboard with \"new dashboard... \" below.">>},
                                        {<<"groups">>,
                                         [<<"11rx2q3dq">>,<<"0l17jptaz">>]},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"hq5gtf09a">>}]},
                                      {[{<<"name">>,<<"All Services">>},
                                        {<<"uiid">>,<<"mn-all-services">>},
                                        {<<"desc">>,
                                         <<"Most common stats, arranged per service. Customize and/or make your own dashboard with \"new dashboard... \" below.">>},
                                        {<<"groups">>,
                                         [<<"hoywddh35">>,<<"6hvu5jnx8">>,
                                          <<"ndtqn252p">>,<<"cf1bvg1sr">>,
                                          <<"fntj7a633">>,<<"afzuv5i81">>,
                                          <<"rmkds99te">>,<<"gwe664c43">>,
                                          <<"9pll8tqmu">>,<<"hlsgofi6p">>]},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"dl7q7bzx5">>}]}]},
                                    {<<"groups">>,
                                     [{[{<<"name">>,<<"Cluster Overview">>},
                                        {<<"uiid">>,
                                         <<"mn-cluster-overview-group">>},
                                        {<<"charts">>,
                                         [<<"9ogsfigym">>,<<"npt3v11wn">>,
                                          <<"mlzthzdj8">>,<<"kjqc7gszc">>,
                                          <<"77ebkjk67">>,<<"ztrhk4daf">>,
                                          <<"scgygdlbb">>,<<"in36w18a8">>]},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"11rx2q3dq">>}]},
                                      {[{<<"name">>,<<"Node Resources">>},
                                        {<<"charts">>,
                                         [<<"f3qns6gtf">>,<<"e75f64p1s">>,
                                          <<"g6cojdhji">>,<<"gk6oc5z0a">>]},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"0l17jptaz">>}]},
                                      {[{<<"name">>,
                                         <<"Data (Docs/Views/XDCR)">>},
                                        {<<"uiid">>,
                                         <<"mn-all-services-data-group">>},
                                        {<<"charts">>,
                                         [<<"fgmr2a75f">>,<<"ny2j9a7yr">>,
                                          <<"atm5mcjnv">>,<<"987tcav28">>,
                                          <<"9sqintxmy">>]},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"hoywddh35">>}]},
                                      {[{<<"name">>,<<"Query">>},
                                        {<<"charts">>,
                                         [<<"n9yk6ymll">>,<<"bfbbc4xz9">>,
                                          <<"rpfv85c41">>,<<"3tfm7aosv">>,
                                          <<"bhneyfyh0">>]},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"6hvu5jnx8">>}]},
                                      {[{<<"name">>,<<"Index">>},
                                        {<<"charts">>,
                                         [<<"gjd29lmz9">>,<<"jidz9ijfn">>,
                                          <<"nogoqkvvk">>,<<"n7p4o3vv8">>,
                                          <<"lv9ha6wam">>]},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"ndtqn252p">>}]},
                                      {[{<<"name">>,<<"Search">>},
                                        {<<"charts">>,
                                         [<<"hp8liq47s">>,<<"13xkxyd0f">>]},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"cf1bvg1sr">>}]},
                                      {[{<<"name">>,<<"Analytics">>},
                                        {<<"enterprise">>,true},
                                        {<<"charts">>,
                                         [<<"pgg7l1tz2">>,<<"ye7aqi8oo">>,
                                          <<"x4t0hojki">>,<<"yomkjapa3">>,
                                          <<"gd3nl721k">>,<<"abs0sq9qm">>,
                                          <<"olv1eleqi">>,<<"goknl7ind">>,
                                          <<"s1z7ypkgc">>,<<"22r3nufy5">>,
                                          <<"uqknixaoy">>,<<"bmqc6drl4">>,
                                          <<"hpe0a55tx">>]},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"fntj7a633">>}]},
                                      {[{<<"name">>,<<"Eventing">>},
                                        {<<"enterprise">>,true},
                                        {<<"charts">>,[<<"f3zcnd6rj">>]},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"afzuv5i81">>}]},
                                      {[{<<"name">>,<<"XDCR">>},
                                        {<<"charts">>,
                                         [<<"o2ts69dxs">>,<<"dblhe1for">>,
                                          <<"ju8mbgh0b">>,<<"pnpspa99w">>]},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"rmkds99te">>}]},
                                      {[{<<"name">>,<<"vBucket Resources">>},
                                        {<<"charts">>,
                                         [<<"deqbnnxp3">>,<<"cdryr96ee">>,
                                          <<"3tsnfxa13">>,<<"8fc2hkons">>,
                                          <<"mdbgi8bjj">>,<<"egmkytqyb">>,
                                          <<"b62ov7bzk">>]},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"gwe664c43">>}]},
                                      {[{<<"name">>,<<"DCP Queues">>},
                                        {<<"charts">>,
                                         [<<"xakh5iyng">>,<<"11874rd3s">>,
                                          <<"dwuy10do6">>,<<"au2z85yrz">>,
                                          <<"ldwxy7sqz">>,<<"zn3jkmlk4">>]},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"9pll8tqmu">>}]},
                                      {[{<<"name">>,<<"Disk Queues">>},
                                        {<<"charts">>,
                                         [<<"56zfuc9ht">>,<<"ativ0hd0v">>,
                                          <<"uel8bwemw">>,<<"lvam0tvln">>]},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"hlsgofi6p">>}]}]},
                                    {<<"charts">>,
                                     [{[{<<"stats">>,
                                         {[{<<"@kv-.kv_ops">>,true},
                                           {<<"@query.n1ql_requests">>,true},
                                           {<<"@fts-.@items.fts_total_queries">>,
                                            true},
                                           {<<"@kv-.kv_ep_tmp_oom_errors">>,
                                            true},
                                           {<<"@kv-.kv_ep_cache_miss_ratio">>,
                                            true},
                                           {<<"@kv-.kv_cmd_get">>,true},
                                           {<<"@kv-.kv_cmd_set">>,true},
                                           {<<"@kv-.kv_delete_hits">>,true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"9ogsfigym">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.kv_mem_used_bytes">>,true},
                                           {<<"@kv-.kv_ep_mem_low_wat">>,true},
                                           {<<"@kv-.kv_ep_mem_high_wat">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"npt3v11wn">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.kv_curr_items">>,true},
                                           {<<"@kv-.kv_vb_replica_curr_items">>,
                                            true},
                                           {<<"@kv-.kv_vb_active_resident_items_ratio">>,
                                            true},
                                           {<<"@kv-.kv_vb_replica_resident_items_ratio">>,
                                            true},
                                           {<<"@kv-.couch_docs_fragmentation">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"mlzthzdj8">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.kv_disk_write_queue">>,
                                            true},
                                           {<<"@kv-.couch_docs_actual_disk_size">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"kjqc7gszc">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.kv_dcp_items_remaining_replication">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"77ebkjk67">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.kv_ep_data_read_failed">>,
                                            true},
                                           {<<"@kv-.kv_ep_data_write_failed">>,
                                            true},
                                           {<<"@query.n1ql_errors">>,true},
                                           {<<"@fts-.@items.fts_total_queries_error">>,
                                            true},
                                           {<<"@eventing.eventing_failed_count">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"ztrhk4daf">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@query.n1ql_requests_250ms">>,
                                            true},
                                           {<<"@query.n1ql_requests_500ms">>,
                                            true},
                                           {<<"@query.n1ql_requests_1000ms">>,
                                            true},
                                           {<<"@query.n1ql_requests_5000ms">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"scgygdlbb">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@xdcr-.xdcr_changes_left_total">>,
                                            true},
                                           {<<"@index-.@items.index_num_docs_pending_and_queued">>,
                                            true},
                                           {<<"@fts-.@items.fts_num_mutations_to_index">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"in36w18a8">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@system.sys_cpu_utilization_rate">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,true},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"f3qns6gtf">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@system.cm_rest_request_leaves_total">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,true},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"e75f64p1s">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@system.sys_mem_actual_free">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"g6cojdhji">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@system.sys_swap_used">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,true},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"gk6oc5z0a">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.kv_mem_used_bytes">>,true},
                                           {<<"@kv-.kv_ep_mem_low_wat">>,true},
                                           {<<"@kv-.kv_ep_mem_high_wat">>,
                                            true},
                                           {<<"@kv-.kv_memory_used_bytes">>,
                                            true},
                                           {<<"@kv-.kv_ep_meta_data_memory_bytes">>,
                                            true},
                                           {<<"@kv-.kv_vb_active_resident_items_ratio">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"fgmr2a75f">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.kv_ops">>,true},
                                           {<<"@kv-.kv_ep_cache_miss_ratio">>,
                                            true},
                                           {<<"@kv-.kv_cmd_get">>,true},
                                           {<<"@kv-.kv_cmd_set">>,true},
                                           {<<"@kv-.kv_delete_hits">>,true},
                                           {<<"@kv-.kv_ep_num_ops_set_meta">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"ny2j9a7yr">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.kv_dcp_items_remaining_views+indexes">>,
                                            true},
                                           {<<"@kv-.kv_dcp_items_remaining_cbas">>,
                                            true},
                                           {<<"@kv-.kv_dcp_items_remaining_replication">>,
                                            true},
                                           {<<"@kv-.kv_dcp_items_remaining_xdcr">>,
                                            true},
                                           {<<"@kv-.kv_dcp_items_remaining_eventing">>,
                                            true},
                                           {<<"@kv-.kv_dcp_items_remaining_other">>,
                                            true},
                                           {<<"@xdcr-.xdcr_changes_left_total">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"atm5mcjnv">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.kv_ep_bg_fetched">>,true},
                                           {<<"@kv-.kv_ep_data_read_failed">>,
                                            true},
                                           {<<"@kv-.kv_ep_data_write_failed">>,
                                            true},
                                           {<<"@kv-.kv_ep_ops_create">>,true},
                                           {<<"@kv-.kv_ep_ops_update">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"987tcav28">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.kv_ep_diskqueue_items">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,true},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"9sqintxmy">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@query.n1ql_requests_1000ms">>,
                                            true},
                                           {<<"@query.n1ql_requests_500ms">>,
                                            true},
                                           {<<"@query.n1ql_requests_5000ms">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"n9yk6ymll">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@query.n1ql_selects">>,true},
                                           {<<"@query.n1ql_requests">>,true},
                                           {<<"@query.n1ql_warnings">>,true},
                                           {<<"@query.n1ql_invalid_requests">>,
                                            true},
                                           {<<"@query.n1ql_errors">>,true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"bfbbc4xz9">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@query.n1ql_avg_req_time">>,
                                            true},
                                           {<<"@query.n1ql_avg_svc_time">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"rpfv85c41">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@query.n1ql_avg_result_count">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,true},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"3tfm7aosv">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@query.n1ql_avg_response_size">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"bhneyfyh0">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@index-.index_num_rows_returned">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,true},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"gjd29lmz9">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@index-.@items.index_num_docs_pending_and_queued">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,true},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"jidz9ijfn">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@index-.index_data_size">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,true},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"nogoqkvvk">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@index-.index_disk_size">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,true},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"n7p4o3vv8">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@index.index_ram_percent">>,
                                            true},
                                           {<<"@index.index_remaining_ram">>,
                                            true},
                                           {<<"@index-.index_data_size">>,
                                            true},
                                           {<<"@index-.index_disk_size">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"lv9ha6wam">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@fts-.fts_num_bytes_used_disk">>,
                                            true},
                                           {<<"@fts.fts_num_bytes_used_ram">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"hp8liq47s">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@fts-.@items.fts_total_queries">>,
                                            true},
                                           {<<"@fts-.@items.fts_total_queries_error">>,
                                            true},
                                           {<<"@fts-.@items.fts_total_queries_slow">>,
                                            true},
                                           {<<"@fts-.@items.fts_total_queries_timeout">>,
                                            true},
                                           {<<"@fts.fts_total_queries_rejected_by_herder">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"13xkxyd0f">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@cbas-.cbas_incoming_records_count">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,true},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"pgg7l1tz2">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@cbas-.cbas_failed_to_parse_records_count">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,true},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"ye7aqi8oo">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@cbas-.cbas_failed_to_parse_records_count_total">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,true},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"x4t0hojki">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@cbas.cbas_heap_memory_used_bytes">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,true},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"yomkjapa3">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@cbas.cbas_heap_memory_committed_bytes">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,true},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"gd3nl721k">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@cbas.cbas_thread_count">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,true},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"abs0sq9qm">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@cbas.cbas_disk_used_bytes_total">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,true},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"olv1eleqi">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@cbas.cbas_io_reads_total">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,true},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"goknl7ind">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@cbas.cbas_io_writes_total">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,true},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"s1z7ypkgc">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@cbas.cbas_system_load_average">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,true},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"22r3nufy5">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@cbas.cbas_pending_merge_ops">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,true},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"uqknixaoy">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@cbas.cbas_pending_flush_ops">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,true},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"bmqc6drl4">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@system.sysproc_mem_resident_java_cbas">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,true},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"hpe0a55tx">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@eventing.eventing_failed_count">>,
                                            true},
                                           {<<"@eventing.eventing_timeout_count">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"f3zcnd6rj">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@xdcr-.xdcr_changes_left_total">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,true},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"o2ts69dxs">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@xdcr-.@items.xdcr_changes_left_total">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,true},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"dblhe1for">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@xdcr-.@items.xdcr_wtavg_docs_latency_seconds">>,
                                            true},
                                           {<<"@xdcr-.@items.xdcr_wtavg_meta_latency_seconds">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"ju8mbgh0b">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@xdcr-.@items.xdcr_docs_failed_cr_source_total">>,
                                            true},
                                           {<<"@xdcr-.@items.xdcr_docs_failed_cr_target_total">>,
                                            true},
                                           {<<"@xdcr-.@items.xdcr_docs_filtered_total">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"pnpspa99w">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.kv_vb_active_num">>,true},
                                           {<<"@kv-.kv_vb_replica_num">>,true},
                                           {<<"@kv-.kv_vb_pending_num">>,true},
                                           {<<"@kv-.kv_ep_vb_total">>,true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"deqbnnxp3">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.kv_curr_items">>,true},
                                           {<<"@kv-.kv_vb_replica_curr_items">>,
                                            true},
                                           {<<"@kv-.kv_vb_pending_curr_items">>,
                                            true},
                                           {<<"@kv-.kv_curr_items_tot">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"cdryr96ee">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.kv_vb_active_resident_items_ratio">>,
                                            true},
                                           {<<"@kv-.kv_vb_replica_resident_items_ratio">>,
                                            true},
                                           {<<"@kv-.kv_vb_pending_resident_items_ratio">>,
                                            true},
                                           {<<"@kv-.kv_ep_resident_items_ratio">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"3tsnfxa13">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.kv_vb_active_ops_create">>,
                                            true},
                                           {<<"@kv-.kv_vb_replica_ops_create">>,
                                            true},
                                           {<<"@kv-.kv_vb_pending_ops_create">>,
                                            true},
                                           {<<"@kv-.kv_ep_ops_create">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"8fc2hkons">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.kv_vb_active_eject">>,
                                            true},
                                           {<<"@kv-.kv_vb_replica_eject">>,
                                            true},
                                           {<<"@kv-.kv_vb_pending_eject">>,
                                            true},
                                           {<<"@kv-.kv_ep_num_value_ejects">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"mdbgi8bjj">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.kv_vb_active_itm_memory_bytes">>,
                                            true},
                                           {<<"@kv-.kv_vb_replica_itm_memory_bytes">>,
                                            true},
                                           {<<"@kv-.kv_vb_pending_itm_memory_bytes">>,
                                            true},
                                           {<<"@kv-.kv_memory_used_bytes">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"egmkytqyb">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.kv_vb_active_meta_data_memory_bytes">>,
                                            true},
                                           {<<"@kv-.kv_vb_replica_meta_data_memory_bytes">>,
                                            true},
                                           {<<"@kv-.kv_vb_pending_meta_data_memory_bytes">>,
                                            true},
                                           {<<"@kv-.kv_ep_meta_data_memory_bytes">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"b62ov7bzk">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.kv_dcp_connection_count_views+indexes">>,
                                            true},
                                           {<<"@kv-.kv_dcp_connection_count_cbas">>,
                                            true},
                                           {<<"@kv-.kv_dcp_connection_count_replication">>,
                                            true},
                                           {<<"@kv-.kv_dcp_connection_count_xdcr">>,
                                            true},
                                           {<<"@kv-.kv_dcp_connection_count_eventing">>,
                                            true},
                                           {<<"@kv-.kv_dcp_connection_count_other">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"xakh5iyng">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.kv_dcp_producer_count_views+indexes">>,
                                            true},
                                           {<<"@kv-.kv_dcp_producer_count_cbas">>,
                                            true},
                                           {<<"@kv-.kv_dcp_producer_count_replication">>,
                                            true},
                                           {<<"@kv-.kv_dcp_producer_count_xdcr">>,
                                            true},
                                           {<<"@kv-.kv_dcp_producer_count_eventing">>,
                                            true},
                                           {<<"@kv-.kv_dcp_producer_count_other">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"11874rd3s">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.kv_dcp_items_remaining_views+indexes">>,
                                            true},
                                           {<<"@kv-.kv_dcp_items_remaining_cbas">>,
                                            true},
                                           {<<"@kv-.kv_dcp_items_remaining_replication">>,
                                            true},
                                           {<<"@kv-.kv_dcp_items_remaining_xdcr">>,
                                            true},
                                           {<<"@kv-.kv_dcp_items_remaining_eventing">>,
                                            true},
                                           {<<"@kv-.kv_dcp_items_remaining_other">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"dwuy10do6">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.kv_dcp_items_sent_views+indexes">>,
                                            true},
                                           {<<"@kv-.kv_dcp_items_sent_cbas">>,
                                            true},
                                           {<<"@kv-.kv_dcp_items_sent_replication">>,
                                            true},
                                           {<<"@kv-.kv_dcp_items_sent_xdcr">>,
                                            true},
                                           {<<"@kv-.kv_dcp_items_sent_eventing">>,
                                            true},
                                           {<<"@kv-.kv_dcp_items_sent_other">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"au2z85yrz">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.kv_dcp_total_data_size_bytes_views+indexes">>,
                                            true},
                                           {<<"@kv-.kv_dcp_total_data_size_bytes_cbas">>,
                                            true},
                                           {<<"@kv-.kv_dcp_total_data_size_bytes_replication">>,
                                            true},
                                           {<<"@kv-.kv_dcp_total_data_size_bytes_xdcr">>,
                                            true},
                                           {<<"@kv-.kv_dcp_total_data_size_bytes_eventing">>,
                                            true},
                                           {<<"@kv-.kv_dcp_total_data_size_bytes_other">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"ldwxy7sqz">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.kv_dcp_backoff_views+indexes">>,
                                            true},
                                           {<<"@kv-.kv_dcp_backoff_cbas">>,
                                            true},
                                           {<<"@kv-.kv_dcp_backoff_replication">>,
                                            true},
                                           {<<"@kv-.kv_dcp_backoff_xdcr">>,
                                            true},
                                           {<<"@kv-.kv_dcp_backoff_eventing">>,
                                            true},
                                           {<<"@kv-.kv_dcp_backoff_other">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"zn3jkmlk4">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.kv_ep_diskqueue_fill">>,
                                            true},
                                           {<<"@kv-.kv_ep_diskqueue_drain">>,
                                            true},
                                           {<<"@kv-.kv_ep_diskqueue_items">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"56zfuc9ht">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.kv_vb_active_queue_fill">>,
                                            true},
                                           {<<"@kv-.kv_vb_active_queue_drain">>,
                                            true},
                                           {<<"@kv-.kv_vb_active_queue_size">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"ativ0hd0v">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.kv_vb_replica_queue_fill">>,
                                            true},
                                           {<<"@kv-.kv_vb_replica_queue_drain">>,
                                            true},
                                           {<<"@kv-.kv_vb_replica_queue_size">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"uel8bwemw">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.kv_vb_pending_queue_fill">>,
                                            true},
                                           {<<"@kv-.kv_vb_pending_queue_drain">>,
                                            true},
                                           {<<"@kv-.kv_vb_pending_queue_size">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"lvam0tvln">>}]}]}]}},
                         {identity,{[{domain,builtin},
                                     {user,<<"<ud>admin</ud>">>}]}}]
[ns_server:debug,2022-09-08T12:12:08.352Z,ns_1@127.0.0.1:users_storage<0.340.0>:replicated_storage:handle_call:107]Writing interactively saved doc {docv2,
                                    {ui_profile,{"<ud>admin</ud>",admin}},
                                    {[{<<"version">>,458753},
                                      {<<"scenarios">>,
                                       [{[{<<"name">>,<<"Cluster Overview">>},
                                          {<<"uiid">>,
                                           <<"mn-cluster-overview">>},
                                          {<<"desc">>,
                                           <<"Stats showing the general health of your cluster. Customize and/or make your own dashboard with \"new dashboard... \" below.">>},
                                          {<<"groups">>,
                                           [<<"11rx2q3dq">>,<<"0l17jptaz">>]},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"hq5gtf09a">>}]},
                                        {[{<<"name">>,<<"All Services">>},
                                          {<<"uiid">>,<<"mn-all-services">>},
                                          {<<"desc">>,
                                           <<"Most common stats, arranged per service. Customize and/or make your own dashboard with \"new dashboard... \" below.">>},
                                          {<<"groups">>,
                                           [<<"hoywddh35">>,<<"6hvu5jnx8">>,
                                            <<"ndtqn252p">>,<<"cf1bvg1sr">>,
                                            <<"fntj7a633">>,<<"afzuv5i81">>,
                                            <<"rmkds99te">>,<<"gwe664c43">>,
                                            <<"9pll8tqmu">>,<<"hlsgofi6p">>]},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"dl7q7bzx5">>}]}]},
                                      {<<"groups">>,
                                       [{[{<<"name">>,<<"Cluster Overview">>},
                                          {<<"uiid">>,
                                           <<"mn-cluster-overview-group">>},
                                          {<<"charts">>,
                                           [<<"9ogsfigym">>,<<"npt3v11wn">>,
                                            <<"mlzthzdj8">>,<<"kjqc7gszc">>,
                                            <<"77ebkjk67">>,<<"ztrhk4daf">>,
                                            <<"scgygdlbb">>,<<"in36w18a8">>]},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"11rx2q3dq">>}]},
                                        {[{<<"name">>,<<"Node Resources">>},
                                          {<<"charts">>,
                                           [<<"f3qns6gtf">>,<<"e75f64p1s">>,
                                            <<"g6cojdhji">>,<<"gk6oc5z0a">>]},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"0l17jptaz">>}]},
                                        {[{<<"name">>,
                                           <<"Data (Docs/Views/XDCR)">>},
                                          {<<"uiid">>,
                                           <<"mn-all-services-data-group">>},
                                          {<<"charts">>,
                                           [<<"fgmr2a75f">>,<<"ny2j9a7yr">>,
                                            <<"atm5mcjnv">>,<<"987tcav28">>,
                                            <<"9sqintxmy">>]},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"hoywddh35">>}]},
                                        {[{<<"name">>,<<"Query">>},
                                          {<<"charts">>,
                                           [<<"n9yk6ymll">>,<<"bfbbc4xz9">>,
                                            <<"rpfv85c41">>,<<"3tfm7aosv">>,
                                            <<"bhneyfyh0">>]},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"6hvu5jnx8">>}]},
                                        {[{<<"name">>,<<"Index">>},
                                          {<<"charts">>,
                                           [<<"gjd29lmz9">>,<<"jidz9ijfn">>,
                                            <<"nogoqkvvk">>,<<"n7p4o3vv8">>,
                                            <<"lv9ha6wam">>]},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"ndtqn252p">>}]},
                                        {[{<<"name">>,<<"Search">>},
                                          {<<"charts">>,
                                           [<<"hp8liq47s">>,<<"13xkxyd0f">>]},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"cf1bvg1sr">>}]},
                                        {[{<<"name">>,<<"Analytics">>},
                                          {<<"enterprise">>,true},
                                          {<<"charts">>,
                                           [<<"pgg7l1tz2">>,<<"ye7aqi8oo">>,
                                            <<"x4t0hojki">>,<<"yomkjapa3">>,
                                            <<"gd3nl721k">>,<<"abs0sq9qm">>,
                                            <<"olv1eleqi">>,<<"goknl7ind">>,
                                            <<"s1z7ypkgc">>,<<"22r3nufy5">>,
                                            <<"uqknixaoy">>,<<"bmqc6drl4">>,
                                            <<"hpe0a55tx">>]},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"fntj7a633">>}]},
                                        {[{<<"name">>,<<"Eventing">>},
                                          {<<"enterprise">>,true},
                                          {<<"charts">>,[<<"f3zcnd6rj">>]},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"afzuv5i81">>}]},
                                        {[{<<"name">>,<<"XDCR">>},
                                          {<<"charts">>,
                                           [<<"o2ts69dxs">>,<<"dblhe1for">>,
                                            <<"ju8mbgh0b">>,<<"pnpspa99w">>]},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"rmkds99te">>}]},
                                        {[{<<"name">>,<<"vBucket Resources">>},
                                          {<<"charts">>,
                                           [<<"deqbnnxp3">>,<<"cdryr96ee">>,
                                            <<"3tsnfxa13">>,<<"8fc2hkons">>,
                                            <<"mdbgi8bjj">>,<<"egmkytqyb">>,
                                            <<"b62ov7bzk">>]},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"gwe664c43">>}]},
                                        {[{<<"name">>,<<"DCP Queues">>},
                                          {<<"charts">>,
                                           [<<"xakh5iyng">>,<<"11874rd3s">>,
                                            <<"dwuy10do6">>,<<"au2z85yrz">>,
                                            <<"ldwxy7sqz">>,<<"zn3jkmlk4">>]},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"9pll8tqmu">>}]},
                                        {[{<<"name">>,<<"Disk Queues">>},
                                          {<<"charts">>,
                                           [<<"56zfuc9ht">>,<<"ativ0hd0v">>,
                                            <<"uel8bwemw">>,<<"lvam0tvln">>]},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"hlsgofi6p">>}]}]},
                                      {<<"charts">>,
                                       [{[{<<"stats">>,
                                           {[{<<"@kv-.kv_ops">>,true},
                                             {<<"@query.n1ql_requests">>,true},
                                             {<<"@fts-.@items.fts_total_queries">>,
                                              true},
                                             {<<"@kv-.kv_ep_tmp_oom_errors">>,
                                              true},
                                             {<<"@kv-.kv_ep_cache_miss_ratio">>,
                                              true},
                                             {<<"@kv-.kv_cmd_get">>,true},
                                             {<<"@kv-.kv_cmd_set">>,true},
                                             {<<"@kv-.kv_delete_hits">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"9ogsfigym">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@kv-.kv_mem_used_bytes">>,
                                              true},
                                             {<<"@kv-.kv_ep_mem_low_wat">>,
                                              true},
                                             {<<"@kv-.kv_ep_mem_high_wat">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"npt3v11wn">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@kv-.kv_curr_items">>,true},
                                             {<<"@kv-.kv_vb_replica_curr_items">>,
                                              true},
                                             {<<"@kv-.kv_vb_active_resident_items_ratio">>,
                                              true},
                                             {<<"@kv-.kv_vb_replica_resident_items_ratio">>,
                                              true},
                                             {<<"@kv-.couch_docs_fragmentation">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"mlzthzdj8">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@kv-.kv_disk_write_queue">>,
                                              true},
                                             {<<"@kv-.couch_docs_actual_disk_size">>,
                                              true}]}},
                                          {<<"size">>,<<"small">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"kjqc7gszc">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@kv-.kv_dcp_items_remaining_replication">>,
                                              true}]}},
                                          {<<"size">>,<<"small">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"77ebkjk67">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@kv-.kv_ep_data_read_failed">>,
                                              true},
                                             {<<"@kv-.kv_ep_data_write_failed">>,
                                              true},
                                             {<<"@query.n1ql_errors">>,true},
                                             {<<"@fts-.@items.fts_total_queries_error">>,
                                              true},
                                             {<<"@eventing.eventing_failed_count">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"ztrhk4daf">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@query.n1ql_requests_250ms">>,
                                              true},
                                             {<<"@query.n1ql_requests_500ms">>,
                                              true},
                                             {<<"@query.n1ql_requests_1000ms">>,
                                              true},
                                             {<<"@query.n1ql_requests_5000ms">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"scgygdlbb">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@xdcr-.xdcr_changes_left_total">>,
                                              true},
                                             {<<"@index-.@items.index_num_docs_pending_and_queued">>,
                                              true},
                                             {<<"@fts-.@items.fts_num_mutations_to_index">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"in36w18a8">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@system.sys_cpu_utilization_rate">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,true},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"f3qns6gtf">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@system.cm_rest_request_leaves_total">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,true},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"e75f64p1s">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@system.sys_mem_actual_free">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"g6cojdhji">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@system.sys_swap_used">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,true},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"gk6oc5z0a">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@kv-.kv_mem_used_bytes">>,
                                              true},
                                             {<<"@kv-.kv_ep_mem_low_wat">>,
                                              true},
                                             {<<"@kv-.kv_ep_mem_high_wat">>,
                                              true},
                                             {<<"@kv-.kv_memory_used_bytes">>,
                                              true},
                                             {<<"@kv-.kv_ep_meta_data_memory_bytes">>,
                                              true},
                                             {<<"@kv-.kv_vb_active_resident_items_ratio">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"fgmr2a75f">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@kv-.kv_ops">>,true},
                                             {<<"@kv-.kv_ep_cache_miss_ratio">>,
                                              true},
                                             {<<"@kv-.kv_cmd_get">>,true},
                                             {<<"@kv-.kv_cmd_set">>,true},
                                             {<<"@kv-.kv_delete_hits">>,true},
                                             {<<"@kv-.kv_ep_num_ops_set_meta">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"ny2j9a7yr">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@kv-.kv_dcp_items_remaining_views+indexes">>,
                                              true},
                                             {<<"@kv-.kv_dcp_items_remaining_cbas">>,
                                              true},
                                             {<<"@kv-.kv_dcp_items_remaining_replication">>,
                                              true},
                                             {<<"@kv-.kv_dcp_items_remaining_xdcr">>,
                                              true},
                                             {<<"@kv-.kv_dcp_items_remaining_eventing">>,
                                              true},
                                             {<<"@kv-.kv_dcp_items_remaining_other">>,
                                              true},
                                             {<<"@xdcr-.xdcr_changes_left_total">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"atm5mcjnv">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@kv-.kv_ep_bg_fetched">>,
                                              true},
                                             {<<"@kv-.kv_ep_data_read_failed">>,
                                              true},
                                             {<<"@kv-.kv_ep_data_write_failed">>,
                                              true},
                                             {<<"@kv-.kv_ep_ops_create">>,
                                              true},
                                             {<<"@kv-.kv_ep_ops_update">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"987tcav28">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@kv-.kv_ep_diskqueue_items">>,
                                              true}]}},
                                          {<<"size">>,<<"small">>},
                                          {<<"specificStat">>,true},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"9sqintxmy">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@query.n1ql_requests_1000ms">>,
                                              true},
                                             {<<"@query.n1ql_requests_500ms">>,
                                              true},
                                             {<<"@query.n1ql_requests_5000ms">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"n9yk6ymll">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@query.n1ql_selects">>,true},
                                             {<<"@query.n1ql_requests">>,true},
                                             {<<"@query.n1ql_warnings">>,true},
                                             {<<"@query.n1ql_invalid_requests">>,
                                              true},
                                             {<<"@query.n1ql_errors">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"bfbbc4xz9">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@query.n1ql_avg_req_time">>,
                                              true},
                                             {<<"@query.n1ql_avg_svc_time">>,
                                              true}]}},
                                          {<<"size">>,<<"small">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"rpfv85c41">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@query.n1ql_avg_result_count">>,
                                              true}]}},
                                          {<<"size">>,<<"small">>},
                                          {<<"specificStat">>,true},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"3tfm7aosv">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@query.n1ql_avg_response_size">>,
                                              true}]}},
                                          {<<"size">>,<<"small">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"bhneyfyh0">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@index-.index_num_rows_returned">>,
                                              true}]}},
                                          {<<"size">>,<<"small">>},
                                          {<<"specificStat">>,true},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"gjd29lmz9">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@index-.@items.index_num_docs_pending_and_queued">>,
                                              true}]}},
                                          {<<"size">>,<<"small">>},
                                          {<<"specificStat">>,true},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"jidz9ijfn">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@index-.index_data_size">>,
                                              true}]}},
                                          {<<"size">>,<<"small">>},
                                          {<<"specificStat">>,true},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"nogoqkvvk">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@index-.index_disk_size">>,
                                              true}]}},
                                          {<<"size">>,<<"small">>},
                                          {<<"specificStat">>,true},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"n7p4o3vv8">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@index.index_ram_percent">>,
                                              true},
                                             {<<"@index.index_remaining_ram">>,
                                              true},
                                             {<<"@index-.index_data_size">>,
                                              true},
                                             {<<"@index-.index_disk_size">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"lv9ha6wam">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@fts-.fts_num_bytes_used_disk">>,
                                              true},
                                             {<<"@fts.fts_num_bytes_used_ram">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"hp8liq47s">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@fts-.@items.fts_total_queries">>,
                                              true},
                                             {<<"@fts-.@items.fts_total_queries_error">>,
                                              true},
                                             {<<"@fts-.@items.fts_total_queries_slow">>,
                                              true},
                                             {<<"@fts-.@items.fts_total_queries_timeout">>,
                                              true},
                                             {<<"@fts.fts_total_queries_rejected_by_herder">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"13xkxyd0f">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@cbas-.cbas_incoming_records_count">>,
                                              true}]}},
                                          {<<"size">>,<<"small">>},
                                          {<<"specificStat">>,true},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"pgg7l1tz2">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@cbas-.cbas_failed_to_parse_records_count">>,
                                              true}]}},
                                          {<<"size">>,<<"small">>},
                                          {<<"specificStat">>,true},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"ye7aqi8oo">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@cbas-.cbas_failed_to_parse_records_count_total">>,
                                              true}]}},
                                          {<<"size">>,<<"small">>},
                                          {<<"specificStat">>,true},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"x4t0hojki">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@cbas.cbas_heap_memory_used_bytes">>,
                                              true}]}},
                                          {<<"size">>,<<"small">>},
                                          {<<"specificStat">>,true},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"yomkjapa3">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@cbas.cbas_heap_memory_committed_bytes">>,
                                              true}]}},
                                          {<<"size">>,<<"small">>},
                                          {<<"specificStat">>,true},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"gd3nl721k">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@cbas.cbas_thread_count">>,
                                              true}]}},
                                          {<<"size">>,<<"small">>},
                                          {<<"specificStat">>,true},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"abs0sq9qm">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@cbas.cbas_disk_used_bytes_total">>,
                                              true}]}},
                                          {<<"size">>,<<"small">>},
                                          {<<"specificStat">>,true},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"olv1eleqi">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@cbas.cbas_io_reads_total">>,
                                              true}]}},
                                          {<<"size">>,<<"small">>},
                                          {<<"specificStat">>,true},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"goknl7ind">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@cbas.cbas_io_writes_total">>,
                                              true}]}},
                                          {<<"size">>,<<"small">>},
                                          {<<"specificStat">>,true},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"s1z7ypkgc">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@cbas.cbas_system_load_average">>,
                                              true}]}},
                                          {<<"size">>,<<"small">>},
                                          {<<"specificStat">>,true},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"22r3nufy5">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@cbas.cbas_pending_merge_ops">>,
                                              true}]}},
                                          {<<"size">>,<<"small">>},
                                          {<<"specificStat">>,true},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"uqknixaoy">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@cbas.cbas_pending_flush_ops">>,
                                              true}]}},
                                          {<<"size">>,<<"small">>},
                                          {<<"specificStat">>,true},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"bmqc6drl4">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@system.sysproc_mem_resident_java_cbas">>,
                                              true}]}},
                                          {<<"size">>,<<"small">>},
                                          {<<"specificStat">>,true},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"hpe0a55tx">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@eventing.eventing_failed_count">>,
                                              true},
                                             {<<"@eventing.eventing_timeout_count">>,
                                              true}]}},
                                          {<<"size">>,<<"small">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"f3zcnd6rj">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@xdcr-.xdcr_changes_left_total">>,
                                              true}]}},
                                          {<<"size">>,<<"small">>},
                                          {<<"specificStat">>,true},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"o2ts69dxs">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@xdcr-.@items.xdcr_changes_left_total">>,
                                              true}]}},
                                          {<<"size">>,<<"small">>},
                                          {<<"specificStat">>,true},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"dblhe1for">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@xdcr-.@items.xdcr_wtavg_docs_latency_seconds">>,
                                              true},
                                             {<<"@xdcr-.@items.xdcr_wtavg_meta_latency_seconds">>,
                                              true}]}},
                                          {<<"size">>,<<"small">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"ju8mbgh0b">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@xdcr-.@items.xdcr_docs_failed_cr_source_total">>,
                                              true},
                                             {<<"@xdcr-.@items.xdcr_docs_failed_cr_target_total">>,
                                              true},
                                             {<<"@xdcr-.@items.xdcr_docs_filtered_total">>,
                                              true}]}},
                                          {<<"size">>,<<"small">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"pnpspa99w">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@kv-.kv_vb_active_num">>,
                                              true},
                                             {<<"@kv-.kv_vb_replica_num">>,
                                              true},
                                             {<<"@kv-.kv_vb_pending_num">>,
                                              true},
                                             {<<"@kv-.kv_ep_vb_total">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"deqbnnxp3">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@kv-.kv_curr_items">>,true},
                                             {<<"@kv-.kv_vb_replica_curr_items">>,
                                              true},
                                             {<<"@kv-.kv_vb_pending_curr_items">>,
                                              true},
                                             {<<"@kv-.kv_curr_items_tot">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"cdryr96ee">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@kv-.kv_vb_active_resident_items_ratio">>,
                                              true},
                                             {<<"@kv-.kv_vb_replica_resident_items_ratio">>,
                                              true},
                                             {<<"@kv-.kv_vb_pending_resident_items_ratio">>,
                                              true},
                                             {<<"@kv-.kv_ep_resident_items_ratio">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"3tsnfxa13">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@kv-.kv_vb_active_ops_create">>,
                                              true},
                                             {<<"@kv-.kv_vb_replica_ops_create">>,
                                              true},
                                             {<<"@kv-.kv_vb_pending_ops_create">>,
                                              true},
                                             {<<"@kv-.kv_ep_ops_create">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"8fc2hkons">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@kv-.kv_vb_active_eject">>,
                                              true},
                                             {<<"@kv-.kv_vb_replica_eject">>,
                                              true},
                                             {<<"@kv-.kv_vb_pending_eject">>,
                                              true},
                                             {<<"@kv-.kv_ep_num_value_ejects">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"mdbgi8bjj">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@kv-.kv_vb_active_itm_memory_bytes">>,
                                              true},
                                             {<<"@kv-.kv_vb_replica_itm_memory_bytes">>,
                                              true},
                                             {<<"@kv-.kv_vb_pending_itm_memory_bytes">>,
                                              true},
                                             {<<"@kv-.kv_memory_used_bytes">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"egmkytqyb">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@kv-.kv_vb_active_meta_data_memory_bytes">>,
                                              true},
                                             {<<"@kv-.kv_vb_replica_meta_data_memory_bytes">>,
                                              true},
                                             {<<"@kv-.kv_vb_pending_meta_data_memory_bytes">>,
                                              true},
                                             {<<"@kv-.kv_ep_meta_data_memory_bytes">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"b62ov7bzk">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@kv-.kv_dcp_connection_count_views+indexes">>,
                                              true},
                                             {<<"@kv-.kv_dcp_connection_count_cbas">>,
                                              true},
                                             {<<"@kv-.kv_dcp_connection_count_replication">>,
                                              true},
                                             {<<"@kv-.kv_dcp_connection_count_xdcr">>,
                                              true},
                                             {<<"@kv-.kv_dcp_connection_count_eventing">>,
                                              true},
                                             {<<"@kv-.kv_dcp_connection_count_other">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"xakh5iyng">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@kv-.kv_dcp_producer_count_views+indexes">>,
                                              true},
                                             {<<"@kv-.kv_dcp_producer_count_cbas">>,
                                              true},
                                             {<<"@kv-.kv_dcp_producer_count_replication">>,
                                              true},
                                             {<<"@kv-.kv_dcp_producer_count_xdcr">>,
                                              true},
                                             {<<"@kv-.kv_dcp_producer_count_eventing">>,
                                              true},
                                             {<<"@kv-.kv_dcp_producer_count_other">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"11874rd3s">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@kv-.kv_dcp_items_remaining_views+indexes">>,
                                              true},
                                             {<<"@kv-.kv_dcp_items_remaining_cbas">>,
                                              true},
                                             {<<"@kv-.kv_dcp_items_remaining_replication">>,
                                              true},
                                             {<<"@kv-.kv_dcp_items_remaining_xdcr">>,
                                              true},
                                             {<<"@kv-.kv_dcp_items_remaining_eventing">>,
                                              true},
                                             {<<"@kv-.kv_dcp_items_remaining_other">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"dwuy10do6">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@kv-.kv_dcp_items_sent_views+indexes">>,
                                              true},
                                             {<<"@kv-.kv_dcp_items_sent_cbas">>,
                                              true},
                                             {<<"@kv-.kv_dcp_items_sent_replication">>,
                                              true},
                                             {<<"@kv-.kv_dcp_items_sent_xdcr">>,
                                              true},
                                             {<<"@kv-.kv_dcp_items_sent_eventing">>,
                                              true},
                                             {<<"@kv-.kv_dcp_items_sent_other">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"au2z85yrz">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@kv-.kv_dcp_total_data_size_bytes_views+indexes">>,
                                              true},
                                             {<<"@kv-.kv_dcp_total_data_size_bytes_cbas">>,
                                              true},
                                             {<<"@kv-.kv_dcp_total_data_size_bytes_replication">>,
                                              true},
                                             {<<"@kv-.kv_dcp_total_data_size_bytes_xdcr">>,
                                              true},
                                             {<<"@kv-.kv_dcp_total_data_size_bytes_eventing">>,
                                              true},
                                             {<<"@kv-.kv_dcp_total_data_size_bytes_other">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"ldwxy7sqz">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@kv-.kv_dcp_backoff_views+indexes">>,
                                              true},
                                             {<<"@kv-.kv_dcp_backoff_cbas">>,
                                              true},
                                             {<<"@kv-.kv_dcp_backoff_replication">>,
                                              true},
                                             {<<"@kv-.kv_dcp_backoff_xdcr">>,
                                              true},
                                             {<<"@kv-.kv_dcp_backoff_eventing">>,
                                              true},
                                             {<<"@kv-.kv_dcp_backoff_other">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"zn3jkmlk4">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@kv-.kv_ep_diskqueue_fill">>,
                                              true},
                                             {<<"@kv-.kv_ep_diskqueue_drain">>,
                                              true},
                                             {<<"@kv-.kv_ep_diskqueue_items">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"56zfuc9ht">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@kv-.kv_vb_active_queue_fill">>,
                                              true},
                                             {<<"@kv-.kv_vb_active_queue_drain">>,
                                              true},
                                             {<<"@kv-.kv_vb_active_queue_size">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"ativ0hd0v">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@kv-.kv_vb_replica_queue_fill">>,
                                              true},
                                             {<<"@kv-.kv_vb_replica_queue_drain">>,
                                              true},
                                             {<<"@kv-.kv_vb_replica_queue_size">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"uel8bwemw">>}]},
                                        {[{<<"stats">>,
                                           {[{<<"@kv-.kv_vb_pending_queue_fill">>,
                                              true},
                                             {<<"@kv-.kv_vb_pending_queue_drain">>,
                                              true},
                                             {<<"@kv-.kv_vb_pending_queue_size">>,
                                              true}]}},
                                          {<<"size">>,<<"medium">>},
                                          {<<"specificStat">>,false},
                                          {<<"preset">>,true},
                                          {<<"id">>,<<"lvam0tvln">>}]}]}]},
                                    [{rev,{3,<<204,234,24,28>>}},
                                     {deleted,false},
                                     {last_modified,1662639128350}]}
[ns_server:debug,2022-09-08T12:12:08.378Z,ns_1@127.0.0.1:ns_audit<0.579.0>:ns_audit:handle_call:148]Audit set_user_profile: [{local,{[{ip,<<"172.18.0.2">>},{port,8091}]}},
                         {remote,{[{ip,<<"172.18.0.1">>},{port,46818}]}},
                         {sessionid,<<"8d72771539648f7a1bc57f49b878927e398368fd">>},
                         {real_userid,{[{domain,builtin},
                                        {user,<<"<ud>admin</ud>">>}]}},
                         {timestamp,<<"2022-09-08T12:12:08.374Z">>},
                         {profile,{[{<<"version">>,458753},
                                    {<<"scenarios">>,
                                     [{[{<<"name">>,<<"Cluster Overview">>},
                                        {<<"uiid">>,<<"mn-cluster-overview">>},
                                        {<<"desc">>,
                                         <<"Stats showing the general health of your cluster. Customize and/or make your own dashboard with \"new dashboard... \" below.">>},
                                        {<<"groups">>,
                                         [<<"11rx2q3dq">>,<<"0l17jptaz">>]},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"hq5gtf09a">>}]},
                                      {[{<<"name">>,<<"All Services">>},
                                        {<<"uiid">>,<<"mn-all-services">>},
                                        {<<"desc">>,
                                         <<"Most common stats, arranged per service. Customize and/or make your own dashboard with \"new dashboard... \" below.">>},
                                        {<<"groups">>,
                                         [<<"hoywddh35">>,<<"6hvu5jnx8">>,
                                          <<"ndtqn252p">>,<<"cf1bvg1sr">>,
                                          <<"fntj7a633">>,<<"afzuv5i81">>,
                                          <<"rmkds99te">>,<<"gwe664c43">>,
                                          <<"9pll8tqmu">>,<<"hlsgofi6p">>]},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"dl7q7bzx5">>}]}]},
                                    {<<"groups">>,
                                     [{[{<<"name">>,<<"Cluster Overview">>},
                                        {<<"uiid">>,
                                         <<"mn-cluster-overview-group">>},
                                        {<<"charts">>,
                                         [<<"9ogsfigym">>,<<"npt3v11wn">>,
                                          <<"mlzthzdj8">>,<<"kjqc7gszc">>,
                                          <<"77ebkjk67">>,<<"ztrhk4daf">>,
                                          <<"scgygdlbb">>,<<"in36w18a8">>]},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"11rx2q3dq">>}]},
                                      {[{<<"name">>,<<"Node Resources">>},
                                        {<<"charts">>,
                                         [<<"f3qns6gtf">>,<<"e75f64p1s">>,
                                          <<"g6cojdhji">>,<<"gk6oc5z0a">>]},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"0l17jptaz">>}]},
                                      {[{<<"name">>,
                                         <<"Data (Docs/Views/XDCR)">>},
                                        {<<"uiid">>,
                                         <<"mn-all-services-data-group">>},
                                        {<<"charts">>,
                                         [<<"fgmr2a75f">>,<<"ny2j9a7yr">>,
                                          <<"atm5mcjnv">>,<<"987tcav28">>,
                                          <<"9sqintxmy">>]},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"hoywddh35">>}]},
                                      {[{<<"name">>,<<"Query">>},
                                        {<<"charts">>,
                                         [<<"n9yk6ymll">>,<<"bfbbc4xz9">>,
                                          <<"rpfv85c41">>,<<"3tfm7aosv">>,
                                          <<"bhneyfyh0">>]},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"6hvu5jnx8">>}]},
                                      {[{<<"name">>,<<"Index">>},
                                        {<<"charts">>,
                                         [<<"gjd29lmz9">>,<<"jidz9ijfn">>,
                                          <<"nogoqkvvk">>,<<"n7p4o3vv8">>,
                                          <<"lv9ha6wam">>]},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"ndtqn252p">>}]},
                                      {[{<<"name">>,<<"Search">>},
                                        {<<"charts">>,
                                         [<<"hp8liq47s">>,<<"13xkxyd0f">>]},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"cf1bvg1sr">>}]},
                                      {[{<<"name">>,<<"Analytics">>},
                                        {<<"enterprise">>,true},
                                        {<<"charts">>,
                                         [<<"pgg7l1tz2">>,<<"ye7aqi8oo">>,
                                          <<"x4t0hojki">>,<<"yomkjapa3">>,
                                          <<"gd3nl721k">>,<<"abs0sq9qm">>,
                                          <<"olv1eleqi">>,<<"goknl7ind">>,
                                          <<"s1z7ypkgc">>,<<"22r3nufy5">>,
                                          <<"uqknixaoy">>,<<"bmqc6drl4">>,
                                          <<"hpe0a55tx">>]},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"fntj7a633">>}]},
                                      {[{<<"name">>,<<"Eventing">>},
                                        {<<"enterprise">>,true},
                                        {<<"charts">>,[<<"f3zcnd6rj">>]},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"afzuv5i81">>}]},
                                      {[{<<"name">>,<<"XDCR">>},
                                        {<<"charts">>,
                                         [<<"o2ts69dxs">>,<<"dblhe1for">>,
                                          <<"ju8mbgh0b">>,<<"pnpspa99w">>]},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"rmkds99te">>}]},
                                      {[{<<"name">>,<<"vBucket Resources">>},
                                        {<<"charts">>,
                                         [<<"deqbnnxp3">>,<<"cdryr96ee">>,
                                          <<"3tsnfxa13">>,<<"8fc2hkons">>,
                                          <<"mdbgi8bjj">>,<<"egmkytqyb">>,
                                          <<"b62ov7bzk">>]},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"gwe664c43">>}]},
                                      {[{<<"name">>,<<"DCP Queues">>},
                                        {<<"charts">>,
                                         [<<"xakh5iyng">>,<<"11874rd3s">>,
                                          <<"dwuy10do6">>,<<"au2z85yrz">>,
                                          <<"ldwxy7sqz">>,<<"zn3jkmlk4">>]},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"9pll8tqmu">>}]},
                                      {[{<<"name">>,<<"Disk Queues">>},
                                        {<<"charts">>,
                                         [<<"56zfuc9ht">>,<<"ativ0hd0v">>,
                                          <<"uel8bwemw">>,<<"lvam0tvln">>]},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"hlsgofi6p">>}]}]},
                                    {<<"charts">>,
                                     [{[{<<"stats">>,
                                         {[{<<"@kv-.kv_ops">>,true},
                                           {<<"@query.n1ql_requests">>,true},
                                           {<<"@fts-.@items.fts_total_queries">>,
                                            true},
                                           {<<"@kv-.kv_ep_tmp_oom_errors">>,
                                            true},
                                           {<<"@kv-.kv_ep_cache_miss_ratio">>,
                                            true},
                                           {<<"@kv-.kv_cmd_get">>,true},
                                           {<<"@kv-.kv_cmd_set">>,true},
                                           {<<"@kv-.kv_delete_hits">>,true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"9ogsfigym">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.kv_mem_used_bytes">>,true},
                                           {<<"@kv-.kv_ep_mem_low_wat">>,true},
                                           {<<"@kv-.kv_ep_mem_high_wat">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"npt3v11wn">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.kv_curr_items">>,true},
                                           {<<"@kv-.kv_vb_replica_curr_items">>,
                                            true},
                                           {<<"@kv-.kv_vb_active_resident_items_ratio">>,
                                            true},
                                           {<<"@kv-.kv_vb_replica_resident_items_ratio">>,
                                            true},
                                           {<<"@kv-.couch_docs_fragmentation">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"mlzthzdj8">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.kv_disk_write_queue">>,
                                            true},
                                           {<<"@kv-.couch_docs_actual_disk_size">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"kjqc7gszc">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.kv_dcp_items_remaining_replication">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"77ebkjk67">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.kv_ep_data_read_failed">>,
                                            true},
                                           {<<"@kv-.kv_ep_data_write_failed">>,
                                            true},
                                           {<<"@query.n1ql_errors">>,true},
                                           {<<"@fts-.@items.fts_total_queries_error">>,
                                            true},
                                           {<<"@eventing.eventing_failed_count">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"ztrhk4daf">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@query.n1ql_requests_250ms">>,
                                            true},
                                           {<<"@query.n1ql_requests_500ms">>,
                                            true},
                                           {<<"@query.n1ql_requests_1000ms">>,
                                            true},
                                           {<<"@query.n1ql_requests_5000ms">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"scgygdlbb">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@xdcr-.xdcr_changes_left_total">>,
                                            true},
                                           {<<"@index-.@items.index_num_docs_pending_and_queued">>,
                                            true},
                                           {<<"@fts-.@items.fts_num_mutations_to_index">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"in36w18a8">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@system.sys_cpu_utilization_rate">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,true},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"f3qns6gtf">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@system.cm_rest_request_leaves_total">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,true},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"e75f64p1s">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@system.sys_mem_actual_free">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"g6cojdhji">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@system.sys_swap_used">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,true},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"gk6oc5z0a">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.kv_mem_used_bytes">>,true},
                                           {<<"@kv-.kv_ep_mem_low_wat">>,true},
                                           {<<"@kv-.kv_ep_mem_high_wat">>,
                                            true},
                                           {<<"@kv-.kv_memory_used_bytes">>,
                                            true},
                                           {<<"@kv-.kv_ep_meta_data_memory_bytes">>,
                                            true},
                                           {<<"@kv-.kv_vb_active_resident_items_ratio">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"fgmr2a75f">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.kv_ops">>,true},
                                           {<<"@kv-.kv_ep_cache_miss_ratio">>,
                                            true},
                                           {<<"@kv-.kv_cmd_get">>,true},
                                           {<<"@kv-.kv_cmd_set">>,true},
                                           {<<"@kv-.kv_delete_hits">>,true},
                                           {<<"@kv-.kv_ep_num_ops_set_meta">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"ny2j9a7yr">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.kv_dcp_items_remaining_views+indexes">>,
                                            true},
                                           {<<"@kv-.kv_dcp_items_remaining_cbas">>,
                                            true},
                                           {<<"@kv-.kv_dcp_items_remaining_replication">>,
                                            true},
                                           {<<"@kv-.kv_dcp_items_remaining_xdcr">>,
                                            true},
                                           {<<"@kv-.kv_dcp_items_remaining_eventing">>,
                                            true},
                                           {<<"@kv-.kv_dcp_items_remaining_other">>,
                                            true},
                                           {<<"@xdcr-.xdcr_changes_left_total">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"atm5mcjnv">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.kv_ep_bg_fetched">>,true},
                                           {<<"@kv-.kv_ep_data_read_failed">>,
                                            true},
                                           {<<"@kv-.kv_ep_data_write_failed">>,
                                            true},
                                           {<<"@kv-.kv_ep_ops_create">>,true},
                                           {<<"@kv-.kv_ep_ops_update">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"987tcav28">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.kv_ep_diskqueue_items">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,true},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"9sqintxmy">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@query.n1ql_requests_1000ms">>,
                                            true},
                                           {<<"@query.n1ql_requests_500ms">>,
                                            true},
                                           {<<"@query.n1ql_requests_5000ms">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"n9yk6ymll">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@query.n1ql_selects">>,true},
                                           {<<"@query.n1ql_requests">>,true},
                                           {<<"@query.n1ql_warnings">>,true},
                                           {<<"@query.n1ql_invalid_requests">>,
                                            true},
                                           {<<"@query.n1ql_errors">>,true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"bfbbc4xz9">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@query.n1ql_avg_req_time">>,
                                            true},
                                           {<<"@query.n1ql_avg_svc_time">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"rpfv85c41">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@query.n1ql_avg_result_count">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,true},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"3tfm7aosv">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@query.n1ql_avg_response_size">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"bhneyfyh0">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@index-.index_num_rows_returned">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,true},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"gjd29lmz9">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@index-.@items.index_num_docs_pending_and_queued">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,true},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"jidz9ijfn">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@index-.index_data_size">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,true},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"nogoqkvvk">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@index-.index_disk_size">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,true},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"n7p4o3vv8">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@index.index_ram_percent">>,
                                            true},
                                           {<<"@index.index_remaining_ram">>,
                                            true},
                                           {<<"@index-.index_data_size">>,
                                            true},
                                           {<<"@index-.index_disk_size">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"lv9ha6wam">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@fts-.fts_num_bytes_used_disk">>,
                                            true},
                                           {<<"@fts.fts_num_bytes_used_ram">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"hp8liq47s">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@fts-.@items.fts_total_queries">>,
                                            true},
                                           {<<"@fts-.@items.fts_total_queries_error">>,
                                            true},
                                           {<<"@fts-.@items.fts_total_queries_slow">>,
                                            true},
                                           {<<"@fts-.@items.fts_total_queries_timeout">>,
                                            true},
                                           {<<"@fts.fts_total_queries_rejected_by_herder">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"13xkxyd0f">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@cbas-.cbas_incoming_records_count">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,true},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"pgg7l1tz2">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@cbas-.cbas_failed_to_parse_records_count">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,true},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"ye7aqi8oo">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@cbas-.cbas_failed_to_parse_records_count_total">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,true},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"x4t0hojki">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@cbas.cbas_heap_memory_used_bytes">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,true},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"yomkjapa3">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@cbas.cbas_heap_memory_committed_bytes">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,true},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"gd3nl721k">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@cbas.cbas_thread_count">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,true},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"abs0sq9qm">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@cbas.cbas_disk_used_bytes_total">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,true},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"olv1eleqi">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@cbas.cbas_io_reads_total">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,true},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"goknl7ind">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@cbas.cbas_io_writes_total">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,true},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"s1z7ypkgc">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@cbas.cbas_system_load_average">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,true},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"22r3nufy5">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@cbas.cbas_pending_merge_ops">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,true},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"uqknixaoy">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@cbas.cbas_pending_flush_ops">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,true},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"bmqc6drl4">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@system.sysproc_mem_resident_java_cbas">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,true},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"hpe0a55tx">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@eventing.eventing_failed_count">>,
                                            true},
                                           {<<"@eventing.eventing_timeout_count">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"f3zcnd6rj">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@xdcr-.xdcr_changes_left_total">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,true},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"o2ts69dxs">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@xdcr-.@items.xdcr_changes_left_total">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,true},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"dblhe1for">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@xdcr-.@items.xdcr_wtavg_docs_latency_seconds">>,
                                            true},
                                           {<<"@xdcr-.@items.xdcr_wtavg_meta_latency_seconds">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"ju8mbgh0b">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@xdcr-.@items.xdcr_docs_failed_cr_source_total">>,
                                            true},
                                           {<<"@xdcr-.@items.xdcr_docs_failed_cr_target_total">>,
                                            true},
                                           {<<"@xdcr-.@items.xdcr_docs_filtered_total">>,
                                            true}]}},
                                        {<<"size">>,<<"small">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"pnpspa99w">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.kv_vb_active_num">>,true},
                                           {<<"@kv-.kv_vb_replica_num">>,true},
                                           {<<"@kv-.kv_vb_pending_num">>,true},
                                           {<<"@kv-.kv_ep_vb_total">>,true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"deqbnnxp3">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.kv_curr_items">>,true},
                                           {<<"@kv-.kv_vb_replica_curr_items">>,
                                            true},
                                           {<<"@kv-.kv_vb_pending_curr_items">>,
                                            true},
                                           {<<"@kv-.kv_curr_items_tot">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"cdryr96ee">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.kv_vb_active_resident_items_ratio">>,
                                            true},
                                           {<<"@kv-.kv_vb_replica_resident_items_ratio">>,
                                            true},
                                           {<<"@kv-.kv_vb_pending_resident_items_ratio">>,
                                            true},
                                           {<<"@kv-.kv_ep_resident_items_ratio">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"3tsnfxa13">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.kv_vb_active_ops_create">>,
                                            true},
                                           {<<"@kv-.kv_vb_replica_ops_create">>,
                                            true},
                                           {<<"@kv-.kv_vb_pending_ops_create">>,
                                            true},
                                           {<<"@kv-.kv_ep_ops_create">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"8fc2hkons">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.kv_vb_active_eject">>,
                                            true},
                                           {<<"@kv-.kv_vb_replica_eject">>,
                                            true},
                                           {<<"@kv-.kv_vb_pending_eject">>,
                                            true},
                                           {<<"@kv-.kv_ep_num_value_ejects">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"mdbgi8bjj">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.kv_vb_active_itm_memory_bytes">>,
                                            true},
                                           {<<"@kv-.kv_vb_replica_itm_memory_bytes">>,
                                            true},
                                           {<<"@kv-.kv_vb_pending_itm_memory_bytes">>,
                                            true},
                                           {<<"@kv-.kv_memory_used_bytes">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"egmkytqyb">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.kv_vb_active_meta_data_memory_bytes">>,
                                            true},
                                           {<<"@kv-.kv_vb_replica_meta_data_memory_bytes">>,
                                            true},
                                           {<<"@kv-.kv_vb_pending_meta_data_memory_bytes">>,
                                            true},
                                           {<<"@kv-.kv_ep_meta_data_memory_bytes">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"b62ov7bzk">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.kv_dcp_connection_count_views+indexes">>,
                                            true},
                                           {<<"@kv-.kv_dcp_connection_count_cbas">>,
                                            true},
                                           {<<"@kv-.kv_dcp_connection_count_replication">>,
                                            true},
                                           {<<"@kv-.kv_dcp_connection_count_xdcr">>,
                                            true},
                                           {<<"@kv-.kv_dcp_connection_count_eventing">>,
                                            true},
                                           {<<"@kv-.kv_dcp_connection_count_other">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"xakh5iyng">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.kv_dcp_producer_count_views+indexes">>,
                                            true},
                                           {<<"@kv-.kv_dcp_producer_count_cbas">>,
                                            true},
                                           {<<"@kv-.kv_dcp_producer_count_replication">>,
                                            true},
                                           {<<"@kv-.kv_dcp_producer_count_xdcr">>,
                                            true},
                                           {<<"@kv-.kv_dcp_producer_count_eventing">>,
                                            true},
                                           {<<"@kv-.kv_dcp_producer_count_other">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"11874rd3s">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.kv_dcp_items_remaining_views+indexes">>,
                                            true},
                                           {<<"@kv-.kv_dcp_items_remaining_cbas">>,
                                            true},
                                           {<<"@kv-.kv_dcp_items_remaining_replication">>,
                                            true},
                                           {<<"@kv-.kv_dcp_items_remaining_xdcr">>,
                                            true},
                                           {<<"@kv-.kv_dcp_items_remaining_eventing">>,
                                            true},
                                           {<<"@kv-.kv_dcp_items_remaining_other">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"dwuy10do6">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.kv_dcp_items_sent_views+indexes">>,
                                            true},
                                           {<<"@kv-.kv_dcp_items_sent_cbas">>,
                                            true},
                                           {<<"@kv-.kv_dcp_items_sent_replication">>,
                                            true},
                                           {<<"@kv-.kv_dcp_items_sent_xdcr">>,
                                            true},
                                           {<<"@kv-.kv_dcp_items_sent_eventing">>,
                                            true},
                                           {<<"@kv-.kv_dcp_items_sent_other">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"au2z85yrz">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.kv_dcp_total_data_size_bytes_views+indexes">>,
                                            true},
                                           {<<"@kv-.kv_dcp_total_data_size_bytes_cbas">>,
                                            true},
                                           {<<"@kv-.kv_dcp_total_data_size_bytes_replication">>,
                                            true},
                                           {<<"@kv-.kv_dcp_total_data_size_bytes_xdcr">>,
                                            true},
                                           {<<"@kv-.kv_dcp_total_data_size_bytes_eventing">>,
                                            true},
                                           {<<"@kv-.kv_dcp_total_data_size_bytes_other">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"ldwxy7sqz">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.kv_dcp_backoff_views+indexes">>,
                                            true},
                                           {<<"@kv-.kv_dcp_backoff_cbas">>,
                                            true},
                                           {<<"@kv-.kv_dcp_backoff_replication">>,
                                            true},
                                           {<<"@kv-.kv_dcp_backoff_xdcr">>,
                                            true},
                                           {<<"@kv-.kv_dcp_backoff_eventing">>,
                                            true},
                                           {<<"@kv-.kv_dcp_backoff_other">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"zn3jkmlk4">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.kv_ep_diskqueue_fill">>,
                                            true},
                                           {<<"@kv-.kv_ep_diskqueue_drain">>,
                                            true},
                                           {<<"@kv-.kv_ep_diskqueue_items">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"56zfuc9ht">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.kv_vb_active_queue_fill">>,
                                            true},
                                           {<<"@kv-.kv_vb_active_queue_drain">>,
                                            true},
                                           {<<"@kv-.kv_vb_active_queue_size">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"ativ0hd0v">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.kv_vb_replica_queue_fill">>,
                                            true},
                                           {<<"@kv-.kv_vb_replica_queue_drain">>,
                                            true},
                                           {<<"@kv-.kv_vb_replica_queue_size">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"uel8bwemw">>}]},
                                      {[{<<"stats">>,
                                         {[{<<"@kv-.kv_vb_pending_queue_fill">>,
                                            true},
                                           {<<"@kv-.kv_vb_pending_queue_drain">>,
                                            true},
                                           {<<"@kv-.kv_vb_pending_queue_size">>,
                                            true}]}},
                                        {<<"size">>,<<"medium">>},
                                        {<<"specificStat">>,false},
                                        {<<"preset">>,true},
                                        {<<"id">>,<<"lvam0tvln">>}]}]}]}},
                         {identity,{[{domain,builtin},
                                     {user,<<"<ud>admin</ud>">>}]}}]
[ns_server:debug,2022-09-08T12:12:08.453Z,ns_1@127.0.0.1:users_storage<0.340.0>:replicated_dets:select_from_table:281][ets] Starting select with {users_storage,
                               [{{docv2,{auth,'_'},'_','_'},[],['$_']}],
                               100}
[ns_server:debug,2022-09-08T12:12:12.535Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:12:12.536Z,ns_1@127.0.0.1:<0.15393.0>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:12:12.540Z,ns_1@127.0.0.1:<0.15395.0>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 429164, disk size is 12654727
[ns_server:debug,2022-09-08T12:12:12.540Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:12:12.540Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:12:12.788Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:12:12.794Z,ns_1@127.0.0.1:<0.15405.0>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:12:12.794Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:12:12.794Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:12:16.044Z,ns_1@127.0.0.1:ns_audit<0.579.0>:ns_audit:handle_call:148]Audit read_doc: [{local,{[{ip,<<"172.18.0.2">>},{port,8091}]}},
                 {remote,{[{ip,<<"172.18.0.1">>},{port,46944}]}},
                 {sessionid,<<"8d72771539648f7a1bc57f49b878927e398368fd">>},
                 {real_userid,{[{domain,builtin},
                                {user,<<"<ud>admin</ud>">>}]}},
                 {timestamp,<<"2022-09-08T12:12:16.044Z">>},
                 {doc_id,<<"<ud>14ba3dce-4616-4ea0-a486-b44347bcc927</ud>">>},
                 {bucket_name,<<"todo">>}]
[ns_server:debug,2022-09-08T12:12:16.045Z,ns_1@127.0.0.1:ns_audit<0.579.0>:ns_audit:handle_call:148]Audit read_doc: [{local,{[{ip,<<"172.18.0.2">>},{port,8091}]}},
                 {remote,{[{ip,<<"172.18.0.1">>},{port,46948}]}},
                 {sessionid,<<"8d72771539648f7a1bc57f49b878927e398368fd">>},
                 {real_userid,{[{domain,builtin},
                                {user,<<"<ud>admin</ud>">>}]}},
                 {timestamp,<<"2022-09-08T12:12:16.044Z">>},
                 {doc_id,<<"<ud>e317bcd3-d668-454b-b1ea-b420aed7def1</ud>">>},
                 {bucket_name,<<"todo">>}]
[ns_server:debug,2022-09-08T12:12:39.578Z,ns_1@127.0.0.1:ldap_auth_cache<0.334.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2022-09-08T12:12:42.541Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:12:42.626Z,ns_1@127.0.0.1:<0.16881.0>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:12:42.627Z,ns_1@127.0.0.1:<0.16883.0>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 429164, disk size is 12654727
[ns_server:debug,2022-09-08T12:12:42.627Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:12:42.627Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:12:42.795Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:12:42.798Z,ns_1@127.0.0.1:<0.16912.0>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:12:42.799Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:12:42.799Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:13:12.628Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:13:12.628Z,ns_1@127.0.0.1:<0.18355.0>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:13:12.629Z,ns_1@127.0.0.1:<0.18357.0>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 429164, disk size is 12654727
[ns_server:debug,2022-09-08T12:13:12.630Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:13:12.630Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:13:12.800Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:13:12.803Z,ns_1@127.0.0.1:<0.18364.0>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:13:12.803Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:13:12.803Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:13:42.631Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:13:42.632Z,ns_1@127.0.0.1:<0.19836.0>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:13:42.634Z,ns_1@127.0.0.1:<0.19838.0>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 429164, disk size is 12654727
[ns_server:debug,2022-09-08T12:13:42.634Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:13:42.634Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:13:42.804Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:13:42.810Z,ns_1@127.0.0.1:<0.19856.0>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:13:42.811Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:13:42.811Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:13:54.579Z,ns_1@127.0.0.1:ldap_auth_cache<0.334.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2022-09-08T12:14:12.635Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:14:12.635Z,ns_1@127.0.0.1:<0.21319.0>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:14:12.636Z,ns_1@127.0.0.1:<0.21321.0>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 429164, disk size is 12654727
[ns_server:debug,2022-09-08T12:14:12.637Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:14:12.637Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:14:12.813Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:14:12.816Z,ns_1@127.0.0.1:<0.21339.0>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:14:12.816Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:14:12.816Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:14:42.637Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:14:42.638Z,ns_1@127.0.0.1:<0.22807.0>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:14:42.641Z,ns_1@127.0.0.1:<0.22809.0>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 429164, disk size is 12654727
[ns_server:debug,2022-09-08T12:14:42.642Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:14:42.642Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:14:42.817Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:14:42.822Z,ns_1@127.0.0.1:<0.22828.0>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:14:42.824Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:14:42.824Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:15:09.580Z,ns_1@127.0.0.1:ldap_auth_cache<0.334.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2022-09-08T12:15:09.589Z,ns_1@127.0.0.1:roles_cache<0.345.0>:active_cache:renew:238]Starting roles_cache cache renewal
[ns_server:debug,2022-09-08T12:15:09.589Z,ns_1@127.0.0.1:roles_cache<0.345.0>:active_cache:renew:244]roles_cache cache renewal process originated 0 queries
[ns_server:debug,2022-09-08T12:15:12.643Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:15:12.644Z,ns_1@127.0.0.1:<0.24287.0>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:15:12.645Z,ns_1@127.0.0.1:<0.24289.0>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 429164, disk size is 12654727
[ns_server:debug,2022-09-08T12:15:12.645Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:15:12.645Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:15:12.825Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:15:12.829Z,ns_1@127.0.0.1:<0.24303.0>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:15:12.830Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:15:12.830Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:15:42.646Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:15:42.647Z,ns_1@127.0.0.1:<0.25781.0>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:15:42.648Z,ns_1@127.0.0.1:<0.25783.0>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 429545, disk size is 12658868
[ns_server:debug,2022-09-08T12:15:42.648Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:15:42.648Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:15:42.831Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:15:42.834Z,ns_1@127.0.0.1:<0.25793.0>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:15:42.835Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:15:42.835Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:16:12.649Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:16:12.650Z,ns_1@127.0.0.1:<0.27261.0>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:16:12.651Z,ns_1@127.0.0.1:<0.27263.0>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T12:16:12.651Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:16:12.651Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:16:12.836Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:16:12.840Z,ns_1@127.0.0.1:<0.27269.0>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:16:12.841Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:16:12.841Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:16:24.581Z,ns_1@127.0.0.1:ldap_auth_cache<0.334.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2022-09-08T12:16:42.652Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:16:42.653Z,ns_1@127.0.0.1:<0.28751.0>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:16:42.656Z,ns_1@127.0.0.1:<0.28753.0>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T12:16:42.656Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:16:42.656Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:16:42.842Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:16:42.848Z,ns_1@127.0.0.1:<0.28756.0>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:16:42.848Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:16:42.849Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:17:12.658Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:17:12.659Z,ns_1@127.0.0.1:<0.30234.0>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:17:12.662Z,ns_1@127.0.0.1:<0.30236.0>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T12:17:12.662Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:17:12.662Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:17:12.850Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:17:12.855Z,ns_1@127.0.0.1:<0.30237.0>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:17:12.856Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:17:12.856Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:17:39.559Z,ns_1@127.0.0.1:roles_cache<0.345.0>:active_cache:cleanup:258]Cache roles_cache cleanup: 0/0 records deleted
[ns_server:debug,2022-09-08T12:17:39.582Z,ns_1@127.0.0.1:ldap_auth_cache<0.334.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2022-09-08T12:17:42.306Z,ns_1@127.0.0.1:prometheus_cfg<0.391.0>:prometheus_cfg:maybe_update_scrape_dynamic_intervals:941]Recalculating prometheus scrape intervals for high cardinality metrics
[ns_server:debug,2022-09-08T12:17:42.319Z,ns_1@127.0.0.1:prometheus_cfg<0.391.0>:prometheus_cfg:maybe_update_scrape_dynamic_intervals:958]Scrape intervals haven't changed:
[]
CBCollect stats dir size estimation: 629596800
Calculated based on scrapes info:
[{backup,low_cardinality,0},
 {cbas,high_cardinality,0},
 {cbas,low_cardinality,14},
 {eventing,high_cardinality,0},
 {eventing,low_cardinality,1},
 {fts,high_cardinality,0},
 {fts,low_cardinality,25},
 {index,low_cardinality,2},
 {index,high_cardinality,24},
 {kv,low_cardinality,291},
 {kv,high_cardinality,1123},
 {n1ql,low_cardinality,33},
 {ns_server,low_cardinality,167},
 {ns_server,high_cardinality,330},
 {xdcr,low_cardinality,0}]
Raw intervals:
[]
[ns_server:debug,2022-09-08T12:17:42.663Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:17:42.664Z,ns_1@127.0.0.1:<0.31731.0>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:17:42.667Z,ns_1@127.0.0.1:<0.31733.0>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T12:17:42.667Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:17:42.667Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:17:42.857Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:17:42.862Z,ns_1@127.0.0.1:<0.31734.0>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:17:42.863Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:17:42.863Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:18:12.668Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:18:12.669Z,ns_1@127.0.0.1:<0.446.1>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:18:12.673Z,ns_1@127.0.0.1:<0.448.1>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T12:18:12.673Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:18:12.673Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:18:12.864Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:18:12.870Z,ns_1@127.0.0.1:<0.449.1>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:18:12.870Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:18:12.871Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:18:42.674Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:18:42.675Z,ns_1@127.0.0.1:<0.1937.1>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:18:42.677Z,ns_1@127.0.0.1:<0.1939.1>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T12:18:42.677Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:18:42.678Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:18:42.872Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:18:42.877Z,ns_1@127.0.0.1:<0.1940.1>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:18:42.878Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:18:42.878Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:18:54.583Z,ns_1@127.0.0.1:ldap_auth_cache<0.334.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2022-09-08T12:19:12.679Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:19:12.680Z,ns_1@127.0.0.1:<0.3420.1>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:19:12.682Z,ns_1@127.0.0.1:<0.3422.1>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T12:19:12.682Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:19:12.682Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:19:12.879Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:19:12.884Z,ns_1@127.0.0.1:<0.3423.1>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:19:12.885Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:19:12.885Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:19:42.683Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:19:42.685Z,ns_1@127.0.0.1:<0.4913.1>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:19:42.687Z,ns_1@127.0.0.1:<0.4915.1>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T12:19:42.687Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:19:42.687Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:19:42.886Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:19:42.890Z,ns_1@127.0.0.1:<0.4916.1>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:19:42.890Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:19:42.890Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:20:09.584Z,ns_1@127.0.0.1:ldap_auth_cache<0.334.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2022-09-08T12:20:12.688Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:20:12.689Z,ns_1@127.0.0.1:<0.6396.1>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:20:12.691Z,ns_1@127.0.0.1:<0.6398.1>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T12:20:12.692Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:20:12.692Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:20:12.891Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:20:12.897Z,ns_1@127.0.0.1:<0.6400.1>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:20:12.898Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:20:12.898Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:20:42.693Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:20:42.694Z,ns_1@127.0.0.1:<0.7887.1>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:20:42.695Z,ns_1@127.0.0.1:<0.7889.1>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T12:20:42.696Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:20:42.696Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:20:42.899Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:20:42.903Z,ns_1@127.0.0.1:<0.7890.1>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:20:42.903Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:20:42.903Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:21:12.696Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:21:12.697Z,ns_1@127.0.0.1:<0.9366.1>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:21:12.700Z,ns_1@127.0.0.1:<0.9368.1>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T12:21:12.700Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:21:12.701Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:21:12.904Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:21:12.909Z,ns_1@127.0.0.1:<0.9373.1>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:21:12.910Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:21:12.910Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:21:24.585Z,ns_1@127.0.0.1:ldap_auth_cache<0.334.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2022-09-08T12:21:42.702Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:21:42.703Z,ns_1@127.0.0.1:<0.10859.1>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:21:42.705Z,ns_1@127.0.0.1:<0.10861.1>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T12:21:42.706Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:21:42.706Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:21:42.911Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:21:42.916Z,ns_1@127.0.0.1:<0.10866.1>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:21:42.917Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:21:42.917Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:22:12.487Z,ns_1@127.0.0.1:ns_audit<0.579.0>:ns_audit:handle_call:148]Audit session_expired: [{timestamp,<<"2022-09-08T12:22:12.487Z">>},
                        {sessionid,<<"8d72771539648f7a1bc57f49b878927e398368fd">>},
                        {real_userid,{[{domain,builtin},
                                       {user,<<"<ud>admin</ud>">>}]}}]
[ns_server:debug,2022-09-08T12:22:12.707Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:22:12.708Z,ns_1@127.0.0.1:<0.12344.1>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:22:12.711Z,ns_1@127.0.0.1:<0.12346.1>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T12:22:12.711Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:22:12.711Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:22:12.918Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:22:12.923Z,ns_1@127.0.0.1:<0.12351.1>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:22:12.923Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:22:12.923Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:22:39.586Z,ns_1@127.0.0.1:ldap_auth_cache<0.334.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2022-09-08T12:22:39.590Z,ns_1@127.0.0.1:roles_cache<0.345.0>:active_cache:renew:238]Starting roles_cache cache renewal
[ns_server:debug,2022-09-08T12:22:39.590Z,ns_1@127.0.0.1:roles_cache<0.345.0>:active_cache:renew:244]roles_cache cache renewal process originated 0 queries
[ns_server:debug,2022-09-08T12:22:42.712Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:22:42.713Z,ns_1@127.0.0.1:<0.13835.1>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:22:42.716Z,ns_1@127.0.0.1:<0.13837.1>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T12:22:42.716Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:22:42.717Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:22:42.924Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:22:42.929Z,ns_1@127.0.0.1:<0.13838.1>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:22:42.930Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:22:42.930Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:23:12.718Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:23:12.719Z,ns_1@127.0.0.1:<0.15318.1>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:23:12.722Z,ns_1@127.0.0.1:<0.15320.1>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T12:23:12.722Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:23:12.722Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:23:12.931Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:23:12.936Z,ns_1@127.0.0.1:<0.15321.1>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:23:12.937Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:23:12.937Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:23:42.723Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:23:42.724Z,ns_1@127.0.0.1:<0.16811.1>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:23:42.727Z,ns_1@127.0.0.1:<0.16813.1>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T12:23:42.727Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:23:42.727Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:23:42.939Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:23:42.943Z,ns_1@127.0.0.1:<0.16814.1>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:23:42.944Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:23:42.944Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:23:54.587Z,ns_1@127.0.0.1:ldap_auth_cache<0.334.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2022-09-08T12:24:12.728Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:24:12.729Z,ns_1@127.0.0.1:<0.18294.1>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:24:12.730Z,ns_1@127.0.0.1:<0.18296.1>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T12:24:12.730Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:24:12.730Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:24:12.945Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:24:12.950Z,ns_1@127.0.0.1:<0.18297.1>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:24:12.951Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:24:12.951Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:24:42.731Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:24:42.732Z,ns_1@127.0.0.1:<0.19759.1>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:24:42.735Z,ns_1@127.0.0.1:<0.19761.1>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T12:24:42.735Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:24:42.735Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:24:42.952Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:24:42.955Z,ns_1@127.0.0.1:<0.19788.1>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:24:42.955Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:24:42.955Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:25:09.589Z,ns_1@127.0.0.1:ldap_auth_cache<0.334.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2022-09-08T12:25:12.736Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:25:12.738Z,ns_1@127.0.0.1:<0.21242.1>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:25:12.740Z,ns_1@127.0.0.1:<0.21244.1>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T12:25:12.741Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:25:12.741Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:25:12.956Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:25:12.959Z,ns_1@127.0.0.1:<0.21245.1>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:25:12.959Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:25:12.959Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:25:42.742Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:25:42.743Z,ns_1@127.0.0.1:<0.22735.1>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:25:42.746Z,ns_1@127.0.0.1:<0.22737.1>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T12:25:42.746Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:25:42.747Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:25:42.961Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:25:42.966Z,ns_1@127.0.0.1:<0.22738.1>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:25:42.966Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:25:42.967Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:26:12.747Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:26:12.748Z,ns_1@127.0.0.1:<0.24218.1>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:26:12.751Z,ns_1@127.0.0.1:<0.24220.1>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T12:26:12.751Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:26:12.751Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:26:12.968Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:26:12.973Z,ns_1@127.0.0.1:<0.24223.1>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:26:12.974Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:26:12.974Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:26:24.591Z,ns_1@127.0.0.1:ldap_auth_cache<0.334.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2022-09-08T12:26:42.752Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:26:42.754Z,ns_1@127.0.0.1:<0.25709.1>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:26:42.756Z,ns_1@127.0.0.1:<0.25711.1>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T12:26:42.757Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:26:42.757Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:26:42.976Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:26:42.980Z,ns_1@127.0.0.1:<0.25712.1>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:26:42.981Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:26:42.981Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:27:12.507Z,ns_1@127.0.0.1:ns_audit<0.579.0>:ns_audit:handle_call:148]Audit session_expired: [{timestamp,<<"2022-09-08T12:27:12.507Z">>},
                        {sessionid,<<"dc0da152bec7c967a04454681389fb6807fa5402">>},
                        {real_userid,{[{domain,builtin},
                                       {user,<<"<ud>admin</ud>">>}]}}]
[ns_server:debug,2022-09-08T12:27:12.758Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:27:12.759Z,ns_1@127.0.0.1:<0.27190.1>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:27:12.762Z,ns_1@127.0.0.1:<0.27192.1>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T12:27:12.762Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:27:12.763Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:27:12.982Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:27:12.986Z,ns_1@127.0.0.1:<0.27197.1>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:27:12.987Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:27:12.987Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:27:39.560Z,ns_1@127.0.0.1:roles_cache<0.345.0>:active_cache:cleanup:258]Cache roles_cache cleanup: 0/0 records deleted
[ns_server:debug,2022-09-08T12:27:39.592Z,ns_1@127.0.0.1:ldap_auth_cache<0.334.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2022-09-08T12:27:42.320Z,ns_1@127.0.0.1:prometheus_cfg<0.391.0>:prometheus_cfg:maybe_update_scrape_dynamic_intervals:941]Recalculating prometheus scrape intervals for high cardinality metrics
[ns_server:debug,2022-09-08T12:27:42.332Z,ns_1@127.0.0.1:prometheus_cfg<0.391.0>:prometheus_cfg:maybe_update_scrape_dynamic_intervals:958]Scrape intervals haven't changed:
[]
CBCollect stats dir size estimation: 629596800
Calculated based on scrapes info:
[{backup,low_cardinality,0},
 {cbas,high_cardinality,0},
 {cbas,low_cardinality,14},
 {eventing,high_cardinality,0},
 {eventing,low_cardinality,1},
 {fts,high_cardinality,0},
 {fts,low_cardinality,25},
 {index,low_cardinality,2},
 {index,high_cardinality,24},
 {kv,low_cardinality,291},
 {kv,high_cardinality,1123},
 {n1ql,low_cardinality,33},
 {ns_server,low_cardinality,167},
 {ns_server,high_cardinality,330},
 {xdcr,low_cardinality,0}]
Raw intervals:
[]
[ns_server:debug,2022-09-08T12:27:42.764Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:27:42.765Z,ns_1@127.0.0.1:<0.28687.1>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:27:42.769Z,ns_1@127.0.0.1:<0.28689.1>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T12:27:42.769Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:27:42.769Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:27:42.988Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:27:42.993Z,ns_1@127.0.0.1:<0.28694.1>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:27:42.994Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:27:42.994Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:28:12.770Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:28:12.771Z,ns_1@127.0.0.1:<0.30170.1>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:28:12.774Z,ns_1@127.0.0.1:<0.30172.1>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T12:28:12.774Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:28:12.775Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:28:12.995Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:28:12.999Z,ns_1@127.0.0.1:<0.30177.1>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:28:12.999Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:28:12.999Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:28:42.775Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:28:42.776Z,ns_1@127.0.0.1:<0.31653.1>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:28:42.777Z,ns_1@127.0.0.1:<0.31655.1>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T12:28:42.778Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:28:42.778Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:28:43.000Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:28:43.005Z,ns_1@127.0.0.1:<0.31668.1>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:28:43.006Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:28:43.006Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:28:54.593Z,ns_1@127.0.0.1:ldap_auth_cache<0.334.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2022-09-08T12:29:12.779Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:29:12.780Z,ns_1@127.0.0.1:<0.368.2>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:29:12.781Z,ns_1@127.0.0.1:<0.370.2>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T12:29:12.782Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:29:12.782Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:29:13.007Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:29:13.012Z,ns_1@127.0.0.1:<0.383.2>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:29:13.013Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:29:13.013Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:29:42.782Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:29:42.783Z,ns_1@127.0.0.1:<0.1861.2>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:29:42.786Z,ns_1@127.0.0.1:<0.1863.2>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T12:29:42.786Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:29:42.787Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:29:43.014Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:29:43.019Z,ns_1@127.0.0.1:<0.1876.2>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:29:43.020Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:29:43.020Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 29s
[ns_server:debug,2022-09-08T12:30:09.591Z,ns_1@127.0.0.1:roles_cache<0.345.0>:active_cache:renew:238]Starting roles_cache cache renewal
[ns_server:debug,2022-09-08T12:30:09.591Z,ns_1@127.0.0.1:roles_cache<0.345.0>:active_cache:renew:244]roles_cache cache renewal process originated 0 queries
[ns_server:debug,2022-09-08T12:30:09.594Z,ns_1@127.0.0.1:ldap_auth_cache<0.334.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2022-09-08T12:30:12.021Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:30:12.027Z,ns_1@127.0.0.1:<0.3317.2>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:30:12.027Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:30:12.028Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:30:12.787Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:30:12.788Z,ns_1@127.0.0.1:<0.3345.2>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:30:12.790Z,ns_1@127.0.0.1:<0.3347.2>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T12:30:12.790Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:30:12.790Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:30:42.029Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:30:42.034Z,ns_1@127.0.0.1:<0.4802.2>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:30:42.035Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:30:42.035Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:30:42.791Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:30:42.792Z,ns_1@127.0.0.1:<0.4836.2>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:30:42.795Z,ns_1@127.0.0.1:<0.4838.2>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T12:30:42.795Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:30:42.795Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:31:12.036Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:31:12.041Z,ns_1@127.0.0.1:<0.6291.2>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:31:12.042Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:31:12.042Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:31:12.796Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:31:12.797Z,ns_1@127.0.0.1:<0.6319.2>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:31:12.800Z,ns_1@127.0.0.1:<0.6321.2>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T12:31:12.800Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:31:12.800Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:31:24.595Z,ns_1@127.0.0.1:ldap_auth_cache<0.334.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2022-09-08T12:31:42.043Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:31:42.048Z,ns_1@127.0.0.1:<0.7778.2>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:31:42.049Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:31:42.049Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:31:42.801Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:31:42.802Z,ns_1@127.0.0.1:<0.7812.2>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:31:42.804Z,ns_1@127.0.0.1:<0.7814.2>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T12:31:42.804Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:31:42.804Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:32:12.050Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:32:12.055Z,ns_1@127.0.0.1:<0.9269.2>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:32:12.056Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:32:12.056Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:32:12.805Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:32:12.806Z,ns_1@127.0.0.1:<0.9296.2>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:32:12.808Z,ns_1@127.0.0.1:<0.9298.2>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T12:32:12.808Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:32:12.809Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:32:27.529Z,ns_1@127.0.0.1:ns_audit<0.579.0>:ns_audit:handle_call:148]Audit session_expired: [{timestamp,<<"2022-09-08T12:32:27.528Z">>},
                        {sessionid,<<"b3ef6495570e6e9441f7298fca727d7168fd68b0">>},
                        {real_userid,{[{domain,builtin},
                                       {user,<<"<ud>admin</ud>">>}]}}]
[ns_server:debug,2022-09-08T12:32:39.596Z,ns_1@127.0.0.1:ldap_auth_cache<0.334.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2022-09-08T12:32:42.057Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:32:42.062Z,ns_1@127.0.0.1:<0.10754.2>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:32:42.063Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:32:42.063Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:32:42.810Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:32:42.811Z,ns_1@127.0.0.1:<0.10788.2>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:32:42.813Z,ns_1@127.0.0.1:<0.10790.2>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T12:32:42.814Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:32:42.814Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:33:12.064Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:33:12.070Z,ns_1@127.0.0.1:<0.12243.2>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:33:12.071Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:33:12.071Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:33:12.815Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:33:12.816Z,ns_1@127.0.0.1:<0.12271.2>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:33:12.819Z,ns_1@127.0.0.1:<0.12273.2>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T12:33:12.819Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:33:12.819Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:33:42.072Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:33:42.078Z,ns_1@127.0.0.1:<0.13726.2>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:33:42.079Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:33:42.079Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:33:42.820Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:33:42.821Z,ns_1@127.0.0.1:<0.13764.2>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:33:42.824Z,ns_1@127.0.0.1:<0.13766.2>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T12:33:42.824Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:33:42.825Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:33:54.597Z,ns_1@127.0.0.1:ldap_auth_cache<0.334.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2022-09-08T12:34:12.080Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:34:12.084Z,ns_1@127.0.0.1:<0.15215.2>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:34:12.085Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:34:12.085Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:34:12.826Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:34:12.827Z,ns_1@127.0.0.1:<0.15247.2>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:34:12.828Z,ns_1@127.0.0.1:<0.15249.2>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T12:34:12.829Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:34:12.829Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:34:42.086Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:34:42.092Z,ns_1@127.0.0.1:<0.16700.2>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:34:42.093Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:34:42.093Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:34:42.830Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:34:42.831Z,ns_1@127.0.0.1:<0.16738.2>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:34:42.834Z,ns_1@127.0.0.1:<0.16740.2>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T12:34:42.834Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:34:42.835Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:35:09.598Z,ns_1@127.0.0.1:ldap_auth_cache<0.334.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2022-09-08T12:35:12.095Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:35:12.100Z,ns_1@127.0.0.1:<0.18163.2>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:35:12.102Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:35:12.102Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:35:12.836Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:35:12.837Z,ns_1@127.0.0.1:<0.18221.2>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:35:12.839Z,ns_1@127.0.0.1:<0.18223.2>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T12:35:12.840Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:35:12.840Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:35:42.103Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:35:42.107Z,ns_1@127.0.0.1:<0.19650.2>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:35:42.107Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:35:42.107Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:35:42.841Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:35:42.843Z,ns_1@127.0.0.1:<0.19706.2>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:35:42.844Z,ns_1@127.0.0.1:<0.19708.2>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T12:35:42.845Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:35:42.845Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:36:12.108Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:36:12.113Z,ns_1@127.0.0.1:<0.21139.2>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:36:12.113Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:36:12.113Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:36:12.846Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:36:12.847Z,ns_1@127.0.0.1:<0.21189.2>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:36:12.851Z,ns_1@127.0.0.1:<0.21191.2>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T12:36:12.851Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:36:12.851Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:36:24.600Z,ns_1@127.0.0.1:ldap_auth_cache<0.334.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2022-09-08T12:36:42.114Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:36:42.119Z,ns_1@127.0.0.1:<0.22624.2>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:36:42.120Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:36:42.120Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:36:42.852Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:36:42.853Z,ns_1@127.0.0.1:<0.22680.2>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:36:42.854Z,ns_1@127.0.0.1:<0.22682.2>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T12:36:42.854Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:36:42.854Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:37:12.121Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:37:12.127Z,ns_1@127.0.0.1:<0.24113.2>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:37:12.128Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:37:12.128Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:37:12.855Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:37:12.856Z,ns_1@127.0.0.1:<0.24137.2>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:37:12.859Z,ns_1@127.0.0.1:<0.24139.2>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T12:37:12.859Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:37:12.859Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:37:27.549Z,ns_1@127.0.0.1:ns_audit<0.579.0>:ns_audit:handle_call:148]Audit session_expired: [{timestamp,<<"2022-09-08T12:37:27.549Z">>},
                        {sessionid,<<"aafcaee367b3a42234129c84d90664ce64b44715">>},
                        {real_userid,{[{domain,builtin},
                                       {user,<<"<ud>admin</ud>">>}]}}]
[ns_server:debug,2022-09-08T12:37:39.561Z,ns_1@127.0.0.1:roles_cache<0.345.0>:active_cache:cleanup:258]Cache roles_cache cleanup: 0/0 records deleted
[ns_server:debug,2022-09-08T12:37:39.592Z,ns_1@127.0.0.1:roles_cache<0.345.0>:active_cache:renew:238]Starting roles_cache cache renewal
[ns_server:debug,2022-09-08T12:37:39.592Z,ns_1@127.0.0.1:roles_cache<0.345.0>:active_cache:renew:244]roles_cache cache renewal process originated 0 queries
[ns_server:debug,2022-09-08T12:37:39.601Z,ns_1@127.0.0.1:ldap_auth_cache<0.334.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2022-09-08T12:37:42.129Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:37:42.135Z,ns_1@127.0.0.1:<0.25594.2>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:37:42.135Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:37:42.135Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:37:42.333Z,ns_1@127.0.0.1:prometheus_cfg<0.391.0>:prometheus_cfg:maybe_update_scrape_dynamic_intervals:941]Recalculating prometheus scrape intervals for high cardinality metrics
[ns_server:debug,2022-09-08T12:37:42.346Z,ns_1@127.0.0.1:prometheus_cfg<0.391.0>:prometheus_cfg:maybe_update_scrape_dynamic_intervals:958]Scrape intervals haven't changed:
[]
CBCollect stats dir size estimation: 629596800
Calculated based on scrapes info:
[{backup,low_cardinality,0},
 {cbas,high_cardinality,0},
 {cbas,low_cardinality,14},
 {eventing,high_cardinality,0},
 {eventing,low_cardinality,1},
 {fts,high_cardinality,0},
 {fts,low_cardinality,25},
 {index,low_cardinality,2},
 {index,high_cardinality,24},
 {kv,low_cardinality,291},
 {kv,high_cardinality,1123},
 {n1ql,low_cardinality,33},
 {ns_server,low_cardinality,167},
 {ns_server,high_cardinality,330},
 {xdcr,low_cardinality,0}]
Raw intervals:
[]
[ns_server:debug,2022-09-08T12:37:42.861Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:37:42.862Z,ns_1@127.0.0.1:<0.25636.2>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:37:42.865Z,ns_1@127.0.0.1:<0.25638.2>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T12:37:42.866Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:37:42.866Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:38:12.136Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:38:12.141Z,ns_1@127.0.0.1:<0.27087.2>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:38:12.142Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:38:12.142Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:38:12.867Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:38:12.868Z,ns_1@127.0.0.1:<0.27116.2>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:38:12.870Z,ns_1@127.0.0.1:<0.27118.2>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T12:38:12.871Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:38:12.871Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:38:42.143Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:38:42.148Z,ns_1@127.0.0.1:<0.28572.2>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:38:42.149Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:38:42.149Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:38:42.872Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:38:42.873Z,ns_1@127.0.0.1:<0.28602.2>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:38:42.876Z,ns_1@127.0.0.1:<0.28604.2>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T12:38:42.876Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:38:42.876Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:38:54.602Z,ns_1@127.0.0.1:ldap_auth_cache<0.334.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2022-09-08T12:39:12.150Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:39:12.156Z,ns_1@127.0.0.1:<0.30056.2>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:39:12.157Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:39:12.157Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:39:12.877Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:39:12.878Z,ns_1@127.0.0.1:<0.30080.2>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:39:12.880Z,ns_1@127.0.0.1:<0.30082.2>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T12:39:12.880Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:39:12.880Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:39:42.158Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:39:42.164Z,ns_1@127.0.0.1:<0.31543.2>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:39:42.165Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:39:42.165Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:39:42.881Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:39:42.882Z,ns_1@127.0.0.1:<0.31570.2>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:39:42.884Z,ns_1@127.0.0.1:<0.31572.2>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T12:39:42.884Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:39:42.884Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:40:09.603Z,ns_1@127.0.0.1:ldap_auth_cache<0.334.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2022-09-08T12:40:12.166Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:40:12.172Z,ns_1@127.0.0.1:<0.264.3>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:40:12.173Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:40:12.174Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:40:12.885Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:40:12.886Z,ns_1@127.0.0.1:<0.285.3>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:40:12.889Z,ns_1@127.0.0.1:<0.287.3>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T12:40:12.889Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:40:12.890Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:40:42.175Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:40:42.181Z,ns_1@127.0.0.1:<0.1749.3>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:40:42.182Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:40:42.182Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:40:42.891Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:40:42.892Z,ns_1@127.0.0.1:<0.1776.3>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:40:42.895Z,ns_1@127.0.0.1:<0.1778.3>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T12:40:42.895Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:40:42.895Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:41:12.183Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:41:12.188Z,ns_1@127.0.0.1:<0.3238.3>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:41:12.189Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:41:12.190Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:41:12.896Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:41:12.897Z,ns_1@127.0.0.1:<0.3259.3>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:41:12.900Z,ns_1@127.0.0.1:<0.3261.3>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T12:41:12.901Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:41:12.901Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:41:24.605Z,ns_1@127.0.0.1:ldap_auth_cache<0.334.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2022-09-08T12:41:42.190Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:41:42.195Z,ns_1@127.0.0.1:<0.4725.3>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:41:42.196Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:41:42.196Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:41:42.903Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:41:42.904Z,ns_1@127.0.0.1:<0.4752.3>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:41:42.906Z,ns_1@127.0.0.1:<0.4754.3>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T12:41:42.906Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:41:42.907Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:42:12.197Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:42:12.201Z,ns_1@127.0.0.1:<0.6214.3>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:42:12.202Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:42:12.202Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:42:12.907Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:42:12.908Z,ns_1@127.0.0.1:<0.6235.3>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:42:12.911Z,ns_1@127.0.0.1:<0.6237.3>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T12:42:12.911Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:42:12.911Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:42:39.606Z,ns_1@127.0.0.1:ldap_auth_cache<0.334.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2022-09-08T12:42:42.203Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:42:42.208Z,ns_1@127.0.0.1:<0.7699.3>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:42:42.208Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:42:42.208Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:42:42.571Z,ns_1@127.0.0.1:ns_audit<0.579.0>:ns_audit:handle_call:148]Audit session_expired: [{timestamp,<<"2022-09-08T12:42:42.571Z">>},
                        {sessionid,<<"3dacacfe12e4bfbd5745d351e907c0247c6a90ac">>},
                        {real_userid,{[{domain,builtin},
                                       {user,<<"<ud>admin</ud>">>}]}}]
[ns_server:debug,2022-09-08T12:42:42.912Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:42:42.913Z,ns_1@127.0.0.1:<0.7725.3>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:42:42.916Z,ns_1@127.0.0.1:<0.7727.3>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T12:42:42.916Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:42:42.916Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:43:12.209Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:43:12.213Z,ns_1@127.0.0.1:<0.9190.3>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:43:12.214Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:43:12.214Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:43:12.917Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:43:12.918Z,ns_1@127.0.0.1:<0.9199.3>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:43:12.921Z,ns_1@127.0.0.1:<0.9201.3>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T12:43:12.921Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:43:12.921Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:43:42.215Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:43:42.221Z,ns_1@127.0.0.1:<0.10677.3>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:43:42.222Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:43:42.222Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:43:42.922Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:43:42.923Z,ns_1@127.0.0.1:<0.10692.3>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:43:42.924Z,ns_1@127.0.0.1:<0.10694.3>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T12:43:42.924Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:43:42.924Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:43:54.607Z,ns_1@127.0.0.1:ldap_auth_cache<0.334.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2022-09-08T12:44:12.223Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:44:12.226Z,ns_1@127.0.0.1:<0.12166.3>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:44:12.227Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:44:12.227Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:44:12.925Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:44:12.926Z,ns_1@127.0.0.1:<0.12176.3>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:44:12.929Z,ns_1@127.0.0.1:<0.12178.3>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T12:44:12.930Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:44:12.930Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:44:42.228Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:44:42.232Z,ns_1@127.0.0.1:<0.13651.3>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:44:42.233Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:44:42.233Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:44:42.931Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:44:42.932Z,ns_1@127.0.0.1:<0.13666.3>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:44:42.935Z,ns_1@127.0.0.1:<0.13668.3>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T12:44:42.936Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:44:42.936Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:45:09.593Z,ns_1@127.0.0.1:roles_cache<0.345.0>:active_cache:renew:238]Starting roles_cache cache renewal
[ns_server:debug,2022-09-08T12:45:09.594Z,ns_1@127.0.0.1:roles_cache<0.345.0>:active_cache:renew:244]roles_cache cache renewal process originated 0 queries
[ns_server:debug,2022-09-08T12:45:09.608Z,ns_1@127.0.0.1:ldap_auth_cache<0.334.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2022-09-08T12:45:12.234Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:45:12.237Z,ns_1@127.0.0.1:<0.15132.3>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:45:12.238Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:45:12.238Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:45:12.937Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:45:12.938Z,ns_1@127.0.0.1:<0.15149.3>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:45:12.941Z,ns_1@127.0.0.1:<0.15151.3>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T12:45:12.941Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:45:12.941Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:45:42.239Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:45:42.245Z,ns_1@127.0.0.1:<0.16619.3>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:45:42.245Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:45:42.246Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:45:42.942Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:45:42.943Z,ns_1@127.0.0.1:<0.16642.3>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:45:42.946Z,ns_1@127.0.0.1:<0.16644.3>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T12:45:42.947Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:45:42.947Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:46:12.247Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:46:12.253Z,ns_1@127.0.0.1:<0.18108.3>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:46:12.254Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:46:12.254Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:46:12.948Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:46:12.949Z,ns_1@127.0.0.1:<0.18125.3>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:46:12.952Z,ns_1@127.0.0.1:<0.18127.3>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T12:46:12.952Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:46:12.952Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:46:24.609Z,ns_1@127.0.0.1:ldap_auth_cache<0.334.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2022-09-08T12:46:42.255Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:46:42.260Z,ns_1@127.0.0.1:<0.19593.3>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:46:42.261Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:46:42.261Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:46:42.953Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:46:42.954Z,ns_1@127.0.0.1:<0.19616.3>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:46:42.957Z,ns_1@127.0.0.1:<0.19618.3>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T12:46:42.957Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:46:42.957Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:47:12.262Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:47:12.265Z,ns_1@127.0.0.1:<0.21082.3>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:47:12.266Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:47:12.266Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:47:12.958Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:47:12.959Z,ns_1@127.0.0.1:<0.21099.3>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:47:12.962Z,ns_1@127.0.0.1:<0.21101.3>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T12:47:12.962Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:47:12.962Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:47:39.562Z,ns_1@127.0.0.1:roles_cache<0.345.0>:active_cache:cleanup:258]Cache roles_cache cleanup: 0/0 records deleted
[ns_server:debug,2022-09-08T12:47:39.610Z,ns_1@127.0.0.1:ldap_auth_cache<0.334.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2022-09-08T12:47:42.267Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:47:42.272Z,ns_1@127.0.0.1:<0.22543.3>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:47:42.273Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:47:42.273Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:47:42.347Z,ns_1@127.0.0.1:prometheus_cfg<0.391.0>:prometheus_cfg:maybe_update_scrape_dynamic_intervals:941]Recalculating prometheus scrape intervals for high cardinality metrics
[ns_server:debug,2022-09-08T12:47:42.358Z,ns_1@127.0.0.1:prometheus_cfg<0.391.0>:prometheus_cfg:maybe_update_scrape_dynamic_intervals:958]Scrape intervals haven't changed:
[]
CBCollect stats dir size estimation: 629596800
Calculated based on scrapes info:
[{backup,low_cardinality,0},
 {cbas,high_cardinality,0},
 {cbas,low_cardinality,14},
 {eventing,high_cardinality,0},
 {eventing,low_cardinality,1},
 {fts,high_cardinality,0},
 {fts,low_cardinality,25},
 {index,low_cardinality,2},
 {index,high_cardinality,24},
 {kv,low_cardinality,291},
 {kv,high_cardinality,1123},
 {n1ql,low_cardinality,33},
 {ns_server,low_cardinality,167},
 {ns_server,high_cardinality,330},
 {xdcr,low_cardinality,0}]
Raw intervals:
[]
[ns_server:debug,2022-09-08T12:47:42.591Z,ns_1@127.0.0.1:ns_audit<0.579.0>:ns_audit:handle_call:148]Audit session_expired: [{timestamp,<<"2022-09-08T12:47:42.591Z">>},
                        {sessionid,<<"664cdf4a8d51a07030a50364db364e9d3e3521c8">>},
                        {real_userid,{[{domain,builtin},
                                       {user,<<"<ud>admin</ud>">>}]}}]
[ns_server:debug,2022-09-08T12:47:42.963Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:47:42.965Z,ns_1@127.0.0.1:<0.22598.3>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:47:42.968Z,ns_1@127.0.0.1:<0.22600.3>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T12:47:42.968Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:47:42.968Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:48:12.274Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:48:12.280Z,ns_1@127.0.0.1:<0.24030.3>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:48:12.281Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:48:12.281Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:48:12.969Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:48:12.970Z,ns_1@127.0.0.1:<0.24081.3>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:48:12.972Z,ns_1@127.0.0.1:<0.24083.3>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T12:48:12.973Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:48:12.973Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:48:42.282Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:48:42.287Z,ns_1@127.0.0.1:<0.25513.3>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:48:42.288Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:48:42.288Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:48:42.974Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:48:42.975Z,ns_1@127.0.0.1:<0.25568.3>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:48:42.978Z,ns_1@127.0.0.1:<0.25570.3>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T12:48:42.978Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:48:42.978Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:48:54.611Z,ns_1@127.0.0.1:ldap_auth_cache<0.334.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2022-09-08T12:49:12.289Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:49:12.294Z,ns_1@127.0.0.1:<0.27002.3>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:49:12.295Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:49:12.295Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:49:12.979Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:49:12.980Z,ns_1@127.0.0.1:<0.27025.3>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:49:12.983Z,ns_1@127.0.0.1:<0.27027.3>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T12:49:12.983Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:49:12.983Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:49:42.296Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:49:42.301Z,ns_1@127.0.0.1:<0.28489.3>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:49:42.302Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:49:42.302Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:49:42.984Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:49:42.985Z,ns_1@127.0.0.1:<0.28518.3>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:49:42.987Z,ns_1@127.0.0.1:<0.28520.3>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T12:49:42.987Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:49:42.988Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:50:09.612Z,ns_1@127.0.0.1:ldap_auth_cache<0.334.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2022-09-08T12:50:12.303Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:50:12.307Z,ns_1@127.0.0.1:<0.29976.3>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:50:12.308Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:50:12.308Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:50:12.988Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:50:12.989Z,ns_1@127.0.0.1:<0.29995.3>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:50:12.991Z,ns_1@127.0.0.1:<0.29997.3>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T12:50:12.991Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:50:12.991Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:50:42.309Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:50:42.315Z,ns_1@127.0.0.1:<0.31459.3>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:50:42.316Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:50:42.316Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:50:42.992Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:50:42.993Z,ns_1@127.0.0.1:<0.31484.3>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:50:42.995Z,ns_1@127.0.0.1:<0.31486.3>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T12:50:42.995Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:50:42.995Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:51:12.317Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:51:12.321Z,ns_1@127.0.0.1:<0.180.4>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:51:12.321Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:51:12.322Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:51:12.996Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:51:12.997Z,ns_1@127.0.0.1:<0.199.4>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:51:13.000Z,ns_1@127.0.0.1:<0.201.4>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T12:51:13.001Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:51:13.001Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:51:24.613Z,ns_1@127.0.0.1:ldap_auth_cache<0.334.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2022-09-08T12:51:42.322Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:51:42.325Z,ns_1@127.0.0.1:<0.1667.4>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:51:42.325Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:51:42.325Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:51:43.002Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:51:43.003Z,ns_1@127.0.0.1:<0.1692.4>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:51:43.005Z,ns_1@127.0.0.1:<0.1694.4>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T12:51:43.005Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:51:43.005Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:52:12.326Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:52:12.332Z,ns_1@127.0.0.1:<0.3156.4>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:52:12.333Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:52:12.333Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:52:13.006Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:52:13.007Z,ns_1@127.0.0.1:<0.3175.4>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:52:13.009Z,ns_1@127.0.0.1:<0.3177.4>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T12:52:13.009Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:52:13.009Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:52:39.595Z,ns_1@127.0.0.1:roles_cache<0.345.0>:active_cache:renew:238]Starting roles_cache cache renewal
[ns_server:debug,2022-09-08T12:52:39.595Z,ns_1@127.0.0.1:roles_cache<0.345.0>:active_cache:renew:244]roles_cache cache renewal process originated 0 queries
[ns_server:debug,2022-09-08T12:52:39.614Z,ns_1@127.0.0.1:ldap_auth_cache<0.334.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2022-09-08T12:52:42.334Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:52:42.339Z,ns_1@127.0.0.1:<0.4633.4>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:52:42.340Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:52:42.340Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:52:42.611Z,ns_1@127.0.0.1:ns_audit<0.579.0>:ns_audit:handle_call:148]Audit session_expired: [{timestamp,<<"2022-09-08T12:52:42.611Z">>},
                        {sessionid,<<"6ddde123540a6124e73c88bc95fca4aafcd6d8ac">>},
                        {real_userid,{[{domain,builtin},
                                       {user,<<"<ud>admin</ud>">>}]}}]
[ns_server:debug,2022-09-08T12:52:43.010Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:52:43.011Z,ns_1@127.0.0.1:<0.4668.4>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:52:43.014Z,ns_1@127.0.0.1:<0.4670.4>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T12:52:43.014Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:52:43.014Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:53:12.341Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:53:12.346Z,ns_1@127.0.0.1:<0.6124.4>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:53:12.347Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:53:12.347Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:53:13.016Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:53:13.017Z,ns_1@127.0.0.1:<0.6151.4>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:53:13.020Z,ns_1@127.0.0.1:<0.6153.4>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T12:53:13.020Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:53:13.021Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 29s
[ns_server:debug,2022-09-08T12:53:42.021Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:53:42.022Z,ns_1@127.0.0.1:<0.7592.4>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:53:42.025Z,ns_1@127.0.0.1:<0.7594.4>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T12:53:42.026Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:53:42.026Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:53:42.348Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:53:42.354Z,ns_1@127.0.0.1:<0.7614.4>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:53:42.355Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:53:42.355Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:53:54.615Z,ns_1@127.0.0.1:ldap_auth_cache<0.334.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2022-09-08T12:54:12.027Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:54:12.029Z,ns_1@127.0.0.1:<0.9077.4>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:54:12.031Z,ns_1@127.0.0.1:<0.9079.4>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T12:54:12.032Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:54:12.032Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:54:12.356Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:54:12.361Z,ns_1@127.0.0.1:<0.9103.4>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:54:12.362Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:54:12.362Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:54:42.033Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:54:42.034Z,ns_1@127.0.0.1:<0.10562.4>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:54:42.036Z,ns_1@127.0.0.1:<0.10564.4>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T12:54:42.036Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:54:42.036Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:54:42.363Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:54:42.369Z,ns_1@127.0.0.1:<0.10588.4>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:54:42.370Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:54:42.370Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:55:09.616Z,ns_1@127.0.0.1:ldap_auth_cache<0.334.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2022-09-08T12:55:12.037Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:55:12.038Z,ns_1@127.0.0.1:<0.12051.4>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:55:12.041Z,ns_1@127.0.0.1:<0.12053.4>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T12:55:12.042Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:55:12.042Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:55:12.371Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:55:12.376Z,ns_1@127.0.0.1:<0.12073.4>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:55:12.377Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:55:12.377Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:55:42.043Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:55:42.044Z,ns_1@127.0.0.1:<0.13538.4>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:55:42.047Z,ns_1@127.0.0.1:<0.13540.4>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T12:55:42.047Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:55:42.047Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:55:42.378Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:55:42.383Z,ns_1@127.0.0.1:<0.13561.4>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:55:42.384Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:55:42.384Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:56:12.048Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:56:12.049Z,ns_1@127.0.0.1:<0.15030.4>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:56:12.052Z,ns_1@127.0.0.1:<0.15032.4>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T12:56:12.052Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:56:12.052Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:56:12.385Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:56:12.391Z,ns_1@127.0.0.1:<0.15046.4>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:56:12.391Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:56:12.392Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:56:24.617Z,ns_1@127.0.0.1:ldap_auth_cache<0.334.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2022-09-08T12:56:42.054Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:56:42.055Z,ns_1@127.0.0.1:<0.16512.4>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:56:42.057Z,ns_1@127.0.0.1:<0.16514.4>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T12:56:42.058Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:56:42.058Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:56:42.393Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:56:42.399Z,ns_1@127.0.0.1:<0.16529.4>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:56:42.400Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:56:42.400Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:57:12.059Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:57:12.060Z,ns_1@127.0.0.1:<0.18001.4>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:57:12.062Z,ns_1@127.0.0.1:<0.18003.4>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T12:57:12.062Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:57:12.062Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:57:12.402Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:57:12.407Z,ns_1@127.0.0.1:<0.18014.4>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:57:12.408Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:57:12.408Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:57:39.563Z,ns_1@127.0.0.1:roles_cache<0.345.0>:active_cache:cleanup:258]Cache roles_cache cleanup: 0/0 records deleted
[ns_server:debug,2022-09-08T12:57:39.618Z,ns_1@127.0.0.1:ldap_auth_cache<0.334.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2022-09-08T12:57:42.063Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:57:42.065Z,ns_1@127.0.0.1:<0.19480.4>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:57:42.068Z,ns_1@127.0.0.1:<0.19482.4>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T12:57:42.068Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:57:42.068Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:57:42.359Z,ns_1@127.0.0.1:prometheus_cfg<0.391.0>:prometheus_cfg:maybe_update_scrape_dynamic_intervals:941]Recalculating prometheus scrape intervals for high cardinality metrics
[ns_server:debug,2022-09-08T12:57:42.370Z,ns_1@127.0.0.1:prometheus_cfg<0.391.0>:prometheus_cfg:maybe_update_scrape_dynamic_intervals:958]Scrape intervals haven't changed:
[]
CBCollect stats dir size estimation: 629596800
Calculated based on scrapes info:
[{backup,low_cardinality,0},
 {cbas,high_cardinality,0},
 {cbas,low_cardinality,14},
 {eventing,high_cardinality,0},
 {eventing,low_cardinality,1},
 {fts,high_cardinality,0},
 {fts,low_cardinality,25},
 {index,low_cardinality,2},
 {index,high_cardinality,24},
 {kv,low_cardinality,291},
 {kv,high_cardinality,1123},
 {n1ql,low_cardinality,33},
 {ns_server,low_cardinality,167},
 {ns_server,high_cardinality,330},
 {xdcr,low_cardinality,0}]
Raw intervals:
[]
[ns_server:debug,2022-09-08T12:57:42.409Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:57:42.413Z,ns_1@127.0.0.1:<0.19501.4>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:57:42.413Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:57:42.414Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:57:57.634Z,ns_1@127.0.0.1:ns_audit<0.579.0>:ns_audit:handle_call:148]Audit session_expired: [{timestamp,<<"2022-09-08T12:57:57.633Z">>},
                        {sessionid,<<"2a26f8c3f3163fa5865e9ade041fb721cb3d47d3">>},
                        {real_userid,{[{domain,builtin},
                                       {user,<<"<ud>admin</ud>">>}]}}]
[ns_server:debug,2022-09-08T12:58:12.069Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:58:12.069Z,ns_1@127.0.0.1:<0.20976.4>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:58:12.071Z,ns_1@127.0.0.1:<0.20978.4>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T12:58:12.071Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:58:12.071Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:58:12.414Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:58:12.417Z,ns_1@127.0.0.1:<0.20989.4>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:58:12.417Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:58:12.417Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:58:42.072Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:58:42.073Z,ns_1@127.0.0.1:<0.22460.4>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:58:42.076Z,ns_1@127.0.0.1:<0.22462.4>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T12:58:42.076Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:58:42.076Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:58:42.418Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:58:42.421Z,ns_1@127.0.0.1:<0.22472.4>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:58:42.422Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:58:42.422Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:58:54.619Z,ns_1@127.0.0.1:ldap_auth_cache<0.334.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2022-09-08T12:59:12.077Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:59:12.079Z,ns_1@127.0.0.1:<0.23923.4>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:59:12.081Z,ns_1@127.0.0.1:<0.23925.4>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T12:59:12.082Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:59:12.082Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:59:12.423Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:59:12.426Z,ns_1@127.0.0.1:<0.23960.4>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:59:12.427Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:59:12.427Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:59:42.083Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:59:42.085Z,ns_1@127.0.0.1:<0.25410.4>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:59:42.088Z,ns_1@127.0.0.1:<0.25412.4>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T12:59:42.088Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:59:42.088Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T12:59:42.428Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T12:59:42.431Z,ns_1@127.0.0.1:<0.25449.4>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T12:59:42.431Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T12:59:42.431Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:00:09.596Z,ns_1@127.0.0.1:roles_cache<0.345.0>:active_cache:renew:238]Starting roles_cache cache renewal
[ns_server:debug,2022-09-08T13:00:09.596Z,ns_1@127.0.0.1:roles_cache<0.345.0>:active_cache:renew:244]roles_cache cache renewal process originated 0 queries
[ns_server:debug,2022-09-08T13:00:09.620Z,ns_1@127.0.0.1:ldap_auth_cache<0.334.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2022-09-08T13:00:12.089Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:00:12.090Z,ns_1@127.0.0.1:<0.26901.4>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:00:12.093Z,ns_1@127.0.0.1:<0.26903.4>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T13:00:12.093Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:00:12.094Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:00:12.432Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:00:12.435Z,ns_1@127.0.0.1:<0.26904.4>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:00:12.436Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:00:12.436Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:00:42.094Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:00:42.095Z,ns_1@127.0.0.1:<0.28386.4>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:00:42.098Z,ns_1@127.0.0.1:<0.28388.4>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T13:00:42.099Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:00:42.099Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:00:42.437Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:00:42.442Z,ns_1@127.0.0.1:<0.28391.4>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:00:42.442Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:00:42.443Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:01:12.100Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:01:12.101Z,ns_1@127.0.0.1:<0.29875.4>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:01:12.104Z,ns_1@127.0.0.1:<0.29877.4>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T13:01:12.104Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:01:12.104Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:01:12.444Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:01:12.449Z,ns_1@127.0.0.1:<0.29878.4>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:01:12.450Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:01:12.450Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:01:24.622Z,ns_1@127.0.0.1:ldap_auth_cache<0.334.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2022-09-08T13:01:42.105Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:01:42.107Z,ns_1@127.0.0.1:<0.31362.4>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:01:42.110Z,ns_1@127.0.0.1:<0.31364.4>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T13:01:42.110Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:01:42.110Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:01:42.451Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:01:42.456Z,ns_1@127.0.0.1:<0.31367.4>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:01:42.457Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:01:42.457Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:02:12.111Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:02:12.112Z,ns_1@127.0.0.1:<0.83.5>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:02:12.115Z,ns_1@127.0.0.1:<0.85.5>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T13:02:12.115Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:02:12.116Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:02:12.458Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:02:12.462Z,ns_1@127.0.0.1:<0.86.5>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:02:12.462Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:02:12.462Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:02:39.623Z,ns_1@127.0.0.1:ldap_auth_cache<0.334.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2022-09-08T13:02:42.116Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:02:42.117Z,ns_1@127.0.0.1:<0.1568.5>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:02:42.120Z,ns_1@127.0.0.1:<0.1570.5>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T13:02:42.121Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:02:42.121Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:02:42.463Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:02:42.468Z,ns_1@127.0.0.1:<0.1573.5>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:02:42.468Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:02:42.468Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:02:57.655Z,ns_1@127.0.0.1:ns_audit<0.579.0>:ns_audit:handle_call:148]Audit session_expired: [{timestamp,<<"2022-09-08T13:02:57.654Z">>},
                        {sessionid,<<"70bf0e99384260f6d560f1dfe39b78317a5b9370">>},
                        {real_userid,{[{domain,builtin},
                                       {user,<<"<ud>admin</ud>">>}]}}]
[ns_server:debug,2022-09-08T13:03:12.122Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:03:12.123Z,ns_1@127.0.0.1:<0.3059.5>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:03:12.126Z,ns_1@127.0.0.1:<0.3061.5>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T13:03:12.126Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:03:12.127Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:03:12.469Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:03:12.472Z,ns_1@127.0.0.1:<0.3062.5>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:03:12.473Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:03:12.473Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:03:42.127Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:03:42.128Z,ns_1@127.0.0.1:<0.4546.5>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:03:42.131Z,ns_1@127.0.0.1:<0.4548.5>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T13:03:42.131Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:03:42.131Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:03:42.474Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:03:42.479Z,ns_1@127.0.0.1:<0.4551.5>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:03:42.480Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:03:42.480Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:03:54.624Z,ns_1@127.0.0.1:ldap_auth_cache<0.334.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2022-09-08T13:04:12.132Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:04:12.133Z,ns_1@127.0.0.1:<0.6035.5>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:04:12.136Z,ns_1@127.0.0.1:<0.6037.5>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T13:04:12.136Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:04:12.136Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:04:12.481Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:04:12.486Z,ns_1@127.0.0.1:<0.6038.5>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:04:12.487Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:04:12.487Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:04:42.137Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:04:42.138Z,ns_1@127.0.0.1:<0.7520.5>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:04:42.140Z,ns_1@127.0.0.1:<0.7522.5>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T13:04:42.141Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:04:42.141Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:04:42.488Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:04:42.492Z,ns_1@127.0.0.1:<0.7525.5>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:04:42.493Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:04:42.493Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:05:09.626Z,ns_1@127.0.0.1:ldap_auth_cache<0.334.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2022-09-08T13:05:12.142Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:05:12.143Z,ns_1@127.0.0.1:<0.9001.5>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:05:12.145Z,ns_1@127.0.0.1:<0.9003.5>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T13:05:12.145Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:05:12.145Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:05:12.494Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:05:12.499Z,ns_1@127.0.0.1:<0.9012.5>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:05:12.500Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:05:12.500Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:05:42.147Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:05:42.148Z,ns_1@127.0.0.1:<0.10488.5>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:05:42.151Z,ns_1@127.0.0.1:<0.10490.5>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T13:05:42.151Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:05:42.152Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:05:42.501Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:05:42.506Z,ns_1@127.0.0.1:<0.10501.5>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:05:42.507Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:05:42.507Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:06:12.153Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:06:12.154Z,ns_1@127.0.0.1:<0.11977.5>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:06:12.157Z,ns_1@127.0.0.1:<0.11979.5>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T13:06:12.157Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:06:12.157Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:06:12.509Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:06:12.514Z,ns_1@127.0.0.1:<0.11988.5>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:06:12.515Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:06:12.515Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:06:24.627Z,ns_1@127.0.0.1:ldap_auth_cache<0.334.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2022-09-08T13:06:42.159Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:06:42.160Z,ns_1@127.0.0.1:<0.13462.5>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:06:42.162Z,ns_1@127.0.0.1:<0.13464.5>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T13:06:42.162Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:06:42.163Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:06:42.516Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:06:42.520Z,ns_1@127.0.0.1:<0.13475.5>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:06:42.521Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:06:42.521Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:07:12.164Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:07:12.165Z,ns_1@127.0.0.1:<0.14950.5>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:07:12.167Z,ns_1@127.0.0.1:<0.14952.5>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T13:07:12.167Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:07:12.167Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:07:12.522Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:07:12.527Z,ns_1@127.0.0.1:<0.14961.5>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:07:12.528Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:07:12.528Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:07:39.564Z,ns_1@127.0.0.1:roles_cache<0.345.0>:active_cache:cleanup:258]Cache roles_cache cleanup: 0/0 records deleted
[ns_server:debug,2022-09-08T13:07:39.597Z,ns_1@127.0.0.1:roles_cache<0.345.0>:active_cache:renew:238]Starting roles_cache cache renewal
[ns_server:debug,2022-09-08T13:07:39.597Z,ns_1@127.0.0.1:roles_cache<0.345.0>:active_cache:renew:244]roles_cache cache renewal process originated 0 queries
[ns_server:debug,2022-09-08T13:07:39.629Z,ns_1@127.0.0.1:ldap_auth_cache<0.334.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2022-09-08T13:07:42.168Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:07:42.169Z,ns_1@127.0.0.1:<0.16436.5>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:07:42.170Z,ns_1@127.0.0.1:<0.16438.5>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T13:07:42.171Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:07:42.171Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:07:42.372Z,ns_1@127.0.0.1:prometheus_cfg<0.391.0>:prometheus_cfg:maybe_update_scrape_dynamic_intervals:941]Recalculating prometheus scrape intervals for high cardinality metrics
[ns_server:debug,2022-09-08T13:07:42.384Z,ns_1@127.0.0.1:prometheus_cfg<0.391.0>:prometheus_cfg:maybe_update_scrape_dynamic_intervals:958]Scrape intervals haven't changed:
[]
CBCollect stats dir size estimation: 629596800
Calculated based on scrapes info:
[{backup,low_cardinality,0},
 {cbas,high_cardinality,0},
 {cbas,low_cardinality,14},
 {eventing,high_cardinality,0},
 {eventing,low_cardinality,1},
 {fts,high_cardinality,0},
 {fts,low_cardinality,25},
 {index,low_cardinality,2},
 {index,high_cardinality,24},
 {kv,low_cardinality,291},
 {kv,high_cardinality,1123},
 {n1ql,low_cardinality,33},
 {ns_server,low_cardinality,167},
 {ns_server,high_cardinality,330},
 {xdcr,low_cardinality,0}]
Raw intervals:
[]
[ns_server:debug,2022-09-08T13:07:42.529Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:07:42.534Z,ns_1@127.0.0.1:<0.16446.5>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:07:42.535Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:07:42.535Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:07:42.736Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_master) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:07:42.737Z,ns_1@127.0.0.1:<0.16461.5>:compaction_daemon:spawn_master_db_compactor:901]Start compaction of master db for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:07:42.740Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:07:42.740Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_master too soon. Next run will be in 3600s
[ns_server:debug,2022-09-08T13:08:12.172Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:08:12.173Z,ns_1@127.0.0.1:<0.17930.5>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:08:12.176Z,ns_1@127.0.0.1:<0.17932.5>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T13:08:12.176Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:08:12.177Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:08:12.536Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:08:12.541Z,ns_1@127.0.0.1:<0.17934.5>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:08:12.542Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:08:12.542Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:08:12.676Z,ns_1@127.0.0.1:ns_audit<0.579.0>:ns_audit:handle_call:148]Audit session_expired: [{timestamp,<<"2022-09-08T13:08:12.676Z">>},
                        {sessionid,<<"47deb49b194c3d0c66afd33f14806311c4771c01">>},
                        {real_userid,{[{domain,builtin},
                                       {user,<<"<ud>admin</ud>">>}]}}]
[ns_server:debug,2022-09-08T13:08:42.179Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:08:42.180Z,ns_1@127.0.0.1:<0.19413.5>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:08:42.182Z,ns_1@127.0.0.1:<0.19415.5>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T13:08:42.183Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:08:42.183Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:08:42.543Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:08:42.546Z,ns_1@127.0.0.1:<0.19427.5>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:08:42.547Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:08:42.547Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:08:54.630Z,ns_1@127.0.0.1:ldap_auth_cache<0.334.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2022-09-08T13:09:12.184Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:09:12.185Z,ns_1@127.0.0.1:<0.20902.5>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:09:12.188Z,ns_1@127.0.0.1:<0.20904.5>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T13:09:12.188Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:09:12.188Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:09:12.548Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:09:12.552Z,ns_1@127.0.0.1:<0.20910.5>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:09:12.553Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:09:12.553Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:09:42.189Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:09:42.190Z,ns_1@127.0.0.1:<0.22389.5>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:09:42.194Z,ns_1@127.0.0.1:<0.22391.5>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T13:09:42.195Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:09:42.195Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:09:42.554Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:09:42.557Z,ns_1@127.0.0.1:<0.22402.5>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:09:42.558Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:09:42.558Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:10:09.631Z,ns_1@127.0.0.1:ldap_auth_cache<0.334.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2022-09-08T13:10:12.196Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:10:12.197Z,ns_1@127.0.0.1:<0.23878.5>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:10:12.200Z,ns_1@127.0.0.1:<0.23880.5>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T13:10:12.200Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:10:12.200Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:10:12.559Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:10:12.562Z,ns_1@127.0.0.1:<0.23885.5>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:10:12.562Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:10:12.562Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:10:42.201Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:10:42.202Z,ns_1@127.0.0.1:<0.25363.5>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:10:42.204Z,ns_1@127.0.0.1:<0.25365.5>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T13:10:42.205Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:10:42.205Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:10:42.563Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:10:42.567Z,ns_1@127.0.0.1:<0.25376.5>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:10:42.568Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:10:42.568Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:11:12.206Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:11:12.207Z,ns_1@127.0.0.1:<0.26852.5>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:11:12.208Z,ns_1@127.0.0.1:<0.26854.5>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T13:11:12.208Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:11:12.208Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:11:12.569Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:11:12.575Z,ns_1@127.0.0.1:<0.26859.5>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:11:12.576Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:11:12.576Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:11:24.632Z,ns_1@127.0.0.1:ldap_auth_cache<0.334.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2022-09-08T13:11:42.209Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:11:42.210Z,ns_1@127.0.0.1:<0.28313.5>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:11:42.211Z,ns_1@127.0.0.1:<0.28315.5>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T13:11:42.211Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:11:42.211Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:11:42.577Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:11:42.580Z,ns_1@127.0.0.1:<0.28352.5>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:11:42.580Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:11:42.580Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:12:12.212Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:12:12.213Z,ns_1@127.0.0.1:<0.29794.5>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:12:12.217Z,ns_1@127.0.0.1:<0.29796.5>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T13:12:12.217Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:12:12.217Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:12:12.581Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:12:12.586Z,ns_1@127.0.0.1:<0.29809.5>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:12:12.587Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:12:12.587Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:12:39.633Z,ns_1@127.0.0.1:ldap_auth_cache<0.334.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2022-09-08T13:12:42.218Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:12:42.219Z,ns_1@127.0.0.1:<0.31279.5>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:12:42.222Z,ns_1@127.0.0.1:<0.31281.5>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T13:12:42.222Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:12:42.222Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:12:42.588Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:12:42.592Z,ns_1@127.0.0.1:<0.31300.5>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:12:42.592Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:12:42.592Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:13:12.223Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:13:12.223Z,ns_1@127.0.0.1:<0.0.6>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:13:12.224Z,ns_1@127.0.0.1:<0.2.6>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T13:13:12.224Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:13:12.224Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:13:12.593Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:13:12.599Z,ns_1@127.0.0.1:<0.15.6>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:13:12.600Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:13:12.600Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:13:12.697Z,ns_1@127.0.0.1:ns_audit<0.579.0>:ns_audit:handle_call:148]Audit session_expired: [{timestamp,<<"2022-09-08T13:13:12.696Z">>},
                        {sessionid,<<"1ea0a993039a7217a0e677b7db2efb1f4749617b">>},
                        {real_userid,{[{domain,builtin},
                                       {user,<<"<ud>admin</ud>">>}]}}]
[ns_server:debug,2022-09-08T13:13:42.225Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:13:42.225Z,ns_1@127.0.0.1:<0.1489.6>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:13:42.226Z,ns_1@127.0.0.1:<0.1491.6>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T13:13:42.227Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:13:42.227Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:13:42.601Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:13:42.606Z,ns_1@127.0.0.1:<0.1510.6>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:13:42.607Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:13:42.607Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:13:54.634Z,ns_1@127.0.0.1:ldap_auth_cache<0.334.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2022-09-08T13:14:12.227Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:14:12.228Z,ns_1@127.0.0.1:<0.2978.6>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:14:12.231Z,ns_1@127.0.0.1:<0.2980.6>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T13:14:12.231Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:14:12.231Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:14:12.608Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:14:12.613Z,ns_1@127.0.0.1:<0.2994.6>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:14:12.614Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:14:12.614Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:14:42.232Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:14:42.233Z,ns_1@127.0.0.1:<0.4463.6>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:14:42.236Z,ns_1@127.0.0.1:<0.4465.6>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T13:14:42.236Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:14:42.236Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:14:42.615Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:14:42.619Z,ns_1@127.0.0.1:<0.4476.6>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:14:42.620Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:14:42.620Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:15:09.598Z,ns_1@127.0.0.1:roles_cache<0.345.0>:active_cache:renew:238]Starting roles_cache cache renewal
[ns_server:debug,2022-09-08T13:15:09.598Z,ns_1@127.0.0.1:roles_cache<0.345.0>:active_cache:renew:244]roles_cache cache renewal process originated 0 queries
[ns_server:debug,2022-09-08T13:15:09.636Z,ns_1@127.0.0.1:ldap_auth_cache<0.334.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2022-09-08T13:15:12.237Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:15:12.238Z,ns_1@127.0.0.1:<0.5952.6>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:15:12.241Z,ns_1@127.0.0.1:<0.5954.6>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T13:15:12.241Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:15:12.242Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:15:12.621Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:15:12.627Z,ns_1@127.0.0.1:<0.5959.6>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:15:12.628Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:15:12.628Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:15:42.243Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:15:42.244Z,ns_1@127.0.0.1:<0.7438.6>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:15:42.246Z,ns_1@127.0.0.1:<0.7440.6>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T13:15:42.247Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:15:42.247Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:15:42.629Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:15:42.634Z,ns_1@127.0.0.1:<0.7452.6>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:15:42.635Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:15:42.635Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:16:12.248Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:16:12.249Z,ns_1@127.0.0.1:<0.8923.6>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:16:12.252Z,ns_1@127.0.0.1:<0.8925.6>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T13:16:12.253Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:16:12.253Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:16:12.636Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:16:12.641Z,ns_1@127.0.0.1:<0.8931.6>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:16:12.642Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:16:12.642Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:16:24.637Z,ns_1@127.0.0.1:ldap_auth_cache<0.334.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2022-09-08T13:16:42.254Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:16:42.255Z,ns_1@127.0.0.1:<0.10404.6>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:16:42.258Z,ns_1@127.0.0.1:<0.10406.6>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T13:16:42.259Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:16:42.259Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:16:42.643Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:16:42.649Z,ns_1@127.0.0.1:<0.10422.6>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:16:42.649Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:16:42.650Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:17:12.260Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:17:12.261Z,ns_1@127.0.0.1:<0.11889.6>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:17:12.262Z,ns_1@127.0.0.1:<0.11891.6>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T13:17:12.262Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:17:12.262Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:17:12.651Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:17:12.655Z,ns_1@127.0.0.1:<0.11905.6>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:17:12.656Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:17:12.656Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:17:39.565Z,ns_1@127.0.0.1:roles_cache<0.345.0>:active_cache:cleanup:258]Cache roles_cache cleanup: 0/0 records deleted
[ns_server:debug,2022-09-08T13:17:39.638Z,ns_1@127.0.0.1:ldap_auth_cache<0.334.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2022-09-08T13:17:42.263Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:17:42.264Z,ns_1@127.0.0.1:<0.13373.6>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:17:42.266Z,ns_1@127.0.0.1:<0.13375.6>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T13:17:42.266Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:17:42.266Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:17:42.385Z,ns_1@127.0.0.1:prometheus_cfg<0.391.0>:prometheus_cfg:maybe_update_scrape_dynamic_intervals:941]Recalculating prometheus scrape intervals for high cardinality metrics
[ns_server:debug,2022-09-08T13:17:42.397Z,ns_1@127.0.0.1:prometheus_cfg<0.391.0>:prometheus_cfg:maybe_update_scrape_dynamic_intervals:958]Scrape intervals haven't changed:
[]
CBCollect stats dir size estimation: 629596800
Calculated based on scrapes info:
[{backup,low_cardinality,0},
 {cbas,high_cardinality,0},
 {cbas,low_cardinality,14},
 {eventing,high_cardinality,0},
 {eventing,low_cardinality,1},
 {fts,high_cardinality,0},
 {fts,low_cardinality,25},
 {index,low_cardinality,2},
 {index,high_cardinality,24},
 {kv,low_cardinality,291},
 {kv,high_cardinality,1123},
 {n1ql,low_cardinality,33},
 {ns_server,low_cardinality,167},
 {ns_server,high_cardinality,330},
 {xdcr,low_cardinality,0}]
Raw intervals:
[]
[ns_server:debug,2022-09-08T13:17:42.657Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:17:42.662Z,ns_1@127.0.0.1:<0.13402.6>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:17:42.663Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:17:42.663Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:18:12.267Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:18:12.268Z,ns_1@127.0.0.1:<0.14867.6>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:18:12.271Z,ns_1@127.0.0.1:<0.14869.6>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T13:18:12.271Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:18:12.272Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:18:12.664Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:18:12.671Z,ns_1@127.0.0.1:<0.14885.6>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:18:12.672Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:18:12.672Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:18:27.718Z,ns_1@127.0.0.1:ns_audit<0.579.0>:ns_audit:handle_call:148]Audit session_expired: [{timestamp,<<"2022-09-08T13:18:27.718Z">>},
                        {sessionid,<<"eba3f25b6498e48ef65edbc9117c24ecf51abb29">>},
                        {real_userid,{[{domain,builtin},
                                       {user,<<"<ud>admin</ud>">>}]}}]
[ns_server:debug,2022-09-08T13:18:42.272Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:18:42.272Z,ns_1@127.0.0.1:<0.16352.6>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:18:42.274Z,ns_1@127.0.0.1:<0.16354.6>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T13:18:42.274Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:18:42.274Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:18:42.673Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:18:42.678Z,ns_1@127.0.0.1:<0.16377.6>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:18:42.678Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:18:42.678Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:18:54.639Z,ns_1@127.0.0.1:ldap_auth_cache<0.334.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2022-09-08T13:19:12.275Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:19:12.277Z,ns_1@127.0.0.1:<0.17831.6>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:19:12.279Z,ns_1@127.0.0.1:<0.17833.6>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T13:19:12.280Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:19:12.280Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:19:12.679Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:19:12.684Z,ns_1@127.0.0.1:<0.17857.6>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:19:12.685Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:19:12.685Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:19:42.281Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:19:42.282Z,ns_1@127.0.0.1:<0.19315.6>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:19:42.285Z,ns_1@127.0.0.1:<0.19317.6>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T13:19:42.286Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:19:42.286Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:19:42.686Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:19:42.691Z,ns_1@127.0.0.1:<0.19347.6>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:19:42.692Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:19:42.692Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:20:09.640Z,ns_1@127.0.0.1:ldap_auth_cache<0.334.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2022-09-08T13:20:12.287Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:20:12.288Z,ns_1@127.0.0.1:<0.20806.6>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:20:12.291Z,ns_1@127.0.0.1:<0.20808.6>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T13:20:12.291Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:20:12.291Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:20:12.693Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:20:12.698Z,ns_1@127.0.0.1:<0.20830.6>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:20:12.698Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:20:12.699Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:20:42.292Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:20:42.293Z,ns_1@127.0.0.1:<0.22288.6>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:20:42.295Z,ns_1@127.0.0.1:<0.22290.6>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T13:20:42.296Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:20:42.296Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:20:42.700Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:20:42.704Z,ns_1@127.0.0.1:<0.22318.6>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:20:42.705Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:20:42.705Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:21:12.297Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:21:12.298Z,ns_1@127.0.0.1:<0.23774.6>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:21:12.301Z,ns_1@127.0.0.1:<0.23776.6>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T13:21:12.301Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:21:12.302Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:21:12.706Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:21:12.711Z,ns_1@127.0.0.1:<0.23798.6>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:21:12.712Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:21:12.712Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:21:24.641Z,ns_1@127.0.0.1:ldap_auth_cache<0.334.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2022-09-08T13:21:42.303Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:21:42.305Z,ns_1@127.0.0.1:<0.25258.6>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:21:42.307Z,ns_1@127.0.0.1:<0.25260.6>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T13:21:42.307Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:21:42.308Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:21:42.714Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:21:42.719Z,ns_1@127.0.0.1:<0.25288.6>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:21:42.720Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:21:42.720Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:22:12.308Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:22:12.309Z,ns_1@127.0.0.1:<0.26744.6>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:22:12.312Z,ns_1@127.0.0.1:<0.26746.6>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T13:22:12.312Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:22:12.312Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:22:12.722Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:22:12.725Z,ns_1@127.0.0.1:<0.26763.6>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:22:12.726Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:22:12.726Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:22:39.600Z,ns_1@127.0.0.1:roles_cache<0.345.0>:active_cache:renew:238]Starting roles_cache cache renewal
[ns_server:debug,2022-09-08T13:22:39.600Z,ns_1@127.0.0.1:roles_cache<0.345.0>:active_cache:renew:244]roles_cache cache renewal process originated 0 queries
[ns_server:debug,2022-09-08T13:22:39.643Z,ns_1@127.0.0.1:ldap_auth_cache<0.334.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2022-09-08T13:22:42.313Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:22:42.314Z,ns_1@127.0.0.1:<0.28229.6>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:22:42.317Z,ns_1@127.0.0.1:<0.28231.6>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T13:22:42.318Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:22:42.318Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:22:42.727Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:22:42.732Z,ns_1@127.0.0.1:<0.28254.6>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:22:42.733Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:22:42.733Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:23:12.319Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:23:12.320Z,ns_1@127.0.0.1:<0.29718.6>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:23:12.322Z,ns_1@127.0.0.1:<0.29720.6>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T13:23:12.323Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:23:12.323Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:23:12.735Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:23:12.739Z,ns_1@127.0.0.1:<0.29737.6>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:23:12.740Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:23:12.740Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:23:27.738Z,ns_1@127.0.0.1:ns_audit<0.579.0>:ns_audit:handle_call:148]Audit session_expired: [{timestamp,<<"2022-09-08T13:23:27.738Z">>},
                        {sessionid,<<"34b36becac4539f57d24d45be3ba076d6c34ba4e">>},
                        {real_userid,{[{domain,builtin},
                                       {user,<<"<ud>admin</ud>">>}]}}]
[ns_server:debug,2022-09-08T13:23:42.324Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:23:42.324Z,ns_1@127.0.0.1:<0.31203.6>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:23:42.325Z,ns_1@127.0.0.1:<0.31205.6>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T13:23:42.325Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:23:42.325Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:23:42.741Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:23:42.745Z,ns_1@127.0.0.1:<0.31232.6>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:23:42.746Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:23:42.746Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:23:54.644Z,ns_1@127.0.0.1:ldap_auth_cache<0.334.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2022-09-08T13:24:12.326Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:24:12.327Z,ns_1@127.0.0.1:<0.32670.6>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:24:12.330Z,ns_1@127.0.0.1:<0.32672.6>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T13:24:12.330Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:24:12.331Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:24:12.747Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:24:12.751Z,ns_1@127.0.0.1:<0.32715.6>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:24:12.752Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:24:12.752Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:24:42.332Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:24:42.333Z,ns_1@127.0.0.1:<0.1387.7>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:24:42.336Z,ns_1@127.0.0.1:<0.1389.7>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T13:24:42.336Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:24:42.337Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:24:42.753Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:24:42.756Z,ns_1@127.0.0.1:<0.1425.7>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:24:42.756Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:24:42.756Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:25:09.645Z,ns_1@127.0.0.1:ldap_auth_cache<0.334.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2022-09-08T13:25:12.337Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:25:12.338Z,ns_1@127.0.0.1:<0.2876.7>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:25:12.341Z,ns_1@127.0.0.1:<0.2878.7>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T13:25:12.341Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:25:12.341Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:25:12.757Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:25:12.760Z,ns_1@127.0.0.1:<0.2890.7>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:25:12.761Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:25:12.761Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:25:42.342Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:25:42.343Z,ns_1@127.0.0.1:<0.4363.7>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:25:42.346Z,ns_1@127.0.0.1:<0.4365.7>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T13:25:42.347Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:25:42.347Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:25:42.762Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:25:42.767Z,ns_1@127.0.0.1:<0.4379.7>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:25:42.769Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:25:42.769Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:26:12.348Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:26:12.349Z,ns_1@127.0.0.1:<0.5844.7>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:26:12.352Z,ns_1@127.0.0.1:<0.5846.7>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T13:26:12.352Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:26:12.353Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:26:12.770Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:26:12.775Z,ns_1@127.0.0.1:<0.5859.7>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:26:12.776Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:26:12.777Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:26:24.646Z,ns_1@127.0.0.1:ldap_auth_cache<0.334.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2022-09-08T13:26:42.354Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:26:42.355Z,ns_1@127.0.0.1:<0.7329.7>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:26:42.358Z,ns_1@127.0.0.1:<0.7331.7>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T13:26:42.358Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:26:42.358Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:26:42.778Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:26:42.783Z,ns_1@127.0.0.1:<0.7346.7>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:26:42.784Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:26:42.784Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:27:12.359Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:27:12.360Z,ns_1@127.0.0.1:<0.8818.7>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:27:12.363Z,ns_1@127.0.0.1:<0.8820.7>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T13:27:12.363Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:27:12.364Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:27:12.785Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:27:12.791Z,ns_1@127.0.0.1:<0.8829.7>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:27:12.792Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:27:12.792Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:27:39.566Z,ns_1@127.0.0.1:roles_cache<0.345.0>:active_cache:cleanup:258]Cache roles_cache cleanup: 0/0 records deleted
[ns_server:debug,2022-09-08T13:27:39.647Z,ns_1@127.0.0.1:ldap_auth_cache<0.334.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2022-09-08T13:27:42.365Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:27:42.366Z,ns_1@127.0.0.1:<0.10305.7>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:27:42.370Z,ns_1@127.0.0.1:<0.10307.7>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T13:27:42.370Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:27:42.370Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:27:42.398Z,ns_1@127.0.0.1:prometheus_cfg<0.391.0>:prometheus_cfg:maybe_update_scrape_dynamic_intervals:941]Recalculating prometheus scrape intervals for high cardinality metrics
[ns_server:debug,2022-09-08T13:27:42.411Z,ns_1@127.0.0.1:prometheus_cfg<0.391.0>:prometheus_cfg:maybe_update_scrape_dynamic_intervals:958]Scrape intervals haven't changed:
[]
CBCollect stats dir size estimation: 629596800
Calculated based on scrapes info:
[{backup,low_cardinality,0},
 {cbas,high_cardinality,0},
 {cbas,low_cardinality,14},
 {eventing,high_cardinality,0},
 {eventing,low_cardinality,1},
 {fts,high_cardinality,0},
 {fts,low_cardinality,25},
 {index,low_cardinality,2},
 {index,high_cardinality,24},
 {kv,low_cardinality,291},
 {kv,high_cardinality,1123},
 {n1ql,low_cardinality,33},
 {ns_server,low_cardinality,167},
 {ns_server,high_cardinality,330},
 {xdcr,low_cardinality,0}]
Raw intervals:
[]
[ns_server:debug,2022-09-08T13:27:42.793Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:27:42.798Z,ns_1@127.0.0.1:<0.10326.7>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:27:42.799Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:27:42.799Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:28:12.371Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:28:12.372Z,ns_1@127.0.0.1:<0.11798.7>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:28:12.375Z,ns_1@127.0.0.1:<0.11800.7>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T13:28:12.375Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:28:12.375Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:28:12.800Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:28:12.805Z,ns_1@127.0.0.1:<0.11809.7>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:28:12.806Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:28:12.806Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:28:27.759Z,ns_1@127.0.0.1:ns_audit<0.579.0>:ns_audit:handle_call:148]Audit session_expired: [{timestamp,<<"2022-09-08T13:28:27.758Z">>},
                        {sessionid,<<"d8be83ed459cf436a33cb4ab0349c608adfe6616">>},
                        {real_userid,{[{domain,builtin},
                                       {user,<<"<ud>admin</ud>">>}]}}]
[ns_server:debug,2022-09-08T13:28:42.378Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:28:42.379Z,ns_1@127.0.0.1:<0.13281.7>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:28:42.382Z,ns_1@127.0.0.1:<0.13283.7>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T13:28:42.382Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:28:42.382Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:28:42.807Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:28:42.812Z,ns_1@127.0.0.1:<0.13302.7>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:28:42.813Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:28:42.813Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:28:54.649Z,ns_1@127.0.0.1:ldap_auth_cache<0.334.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2022-09-08T13:29:12.383Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:29:12.384Z,ns_1@127.0.0.1:<0.14770.7>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:29:12.387Z,ns_1@127.0.0.1:<0.14772.7>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T13:29:12.387Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:29:12.387Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:29:12.814Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:29:12.819Z,ns_1@127.0.0.1:<0.14785.7>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:29:12.820Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:29:12.820Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:29:42.388Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:29:42.390Z,ns_1@127.0.0.1:<0.16257.7>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:29:42.393Z,ns_1@127.0.0.1:<0.16259.7>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T13:29:42.393Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:29:42.393Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:29:42.821Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:29:42.826Z,ns_1@127.0.0.1:<0.16274.7>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:29:42.827Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:29:42.827Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:30:09.601Z,ns_1@127.0.0.1:roles_cache<0.345.0>:active_cache:renew:238]Starting roles_cache cache renewal
[ns_server:debug,2022-09-08T13:30:09.601Z,ns_1@127.0.0.1:roles_cache<0.345.0>:active_cache:renew:244]roles_cache cache renewal process originated 0 queries
[ns_server:debug,2022-09-08T13:30:09.649Z,ns_1@127.0.0.1:ldap_auth_cache<0.334.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2022-09-08T13:30:12.394Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:30:12.396Z,ns_1@127.0.0.1:<0.17746.7>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:30:12.399Z,ns_1@127.0.0.1:<0.17748.7>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T13:30:12.400Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:30:12.400Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:30:12.828Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:30:12.833Z,ns_1@127.0.0.1:<0.17757.7>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:30:12.834Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:30:12.834Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:30:42.401Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:30:42.402Z,ns_1@127.0.0.1:<0.19231.7>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:30:42.405Z,ns_1@127.0.0.1:<0.19233.7>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T13:30:42.406Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:30:42.406Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:30:42.835Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:30:42.841Z,ns_1@127.0.0.1:<0.19248.7>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:30:42.842Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:30:42.842Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:31:12.407Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:31:12.408Z,ns_1@127.0.0.1:<0.20720.7>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:31:12.410Z,ns_1@127.0.0.1:<0.20722.7>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T13:31:12.410Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:31:12.410Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:31:12.843Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:31:12.848Z,ns_1@127.0.0.1:<0.20731.7>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:31:12.849Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:31:12.849Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:31:24.650Z,ns_1@127.0.0.1:ldap_auth_cache<0.334.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2022-09-08T13:31:42.411Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:31:42.412Z,ns_1@127.0.0.1:<0.22207.7>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:31:42.413Z,ns_1@127.0.0.1:<0.22209.7>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T13:31:42.413Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:31:42.413Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:31:42.852Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:31:42.856Z,ns_1@127.0.0.1:<0.22220.7>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:31:42.857Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:31:42.857Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:32:12.414Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:32:12.415Z,ns_1@127.0.0.1:<0.23696.7>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:32:12.416Z,ns_1@127.0.0.1:<0.23698.7>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T13:32:12.416Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:32:12.416Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:32:12.858Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:32:12.863Z,ns_1@127.0.0.1:<0.23704.7>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:32:12.864Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:32:12.864Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:32:39.651Z,ns_1@127.0.0.1:ldap_auth_cache<0.334.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2022-09-08T13:32:42.417Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:32:42.418Z,ns_1@127.0.0.1:<0.25180.7>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:32:42.420Z,ns_1@127.0.0.1:<0.25182.7>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T13:32:42.420Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:32:42.420Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:32:42.865Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:32:42.870Z,ns_1@127.0.0.1:<0.25194.7>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:32:42.871Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:32:42.871Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:33:12.421Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:33:12.422Z,ns_1@127.0.0.1:<0.26668.7>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:33:12.423Z,ns_1@127.0.0.1:<0.26670.7>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T13:33:12.424Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:33:12.424Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:33:12.872Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:33:12.877Z,ns_1@127.0.0.1:<0.26677.7>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:33:12.878Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:33:12.878Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:33:27.779Z,ns_1@127.0.0.1:ns_audit<0.579.0>:ns_audit:handle_call:148]Audit session_expired: [{timestamp,<<"2022-09-08T13:33:27.779Z">>},
                        {sessionid,<<"e5370c96fd7005e80271f52af9d461fdb08899cb">>},
                        {real_userid,{[{domain,builtin},
                                       {user,<<"<ud>admin</ud>">>}]}}]
[ns_server:debug,2022-09-08T13:33:42.425Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:33:42.426Z,ns_1@127.0.0.1:<0.28154.7>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:33:42.428Z,ns_1@127.0.0.1:<0.28156.7>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T13:33:42.429Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:33:42.429Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:33:42.879Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:33:42.884Z,ns_1@127.0.0.1:<0.28172.7>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:33:42.885Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:33:42.885Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:33:54.652Z,ns_1@127.0.0.1:ldap_auth_cache<0.334.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2022-09-08T13:34:12.430Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:34:12.431Z,ns_1@127.0.0.1:<0.29642.7>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:34:12.434Z,ns_1@127.0.0.1:<0.29644.7>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T13:34:12.434Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:34:12.434Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:34:12.886Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:34:12.891Z,ns_1@127.0.0.1:<0.29655.7>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:34:12.892Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:34:12.892Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:34:42.436Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:34:42.436Z,ns_1@127.0.0.1:<0.31128.7>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:34:42.439Z,ns_1@127.0.0.1:<0.31130.7>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T13:34:42.439Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:34:42.439Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:34:42.893Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:34:42.899Z,ns_1@127.0.0.1:<0.31146.7>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:34:42.900Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:34:42.900Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:35:09.653Z,ns_1@127.0.0.1:ldap_auth_cache<0.334.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2022-09-08T13:35:12.440Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:35:12.441Z,ns_1@127.0.0.1:<0.32616.7>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:35:12.444Z,ns_1@127.0.0.1:<0.32618.7>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T13:35:12.444Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:35:12.444Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:35:12.901Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:35:12.907Z,ns_1@127.0.0.1:<0.32629.7>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:35:12.908Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:35:12.908Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:35:42.445Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:35:42.446Z,ns_1@127.0.0.1:<0.1336.8>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:35:42.449Z,ns_1@127.0.0.1:<0.1338.8>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T13:35:42.449Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:35:42.449Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:35:42.909Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:35:42.914Z,ns_1@127.0.0.1:<0.1354.8>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:35:42.915Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:35:42.915Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:36:12.450Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:36:12.450Z,ns_1@127.0.0.1:<0.2807.8>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:36:12.452Z,ns_1@127.0.0.1:<0.2810.8>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T13:36:12.452Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:36:12.452Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:36:12.916Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:36:12.922Z,ns_1@127.0.0.1:<0.2837.8>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:36:12.922Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:36:12.923Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:36:24.654Z,ns_1@127.0.0.1:ldap_auth_cache<0.334.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2022-09-08T13:36:42.453Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:36:42.454Z,ns_1@127.0.0.1:<0.4284.8>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:36:42.457Z,ns_1@127.0.0.1:<0.4286.8>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T13:36:42.458Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:36:42.458Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:36:42.924Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:36:42.929Z,ns_1@127.0.0.1:<0.4328.8>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:36:42.930Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:36:42.930Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:37:12.459Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:37:12.460Z,ns_1@127.0.0.1:<0.5772.8>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:37:12.463Z,ns_1@127.0.0.1:<0.5774.8>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T13:37:12.463Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:37:12.463Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:37:12.931Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:37:12.936Z,ns_1@127.0.0.1:<0.5807.8>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:37:12.937Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:37:12.937Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:37:39.567Z,ns_1@127.0.0.1:roles_cache<0.345.0>:active_cache:cleanup:258]Cache roles_cache cleanup: 0/0 records deleted
[ns_server:debug,2022-09-08T13:37:39.603Z,ns_1@127.0.0.1:roles_cache<0.345.0>:active_cache:renew:238]Starting roles_cache cache renewal
[ns_server:debug,2022-09-08T13:37:39.603Z,ns_1@127.0.0.1:roles_cache<0.345.0>:active_cache:renew:244]roles_cache cache renewal process originated 0 queries
[ns_server:debug,2022-09-08T13:37:39.655Z,ns_1@127.0.0.1:ldap_auth_cache<0.334.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2022-09-08T13:37:42.412Z,ns_1@127.0.0.1:prometheus_cfg<0.391.0>:prometheus_cfg:maybe_update_scrape_dynamic_intervals:941]Recalculating prometheus scrape intervals for high cardinality metrics
[ns_server:debug,2022-09-08T13:37:42.417Z,ns_1@127.0.0.1:prometheus_cfg<0.391.0>:prometheus_cfg:maybe_update_scrape_dynamic_intervals:958]Scrape intervals haven't changed:
[]
CBCollect stats dir size estimation: 629596800
Calculated based on scrapes info:
[{backup,low_cardinality,0},
 {cbas,high_cardinality,0},
 {cbas,low_cardinality,14},
 {eventing,high_cardinality,0},
 {eventing,low_cardinality,1},
 {fts,high_cardinality,0},
 {fts,low_cardinality,25},
 {index,low_cardinality,2},
 {index,high_cardinality,24},
 {kv,low_cardinality,291},
 {kv,high_cardinality,1123},
 {n1ql,low_cardinality,33},
 {ns_server,low_cardinality,167},
 {ns_server,high_cardinality,330},
 {xdcr,low_cardinality,0}]
Raw intervals:
[]
[ns_server:debug,2022-09-08T13:37:42.464Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:37:42.465Z,ns_1@127.0.0.1:<0.7263.8>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:37:42.466Z,ns_1@127.0.0.1:<0.7265.8>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T13:37:42.466Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:37:42.466Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:37:42.939Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:37:42.944Z,ns_1@127.0.0.1:<0.7278.8>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:37:42.945Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:37:42.945Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:38:12.467Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:38:12.469Z,ns_1@127.0.0.1:<0.8751.8>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:38:12.471Z,ns_1@127.0.0.1:<0.8753.8>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T13:38:12.472Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:38:12.472Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:38:12.946Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:38:12.952Z,ns_1@127.0.0.1:<0.8759.8>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:38:12.952Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:38:12.953Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:38:42.473Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:38:42.474Z,ns_1@127.0.0.1:<0.10238.8>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:38:42.477Z,ns_1@127.0.0.1:<0.10240.8>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T13:38:42.477Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:38:42.477Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:38:42.800Z,ns_1@127.0.0.1:ns_audit<0.579.0>:ns_audit:handle_call:148]Audit session_expired: [{timestamp,<<"2022-09-08T13:38:42.800Z">>},
                        {sessionid,<<"695baadbe8cf103029524e69843a0b01d2d0fe34">>},
                        {real_userid,{[{domain,builtin},
                                       {user,<<"<ud>admin</ud>">>}]}}]
[ns_server:debug,2022-09-08T13:38:42.954Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:38:42.959Z,ns_1@127.0.0.1:<0.10250.8>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:38:42.960Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:38:42.960Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:38:54.657Z,ns_1@127.0.0.1:ldap_auth_cache<0.334.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2022-09-08T13:46:39.355Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:debug,2022-09-08T13:46:39.355Z,ns_1@127.0.0.1:ldap_auth_cache<0.334.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2022-09-08T13:46:39.366Z,ns_1@127.0.0.1:roles_cache<0.345.0>:active_cache:renew:238]Starting roles_cache cache renewal
[ns_server:debug,2022-09-08T13:46:39.457Z,ns_1@127.0.0.1:leader_lease_agent<0.729.0>:leader_lease_agent:handle_lease_expired:277]Lease held by {lease_holder,<<"6739a7645d0d3cb96437d1c2d542cfcc">>,
                            'ns_1@127.0.0.1'} expired. Starting expirer.
[ns_server:debug,2022-09-08T13:46:39.367Z,ns_1@127.0.0.1:roles_cache<0.345.0>:active_cache:renew:244]roles_cache cache renewal process originated 0 queries
[ns_server:warn,2022-09-08T13:46:39.492Z,ns_1@127.0.0.1:<0.743.0>:leader_lease_acquire_worker:handle_acquire_timeout:106]Timeout while trying to acquire lease from 'ns_1@127.0.0.1'.
Acquire options were [{timeout,0},{period,15000}]
[ns_server:debug,2022-09-08T13:46:39.526Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:46:39.527Z,ns_1@127.0.0.1:<0.11766.8>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:46:39.541Z,ns_1@127.0.0.1:<0.11768.8>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T13:46:39.541Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:46:39.541Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:46:39.544Z,ns_1@127.0.0.1:ns_audit<0.579.0>:ns_audit:handle_call:148]Audit session_expired: [{timestamp,<<"2022-09-08T13:46:39.474Z">>},
                        {sessionid,<<"83ef1618b893a76bfdaeb6fdd45f638d8b12eece">>},
                        {real_userid,{[{domain,builtin},
                                       {user,<<"<ud>admin</ud>">>}]}}]
[error_logger:error,2022-09-08T13:46:39.762Z,ns_1@127.0.0.1:timer_lag_recorder<0.389.0>:ale_error_logger_handler:do_log:101]Detected time forward jump (or too large erlang scheduling latency).  Skipping 448 samples (or 448385 milliseconds)
[ns_server:info,2022-09-08T13:46:39.845Z,ns_1@127.0.0.1:<0.11669.8>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:46:39.848Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:46:39.848Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[error_logger:error,2022-09-08T13:46:39.862Z,ns_1@127.0.0.1:<0.1174.0>:ale_error_logger_handler:do_log:101]
=========================CRASH REPORT=========================
  crasher:
    initial call: service_agent:'-start_long_poll_worker/4-fun-0-'/0
    pid: <0.1174.0>
    registered_name: []
    exception exit: {timeout,
                        {gen_server,call,
                            [<0.1171.0>,
                             {call,"ServiceAPI.GetCurrentTopology",
                                 #Fun<json_rpc_connection.0.86436583>,
                                 #{timeout => 60000}},
                             60000]}}
      in function  gen_server:call/3 (gen_server.erl, line 247)
      in call from service_api:perform_call/4 (src/service_api.erl, line 76)
      in call from service_agent:grab_topology/2 (src/service_agent.erl, line 535)
      in call from service_agent:long_poll_worker_loop/5 (src/service_agent.erl, line 600)
    ancestors: ['service_agent-fts',service_agent_children_sup,
                  service_agent_sup,ns_server_sup,ns_server_nodes_sup,
                  <0.274.0>,ns_server_cluster_sup,root_sup,<0.145.0>]
    message_queue_len: 0
    messages: []
    links: [<0.552.0>]
    dictionary: []
    trap_exit: false
    status: running
    heap_size: 1598
    stack_size: 29
    reductions: 24460
  neighbours:

[error_logger:error,2022-09-08T13:46:39.863Z,ns_1@127.0.0.1:<0.1437.0>:ale_error_logger_handler:do_log:101]
=========================CRASH REPORT=========================
  crasher:
    initial call: service_agent:'-start_long_poll_worker/4-fun-0-'/0
    pid: <0.1437.0>
    registered_name: []
    exception exit: {timeout,
                        {gen_server,call,
                            [<0.1435.0>,
                             {call,"ServiceAPI.GetTaskList",
                                 #Fun<json_rpc_connection.0.86436583>,
                                 #{timeout => 60000}},
                             60000]}}
      in function  gen_server:call/3 (gen_server.erl, line 247)
      in call from service_api:perform_call/4 (src/service_api.erl, line 76)
      in call from service_agent:grab_tasks/2 (src/service_agent.erl, line 519)
      in call from service_agent:long_poll_worker_loop/5 (src/service_agent.erl, line 600)
    ancestors: ['service_agent-n1ql',service_agent_children_sup,
                  service_agent_sup,ns_server_sup,ns_server_nodes_sup,
                  <0.274.0>,ns_server_cluster_sup,root_sup,<0.145.0>]
    message_queue_len: 0
    messages: []
    links: [<0.560.0>]
    dictionary: []
    trap_exit: false
    status: running
    heap_size: 610
    stack_size: 29
    reductions: 15683
  neighbours:

[error_logger:error,2022-09-08T13:46:39.863Z,ns_1@127.0.0.1:<0.1438.0>:ale_error_logger_handler:do_log:101]
=========================CRASH REPORT=========================
  crasher:
    initial call: service_agent:'-start_long_poll_worker/4-fun-0-'/0
    pid: <0.1438.0>
    registered_name: []
    exception exit: {timeout,
                        {gen_server,call,
                            [<0.1435.0>,
                             {call,"ServiceAPI.GetCurrentTopology",
                                 #Fun<json_rpc_connection.0.86436583>,
                                 #{timeout => 60000}},
                             60000]}}
      in function  gen_server:call/3 (gen_server.erl, line 247)
      in call from service_api:perform_call/4 (src/service_api.erl, line 76)
      in call from service_agent:grab_topology/2 (src/service_agent.erl, line 535)
      in call from service_agent:long_poll_worker_loop/5 (src/service_agent.erl, line 600)
    ancestors: ['service_agent-n1ql',service_agent_children_sup,
                  service_agent_sup,ns_server_sup,ns_server_nodes_sup,
                  <0.274.0>,ns_server_cluster_sup,root_sup,<0.145.0>]
    message_queue_len: 0
    messages: []
    links: [<0.560.0>]
    dictionary: []
    trap_exit: false
    status: running
    heap_size: 1598
    stack_size: 29
    reductions: 16316
  neighbours:

[error_logger:error,2022-09-08T13:46:39.863Z,ns_1@127.0.0.1:<0.919.0>:ale_error_logger_handler:do_log:101]
=========================CRASH REPORT=========================
  crasher:
    initial call: service_agent:'-start_long_poll_worker/4-fun-0-'/0
    pid: <0.919.0>
    registered_name: []
    exception exit: {timeout,
                        {gen_server,call,
                            [<0.913.0>,
                             {call,"ServiceAPI.GetCurrentTopology",
                                 #Fun<json_rpc_connection.0.86436583>,
                                 #{timeout => 60000}},
                             60000]}}
      in function  gen_server:call/3 (gen_server.erl, line 247)
      in call from service_api:perform_call/4 (src/service_api.erl, line 76)
      in call from service_agent:grab_topology/2 (src/service_agent.erl, line 535)
      in call from service_agent:long_poll_worker_loop/5 (src/service_agent.erl, line 600)
    ancestors: ['service_agent-cbas',service_agent_children_sup,
                  service_agent_sup,ns_server_sup,ns_server_nodes_sup,
                  <0.274.0>,ns_server_cluster_sup,root_sup,<0.145.0>]
    message_queue_len: 0
    messages: []
    links: [<0.544.0>]
    dictionary: []
    trap_exit: false
    status: running
    heap_size: 1598
    stack_size: 29
    reductions: 16406
  neighbours:

[error_logger:error,2022-09-08T13:46:39.864Z,ns_1@127.0.0.1:<0.1093.0>:ale_error_logger_handler:do_log:101]
=========================CRASH REPORT=========================
  crasher:
    initial call: service_agent:'-start_long_poll_worker/4-fun-0-'/0
    pid: <0.1093.0>
    registered_name: []
    exception exit: {timeout,
                        {gen_server,call,
                            [<0.1090.0>,
                             {call,"ServiceAPI.GetTaskList",
                                 #Fun<json_rpc_connection.0.86436583>,
                                 #{timeout => 60000}},
                             60000]}}
      in function  gen_server:call/3 (gen_server.erl, line 247)
      in call from service_api:perform_call/4 (src/service_api.erl, line 76)
      in call from service_agent:grab_tasks/2 (src/service_agent.erl, line 519)
      in call from service_agent:long_poll_worker_loop/5 (src/service_agent.erl, line 600)
    ancestors: ['service_agent-backup',service_agent_children_sup,
                  service_agent_sup,ns_server_sup,ns_server_nodes_sup,
                  <0.274.0>,ns_server_cluster_sup,root_sup,<0.145.0>]
    message_queue_len: 0
    messages: []
    links: [<0.540.0>]
    dictionary: []
    trap_exit: false
    status: running
    heap_size: 610
    stack_size: 29
    reductions: 15683
  neighbours:

[ns_server:error,2022-09-08T13:46:39.863Z,ns_1@127.0.0.1:service_agent-n1ql<0.560.0>:service_agent:handle_info:230]Linked process <0.1437.0> died with reason {timeout,
                                            {gen_server,call,
                                             [<0.1435.0>,
                                              {call,"ServiceAPI.GetTaskList",
                                               #Fun<json_rpc_connection.0.86436583>,
                                               #{timeout => 60000}},
                                              60000]}}. Terminating
[error_logger:error,2022-09-08T13:46:39.864Z,ns_1@127.0.0.1:<0.1238.0>:ale_error_logger_handler:do_log:101]
=========================CRASH REPORT=========================
  crasher:
    initial call: service_agent:'-start_long_poll_worker/4-fun-0-'/0
    pid: <0.1238.0>
    registered_name: []
    exception exit: {timeout,
                        {gen_server,call,
                            [<0.1218.0>,
                             {call,"ServiceAPI.GetCurrentTopology",
                                 #Fun<json_rpc_connection.0.86436583>,
                                 #{timeout => 60000}},
                             60000]}}
      in function  gen_server:call/3 (gen_server.erl, line 247)
      in call from service_api:perform_call/4 (src/service_api.erl, line 76)
      in call from service_agent:grab_topology/2 (src/service_agent.erl, line 535)
      in call from service_agent:long_poll_worker_loop/5 (src/service_agent.erl, line 600)
    ancestors: ['service_agent-index',service_agent_children_sup,
                  service_agent_sup,ns_server_sup,ns_server_nodes_sup,
                  <0.274.0>,ns_server_cluster_sup,root_sup,<0.145.0>]
    message_queue_len: 0
    messages: []
    links: [<0.556.0>]
    dictionary: []
    trap_exit: false
    status: running
    heap_size: 1598
    stack_size: 29
    reductions: 16316
  neighbours:

[ns_server:error,2022-09-08T13:46:39.864Z,ns_1@127.0.0.1:service_agent-cbas<0.544.0>:service_agent:handle_info:230]Linked process <0.919.0> died with reason {timeout,
                                           {gen_server,call,
                                            [<0.913.0>,
                                             {call,
                                              "ServiceAPI.GetCurrentTopology",
                                              #Fun<json_rpc_connection.0.86436583>,
                                              #{timeout => 60000}},
                                             60000]}}. Terminating
[ns_server:error,2022-09-08T13:46:39.864Z,ns_1@127.0.0.1:service_agent-backup<0.540.0>:service_agent:handle_info:230]Linked process <0.1093.0> died with reason {timeout,
                                            {gen_server,call,
                                             [<0.1090.0>,
                                              {call,"ServiceAPI.GetTaskList",
                                               #Fun<json_rpc_connection.0.86436583>,
                                               #{timeout => 60000}},
                                              60000]}}. Terminating
[error_logger:error,2022-09-08T13:46:39.864Z,ns_1@127.0.0.1:<0.1041.0>:ale_error_logger_handler:do_log:101]
=========================CRASH REPORT=========================
  crasher:
    initial call: service_agent:'-start_long_poll_worker/4-fun-0-'/0
    pid: <0.1041.0>
    registered_name: []
    exception exit: {timeout,
                        {gen_server,call,
                            [<0.1038.0>,
                             {call,"ServiceAPI.GetTaskList",
                                 #Fun<json_rpc_connection.0.86436583>,
                                 #{timeout => 60000}},
                             60000]}}
      in function  gen_server:call/3 (gen_server.erl, line 247)
      in call from service_api:perform_call/4 (src/service_api.erl, line 76)
      in call from service_agent:grab_tasks/2 (src/service_agent.erl, line 519)
      in call from service_agent:long_poll_worker_loop/5 (src/service_agent.erl, line 600)
    ancestors: ['service_agent-eventing',service_agent_children_sup,
                  service_agent_sup,ns_server_sup,ns_server_nodes_sup,
                  <0.274.0>,ns_server_cluster_sup,root_sup,<0.145.0>]
    message_queue_len: 0
    messages: []
    links: [<0.548.0>]
    dictionary: []
    trap_exit: false
    status: running
    heap_size: 610
    stack_size: 29
    reductions: 16140
  neighbours:

[ns_server:error,2022-09-08T13:46:39.864Z,ns_1@127.0.0.1:service_agent-n1ql<0.560.0>:service_agent:terminate:259]Terminating abnormally
[ns_server:error,2022-09-08T13:46:39.864Z,ns_1@127.0.0.1:service_agent-n1ql<0.560.0>:service_agent:terminate:264]Terminating json rpc connection for n1ql: <0.1435.0>
[ns_server:error,2022-09-08T13:46:39.864Z,ns_1@127.0.0.1:service_agent-index<0.556.0>:service_agent:handle_info:230]Linked process <0.1238.0> died with reason {timeout,
                                            {gen_server,call,
                                             [<0.1218.0>,
                                              {call,
                                               "ServiceAPI.GetCurrentTopology",
                                               #Fun<json_rpc_connection.0.86436583>,
                                               #{timeout => 60000}},
                                              60000]}}. Terminating
[ns_server:error,2022-09-08T13:46:39.865Z,ns_1@127.0.0.1:service_agent-cbas<0.544.0>:service_agent:terminate:259]Terminating abnormally
[ns_server:error,2022-09-08T13:46:39.865Z,ns_1@127.0.0.1:service_agent-backup<0.540.0>:service_agent:terminate:259]Terminating abnormally
[ns_server:error,2022-09-08T13:46:39.865Z,ns_1@127.0.0.1:service_agent-eventing<0.548.0>:service_agent:handle_info:230]Linked process <0.1041.0> died with reason {timeout,
                                            {gen_server,call,
                                             [<0.1038.0>,
                                              {call,"ServiceAPI.GetTaskList",
                                               #Fun<json_rpc_connection.0.86436583>,
                                               #{timeout => 60000}},
                                              60000]}}. Terminating
[ns_server:error,2022-09-08T13:46:39.865Z,ns_1@127.0.0.1:service_agent-cbas<0.544.0>:service_agent:terminate:264]Terminating json rpc connection for cbas: <0.913.0>
[ns_server:error,2022-09-08T13:46:39.865Z,ns_1@127.0.0.1:service_agent-backup<0.540.0>:service_agent:terminate:264]Terminating json rpc connection for backup: <0.1090.0>
[ns_server:error,2022-09-08T13:46:39.865Z,ns_1@127.0.0.1:service_agent-index<0.556.0>:service_agent:terminate:259]Terminating abnormally
[ns_server:error,2022-09-08T13:46:39.865Z,ns_1@127.0.0.1:service_agent-index<0.556.0>:service_agent:terminate:264]Terminating json rpc connection for index: <0.1218.0>
[ns_server:error,2022-09-08T13:46:39.865Z,ns_1@127.0.0.1:service_agent-eventing<0.548.0>:service_agent:terminate:259]Terminating abnormally
[error_logger:error,2022-09-08T13:46:39.865Z,ns_1@127.0.0.1:json_rpc_connection_sup<0.273.0>:ale_error_logger_handler:do_log:101]
=========================SUPERVISOR REPORT=========================
    supervisor: {local,json_rpc_connection_sup}
    errorContext: child_terminated
    reason: {service_agent_died,
                {linked_process_died,<0.1437.0>,
                    {'ns_1@127.0.0.1',
                        {timeout,
                            {gen_server,call,
                                [<0.1435.0>,
                                 {call,"ServiceAPI.GetTaskList",
                                     #Fun<json_rpc_connection.0.86436583>,
                                     #{timeout => 60000}},
                                 60000]}}}}}
    offender: [{pid,<0.1435.0>},
               {id,json_rpc_connection},
               {mfargs,{json_rpc_connection,start_link,undefined}},
               {restart_type,temporary},
               {significant,false},
               {shutdown,brutal_kill},
               {child_type,worker}]

[ns_server:error,2022-09-08T13:46:39.865Z,ns_1@127.0.0.1:service_agent-eventing<0.548.0>:service_agent:terminate:264]Terminating json rpc connection for eventing: <0.1038.0>
[error_logger:error,2022-09-08T13:46:39.865Z,ns_1@127.0.0.1:json_rpc_connection_sup<0.273.0>:ale_error_logger_handler:do_log:101]
=========================SUPERVISOR REPORT=========================
    supervisor: {local,json_rpc_connection_sup}
    errorContext: child_terminated
    reason: {service_agent_died,
                {linked_process_died,<0.919.0>,
                    {'ns_1@127.0.0.1',
                        {timeout,
                            {gen_server,call,
                                [<0.913.0>,
                                 {call,"ServiceAPI.GetCurrentTopology",
                                     #Fun<json_rpc_connection.0.86436583>,
                                     #{timeout => 60000}},
                                 60000]}}}}}
    offender: [{pid,<0.913.0>},
               {id,json_rpc_connection},
               {mfargs,{json_rpc_connection,start_link,undefined}},
               {restart_type,temporary},
               {significant,false},
               {shutdown,brutal_kill},
               {child_type,worker}]

[error_logger:error,2022-09-08T13:46:39.865Z,ns_1@127.0.0.1:json_rpc_connection_sup<0.273.0>:ale_error_logger_handler:do_log:101]
=========================SUPERVISOR REPORT=========================
    supervisor: {local,json_rpc_connection_sup}
    errorContext: child_terminated
    reason: {service_agent_died,
                {linked_process_died,<0.1093.0>,
                    {'ns_1@127.0.0.1',
                        {timeout,
                            {gen_server,call,
                                [<0.1090.0>,
                                 {call,"ServiceAPI.GetTaskList",
                                     #Fun<json_rpc_connection.0.86436583>,
                                     #{timeout => 60000}},
                                 60000]}}}}}
    offender: [{pid,<0.1090.0>},
               {id,json_rpc_connection},
               {mfargs,{json_rpc_connection,start_link,undefined}},
               {restart_type,temporary},
               {significant,false},
               {shutdown,brutal_kill},
               {child_type,worker}]

[error_logger:error,2022-09-08T13:46:39.865Z,ns_1@127.0.0.1:json_rpc_connection_sup<0.273.0>:ale_error_logger_handler:do_log:101]
=========================SUPERVISOR REPORT=========================
    supervisor: {local,json_rpc_connection_sup}
    errorContext: child_terminated
    reason: {service_agent_died,
                {linked_process_died,<0.1238.0>,
                    {'ns_1@127.0.0.1',
                        {timeout,
                            {gen_server,call,
                                [<0.1218.0>,
                                 {call,"ServiceAPI.GetCurrentTopology",
                                     #Fun<json_rpc_connection.0.86436583>,
                                     #{timeout => 60000}},
                                 60000]}}}}}
    offender: [{pid,<0.1218.0>},
               {id,json_rpc_connection},
               {mfargs,{json_rpc_connection,start_link,undefined}},
               {restart_type,temporary},
               {significant,false},
               {shutdown,brutal_kill},
               {child_type,worker}]

[error_logger:error,2022-09-08T13:46:39.866Z,ns_1@127.0.0.1:json_rpc_connection_sup<0.273.0>:ale_error_logger_handler:do_log:101]
=========================SUPERVISOR REPORT=========================
    supervisor: {local,json_rpc_connection_sup}
    errorContext: child_terminated
    reason: {service_agent_died,
                {linked_process_died,<0.1041.0>,
                    {'ns_1@127.0.0.1',
                        {timeout,
                            {gen_server,call,
                                [<0.1038.0>,
                                 {call,"ServiceAPI.GetTaskList",
                                     #Fun<json_rpc_connection.0.86436583>,
                                     #{timeout => 60000}},
                                 60000]}}}}}
    offender: [{pid,<0.1038.0>},
               {id,json_rpc_connection},
               {mfargs,{json_rpc_connection,start_link,undefined}},
               {restart_type,temporary},
               {significant,false},
               {shutdown,brutal_kill},
               {child_type,worker}]

[ns_server:error,2022-09-08T13:46:39.863Z,ns_1@127.0.0.1:service_agent-fts<0.552.0>:service_agent:handle_info:230]Linked process <0.1174.0> died with reason {timeout,
                                            {gen_server,call,
                                             [<0.1171.0>,
                                              {call,
                                               "ServiceAPI.GetCurrentTopology",
                                               #Fun<json_rpc_connection.0.86436583>,
                                               #{timeout => 60000}},
                                              60000]}}. Terminating
[ns_server:error,2022-09-08T13:46:39.870Z,ns_1@127.0.0.1:service_agent-fts<0.552.0>:service_agent:terminate:259]Terminating abnormally
[ns_server:error,2022-09-08T13:46:39.870Z,ns_1@127.0.0.1:service_agent-fts<0.552.0>:service_agent:terminate:264]Terminating json rpc connection for fts: <0.1171.0>
[error_logger:error,2022-09-08T13:46:39.871Z,ns_1@127.0.0.1:json_rpc_connection_sup<0.273.0>:ale_error_logger_handler:do_log:101]
=========================SUPERVISOR REPORT=========================
    supervisor: {local,json_rpc_connection_sup}
    errorContext: child_terminated
    reason: {service_agent_died,
                {linked_process_died,<0.1174.0>,
                    {'ns_1@127.0.0.1',
                        {timeout,
                            {gen_server,call,
                                [<0.1171.0>,
                                 {call,"ServiceAPI.GetCurrentTopology",
                                     #Fun<json_rpc_connection.0.86436583>,
                                     #{timeout => 60000}},
                                 60000]}}}}}
    offender: [{pid,<0.1171.0>},
               {id,json_rpc_connection},
               {mfargs,{json_rpc_connection,start_link,undefined}},
               {restart_type,temporary},
               {significant,false},
               {shutdown,brutal_kill},
               {child_type,worker}]

[error_logger:error,2022-09-08T13:46:39.895Z,ns_1@127.0.0.1:service_agent-n1ql<0.560.0>:ale_error_logger_handler:do_log:101]
=========================ERROR REPORT=========================
** Generic server 'service_agent-n1ql' terminating 
** Last message in was {'EXIT',<0.1437.0>,
                        {timeout,
                         {gen_server,call,
                          [<0.1435.0>,
                           {call,"ServiceAPI.GetTaskList",
                            #Fun<json_rpc_connection.0.86436583>,
                            #{timeout => 60000}},
                           60000]}}}
** When Server state == {state,n1ql,
                         {dict,2,16,16,8,80,48,
                          {[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]},
                          {{[],[],
                            [[{node,'ns_1@127.0.0.1'}|
                              <<"1678dfae96c38e07dff43c49b9f6967b">>]],
                            [],[],[],[],[],[],[],[],[],[],
                            [[{uuid,<<"1678dfae96c38e07dff43c49b9f6967b">>}|
                              'ns_1@127.0.0.1']],
                            [],[]}}},
                         <0.1435.0>,#Ref<0.1778819075.971767810.232171>,
                         undefined,undefined,undefined,undefined,undefined,
                         {<<"AAAAAAAAAAE=">>,[]},
                         {<<"AAAAAAAAAAE=">>,
                          {topology,
                           ['ns_1@127.0.0.1'],
                           [<<"1678dfae96c38e07dff43c49b9f6967b">>],
                           true,[]}},
                         <0.1437.0>,<0.1438.0>}
** Reason for termination ==
** {linked_process_died,<0.1437.0>,
       {'ns_1@127.0.0.1',
           {timeout,
               {gen_server,call,
                   [<0.1435.0>,
                    {call,"ServiceAPI.GetTaskList",
                        #Fun<json_rpc_connection.0.86436583>,
                        #{timeout => 60000}},
                    60000]}}}}

[error_logger:error,2022-09-08T13:46:39.895Z,ns_1@127.0.0.1:service_agent-backup<0.540.0>:ale_error_logger_handler:do_log:101]
=========================ERROR REPORT=========================
** Generic server 'service_agent-backup' terminating 
** Last message in was {'EXIT',<0.1093.0>,
                        {timeout,
                         {gen_server,call,
                          [<0.1090.0>,
                           {call,"ServiceAPI.GetTaskList",
                            #Fun<json_rpc_connection.0.86436583>,
                            #{timeout => 60000}},
                           60000]}}}
** When Server state == {state,backup,
                         {dict,2,16,16,8,80,48,
                          {[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]},
                          {{[],[],
                            [[{node,'ns_1@127.0.0.1'}|
                              <<"1678dfae96c38e07dff43c49b9f6967b">>]],
                            [],[],[],[],[],[],[],[],[],[],
                            [[{uuid,<<"1678dfae96c38e07dff43c49b9f6967b">>}|
                              'ns_1@127.0.0.1']],
                            [],[]}}},
                         <0.1090.0>,#Ref<0.1778819075.971767816.231545>,
                         undefined,undefined,undefined,undefined,undefined,
                         {<<"AAAAAAAAAAI=">>,[]},
                         {<<"AAAAAAAAAAI=">>,
                          {topology,
                           ['ns_1@127.0.0.1'],
                           [<<"1678dfae96c38e07dff43c49b9f6967b">>],
                           true,[]}},
                         <0.1093.0>,<0.1094.0>}
** Reason for termination ==
** {linked_process_died,<0.1093.0>,
       {'ns_1@127.0.0.1',
           {timeout,
               {gen_server,call,
                   [<0.1090.0>,
                    {call,"ServiceAPI.GetTaskList",
                        #Fun<json_rpc_connection.0.86436583>,
                        #{timeout => 60000}},
                    60000]}}}}

[error_logger:error,2022-09-08T13:46:39.896Z,ns_1@127.0.0.1:service_agent-n1ql<0.560.0>:ale_error_logger_handler:do_log:101]
=========================CRASH REPORT=========================
  crasher:
    initial call: service_agent:init/1
    pid: <0.560.0>
    registered_name: 'service_agent-n1ql'
    exception exit: {linked_process_died,<0.1437.0>,
                     {'ns_1@127.0.0.1',
                      {timeout,
                       {gen_server,call,
                        [<0.1435.0>,
                         {call,"ServiceAPI.GetTaskList",
                          #Fun<json_rpc_connection.0.86436583>,
                          #{timeout => 60000}},
                         60000]}}}}
      in function  gen_server:handle_common_reply/8 (gen_server.erl, line 811)
    ancestors: [service_agent_children_sup,service_agent_sup,ns_server_sup,
                  ns_server_nodes_sup,<0.274.0>,ns_server_cluster_sup,
                  root_sup,<0.145.0>]
    message_queue_len: 2
    messages: [{'EXIT',<0.1438.0>,
                   {timeout,
                    {gen_server,call,
                     [<0.1435.0>,
                      {call,"ServiceAPI.GetCurrentTopology",
                       #Fun<json_rpc_connection.0.86436583>,
                       #{timeout => 60000}},
                      60000]}}},
                  {'DOWN',#Ref<0.1778819075.971767810.232171>,process,
                   <0.1435.0>,
                   {service_agent_died,
                    {linked_process_died,<0.1437.0>,
                     {'ns_1@127.0.0.1',
                      {timeout,
                       {gen_server,call,
                        [<0.1435.0>,
                         {call,"ServiceAPI.GetTaskList",
                          #Fun<json_rpc_connection.0.86436583>,
                          #{timeout => 60000}},
                         60000]}}}}}}]
    links: [<0.562.0>,<0.537.0>]
    dictionary: []
    trap_exit: true
    status: running
    heap_size: 6772
    stack_size: 29
    reductions: 123619
  neighbours:

[error_logger:error,2022-09-08T13:46:39.896Z,ns_1@127.0.0.1:service_agent-backup<0.540.0>:ale_error_logger_handler:do_log:101]
=========================CRASH REPORT=========================
  crasher:
    initial call: service_agent:init/1
    pid: <0.540.0>
    registered_name: 'service_agent-backup'
    exception exit: {linked_process_died,<0.1093.0>,
                     {'ns_1@127.0.0.1',
                      {timeout,
                       {gen_server,call,
                        [<0.1090.0>,
                         {call,"ServiceAPI.GetTaskList",
                          #Fun<json_rpc_connection.0.86436583>,
                          #{timeout => 60000}},
                         60000]}}}}
      in function  gen_server:handle_common_reply/8 (gen_server.erl, line 811)
    ancestors: [service_agent_children_sup,service_agent_sup,ns_server_sup,
                  ns_server_nodes_sup,<0.274.0>,ns_server_cluster_sup,
                  root_sup,<0.145.0>]
    message_queue_len: 2
    messages: [{'EXIT',<0.1094.0>,
                   {linked_process_died,<0.1093.0>,
                    {'ns_1@127.0.0.1',
                     {timeout,
                      {gen_server,call,
                       [<0.1090.0>,
                        {call,"ServiceAPI.GetTaskList",
                         #Fun<json_rpc_connection.0.86436583>,
                         #{timeout => 60000}},
                        60000]}}}}},
                  {'DOWN',#Ref<0.1778819075.971767816.231545>,process,
                   <0.1090.0>,
                   {service_agent_died,
                    {linked_process_died,<0.1093.0>,
                     {'ns_1@127.0.0.1',
                      {timeout,
                       {gen_server,call,
                        [<0.1090.0>,
                         {call,"ServiceAPI.GetTaskList",
                          #Fun<json_rpc_connection.0.86436583>,
                          #{timeout => 60000}},
                         60000]}}}}}}]
    links: [<0.542.0>,<0.537.0>]
    dictionary: []
    trap_exit: true
    status: running
    heap_size: 6772
    stack_size: 29
    reductions: 123660
  neighbours:

[error_logger:error,2022-09-08T13:46:39.897Z,ns_1@127.0.0.1:service_agent-fts<0.552.0>:ale_error_logger_handler:do_log:101]
=========================ERROR REPORT=========================
** Generic server 'service_agent-fts' terminating 
** Last message in was {'EXIT',<0.1174.0>,
                        {timeout,
                         {gen_server,call,
                          [<0.1171.0>,
                           {call,"ServiceAPI.GetCurrentTopology",
                            #Fun<json_rpc_connection.0.86436583>,
                            #{timeout => 60000}},
                           60000]}}}
** When Server state == {state,fts,
                         {dict,2,16,16,8,80,48,
                          {[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]},
                          {{[],[],
                            [[{node,'ns_1@127.0.0.1'}|
                              <<"1678dfae96c38e07dff43c49b9f6967b">>]],
                            [],[],[],[],[],[],[],[],[],[],
                            [[{uuid,<<"1678dfae96c38e07dff43c49b9f6967b">>}|
                              'ns_1@127.0.0.1']],
                            [],[]}}},
                         <0.1171.0>,#Ref<0.1778819075.971767815.231263>,
                         undefined,undefined,undefined,undefined,undefined,
                         {<<"MA==">>,[]},
                         {<<"NA==">>,
                          {topology,
                           ['ns_1@127.0.0.1'],
                           [<<"1678dfae96c38e07dff43c49b9f6967b">>],
                           true,[]}},
                         <0.1173.0>,<0.1174.0>}
** Reason for termination ==
** {linked_process_died,<0.1174.0>,
       {'ns_1@127.0.0.1',
           {timeout,
               {gen_server,call,
                   [<0.1171.0>,
                    {call,"ServiceAPI.GetCurrentTopology",
                        #Fun<json_rpc_connection.0.86436583>,
                        #{timeout => 60000}},
                    60000]}}}}

[error_logger:error,2022-09-08T13:46:39.897Z,ns_1@127.0.0.1:service_agent-index<0.556.0>:ale_error_logger_handler:do_log:101]
=========================ERROR REPORT=========================
** Generic server 'service_agent-index' terminating 
** Last message in was {'EXIT',<0.1238.0>,
                        {timeout,
                         {gen_server,call,
                          [<0.1218.0>,
                           {call,"ServiceAPI.GetCurrentTopology",
                            #Fun<json_rpc_connection.0.86436583>,
                            #{timeout => 60000}},
                           60000]}}}
** When Server state == {state,index,
                         {dict,2,16,16,8,80,48,
                          {[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]},
                          {{[],[],
                            [[{node,'ns_1@127.0.0.1'}|
                              <<"1678dfae96c38e07dff43c49b9f6967b">>]],
                            [],[],[],[],[],[],[],[],[],[],
                            [[{uuid,<<"1678dfae96c38e07dff43c49b9f6967b">>}|
                              'ns_1@127.0.0.1']],
                            [],[]}}},
                         <0.1218.0>,#Ref<0.1778819075.971767813.231753>,
                         undefined,undefined,undefined,undefined,undefined,
                         {<<"AAAAAAAAAAE=">>,[]},
                         {<<"AAAAAAAAAAE=">>,
                          {topology,
                           ['ns_1@127.0.0.1'],
                           [<<"1678dfae96c38e07dff43c49b9f6967b">>],
                           true,[]}},
                         <0.1237.0>,<0.1238.0>}
** Reason for termination ==
** {linked_process_died,<0.1238.0>,
       {'ns_1@127.0.0.1',
           {timeout,
               {gen_server,call,
                   [<0.1218.0>,
                    {call,"ServiceAPI.GetCurrentTopology",
                        #Fun<json_rpc_connection.0.86436583>,
                        #{timeout => 60000}},
                    60000]}}}}

[error_logger:error,2022-09-08T13:46:39.897Z,ns_1@127.0.0.1:service_agent-cbas<0.544.0>:ale_error_logger_handler:do_log:101]
=========================ERROR REPORT=========================
** Generic server 'service_agent-cbas' terminating 
** Last message in was {'EXIT',<0.919.0>,
                        {timeout,
                         {gen_server,call,
                          [<0.913.0>,
                           {call,"ServiceAPI.GetCurrentTopology",
                            #Fun<json_rpc_connection.0.86436583>,
                            #{timeout => 60000}},
                           60000]}}}
** When Server state == {state,cbas,
                         {dict,2,16,16,8,80,48,
                          {[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]},
                          {{[],[],
                            [[{node,'ns_1@127.0.0.1'}|
                              <<"1678dfae96c38e07dff43c49b9f6967b">>]],
                            [],[],[],[],[],[],[],[],[],[],
                            [[{uuid,<<"1678dfae96c38e07dff43c49b9f6967b">>}|
                              'ns_1@127.0.0.1']],
                            [],[]}}},
                         <0.913.0>,#Ref<0.1778819075.971767811.232094>,
                         undefined,undefined,undefined,undefined,undefined,
                         {<<"Mg==">>,[]},
                         {<<"Mg==">>,
                          {topology,
                           ['ns_1@127.0.0.1'],
                           [<<"1678dfae96c38e07dff43c49b9f6967b">>],
                           true,[]}},
                         <0.918.0>,<0.919.0>}
** Reason for termination ==
** {linked_process_died,<0.919.0>,
       {'ns_1@127.0.0.1',
           {timeout,
               {gen_server,call,
                   [<0.913.0>,
                    {call,"ServiceAPI.GetCurrentTopology",
                        #Fun<json_rpc_connection.0.86436583>,
                        #{timeout => 60000}},
                    60000]}}}}

[error_logger:error,2022-09-08T13:46:39.899Z,ns_1@127.0.0.1:service_agent-eventing<0.548.0>:ale_error_logger_handler:do_log:101]
=========================ERROR REPORT=========================
** Generic server 'service_agent-eventing' terminating 
** Last message in was {'EXIT',<0.1041.0>,
                        {timeout,
                         {gen_server,call,
                          [<0.1038.0>,
                           {call,"ServiceAPI.GetTaskList",
                            #Fun<json_rpc_connection.0.86436583>,
                            #{timeout => 60000}},
                           60000]}}}
** When Server state == {state,eventing,
                         {dict,2,16,16,8,80,48,
                          {[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]},
                          {{[],[],
                            [[{node,'ns_1@127.0.0.1'}|
                              <<"1678dfae96c38e07dff43c49b9f6967b">>]],
                            [],[],[],[],[],[],[],[],[],[],
                            [[{uuid,<<"1678dfae96c38e07dff43c49b9f6967b">>}|
                              'ns_1@127.0.0.1']],
                            [],[]}}},
                         <0.1038.0>,#Ref<0.1778819075.971767816.231445>,
                         undefined,undefined,undefined,undefined,undefined,
                         {<<"AAAAAAAAAAA=">>,[]},
                         {<<"AAAAAAAAAAA=">>,
                          {topology,
                           ['ns_1@127.0.0.1'],
                           [<<"1678dfae96c38e07dff43c49b9f6967b">>],
                           true,[]}},
                         <0.1041.0>,<0.1042.0>}
** Reason for termination ==
** {linked_process_died,<0.1041.0>,
       {'ns_1@127.0.0.1',
           {timeout,
               {gen_server,call,
                   [<0.1038.0>,
                    {call,"ServiceAPI.GetTaskList",
                        #Fun<json_rpc_connection.0.86436583>,
                        #{timeout => 60000}},
                    60000]}}}}

[ns_server:debug,2022-09-08T13:46:39.899Z,ns_1@127.0.0.1:<0.562.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_config_events,<0.560.0>} exited with reason {linked_process_died,
                                                                                <0.1437.0>,
                                                                                {'ns_1@127.0.0.1',
                                                                                 {timeout,
                                                                                  {gen_server,
                                                                                   call,
                                                                                   [<0.1435.0>,
                                                                                    {call,
                                                                                     "ServiceAPI.GetTaskList",
                                                                                     #Fun<json_rpc_connection.0.86436583>,
                                                                                     #{timeout =>
                                                                                        60000}},
                                                                                    60000]}}}}
[error_logger:error,2022-09-08T13:46:39.900Z,ns_1@127.0.0.1:service_agent-fts<0.552.0>:ale_error_logger_handler:do_log:101]
=========================CRASH REPORT=========================
  crasher:
    initial call: service_agent:init/1
    pid: <0.552.0>
    registered_name: 'service_agent-fts'
    exception exit: {linked_process_died,<0.1174.0>,
                     {'ns_1@127.0.0.1',
                      {timeout,
                       {gen_server,call,
                        [<0.1171.0>,
                         {call,"ServiceAPI.GetCurrentTopology",
                          #Fun<json_rpc_connection.0.86436583>,
                          #{timeout => 60000}},
                         60000]}}}}
      in function  gen_server:handle_common_reply/8 (gen_server.erl, line 811)
    ancestors: [service_agent_children_sup,service_agent_sup,ns_server_sup,
                  ns_server_nodes_sup,<0.274.0>,ns_server_cluster_sup,
                  root_sup,<0.145.0>]
    message_queue_len: 2
    messages: [{'EXIT',<0.1173.0>,
                   {linked_process_died,<0.1174.0>,
                    {'ns_1@127.0.0.1',
                     {timeout,
                      {gen_server,call,
                       [<0.1171.0>,
                        {call,"ServiceAPI.GetCurrentTopology",
                         #Fun<json_rpc_connection.0.86436583>,
                         #{timeout => 60000}},
                        60000]}}}}},
                  {'DOWN',#Ref<0.1778819075.971767815.231263>,process,
                   <0.1171.0>,
                   {service_agent_died,
                    {linked_process_died,<0.1174.0>,
                     {'ns_1@127.0.0.1',
                      {timeout,
                       {gen_server,call,
                        [<0.1171.0>,
                         {call,"ServiceAPI.GetCurrentTopology",
                          #Fun<json_rpc_connection.0.86436583>,
                          #{timeout => 60000}},
                         60000]}}}}}}]
    links: [<0.554.0>,<0.537.0>]
    dictionary: []
    trap_exit: true
    status: running
    heap_size: 10958
    stack_size: 29
    reductions: 123401
  neighbours:

[ns_server:debug,2022-09-08T13:46:39.901Z,ns_1@127.0.0.1:<0.554.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_config_events,<0.552.0>} exited with reason {linked_process_died,
                                                                                <0.1174.0>,
                                                                                {'ns_1@127.0.0.1',
                                                                                 {timeout,
                                                                                  {gen_server,
                                                                                   call,
                                                                                   [<0.1171.0>,
                                                                                    {call,
                                                                                     "ServiceAPI.GetCurrentTopology",
                                                                                     #Fun<json_rpc_connection.0.86436583>,
                                                                                     #{timeout =>
                                                                                        60000}},
                                                                                    60000]}}}}
[error_logger:error,2022-09-08T13:46:39.901Z,ns_1@127.0.0.1:service_agent_children_sup<0.537.0>:ale_error_logger_handler:do_log:101]
=========================SUPERVISOR REPORT=========================
    supervisor: {local,service_agent_children_sup}
    errorContext: child_terminated
    reason: {linked_process_died,<0.1437.0>,
                {'ns_1@127.0.0.1',
                    {timeout,
                        {gen_server,call,
                            [<0.1435.0>,
                             {call,"ServiceAPI.GetTaskList",
                                 #Fun<json_rpc_connection.0.86436583>,
                                 #{timeout => 60000}},
                             60000]}}}}
    offender: [{pid,<0.560.0>},
               {id,{service_agent,n1ql}},
               {mfargs,{service_agent,start_link,[n1ql]}},
               {restart_type,permanent},
               {significant,false},
               {shutdown,1000},
               {child_type,worker}]

[error_logger:error,2022-09-08T13:46:39.901Z,ns_1@127.0.0.1:service_agent-cbas<0.544.0>:ale_error_logger_handler:do_log:101]
=========================CRASH REPORT=========================
  crasher:
    initial call: service_agent:init/1
    pid: <0.544.0>
    registered_name: 'service_agent-cbas'
    exception exit: {linked_process_died,<0.919.0>,
                     {'ns_1@127.0.0.1',
                      {timeout,
                       {gen_server,call,
                        [<0.913.0>,
                         {call,"ServiceAPI.GetCurrentTopology",
                          #Fun<json_rpc_connection.0.86436583>,
                          #{timeout => 60000}},
                         60000]}}}}
      in function  gen_server:handle_common_reply/8 (gen_server.erl, line 811)
    ancestors: [service_agent_children_sup,service_agent_sup,ns_server_sup,
                  ns_server_nodes_sup,<0.274.0>,ns_server_cluster_sup,
                  root_sup,<0.145.0>]
    message_queue_len: 2
    messages: [{'EXIT',<0.918.0>,
                   {linked_process_died,<0.919.0>,
                    {'ns_1@127.0.0.1',
                     {timeout,
                      {gen_server,call,
                       [<0.913.0>,
                        {call,"ServiceAPI.GetCurrentTopology",
                         #Fun<json_rpc_connection.0.86436583>,
                         #{timeout => 60000}},
                        60000]}}}}},
                  {'DOWN',#Ref<0.1778819075.971767811.232094>,process,
                   <0.913.0>,
                   {service_agent_died,
                    {linked_process_died,<0.919.0>,
                     {'ns_1@127.0.0.1',
                      {timeout,
                       {gen_server,call,
                        [<0.913.0>,
                         {call,"ServiceAPI.GetCurrentTopology",
                          #Fun<json_rpc_connection.0.86436583>,
                          #{timeout => 60000}},
                         60000]}}}}}}]
    links: [<0.546.0>,<0.537.0>]
    dictionary: []
    trap_exit: true
    status: running
    heap_size: 6772
    stack_size: 29
    reductions: 123551
  neighbours:

[ns_server:debug,2022-09-08T13:46:39.901Z,ns_1@127.0.0.1:<0.546.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_config_events,<0.544.0>} exited with reason {linked_process_died,
                                                                                <0.919.0>,
                                                                                {'ns_1@127.0.0.1',
                                                                                 {timeout,
                                                                                  {gen_server,
                                                                                   call,
                                                                                   [<0.913.0>,
                                                                                    {call,
                                                                                     "ServiceAPI.GetCurrentTopology",
                                                                                     #Fun<json_rpc_connection.0.86436583>,
                                                                                     #{timeout =>
                                                                                        60000}},
                                                                                    60000]}}}}
[ns_server:debug,2022-09-08T13:46:39.902Z,ns_1@127.0.0.1:menelaus_cbauth<0.527.0>:menelaus_cbauth:handle_cast:101]Observed json rpc process {"backup-cbauth",<0.811.0>} needs_update
[error_logger:error,2022-09-08T13:46:39.904Z,ns_1@127.0.0.1:service_agent-eventing<0.548.0>:ale_error_logger_handler:do_log:101]
=========================CRASH REPORT=========================
  crasher:
    initial call: service_agent:init/1
    pid: <0.548.0>
    registered_name: 'service_agent-eventing'
    exception exit: {linked_process_died,<0.1041.0>,
                     {'ns_1@127.0.0.1',
                      {timeout,
                       {gen_server,call,
                        [<0.1038.0>,
                         {call,"ServiceAPI.GetTaskList",
                          #Fun<json_rpc_connection.0.86436583>,
                          #{timeout => 60000}},
                         60000]}}}}
      in function  gen_server:handle_common_reply/8 (gen_server.erl, line 811)
    ancestors: [service_agent_children_sup,service_agent_sup,ns_server_sup,
                  ns_server_nodes_sup,<0.274.0>,ns_server_cluster_sup,
                  root_sup,<0.145.0>]
    message_queue_len: 2
    messages: [{'EXIT',<0.1042.0>,
                   {linked_process_died,<0.1041.0>,
                    {'ns_1@127.0.0.1',
                     {timeout,
                      {gen_server,call,
                       [<0.1038.0>,
                        {call,"ServiceAPI.GetTaskList",
                         #Fun<json_rpc_connection.0.86436583>,
                         #{timeout => 60000}},
                        60000]}}}}},
                  {'DOWN',#Ref<0.1778819075.971767816.231445>,process,
                   <0.1038.0>,
                   {service_agent_died,
                    {linked_process_died,<0.1041.0>,
                     {'ns_1@127.0.0.1',
                      {timeout,
                       {gen_server,call,
                        [<0.1038.0>,
                         {call,"ServiceAPI.GetTaskList",
                          #Fun<json_rpc_connection.0.86436583>,
                          #{timeout => 60000}},
                         60000]}}}}}}]
    links: [<0.550.0>,<0.537.0>]
    dictionary: []
    trap_exit: true
    status: running
    heap_size: 6772
    stack_size: 29
    reductions: 123701
  neighbours:

[ns_server:debug,2022-09-08T13:46:39.905Z,ns_1@127.0.0.1:<0.550.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_config_events,<0.548.0>} exited with reason {linked_process_died,
                                                                                <0.1041.0>,
                                                                                {'ns_1@127.0.0.1',
                                                                                 {timeout,
                                                                                  {gen_server,
                                                                                   call,
                                                                                   [<0.1038.0>,
                                                                                    {call,
                                                                                     "ServiceAPI.GetTaskList",
                                                                                     #Fun<json_rpc_connection.0.86436583>,
                                                                                     #{timeout =>
                                                                                        60000}},
                                                                                    60000]}}}}
[error_logger:error,2022-09-08T13:46:39.901Z,ns_1@127.0.0.1:service_agent-index<0.556.0>:ale_error_logger_handler:do_log:101]
=========================CRASH REPORT=========================
  crasher:
    initial call: service_agent:init/1
    pid: <0.556.0>
    registered_name: 'service_agent-index'
    exception exit: {linked_process_died,<0.1238.0>,
                     {'ns_1@127.0.0.1',
                      {timeout,
                       {gen_server,call,
                        [<0.1218.0>,
                         {call,"ServiceAPI.GetCurrentTopology",
                          #Fun<json_rpc_connection.0.86436583>,
                          #{timeout => 60000}},
                         60000]}}}}
      in function  gen_server:handle_common_reply/8 (gen_server.erl, line 811)
    ancestors: [service_agent_children_sup,service_agent_sup,ns_server_sup,
                  ns_server_nodes_sup,<0.274.0>,ns_server_cluster_sup,
                  root_sup,<0.145.0>]
    message_queue_len: 2
    messages: [{'EXIT',<0.1237.0>,
                   {linked_process_died,<0.1238.0>,
                    {'ns_1@127.0.0.1',
                     {timeout,
                      {gen_server,call,
                       [<0.1218.0>,
                        {call,"ServiceAPI.GetCurrentTopology",
                         #Fun<json_rpc_connection.0.86436583>,
                         #{timeout => 60000}},
                        60000]}}}}},
                  {'DOWN',#Ref<0.1778819075.971767813.231753>,process,
                   <0.1218.0>,
                   {service_agent_died,
                    {linked_process_died,<0.1238.0>,
                     {'ns_1@127.0.0.1',
                      {timeout,
                       {gen_server,call,
                        [<0.1218.0>,
                         {call,"ServiceAPI.GetCurrentTopology",
                          #Fun<json_rpc_connection.0.86436583>,
                          #{timeout => 60000}},
                         60000]}}}}}}]
    links: [<0.558.0>,<0.537.0>]
    dictionary: []
    trap_exit: true
    status: running
    heap_size: 10958
    stack_size: 29
    reductions: 123493
  neighbours:

[ns_server:debug,2022-09-08T13:46:39.905Z,ns_1@127.0.0.1:<0.558.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_config_events,<0.556.0>} exited with reason {linked_process_died,
                                                                                <0.1238.0>,
                                                                                {'ns_1@127.0.0.1',
                                                                                 {timeout,
                                                                                  {gen_server,
                                                                                   call,
                                                                                   [<0.1218.0>,
                                                                                    {call,
                                                                                     "ServiceAPI.GetCurrentTopology",
                                                                                     #Fun<json_rpc_connection.0.86436583>,
                                                                                     #{timeout =>
                                                                                        60000}},
                                                                                    60000]}}}}
[error_logger:info,2022-09-08T13:46:39.905Z,ns_1@127.0.0.1:service_agent_children_sup<0.537.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,service_agent_children_sup}
    started: [{pid,<0.11802.8>},
              {id,{service_agent,n1ql}},
              {mfargs,{service_agent,start_link,[n1ql]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:error,2022-09-08T13:46:39.906Z,ns_1@127.0.0.1:service_agent_children_sup<0.537.0>:ale_error_logger_handler:do_log:101]
=========================SUPERVISOR REPORT=========================
    supervisor: {local,service_agent_children_sup}
    errorContext: child_terminated
    reason: {linked_process_died,<0.1093.0>,
                {'ns_1@127.0.0.1',
                    {timeout,
                        {gen_server,call,
                            [<0.1090.0>,
                             {call,"ServiceAPI.GetTaskList",
                                 #Fun<json_rpc_connection.0.86436583>,
                                 #{timeout => 60000}},
                             60000]}}}}
    offender: [{pid,<0.540.0>},
               {id,{service_agent,backup}},
               {mfargs,{service_agent,start_link,[backup]}},
               {restart_type,permanent},
               {significant,false},
               {shutdown,1000},
               {child_type,worker}]

[error_logger:info,2022-09-08T13:46:39.906Z,ns_1@127.0.0.1:service_agent_children_sup<0.537.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,service_agent_children_sup}
    started: [{pid,<0.11806.8>},
              {id,{service_agent,backup}},
              {mfargs,{service_agent,start_link,[backup]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:error,2022-09-08T13:46:39.906Z,ns_1@127.0.0.1:service_agent_children_sup<0.537.0>:ale_error_logger_handler:do_log:101]
=========================SUPERVISOR REPORT=========================
    supervisor: {local,service_agent_children_sup}
    errorContext: child_terminated
    reason: {linked_process_died,<0.1174.0>,
                {'ns_1@127.0.0.1',
                    {timeout,
                        {gen_server,call,
                            [<0.1171.0>,
                             {call,"ServiceAPI.GetCurrentTopology",
                                 #Fun<json_rpc_connection.0.86436583>,
                                 #{timeout => 60000}},
                             60000]}}}}
    offender: [{pid,<0.552.0>},
               {id,{service_agent,fts}},
               {mfargs,{service_agent,start_link,[fts]}},
               {restart_type,permanent},
               {significant,false},
               {shutdown,1000},
               {child_type,worker}]

[error_logger:info,2022-09-08T13:46:39.908Z,ns_1@127.0.0.1:service_agent_children_sup<0.537.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,service_agent_children_sup}
    started: [{pid,<0.11810.8>},
              {id,{service_agent,fts}},
              {mfargs,{service_agent,start_link,[fts]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:error,2022-09-08T13:46:39.908Z,ns_1@127.0.0.1:service_agent_children_sup<0.537.0>:ale_error_logger_handler:do_log:101]
=========================SUPERVISOR REPORT=========================
    supervisor: {local,service_agent_children_sup}
    errorContext: child_terminated
    reason: {linked_process_died,<0.919.0>,
                {'ns_1@127.0.0.1',
                    {timeout,
                        {gen_server,call,
                            [<0.913.0>,
                             {call,"ServiceAPI.GetCurrentTopology",
                                 #Fun<json_rpc_connection.0.86436583>,
                                 #{timeout => 60000}},
                             60000]}}}}
    offender: [{pid,<0.544.0>},
               {id,{service_agent,cbas}},
               {mfargs,{service_agent,start_link,[cbas]}},
               {restart_type,permanent},
               {significant,false},
               {shutdown,1000},
               {child_type,worker}]

[error_logger:info,2022-09-08T13:46:39.909Z,ns_1@127.0.0.1:service_agent_children_sup<0.537.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,service_agent_children_sup}
    started: [{pid,<0.11814.8>},
              {id,{service_agent,cbas}},
              {mfargs,{service_agent,start_link,[cbas]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:error,2022-09-08T13:46:39.909Z,ns_1@127.0.0.1:service_agent_children_sup<0.537.0>:ale_error_logger_handler:do_log:101]
=========================SUPERVISOR REPORT=========================
    supervisor: {local,service_agent_children_sup}
    errorContext: child_terminated
    reason: {linked_process_died,<0.1041.0>,
                {'ns_1@127.0.0.1',
                    {timeout,
                        {gen_server,call,
                            [<0.1038.0>,
                             {call,"ServiceAPI.GetTaskList",
                                 #Fun<json_rpc_connection.0.86436583>,
                                 #{timeout => 60000}},
                             60000]}}}}
    offender: [{pid,<0.548.0>},
               {id,{service_agent,eventing}},
               {mfargs,{service_agent,start_link,[eventing]}},
               {restart_type,permanent},
               {significant,false},
               {shutdown,1000},
               {child_type,worker}]

[error_logger:info,2022-09-08T13:46:39.909Z,ns_1@127.0.0.1:service_agent_children_sup<0.537.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,service_agent_children_sup}
    started: [{pid,<0.11818.8>},
              {id,{service_agent,eventing}},
              {mfargs,{service_agent,start_link,[eventing]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2022-09-08T13:46:39.900Z,ns_1@127.0.0.1:<0.542.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_config_events,<0.540.0>} exited with reason {linked_process_died,
                                                                                <0.1093.0>,
                                                                                {'ns_1@127.0.0.1',
                                                                                 {timeout,
                                                                                  {gen_server,
                                                                                   call,
                                                                                   [<0.1090.0>,
                                                                                    {call,
                                                                                     "ServiceAPI.GetTaskList",
                                                                                     #Fun<json_rpc_connection.0.86436583>,
                                                                                     #{timeout =>
                                                                                        60000}},
                                                                                    60000]}}}}
[error_logger:error,2022-09-08T13:46:39.909Z,ns_1@127.0.0.1:service_agent_children_sup<0.537.0>:ale_error_logger_handler:do_log:101]
=========================SUPERVISOR REPORT=========================
    supervisor: {local,service_agent_children_sup}
    errorContext: child_terminated
    reason: {linked_process_died,<0.1238.0>,
                {'ns_1@127.0.0.1',
                    {timeout,
                        {gen_server,call,
                            [<0.1218.0>,
                             {call,"ServiceAPI.GetCurrentTopology",
                                 #Fun<json_rpc_connection.0.86436583>,
                                 #{timeout => 60000}},
                             60000]}}}}
    offender: [{pid,<0.556.0>},
               {id,{service_agent,index}},
               {mfargs,{service_agent,start_link,[index]}},
               {restart_type,permanent},
               {significant,false},
               {shutdown,1000},
               {child_type,worker}]

[error_logger:info,2022-09-08T13:46:39.910Z,ns_1@127.0.0.1:service_agent_children_sup<0.537.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,service_agent_children_sup}
    started: [{pid,<0.11822.8>},
              {id,{service_agent,index}},
              {mfargs,{service_agent,start_link,[index]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2022-09-08T13:46:39.974Z,ns_1@127.0.0.1:menelaus_cbauth<0.527.0>:menelaus_cbauth:handle_cast:101]Observed json rpc process {"cbas-cbauth",<0.820.0>} needs_update
[ns_server:debug,2022-09-08T13:46:40.015Z,ns_1@127.0.0.1:leader_lease_agent<0.729.0>:leader_lease_agent:do_handle_acquire_lease:140]Granting lease to {lease_holder,<<"6739a7645d0d3cb96437d1c2d542cfcc">>,
                                'ns_1@127.0.0.1'} for 15000ms
[ns_server:debug,2022-09-08T13:46:40.086Z,ns_1@127.0.0.1:menelaus_cbauth<0.527.0>:menelaus_cbauth:handle_cast:101]Observed json rpc process {"eventing-cbauth",<0.825.0>} needs_update
[ns_server:debug,2022-09-08T13:46:40.095Z,ns_1@127.0.0.1:menelaus_cbauth<0.527.0>:menelaus_cbauth:handle_cast:101]Observed json rpc process {"goxdcr-cbauth",<0.839.0>} needs_update
[ns_server:debug,2022-09-08T13:46:40.127Z,ns_1@127.0.0.1:menelaus_cbauth<0.527.0>:menelaus_cbauth:handle_cast:101]Observed json rpc process {"index-cbauth",<0.865.0>} needs_update
[ns_server:debug,2022-09-08T13:46:40.168Z,ns_1@127.0.0.1:menelaus_cbauth<0.527.0>:menelaus_cbauth:handle_cast:101]Observed json rpc process {"projector-cbauth",<0.871.0>} needs_update
[ns_server:debug,2022-09-08T13:46:40.185Z,ns_1@127.0.0.1:menelaus_cbauth<0.527.0>:menelaus_cbauth:handle_cast:101]Observed json rpc process {"fts-cbauth",<0.830.0>} needs_update
[ns_server:debug,2022-09-08T13:46:40.193Z,ns_1@127.0.0.1:menelaus_cbauth<0.527.0>:menelaus_cbauth:handle_cast:101]Observed json rpc process {"cbq-engine-cbauth",<0.880.0>} needs_update
[ns_server:debug,2022-09-08T13:46:40.197Z,ns_1@127.0.0.1:menelaus_cbauth<0.527.0>:menelaus_cbauth:handle_cast:101]Observed json rpc process {"cbas-cbauth",<0.820.0>} needs_update
[ns_server:debug,2022-09-08T13:46:40.198Z,ns_1@127.0.0.1:menelaus_cbauth<0.527.0>:menelaus_cbauth:handle_cast:101]Observed json rpc process {"eventing-cbauth",<0.825.0>} needs_update
[ns_server:debug,2022-09-08T13:46:40.199Z,ns_1@127.0.0.1:menelaus_cbauth<0.527.0>:menelaus_cbauth:handle_cast:101]Observed json rpc process {"fts-cbauth",<0.830.0>} needs_update
[ns_server:debug,2022-09-08T13:46:40.226Z,ns_1@127.0.0.1:menelaus_cbauth<0.527.0>:menelaus_cbauth:handle_cast:101]Observed json rpc process {"backup-cbauth",<0.811.0>} needs_update
[ns_server:debug,2022-09-08T13:46:40.231Z,ns_1@127.0.0.1:menelaus_cbauth<0.527.0>:menelaus_cbauth:handle_cast:101]Observed json rpc process {"goxdcr-cbauth",<0.839.0>} needs_update
[ns_server:debug,2022-09-08T13:46:40.232Z,ns_1@127.0.0.1:menelaus_cbauth<0.527.0>:menelaus_cbauth:handle_cast:101]Observed json rpc process {"index-cbauth",<0.865.0>} needs_update
[ns_server:debug,2022-09-08T13:46:40.233Z,ns_1@127.0.0.1:menelaus_cbauth<0.527.0>:menelaus_cbauth:handle_cast:101]Observed json rpc process {"projector-cbauth",<0.871.0>} needs_update
[ns_server:debug,2022-09-08T13:46:40.235Z,ns_1@127.0.0.1:menelaus_cbauth<0.527.0>:menelaus_cbauth:handle_cast:101]Observed json rpc process {"cbq-engine-cbauth",<0.880.0>} needs_update
[ns_server:debug,2022-09-08T13:46:40.237Z,ns_1@127.0.0.1:menelaus_cbauth<0.527.0>:menelaus_cbauth:handle_cast:101]Observed json rpc process {"backup-cbauth",<0.811.0>} needs_update
[ns_server:debug,2022-09-08T13:46:40.238Z,ns_1@127.0.0.1:menelaus_cbauth<0.527.0>:menelaus_cbauth:handle_cast:101]Observed json rpc process {"cbas-cbauth",<0.820.0>} needs_update
[ns_server:debug,2022-09-08T13:46:40.240Z,ns_1@127.0.0.1:menelaus_cbauth<0.527.0>:menelaus_cbauth:handle_cast:101]Observed json rpc process {"eventing-cbauth",<0.825.0>} needs_update
[ns_server:debug,2022-09-08T13:46:40.241Z,ns_1@127.0.0.1:menelaus_cbauth<0.527.0>:menelaus_cbauth:handle_cast:101]Observed json rpc process {"fts-cbauth",<0.830.0>} needs_update
[ns_server:debug,2022-09-08T13:46:40.243Z,ns_1@127.0.0.1:menelaus_cbauth<0.527.0>:menelaus_cbauth:handle_cast:101]Observed json rpc process {"goxdcr-cbauth",<0.839.0>} needs_update
[ns_server:debug,2022-09-08T13:46:40.244Z,ns_1@127.0.0.1:menelaus_cbauth<0.527.0>:menelaus_cbauth:handle_cast:101]Observed json rpc process {"index-cbauth",<0.865.0>} needs_update
[ns_server:debug,2022-09-08T13:46:40.245Z,ns_1@127.0.0.1:menelaus_cbauth<0.527.0>:menelaus_cbauth:handle_cast:101]Observed json rpc process {"projector-cbauth",<0.871.0>} needs_update
[ns_server:debug,2022-09-08T13:46:40.246Z,ns_1@127.0.0.1:menelaus_cbauth<0.527.0>:menelaus_cbauth:handle_cast:101]Observed json rpc process {"cbq-engine-cbauth",<0.880.0>} needs_update
[ns_server:debug,2022-09-08T13:46:40.248Z,ns_1@127.0.0.1:menelaus_cbauth<0.527.0>:menelaus_cbauth:handle_cast:101]Observed json rpc process {"backup-cbauth",<0.811.0>} needs_update
[ns_server:debug,2022-09-08T13:46:40.249Z,ns_1@127.0.0.1:menelaus_cbauth<0.527.0>:menelaus_cbauth:handle_cast:101]Observed json rpc process {"cbas-cbauth",<0.820.0>} needs_update
[ns_server:debug,2022-09-08T13:46:40.262Z,ns_1@127.0.0.1:menelaus_cbauth<0.527.0>:menelaus_cbauth:handle_cast:101]Observed json rpc process {"eventing-cbauth",<0.825.0>} needs_update
[ns_server:debug,2022-09-08T13:46:40.264Z,ns_1@127.0.0.1:menelaus_cbauth<0.527.0>:menelaus_cbauth:handle_cast:101]Observed json rpc process {"fts-cbauth",<0.830.0>} needs_update
[ns_server:debug,2022-09-08T13:46:40.265Z,ns_1@127.0.0.1:menelaus_cbauth<0.527.0>:menelaus_cbauth:handle_cast:101]Observed json rpc process {"goxdcr-cbauth",<0.839.0>} needs_update
[ns_server:debug,2022-09-08T13:46:40.266Z,ns_1@127.0.0.1:menelaus_cbauth<0.527.0>:menelaus_cbauth:handle_cast:101]Observed json rpc process {"index-cbauth",<0.865.0>} needs_update
[ns_server:debug,2022-09-08T13:46:40.267Z,ns_1@127.0.0.1:menelaus_cbauth<0.527.0>:menelaus_cbauth:handle_cast:101]Observed json rpc process {"projector-cbauth",<0.871.0>} needs_update
[ns_server:debug,2022-09-08T13:46:40.269Z,ns_1@127.0.0.1:menelaus_cbauth<0.527.0>:menelaus_cbauth:handle_cast:101]Observed json rpc process {"cbq-engine-cbauth",<0.880.0>} needs_update
[ns_server:debug,2022-09-08T13:46:40.271Z,ns_1@127.0.0.1:menelaus_cbauth<0.527.0>:menelaus_cbauth:handle_cast:101]Observed json rpc process {"backup-cbauth",<0.811.0>} needs_update
[ns_server:debug,2022-09-08T13:46:40.272Z,ns_1@127.0.0.1:menelaus_cbauth<0.527.0>:menelaus_cbauth:handle_cast:101]Observed json rpc process {"cbas-cbauth",<0.820.0>} needs_update
[ns_server:debug,2022-09-08T13:46:40.273Z,ns_1@127.0.0.1:menelaus_cbauth<0.527.0>:menelaus_cbauth:handle_cast:101]Observed json rpc process {"eventing-cbauth",<0.825.0>} needs_update
[ns_server:debug,2022-09-08T13:46:40.275Z,ns_1@127.0.0.1:menelaus_cbauth<0.527.0>:menelaus_cbauth:handle_cast:101]Observed json rpc process {"fts-cbauth",<0.830.0>} needs_update
[ns_server:debug,2022-09-08T13:46:40.276Z,ns_1@127.0.0.1:menelaus_cbauth<0.527.0>:menelaus_cbauth:handle_cast:101]Observed json rpc process {"goxdcr-cbauth",<0.839.0>} needs_update
[ns_server:debug,2022-09-08T13:46:40.277Z,ns_1@127.0.0.1:menelaus_cbauth<0.527.0>:menelaus_cbauth:handle_cast:101]Observed json rpc process {"index-cbauth",<0.865.0>} needs_update
[ns_server:debug,2022-09-08T13:46:40.279Z,ns_1@127.0.0.1:menelaus_cbauth<0.527.0>:menelaus_cbauth:handle_cast:101]Observed json rpc process {"projector-cbauth",<0.871.0>} needs_update
[ns_server:debug,2022-09-08T13:46:40.285Z,ns_1@127.0.0.1:menelaus_cbauth<0.527.0>:menelaus_cbauth:handle_cast:101]Observed json rpc process {"cbq-engine-cbauth",<0.880.0>} needs_update
[ns_server:debug,2022-09-08T13:46:40.286Z,ns_1@127.0.0.1:menelaus_cbauth<0.527.0>:menelaus_cbauth:handle_cast:101]Observed json rpc process {"backup-cbauth",<0.811.0>} needs_update
[ns_server:debug,2022-09-08T13:46:40.288Z,ns_1@127.0.0.1:menelaus_cbauth<0.527.0>:menelaus_cbauth:handle_cast:101]Observed json rpc process {"cbas-cbauth",<0.820.0>} needs_update
[ns_server:debug,2022-09-08T13:46:40.290Z,ns_1@127.0.0.1:menelaus_cbauth<0.527.0>:menelaus_cbauth:handle_cast:101]Observed json rpc process {"eventing-cbauth",<0.825.0>} needs_update
[ns_server:debug,2022-09-08T13:46:40.291Z,ns_1@127.0.0.1:menelaus_cbauth<0.527.0>:menelaus_cbauth:handle_cast:101]Observed json rpc process {"fts-cbauth",<0.830.0>} needs_update
[ns_server:debug,2022-09-08T13:46:40.292Z,ns_1@127.0.0.1:menelaus_cbauth<0.527.0>:menelaus_cbauth:handle_cast:101]Observed json rpc process {"goxdcr-cbauth",<0.839.0>} needs_update
[ns_server:debug,2022-09-08T13:46:40.294Z,ns_1@127.0.0.1:menelaus_cbauth<0.527.0>:menelaus_cbauth:handle_cast:101]Observed json rpc process {"index-cbauth",<0.865.0>} needs_update
[ns_server:debug,2022-09-08T13:46:40.295Z,ns_1@127.0.0.1:menelaus_cbauth<0.527.0>:menelaus_cbauth:handle_cast:101]Observed json rpc process {"projector-cbauth",<0.871.0>} needs_update
[ns_server:debug,2022-09-08T13:46:40.296Z,ns_1@127.0.0.1:menelaus_cbauth<0.527.0>:menelaus_cbauth:handle_cast:101]Observed json rpc process {"cbq-engine-cbauth",<0.880.0>} needs_update
[ns_server:debug,2022-09-08T13:46:40.361Z,ns_1@127.0.0.1:<0.759.0>:auto_failover:log_down_nodes_reason:368]Node 'ns_1@127.0.0.1' is considered down. Reason:"The data service did not respond for the duration of the auto-failover threshold. Either none of the buckets have warmed up or there is an issue with the data service. "
[ns_server:debug,2022-09-08T13:46:40.361Z,ns_1@127.0.0.1:<0.759.0>:auto_failover_logic:log_master_activity:143]Incremented down state:
{node_state,{'ns_1@127.0.0.1',<<"1678dfae96c38e07dff43c49b9f6967b">>},
            0,up,undefined}
->{node_state,{'ns_1@127.0.0.1',<<"1678dfae96c38e07dff43c49b9f6967b">>},
              0,half_down,undefined}
[ns_server:info,2022-09-08T13:46:40.778Z,ns_1@127.0.0.1:<0.743.0>:leader_lease_acquire_worker:handle_fresh_lease_acquired:296]Acquired lease from node 'ns_1@127.0.0.1' (lease uuid: <<"6739a7645d0d3cb96437d1c2d542cfcc">>)
[ns_server:debug,2022-09-08T13:46:40.871Z,ns_1@127.0.0.1:json_rpc_connection-cbq-engine-service_api<0.11898.8>:json_rpc_connection:init:68]Observed revrpc connection: label "cbq-engine-service_api", handling process <0.11898.8>
[ns_server:debug,2022-09-08T13:46:40.871Z,ns_1@127.0.0.1:service_agent-n1ql<0.11802.8>:service_agent:do_handle_connection:319]Observed new json rpc connection for n1ql: <0.11898.8>
[ns_server:debug,2022-09-08T13:46:40.871Z,ns_1@127.0.0.1:<0.11805.8>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {json_rpc_events,<0.11803.8>} exited with reason normal
[ns_server:debug,2022-09-08T13:46:41.356Z,ns_1@127.0.0.1:<0.759.0>:auto_failover_logic:log_master_activity:141]Transitioned node {'ns_1@127.0.0.1',<<"1678dfae96c38e07dff43c49b9f6967b">>} state half_down -> up
[ns_server:debug,2022-09-08T13:47:00.566Z,ns_1@127.0.0.1:json_rpc_connection-fts-service_api<0.12861.8>:json_rpc_connection:init:68]Observed revrpc connection: label "fts-service_api", handling process <0.12861.8>
[ns_server:debug,2022-09-08T13:47:00.567Z,ns_1@127.0.0.1:service_agent-fts<0.11810.8>:service_agent:do_handle_connection:319]Observed new json rpc connection for fts: <0.12861.8>
[ns_server:debug,2022-09-08T13:47:00.567Z,ns_1@127.0.0.1:<0.11813.8>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {json_rpc_events,<0.11811.8>} exited with reason normal
[ns_server:debug,2022-09-08T13:47:09.544Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:47:09.544Z,ns_1@127.0.0.1:<0.13306.8>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:47:09.546Z,ns_1@127.0.0.1:<0.13308.8>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T13:47:09.546Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:47:09.546Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:47:09.863Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:47:09.865Z,ns_1@127.0.0.1:<0.13335.8>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:47:09.866Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:47:09.866Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:47:10.495Z,ns_1@127.0.0.1:json_rpc_connection-eventing-service_api<0.13353.8>:json_rpc_connection:init:68]Observed revrpc connection: label "eventing-service_api", handling process <0.13353.8>
[ns_server:debug,2022-09-08T13:47:10.496Z,ns_1@127.0.0.1:service_agent-eventing<0.11818.8>:service_agent:do_handle_connection:319]Observed new json rpc connection for eventing: <0.13353.8>
[ns_server:debug,2022-09-08T13:47:10.496Z,ns_1@127.0.0.1:<0.11821.8>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {json_rpc_events,<0.11819.8>} exited with reason normal
[ns_server:debug,2022-09-08T13:47:10.532Z,ns_1@127.0.0.1:json_rpc_connection-cbas-service_api<0.13358.8>:json_rpc_connection:init:68]Observed revrpc connection: label "cbas-service_api", handling process <0.13358.8>
[ns_server:debug,2022-09-08T13:47:10.533Z,ns_1@127.0.0.1:service_agent-cbas<0.11814.8>:service_agent:do_handle_connection:319]Observed new json rpc connection for cbas: <0.13358.8>
[ns_server:debug,2022-09-08T13:47:10.533Z,ns_1@127.0.0.1:<0.11817.8>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {json_rpc_events,<0.11815.8>} exited with reason normal
[ns_server:debug,2022-09-08T13:47:10.578Z,ns_1@127.0.0.1:json_rpc_connection-index-service_api<0.13371.8>:json_rpc_connection:init:68]Observed revrpc connection: label "index-service_api", handling process <0.13371.8>
[ns_server:debug,2022-09-08T13:47:10.578Z,ns_1@127.0.0.1:service_agent-index<0.11822.8>:service_agent:do_handle_connection:319]Observed new json rpc connection for index: <0.13371.8>
[ns_server:debug,2022-09-08T13:47:10.578Z,ns_1@127.0.0.1:<0.11825.8>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {json_rpc_events,<0.11823.8>} exited with reason normal
[ns_server:debug,2022-09-08T13:47:14.545Z,ns_1@127.0.0.1:json_rpc_connection-backup-service_api<0.13549.8>:json_rpc_connection:init:68]Observed revrpc connection: label "backup-service_api", handling process <0.13549.8>
[ns_server:debug,2022-09-08T13:47:14.546Z,ns_1@127.0.0.1:service_agent-backup<0.11806.8>:service_agent:do_handle_connection:319]Observed new json rpc connection for backup: <0.13549.8>
[ns_server:debug,2022-09-08T13:47:14.546Z,ns_1@127.0.0.1:<0.11809.8>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {json_rpc_events,<0.11807.8>} exited with reason normal
[ns_server:debug,2022-09-08T13:47:39.547Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:47:39.547Z,ns_1@127.0.0.1:<0.14807.8>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:47:39.548Z,ns_1@127.0.0.1:<0.14809.8>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T13:47:39.548Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:47:39.548Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:47:39.569Z,ns_1@127.0.0.1:roles_cache<0.345.0>:active_cache:cleanup:258]Cache roles_cache cleanup: 0/0 records deleted
[ns_server:debug,2022-09-08T13:47:39.867Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:47:39.869Z,ns_1@127.0.0.1:<0.14838.8>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:47:39.870Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:47:39.870Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:47:42.419Z,ns_1@127.0.0.1:prometheus_cfg<0.391.0>:prometheus_cfg:maybe_update_scrape_dynamic_intervals:941]Recalculating prometheus scrape intervals for high cardinality metrics
[ns_server:debug,2022-09-08T13:47:42.424Z,ns_1@127.0.0.1:prometheus_cfg<0.391.0>:prometheus_cfg:maybe_update_scrape_dynamic_intervals:958]Scrape intervals haven't changed:
[]
CBCollect stats dir size estimation: 629596800
Calculated based on scrapes info:
[{backup,low_cardinality,0},
 {cbas,high_cardinality,0},
 {cbas,low_cardinality,14},
 {eventing,high_cardinality,0},
 {eventing,low_cardinality,1},
 {fts,high_cardinality,0},
 {fts,low_cardinality,25},
 {index,low_cardinality,2},
 {index,high_cardinality,24},
 {kv,low_cardinality,291},
 {kv,high_cardinality,1123},
 {n1ql,low_cardinality,33},
 {ns_server,low_cardinality,167},
 {ns_server,high_cardinality,330},
 {xdcr,low_cardinality,0}]
Raw intervals:
[]
[ns_server:debug,2022-09-08T13:47:54.357Z,ns_1@127.0.0.1:ldap_auth_cache<0.334.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2022-09-08T13:48:09.549Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:48:09.549Z,ns_1@127.0.0.1:<0.16258.8>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:48:09.550Z,ns_1@127.0.0.1:<0.16260.8>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T13:48:09.550Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:48:09.550Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:48:09.871Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:48:09.873Z,ns_1@127.0.0.1:<0.16320.8>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:48:09.874Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:48:09.874Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:48:39.551Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:48:39.552Z,ns_1@127.0.0.1:<0.17748.8>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:48:39.552Z,ns_1@127.0.0.1:ns_audit<0.579.0>:ns_audit:handle_call:148]Audit session_expired: [{timestamp,<<"2022-09-08T13:48:39.552Z">>},
                        {sessionid,<<"083cedca0c0ff84ffaaeba86e628a40119f75e8d">>},
                        {real_userid,{[{domain,builtin},
                                       {user,<<"<ud>admin</ud>">>}]}}]
[ns_server:debug,2022-09-08T13:48:39.553Z,ns_1@127.0.0.1:<0.17750.8>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T13:48:39.553Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:48:39.553Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:48:39.875Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:48:39.877Z,ns_1@127.0.0.1:<0.17812.8>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:48:39.878Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:48:39.878Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:49:09.359Z,ns_1@127.0.0.1:ldap_auth_cache<0.334.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2022-09-08T13:49:09.554Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:49:09.554Z,ns_1@127.0.0.1:<0.19229.8>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:49:09.556Z,ns_1@127.0.0.1:<0.19231.8>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T13:49:09.556Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:49:09.556Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:49:09.878Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:49:09.880Z,ns_1@127.0.0.1:<0.19292.8>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:49:09.881Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:49:09.881Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:49:39.557Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:49:39.557Z,ns_1@127.0.0.1:<0.20713.8>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:49:39.558Z,ns_1@127.0.0.1:<0.20715.8>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T13:49:39.559Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:49:39.559Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:49:39.881Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:49:39.883Z,ns_1@127.0.0.1:<0.20772.8>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:49:39.884Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:49:39.884Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:50:09.559Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:50:09.559Z,ns_1@127.0.0.1:<0.22193.8>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:50:09.560Z,ns_1@127.0.0.1:<0.22195.8>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T13:50:09.561Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:50:09.561Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:50:09.885Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:50:09.888Z,ns_1@127.0.0.1:<0.22250.8>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:50:09.888Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:50:09.888Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:50:24.361Z,ns_1@127.0.0.1:ldap_auth_cache<0.334.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2022-09-08T13:50:39.562Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:50:39.562Z,ns_1@127.0.0.1:<0.23683.8>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:50:39.563Z,ns_1@127.0.0.1:<0.23685.8>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T13:50:39.564Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:50:39.564Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:50:39.889Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:50:39.892Z,ns_1@127.0.0.1:<0.23740.8>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:50:39.892Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:50:39.892Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:51:09.564Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:51:09.564Z,ns_1@127.0.0.1:<0.25163.8>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:51:09.565Z,ns_1@127.0.0.1:<0.25165.8>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T13:51:09.566Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:51:09.566Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:51:09.893Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:51:09.895Z,ns_1@127.0.0.1:<0.25220.8>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:51:09.895Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:51:09.895Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:51:39.362Z,ns_1@127.0.0.1:ldap_auth_cache<0.334.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2022-09-08T13:51:39.566Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:51:39.567Z,ns_1@127.0.0.1:<0.26651.8>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:51:39.568Z,ns_1@127.0.0.1:<0.26653.8>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T13:51:39.568Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:51:39.568Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:51:39.896Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:51:39.898Z,ns_1@127.0.0.1:<0.26708.8>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:51:39.899Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:51:39.899Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:52:09.569Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:52:09.569Z,ns_1@127.0.0.1:<0.28132.8>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:52:09.571Z,ns_1@127.0.0.1:<0.28134.8>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T13:52:09.571Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:52:09.571Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:52:09.900Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:52:09.902Z,ns_1@127.0.0.1:<0.28186.8>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:52:09.902Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:52:09.903Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:52:39.571Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:52:39.572Z,ns_1@127.0.0.1:<0.29622.8>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:52:39.573Z,ns_1@127.0.0.1:<0.29624.8>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T13:52:39.573Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:52:39.573Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:52:39.903Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:52:39.905Z,ns_1@127.0.0.1:<0.29676.8>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:52:39.906Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:52:39.906Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:52:54.363Z,ns_1@127.0.0.1:ldap_auth_cache<0.334.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2022-09-08T13:53:09.574Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:53:09.575Z,ns_1@127.0.0.1:<0.31087.8>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:53:09.576Z,ns_1@127.0.0.1:<0.31089.8>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T13:53:09.576Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:53:09.576Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:53:09.907Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:53:09.909Z,ns_1@127.0.0.1:<0.31157.8>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:53:09.910Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:53:09.910Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:53:39.577Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:53:39.577Z,ns_1@127.0.0.1:<0.32575.8>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:53:39.578Z,ns_1@127.0.0.1:<0.32577.8>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T13:53:39.579Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:53:39.579Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:53:39.911Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:53:39.913Z,ns_1@127.0.0.1:<0.32645.8>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:53:39.913Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:53:39.913Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:54:09.364Z,ns_1@127.0.0.1:ldap_auth_cache<0.334.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2022-09-08T13:54:09.580Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:54:09.580Z,ns_1@127.0.0.1:<0.1287.9>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:54:09.581Z,ns_1@127.0.0.1:<0.1289.9>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T13:54:09.581Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:54:09.581Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:54:09.655Z,ns_1@127.0.0.1:roles_cache<0.345.0>:active_cache:renew:238]Starting roles_cache cache renewal
[ns_server:debug,2022-09-08T13:54:09.655Z,ns_1@127.0.0.1:roles_cache<0.345.0>:active_cache:renew:244]roles_cache cache renewal process originated 0 queries
[ns_server:debug,2022-09-08T13:54:09.914Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:54:09.916Z,ns_1@127.0.0.1:<0.1357.9>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:54:09.917Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:54:09.917Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:54:39.582Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:54:39.582Z,ns_1@127.0.0.1:<0.2771.9>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:54:39.584Z,ns_1@127.0.0.1:<0.2773.9>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T13:54:39.584Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:54:39.584Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:54:39.918Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:54:39.920Z,ns_1@127.0.0.1:<0.2847.9>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:54:39.921Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:54:39.921Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:55:09.585Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:55:09.585Z,ns_1@127.0.0.1:<0.4251.9>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:55:09.586Z,ns_1@127.0.0.1:<0.4253.9>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T13:55:09.587Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:55:09.587Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:55:09.922Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:55:09.924Z,ns_1@127.0.0.1:<0.4327.9>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:55:09.924Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:55:09.924Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:55:24.365Z,ns_1@127.0.0.1:ldap_auth_cache<0.334.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2022-09-08T13:55:39.587Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:55:39.587Z,ns_1@127.0.0.1:<0.5713.9>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:55:39.588Z,ns_1@127.0.0.1:<0.5715.9>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T13:55:39.588Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:55:39.588Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:55:39.926Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:55:39.930Z,ns_1@127.0.0.1:<0.5811.9>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:55:39.931Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:55:39.931Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:55:58.731Z,ns_1@127.0.0.1:ns_audit<0.579.0>:ns_audit:handle_call:148]Audit auth_failure: [{local,{[{ip,<<"172.18.0.2">>},{port,8091}]}},
                     {remote,{[{ip,<<"172.18.0.1">>},{port,34420}]}},
                     {timestamp,<<"2022-09-08T13:55:58.731Z">>},
                     {raw_url,<<"<ud>/pools/default/pendingRetryRebalance</ud>">>}]
[ns_server:debug,2022-09-08T13:55:58.732Z,ns_1@127.0.0.1:ns_audit<0.579.0>:ns_audit:handle_call:148]Audit auth_failure: [{local,{[{ip,<<"172.18.0.2">>},{port,8091}]}},
                     {remote,{[{ip,<<"172.18.0.1">>},{port,34422}]}},
                     {timestamp,<<"2022-09-08T13:55:58.731Z">>},
                     {raw_url,<<"<ud>/settings/autoFailover</ud>">>}]
[ns_server:debug,2022-09-08T13:55:58.778Z,ns_1@127.0.0.1:ns_audit<0.579.0>:ns_audit:handle_call:148]Audit auth_failure: [{local,{[{ip,<<"172.18.0.2">>},{port,8091}]}},
                     {remote,{[{ip,<<"172.18.0.1">>},{port,34422}]}},
                     {timestamp,<<"2022-09-08T13:55:58.778Z">>},
                     {raw_url,<<"<ud>/pools/default/tasks</ud>">>}]
[ns_server:debug,2022-09-08T13:56:09.589Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:56:09.589Z,ns_1@127.0.0.1:<0.7199.9>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:56:09.591Z,ns_1@127.0.0.1:<0.7201.9>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T13:56:09.591Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:56:09.591Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:56:09.932Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:56:09.935Z,ns_1@127.0.0.1:<0.7297.9>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:56:09.937Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:56:09.938Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:56:39.367Z,ns_1@127.0.0.1:ldap_auth_cache<0.334.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2022-09-08T13:56:39.592Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:56:39.593Z,ns_1@127.0.0.1:<0.8689.9>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:56:39.593Z,ns_1@127.0.0.1:<0.8691.9>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T13:56:39.594Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:56:39.594Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:56:39.938Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:56:39.940Z,ns_1@127.0.0.1:<0.8761.9>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:56:39.940Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:56:39.940Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:56:45.150Z,ns_1@127.0.0.1:ns_audit<0.579.0>:ns_audit:handle_call:148]Audit auth_failure: [{local,{[{ip,<<"172.18.0.2">>},{port,8091}]}},
                     {remote,{[{ip,<<"172.18.0.1">>},{port,34422}]}},
                     {timestamp,<<"2022-09-08T13:56:45.149Z">>},
                     {raw_url,<<"<ud>/pools/default?waitChange=0</ud>">>}]
[ns_server:debug,2022-09-08T13:56:46.178Z,ns_1@127.0.0.1:ns_audit<0.579.0>:ns_audit:handle_call:148]Audit auth_failure: [{local,{[{ip,<<"172.18.0.2">>},{port,8091}]}},
                     {remote,{[{ip,<<"172.18.0.1">>},{port,34420}]}},
                     {timestamp,<<"2022-09-08T13:56:46.178Z">>},
                     {raw_url,<<"<ud>/pools</ud>">>}]
[ns_server:debug,2022-09-08T13:56:54.585Z,ns_1@127.0.0.1:ns_audit<0.579.0>:ns_audit:handle_call:148]Audit session_expired: [{timestamp,<<"2022-09-08T13:56:54.585Z">>},
                        {sessionid,<<"a3a7d5f83341b4cab06db880273dd972878608a7">>},
                        {real_userid,{[{domain,builtin},
                                       {user,<<"<ud>admin</ud>">>}]}}]
[ns_server:debug,2022-09-08T13:57:09.594Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:57:09.594Z,ns_1@127.0.0.1:<0.10176.9>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:57:09.596Z,ns_1@127.0.0.1:<0.10178.9>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T13:57:09.596Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:57:09.596Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:57:09.941Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:57:09.946Z,ns_1@127.0.0.1:<0.10244.9>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:57:09.948Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:57:09.948Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:57:39.570Z,ns_1@127.0.0.1:roles_cache<0.345.0>:active_cache:cleanup:258]Cache roles_cache cleanup: 0/0 records deleted
[ns_server:debug,2022-09-08T13:57:39.597Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:57:39.597Z,ns_1@127.0.0.1:<0.11664.9>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:57:39.598Z,ns_1@127.0.0.1:<0.11666.9>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T13:57:39.598Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:57:39.599Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:57:39.949Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:57:39.951Z,ns_1@127.0.0.1:<0.11732.9>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:57:39.952Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:57:39.952Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:57:42.425Z,ns_1@127.0.0.1:prometheus_cfg<0.391.0>:prometheus_cfg:maybe_update_scrape_dynamic_intervals:941]Recalculating prometheus scrape intervals for high cardinality metrics
[ns_server:debug,2022-09-08T13:57:42.429Z,ns_1@127.0.0.1:prometheus_cfg<0.391.0>:prometheus_cfg:maybe_update_scrape_dynamic_intervals:958]Scrape intervals haven't changed:
[]
CBCollect stats dir size estimation: 629596800
Calculated based on scrapes info:
[{backup,low_cardinality,0},
 {cbas,high_cardinality,0},
 {cbas,low_cardinality,14},
 {eventing,high_cardinality,0},
 {eventing,low_cardinality,1},
 {fts,high_cardinality,0},
 {fts,low_cardinality,25},
 {index,low_cardinality,2},
 {index,high_cardinality,24},
 {kv,low_cardinality,291},
 {kv,high_cardinality,1123},
 {n1ql,low_cardinality,33},
 {ns_server,low_cardinality,167},
 {ns_server,high_cardinality,330},
 {xdcr,low_cardinality,0}]
Raw intervals:
[]
[ns_server:debug,2022-09-08T13:57:54.368Z,ns_1@127.0.0.1:ldap_auth_cache<0.334.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2022-09-08T13:58:09.599Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:58:09.599Z,ns_1@127.0.0.1:<0.13148.9>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:58:09.600Z,ns_1@127.0.0.1:<0.13150.9>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T13:58:09.600Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:58:09.601Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:58:09.952Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:58:09.954Z,ns_1@127.0.0.1:<0.13216.9>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:58:09.954Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:58:09.954Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:58:39.601Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:58:39.601Z,ns_1@127.0.0.1:<0.14638.9>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:58:39.603Z,ns_1@127.0.0.1:<0.14640.9>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T13:58:39.603Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:58:39.603Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:58:39.955Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:58:39.957Z,ns_1@127.0.0.1:<0.14706.9>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:58:39.957Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:58:39.957Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:59:09.369Z,ns_1@127.0.0.1:ldap_auth_cache<0.334.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2022-09-08T13:59:09.604Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:59:09.604Z,ns_1@127.0.0.1:<0.16118.9>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:59:09.605Z,ns_1@127.0.0.1:<0.16120.9>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T13:59:09.605Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:59:09.606Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:59:09.958Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:59:09.963Z,ns_1@127.0.0.1:<0.16186.9>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:59:09.964Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:59:09.964Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:59:39.606Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:59:39.606Z,ns_1@127.0.0.1:<0.17606.9>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:59:39.607Z,ns_1@127.0.0.1:<0.17608.9>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T13:59:39.608Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:59:39.608Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T13:59:39.965Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T13:59:39.967Z,ns_1@127.0.0.1:<0.17674.9>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T13:59:39.967Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T13:59:39.967Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T14:00:09.608Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T14:00:09.608Z,ns_1@127.0.0.1:<0.19082.9>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T14:00:09.610Z,ns_1@127.0.0.1:<0.19084.9>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T14:00:09.610Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T14:00:09.610Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T14:00:09.968Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T14:00:09.970Z,ns_1@127.0.0.1:<0.19154.9>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T14:00:09.970Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T14:00:09.970Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T14:00:24.370Z,ns_1@127.0.0.1:ldap_auth_cache<0.334.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2022-09-08T14:00:39.611Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T14:00:39.611Z,ns_1@127.0.0.1:<0.20572.9>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T14:00:39.612Z,ns_1@127.0.0.1:<0.20574.9>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T14:00:39.612Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T14:00:39.612Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T14:00:39.971Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T14:00:39.973Z,ns_1@127.0.0.1:<0.20644.9>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T14:00:39.973Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T14:00:39.973Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T14:01:09.613Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T14:01:09.613Z,ns_1@127.0.0.1:<0.22052.9>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T14:01:09.614Z,ns_1@127.0.0.1:<0.22054.9>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T14:01:09.615Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T14:01:09.615Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T14:01:09.974Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T14:01:09.977Z,ns_1@127.0.0.1:<0.22124.9>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T14:01:09.977Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T14:01:09.977Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T14:01:39.371Z,ns_1@127.0.0.1:ldap_auth_cache<0.334.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2022-09-08T14:01:39.615Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T14:01:39.615Z,ns_1@127.0.0.1:<0.23540.9>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T14:01:39.617Z,ns_1@127.0.0.1:<0.23542.9>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T14:01:39.617Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T14:01:39.617Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T14:01:39.656Z,ns_1@127.0.0.1:roles_cache<0.345.0>:active_cache:renew:238]Starting roles_cache cache renewal
[ns_server:debug,2022-09-08T14:01:39.656Z,ns_1@127.0.0.1:roles_cache<0.345.0>:active_cache:renew:244]roles_cache cache renewal process originated 0 queries
[ns_server:debug,2022-09-08T14:01:39.978Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T14:01:39.981Z,ns_1@127.0.0.1:<0.23612.9>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T14:01:39.981Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T14:01:39.981Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T14:02:09.618Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T14:02:09.619Z,ns_1@127.0.0.1:<0.25020.9>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T14:02:09.620Z,ns_1@127.0.0.1:<0.25022.9>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T14:02:09.620Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T14:02:09.620Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T14:02:09.982Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T14:02:09.986Z,ns_1@127.0.0.1:<0.25092.9>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T14:02:09.986Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T14:02:09.986Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T14:02:39.621Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T14:02:39.621Z,ns_1@127.0.0.1:<0.26510.9>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T14:02:39.623Z,ns_1@127.0.0.1:<0.26512.9>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T14:02:39.623Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T14:02:39.623Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T14:02:39.987Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T14:02:39.990Z,ns_1@127.0.0.1:<0.26582.9>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T14:02:39.990Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T14:02:39.990Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T14:02:54.372Z,ns_1@127.0.0.1:ldap_auth_cache<0.334.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2022-09-08T14:03:09.624Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T14:03:09.626Z,ns_1@127.0.0.1:<0.27990.9>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T14:03:09.629Z,ns_1@127.0.0.1:<0.27992.9>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T14:03:09.630Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T14:03:09.630Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T14:03:09.991Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T14:03:09.993Z,ns_1@127.0.0.1:<0.28062.9>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T14:03:09.993Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T14:03:09.993Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T14:03:39.631Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T14:03:39.632Z,ns_1@127.0.0.1:<0.29474.9>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T14:03:39.633Z,ns_1@127.0.0.1:<0.29476.9>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T14:03:39.633Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T14:03:39.633Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T14:03:39.994Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T14:03:39.997Z,ns_1@127.0.0.1:<0.29550.9>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T14:03:39.998Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T14:03:39.998Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T14:04:09.373Z,ns_1@127.0.0.1:ldap_auth_cache<0.334.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2022-09-08T14:04:09.634Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T14:04:09.634Z,ns_1@127.0.0.1:<0.30954.9>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T14:04:09.635Z,ns_1@127.0.0.1:<0.30956.9>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T14:04:09.635Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T14:04:09.636Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T14:04:09.999Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T14:04:10.004Z,ns_1@127.0.0.1:<0.31030.9>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T14:04:10.004Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T14:04:10.004Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T14:04:39.636Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T14:04:39.637Z,ns_1@127.0.0.1:<0.32444.9>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T14:04:39.638Z,ns_1@127.0.0.1:<0.32446.9>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T14:04:39.638Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T14:04:39.638Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T14:04:40.005Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T14:04:40.007Z,ns_1@127.0.0.1:<0.32520.9>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T14:04:40.008Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T14:04:40.008Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T14:05:09.639Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T14:05:09.639Z,ns_1@127.0.0.1:<0.1156.10>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T14:05:09.640Z,ns_1@127.0.0.1:<0.1158.10>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T14:05:09.640Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T14:05:09.641Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T14:05:10.009Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T14:05:10.010Z,ns_1@127.0.0.1:<0.1232.10>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T14:05:10.010Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T14:05:10.010Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T14:05:24.374Z,ns_1@127.0.0.1:ldap_auth_cache<0.334.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2022-09-08T14:05:39.641Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_kv) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T14:05:39.642Z,ns_1@127.0.0.1:<0.2644.10>:compaction_daemon:spawn_scheduled_kv_compactor:522]Start compaction of vbuckets for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T14:05:39.643Z,ns_1@127.0.0.1:<0.2646.10>:compaction_daemon:bucket_needs_compaction:1023]`todo` data size is 430618, disk size is 12671246
[ns_server:debug,2022-09-08T14:05:39.643Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T14:05:39.643Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2022-09-08T14:05:40.011Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_scheduler_message:1362]Starting compaction (compact_views) for the following buckets: 
[<<"todo">>]
[ns_server:info,2022-09-08T14:05:40.013Z,ns_1@127.0.0.1:<0.2720.10>:compaction_daemon:spawn_scheduled_views_compactor:548]Start compaction of indexes for bucket todo with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:debug,2022-09-08T14:05:40.013Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_daemon:process_compactors_exit:1403]Finished compaction iteration.
[ns_server:debug,2022-09-08T14:05:40.013Z,ns_1@127.0.0.1:compaction_daemon<0.691.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
